<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PyPy (Posts by Richard Plangger)</title><link>https://www.pypy.org/</link><description></description><atom:link href="https://www.pypy.org/authors/richard-plangger.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2025 &lt;a href="mailto:pypy-dev@pypy.org"&gt;The PyPy Team&lt;/a&gt; </copyright><lastBuildDate>Mon, 07 Jul 2025 11:01:29 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Native profiling in VMProf</title><link>https://www.pypy.org/posts/2017/04/native-profiling-in-vmprof-6949065546884243105.html</link><dc:creator>Richard Plangger</dc:creator><description>&lt;p&gt;We are happy to announce a new release for the PyPI package &lt;span&gt;vmprof&lt;/span&gt;.&lt;br&gt;
It is now able to capture native stack frames on Linux and Mac OS X to show you bottle necks in compiled code (such as CFFI modules, Cython or C Python extensions). It supports PyPy, CPython versions 2.7, 3.4, 3.5 and 3.6. Special thanks to Jetbrains for funding the native profiling support.&lt;br&gt;
&lt;br&gt;
&lt;/p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://3.bp.blogspot.com/-94RAR1lkAP8/WNmQn-kpLhI/AAAAAAAAAqE/RXg6T4hptnQtH-8fdi87yh_BI37eN6COQCLcB/s1600/vmprof-logo.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="vmprof logo" border="0" src="https://3.bp.blogspot.com/-94RAR1lkAP8/WNmQn-kpLhI/AAAAAAAAAqE/RXg6T4hptnQtH-8fdi87yh_BI37eN6COQCLcB/s1600/vmprof-logo.png" title="vmprof logo"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;What is vmprof?&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;If you have already worked with vmprof you can skip the next two section. If not, here is a short introduction:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;The goal of vmprof package is to give you more insight into your program. It is a statistical profiler. Another prominent profiler you might already have worked with is cProfile. It is bundled with the Python standard library.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;vmprof's distinct feature (from most other profilers) is that it does not significantly slow down your program execution. The employed strategy is statistical, rather than deterministic. Not every function call is intercepted, but it samples stack traces and memory usage at a configured sample rate (usually around 100hz). You can imagine that this creates a lot less contention than doing work before and after each function call.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;As mentioned earlier cProfile gives you a complete profile, but it needs to intercept every function call (it is a deterministic profiler). Usually this means that you have to capture and record every function call, but this takes an significant amount time.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt; &lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;The overhead vmprof consumes is roughly 3-4% of your total program runtime or even less if you reduce the sampling frequency. Indeed it lets you sample and inspect much larger programs. If you failed to profile a large application with cProfile, please give vmprof a shot.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;span style="font-size: large;"&gt;vmprof.com or PyCharm&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;div&gt;
&lt;div&gt;
There are two major alternatives to the command-line tools shipped with vmprof:&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;A web service on &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyCharm Professional Edition &lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;
While the command line tool is only good for quick inspections, &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt;
 and PyCharm compliment each other providing deeper insight into your 
program. With PyCharm you can view the per-line profiling results inside
 the editor. With the &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt; you get a handy visualization of the profiling results as a flame chart and memory usage graph.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;br&gt;&lt;/div&gt;
&lt;div&gt;
&lt;div&gt;
Since the PyPy Team runs and maintains the service on &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt; (which is by the way free and open-source), I’ll explain some more details here. On &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt; you can inspect the generated profile interactively instead of looking at console output. What is sent to &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt;? You can find details &lt;a href="https://vmprof.readthedocs.io/en/latest/data.html" target="_blank"&gt;here&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;b&gt;Flamegraph&lt;/b&gt;: &lt;/span&gt;&lt;/span&gt;Accumulates and displays the most frequent codepaths. It allows you to quickly and accurately identify hot spots in your code. The flame graph below is a very short run of richards.py (Thus it shows a lot of time spent in PyPy's JIT compiler).&lt;br&gt;
&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://4.bp.blogspot.com/-n5LoH2hf7qI/WNvtNvIAbsI/AAAAAAAAAqc/zn0AXv8fkzIMQXWUwMLtLFpjochspz5MwCLcB/s1600/flamegraph.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="231" src="https://4.bp.blogspot.com/-n5LoH2hf7qI/WNvtNvIAbsI/AAAAAAAAAqc/zn0AXv8fkzIMQXWUwMLtLFpjochspz5MwCLcB/s400/flamegraph.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;b&gt;List all functions (optionally sorted)&lt;/b&gt;: the equivalent of the vmprof command line output in the web.&lt;br&gt;
&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://3.bp.blogspot.com/-zzAmBuf-3KM/WNvtNze_sZI/AAAAAAAAAqg/9u4Kxv_OzMsTV7KgRx9PvXGHOAPdfXYUgCLcB/s1600/list-of-functions.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="215" src="https://3.bp.blogspot.com/-zzAmBuf-3KM/WNvtNze_sZI/AAAAAAAAAqg/9u4Kxv_OzMsTV7KgRx9PvXGHOAPdfXYUgCLcB/s400/list-of-functions.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
 &lt;b&gt;Memory curve&lt;/b&gt;: A line plot that shows how how many MBytes have been consumed over the lifetime of your program (see more info in the section below).&lt;br&gt;
&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://cloud.githubusercontent.com/assets/175722/17400119/70d43a84-5a46-11e6-974b-913cfa22a531.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="187" src="https://cloud.githubusercontent.com/assets/175722/17400119/70d43a84-5a46-11e6-974b-913cfa22a531.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;span style="font-size: large;"&gt;Native programs&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;The new feature introduced in vmprof 0.4.x allows you to look beyond the Python level. As you might know, Python maintains a stack of frames to save the execution. Up to now the vmprof profiles only contained that level of information. But what if you program jumps to native code (such as calling gzip compression on a large file)? Up to now you would not see that information.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;Many packages make use of the CPython C API (which we discurage, please lookup &lt;a href="https://cffi.readthedocs.org/" target="_blank"&gt;cffi&lt;/a&gt; for a better way to call C). Have you ever had the issue that you know that your performance problems reach down to, but you could not profile it properly?&lt;b&gt; Now you can!&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt; &lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;Let's inspect a very simple Python program to find out why a program is significantly slower on Linux than on Mac:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;span&gt;import numpy as np&lt;br&gt;
n = 1000&lt;br&gt;
a = np.random.random((n, n))&lt;br&gt;
b = np.random.random((n, n))&lt;br&gt;
c = np.dot(np.abs(a), b)&lt;/span&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
Take two NxN random matrix objects and create a dot product. The first argument to the dot product provides the absolute value of the random matrix.&lt;br&gt;
&lt;br&gt;
&lt;table border="1" style="border: 1px solid silver;"&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Run&lt;/td&gt;&lt;td&gt;Python&lt;/td&gt;&lt;td&gt;NumPy&lt;/td&gt;&lt;td&gt;OS&lt;/td&gt;&lt;td&gt;n=...&lt;/td&gt; &lt;td&gt;Took&lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;[1]&lt;/td&gt;&lt;td&gt;CPython 3.5.2&lt;/td&gt;&lt;td&gt;NumPy 1.12.1&lt;/td&gt;&lt;td&gt;Mac OS X, 10.12.3&lt;/td&gt;&lt;td&gt;n=5000&lt;/td&gt;&lt;td&gt;~9 sec&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;[2]&lt;/td&gt;&lt;td&gt;CPython 3.6.0&lt;/td&gt;&lt;td&gt;NumPy 1.12.1&lt;/td&gt;&lt;td&gt;Linux 64, Kernel 4.9.14&lt;/td&gt;&lt;td&gt;n=1000&lt;/td&gt;&lt;td&gt;~26 sec&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;br&gt;
Note that the Linux machine operates on a 5 times smaller matrix, still it takes much longer. What is wrong? Is Linux slow? CPython 3.6.0? Well no, lets inspect and &lt;a href="https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac" target="_blank"&gt;[1]&lt;/a&gt; and &lt;a href="https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c" target="_blank"&gt;[2]&lt;/a&gt; (shown below in that order).&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://3.bp.blogspot.com/-WF-JpMQhJaI/WNvx8CPNpTI/AAAAAAAAAqw/ixZpWng6TDc4kIlEHu9zhqrNX4tx0S4rgCLcB/s1600/macosx-profile-blog.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="105" src="https://3.bp.blogspot.com/-WF-JpMQhJaI/WNvx8CPNpTI/AAAAAAAAAqw/ixZpWng6TDc4kIlEHu9zhqrNX4tx0S4rgCLcB/s400/macosx-profile-blog.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://1.bp.blogspot.com/-gjM2uj5Ko_E/WNvx73qcXEI/AAAAAAAAAqs/cMvDfcHQ2eAti4BRU0ldwGQ5M-1_TQ2FACEw/s1600/linux-blog.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="113" src="https://1.bp.blogspot.com/-gjM2uj5Ko_E/WNvx73qcXEI/AAAAAAAAAqs/cMvDfcHQ2eAti4BRU0ldwGQ5M-1_TQ2FACEw/s400/linux-blog.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;a href="https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c" target="_blank"&gt;[2]&lt;/a&gt; runs on Linux, spends nearly all of the time in PyArray_MatrixProduct2, if you compare to &lt;a href="https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac" target="_blank"&gt;[1]&lt;/a&gt; on Mac OS X, you'll see that a lot of time is spent in generating the random numbers and the rest in cblas_matrixproduct.&lt;br&gt;
&lt;br&gt;
Blas has a very efficient implementation so you can achieve the same on Linux if you install a blas implementation (such as openblas).&lt;br&gt;
&lt;br&gt;
Usually you can spot potential program source locations that take a lot of time and might be the first starting point to resolve performance issues.&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;Beyond Python programs &lt;/span&gt;&lt;br&gt;
&lt;br&gt;
It is not unthinkable that the strategy can be reused for native programs. Indeed this can already be done by creating a small cffi wrapper around an entry point of a compiled C program. It would even work for programs compiled from other languages (e.g. C++ or Fortran). The resulting function names are the full symbol name embedded into either the executable symboltable or extracted from the dwarf debugging information. Most of those will be compiler specific and contain some cryptic information.&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;Memory profiling&lt;/span&gt;&lt;br&gt;
We thankfully received a code contribution from the company Blue Yonder. They have built a memory profiler (for Linux and Mac OS X) on top of vmprof.com that displays the memory consumption for the runtime of your process.&lt;br&gt;
&lt;br&gt;
You can run it the following way:&lt;br&gt;
&lt;br&gt;
&lt;span&gt;$ python -m vmprof --mem --web script.py&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
By adding --mem, vmprof will capture memory information and display it in the dedicated view on vmprof.com. You can view it by by clicking the 'Memory' switch in the flamegraph view.&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;There is more&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
Some more minor highlights contained in 0.4.x:&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;VMProf support for Windows 64 bit (No native profiling)&lt;/li&gt;
&lt;li&gt;VMProf can read profiles generated by another host system&lt;/li&gt;
&lt;li&gt;VMProf is now bundled in several binary wheel for fast and easy installation (Mac OS X, Linux 32/64 for CPython 2.7, 3.4, 3.5, 3.6)&lt;/li&gt;
&lt;/ul&gt;
&lt;span style="font-size: large;"&gt;Future plans - Profile Streaming&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
vmprof has not reached the end of development. There are many features we could implement. But there is one feature that could be a great asset to many Python developers.&lt;br&gt;
&lt;br&gt;
Continuous delivery of your statistical profile, or in short, profile streaming. One of the great strengths of vmprof is that is consumes very little overhead. It is not a crazy idea to run this in production.&lt;br&gt;
&lt;br&gt;
It would require a smart way to stream the profile in the background to vmprof.com and new visualizations to look at much more data your Python service produces.&lt;br&gt;
&lt;br&gt;
If that sounds like a solid vmprof improvement, don't hesitate to get in touch with us (e.g. IRC #pypy, mailing list pypy-dev, or comment below)&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;You can help! &lt;/span&gt;&lt;br&gt;
&lt;br&gt;
There are some immediate things other people could help with. Either by donating time or money (yes we have occasional contributors which is great)!&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;We gladly received code contribution for the memory profiler. But it was not enough time to finish the migration completely. Sadly it is a bit brittle right now.&lt;/li&gt;
&lt;li&gt;We would like to spend more time on other visualizations. This should include to give a much better user experience on vmprof.com (like a tutorial that explains the visualization that we already have). &lt;/li&gt;
&lt;li&gt;Build Windows 32/64 bit wheels (for all CPython versions we currently support)&lt;/li&gt;
&lt;/ul&gt;
We are also happy to accept google summer of code projects on vmprof for new visualizations and other improvements. If you qualify and are interested, don't hesitate to ask!&lt;br&gt;
&lt;br&gt;
Richard Plangger (plan_rich) and the PyPy Team&lt;br&gt;
&lt;br&gt;
[1] Mac OS X &lt;a href="https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac"&gt;https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac&lt;/a&gt;&lt;br&gt;
[2] Linux64 &lt;a href="https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c"&gt;https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c&lt;/a&gt;</description><category>profiling</category><category>vmprof</category><guid>https://www.pypy.org/posts/2017/04/native-profiling-in-vmprof-6949065546884243105.html</guid><pubDate>Sat, 01 Apr 2017 14:17:00 GMT</pubDate></item><item><title>Leysin Winter Sprint Summary</title><link>https://www.pypy.org/posts/2017/03/leysin-winter-sprint-summary-4587213628578490701.html</link><dc:creator>Richard Plangger</dc:creator><description>&lt;span class=""&gt;Today
 is the last day of our yearly sprint event in Leysin. We had lots of 
ideas on how to enhance the current state of PyPy, we went skiing and 
had interesting discussions around virtual machines, the Python 
ecosystem, and other real world problems.&lt;/span&gt;&lt;div class="ace-line" id="magicdomid662"&gt;
&lt;span class=""&gt; &lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid664"&gt;
&lt;h2&gt;
&lt;span class=""&gt;Why don't you join us next time?&lt;/span&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid667"&gt;
&lt;span class=""&gt;A usual PyPy sprints day goes through the following stages:&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid668"&gt;
&lt;br&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid669"&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class=""&gt; &lt;b&gt;Planning Session:&lt;/b&gt; Tasks from previous days that have seen progress or 
are completed are noted in a shared document. Everyone adds new tasks 
and then assigns themselves to one or more tasks (usually in pairs). As 
soon as everybody is happy with their task and has a partner to work 
with, the planning session is concluded and the work can start.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Discussions&lt;/b&gt;&lt;span class=""&gt;&lt;b&gt;:&lt;/b&gt; A sprint is a good occasion to discuss difficult 
and important topics in person. We usually sit down in a separate area 
in the sprint room and discuss until a) nobody wants to discuss anymore 
or b) we found a solution to the problem. The good thing is that usally 
the outcome is b).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;b&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;/b&gt;&lt;span class=""&gt;&lt;b&gt;Lunch:&lt;/b&gt; For lunch we prepare sandwiches and other finger food.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;span class=""&gt;&lt;b&gt;Continue working&lt;/b&gt; until dinner, which we eat at a random restaurant in Leysin.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;b&gt;Goto 1 the next day, &lt;/b&gt;if sprint has not ended.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid677"&gt;
&lt;span class=""&gt;Sprints
 are open to everybody and help newcomers to get started with PyPy (we usually
 pair you with a developer familiar with PyPy). They are perfect to 
discuss and find solutions to problems we currently face. If you are 
eager to join next year, please don't hesitate to register next year 
around January.&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid677"&gt;
&lt;span class=""&gt; &lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid679"&gt;
&lt;h2&gt;
&lt;span class=""&gt;Sprint Summary  &lt;/span&gt;&lt;span class=""&gt; &lt;/span&gt;&lt;/h2&gt;
&lt;span class=""&gt;Sprint goals included to work on the following topics: &lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid682"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=""&gt;Work towards releasing PyPy 3.5 (it will be released soon)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;span class=""&gt;CPython Extension&lt;/span&gt;&lt;span class="author-g-s5b0glqpe67hakm8"&gt;s&lt;/span&gt;&lt;span class=""&gt;ion (CPyExt) modules on PyPy&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;Have fun in winter sports (a side goal)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid1290"&gt;
&lt;h3&gt;
&lt;span class=""&gt;Highlights&lt;/span&gt;&lt;/h3&gt;
&lt;h3&gt;
&lt;/h3&gt;
&lt;h3&gt;
&lt;span class=""&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7007"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;span class=""&gt;We have spent lots of time debugging and fixing memory issues on CPyExt.&lt;/span&gt;&lt;span class="author-g-fhxuve7d2vlo71s1"&gt;
 In particular, we fixed a serious memory leak where taking a memoryview
 would prevent numpy arrays from ever being freed. More work is still required to ensure that our GC always releases arrays in a timely 
manner.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;span class=""&gt;Fruitful discussions and progress &lt;/span&gt;&lt;span class="author-g-s5b0glqpe67hakm8"&gt;about &lt;/span&gt;&lt;span class=""&gt;how &lt;/span&gt;&lt;span class="author-g-fhxuve7d2vlo71s1"&gt;to&lt;/span&gt;&lt;span class=""&gt; flesh out some details about&lt;/span&gt;&lt;span class="author-g-s5b0glqpe67hakm8"&gt; the&lt;/span&gt;&lt;span class=""&gt; unicode representation in PyPy. Our current goal is to use utf-8 as&lt;/span&gt;&lt;span class="author-g-s5b0glqpe67hakm8"&gt; the&lt;/span&gt;&lt;span class=""&gt; unicode representation internally and have fast vectorized &lt;/span&gt;&lt;span class="author-g-s5b0glqpe67hakm8"&gt;operations&lt;/span&gt;&lt;span class=""&gt; (indexing, check if valid, ...).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;span class=""&gt;PyPy will participate in GSoC 2017 and we will try to allocate more resources to that than last year.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;span class=""&gt;Profile and think about some details how to reduce the starting size of the interpreter. The starting point would be to &lt;/span&gt;&lt;span class="author-g-s5b0glqpe67hakm8"&gt;look at&lt;/span&gt;&lt;span class=""&gt; the parser and reduce the amount of strings to ke&lt;/span&gt;&lt;span class="author-g-fhxuve7d2vlo71s1"&gt;ep&lt;/span&gt;&lt;span class=""&gt; alive.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;span class=""&gt;Found a topic for a student's master thesis&lt;/span&gt;&lt;span class="author-g-fhxuve7d2vlo71s1"&gt;:&lt;/span&gt;&lt;span class=""&gt; correctly free&lt;/span&gt;&lt;span class="author-g-fhxuve7d2vlo71s1"&gt;ing&lt;/span&gt;&lt;span class=""&gt; cpyext reference cycles.&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;span class=""&gt;&lt;span class=""&gt;Run lots of Python3 code on top of PyPy3 and resolve issues we found along the way.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=""&gt;&lt;span class=""&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=""&gt;Initial work on making RPython thread-safe without a GIL.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7022"&gt;
&lt;h3&gt;
&lt;span class=""&gt;List of attendees&lt;/span&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7025"&gt;
&lt;span class=""&gt;- Stefan Beyer&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7026"&gt;
&lt;span class=""&gt;- Antonio Cuni&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7027"&gt;
&lt;span class=""&gt;- Maciej Fijalkowski&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7028"&gt;
&lt;span class=""&gt;- Manuel Jacob&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7029"&gt;
&lt;span class=""&gt;- Ronan Lamy&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7030"&gt;
&lt;span class=""&gt;- Remi Meier&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7031"&gt;
&lt;span class=""&gt;- Richard Plangger&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7032"&gt;
&lt;span class=""&gt;- Armin Rigo&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7033"&gt;
&lt;span class=""&gt;- Robert Zaremba&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7033"&gt;
&lt;span class=""&gt; &lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7033"&gt;
&lt;span class=""&gt;&lt;a href="https://1.bp.blogspot.com/-DXQBN6Crkkg/WLqLRCyUQoI/AAAAAAAAAoI/YJkwrYbCX1Y3fS97pcrzx1DLAAbEWSK5wCK4B/s1600/C5r2PnqXQAIHFew.jpg"&gt;&lt;img border="0" height="221" src="https://1.bp.blogspot.com/-DXQBN6Crkkg/WLqLRCyUQoI/AAAAAAAAAoI/YJkwrYbCX1Y3fS97pcrzx1DLAAbEWSK5wCK4B/s400/C5r2PnqXQAIHFew.jpg" width="400"&gt;&lt;/a&gt;  &lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7033"&gt;
&lt;span class=""&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7033"&gt;
&lt;div class="ace-line" id="magicdomid7035"&gt;
&lt;a href="https://2.bp.blogspot.com/-GW3_ikXNLnM/WLqLfyj2F1I/AAAAAAAAAoY/rhzM8oMWHCYyIJoZK30GcDCOg8XqIZXagCK4B/s1600/C52T_j3XQAACB0f.jpg%253Alarge.jpg"&gt;&lt;img border="0" height="150" src="https://2.bp.blogspot.com/-GW3_ikXNLnM/WLqLfyj2F1I/AAAAAAAAAoY/rhzM8oMWHCYyIJoZK30GcDCOg8XqIZXagCK4B/s200/C52T_j3XQAACB0f.jpg%253Alarge.jpg" width="200"&gt;&lt;/a&gt;&lt;a href="https://2.bp.blogspot.com/-pr6jpOXCwOA/WLqLig_veDI/AAAAAAAAAog/bDmzrAyn4k8xserwrxB8TpGt8WK49LxDACK4B/s1600/C6AjnJHWAAUogZ-.jpg"&gt;&lt;img border="0" height="150" src="https://2.bp.blogspot.com/-pr6jpOXCwOA/WLqLig_veDI/AAAAAAAAAog/bDmzrAyn4k8xserwrxB8TpGt8WK49LxDACK4B/s200/C6AjnJHWAAUogZ-.jpg" width="200"&gt;&lt;/a&gt;&lt;a href="https://3.bp.blogspot.com/-Wy9pnXLtepg/WLqOnQR9gcI/AAAAAAAAApA/KaFuiYY1oPAZtqAOK10-lqdS8BNk9v7NgCK4B/s1600/IMG_20170302_173339.jpg"&gt;&lt;img border="0" height="150" src="https://3.bp.blogspot.com/-Wy9pnXLtepg/WLqOnQR9gcI/AAAAAAAAApA/KaFuiYY1oPAZtqAOK10-lqdS8BNk9v7NgCK4B/s200/IMG_20170302_173339.jpg" width="200"&gt;&lt;/a&gt;&lt;span class=""&gt;&lt;/span&gt;&lt;span class=""&gt;&lt;a href="https://4.bp.blogspot.com/-XsqEquoT_CE/WLqPlTTeP0I/AAAAAAAAApc/U40smIsLpw4I5HqF5-xWfEPTzq_HsSr2QCK4B/s1600/C5wDgCdXQAAedGq.jpg%253Alarge.jpg"&gt;&lt;img border="0" height="133" src="https://4.bp.blogspot.com/-XsqEquoT_CE/WLqPlTTeP0I/AAAAAAAAApc/U40smIsLpw4I5HqF5-xWfEPTzq_HsSr2QCK4B/s200/C5wDgCdXQAAedGq.jpg%253Alarge.jpg" width="200"&gt;&lt;/a&gt;&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7035"&gt;
&lt;span class=""&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7035"&gt;
&lt;span class=""&gt;We
 would like to thank our donors for the continued support of the PyPy 
project and we looking forward to next years sprint in Leysin.&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7036"&gt;
&lt;br&gt;&lt;/div&gt;
&lt;div class="ace-line" id="magicdomid7037"&gt;
&lt;span class=""&gt;The PyPy Team&lt;/span&gt;&lt;/div&gt;
&lt;div class="" id="magicdomid210"&gt;
&lt;br&gt;&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;</description><guid>https://www.pypy.org/posts/2017/03/leysin-winter-sprint-summary-4587213628578490701.html</guid><pubDate>Sat, 04 Mar 2017 09:59:00 GMT</pubDate></item><item><title>Vectorization extended. PowerPC and s390x</title><link>https://www.pypy.org/posts/2016/11/vectorization-extended-powerpc-and-s390x-4042433015460084057.html</link><dc:creator>Richard Plangger</dc:creator><description>&lt;div dir="ltr" style="text-align: left;"&gt;
We are happy to announce that JIT support in both the PowerPC backend and the&lt;br&gt;
s390x backend have been enhanced. Both can now vectorize loops via SIMD&lt;br&gt;
instructions. Special thanks to IBM for funding this work.&lt;br&gt;
&lt;br&gt;
If you are not familiar with this topic you can read more details &lt;a href="https://pypyvecopt.blogspot.co.at/"&gt;here&lt;/a&gt;.&lt;br&gt;
&lt;div&gt;
&lt;br&gt;
There are many more enhancements under the hood. Most notably, all pure operations are now delayed until the latest possible point. In some cases indices have been calculated more than once or they needed an additional register, because the old value is still used. Additionally it is now possible to load quadword-aligned memory in both PPC and s390x (x86 currently cannot do that).&lt;br&gt;
&lt;h3&gt;
&lt;span style="font-size: large;"&gt;NumPy &amp;amp; CPyExt&lt;/span&gt;&lt;/h3&gt;
The community and core developers have been moving CPyExt towards a complete, but emulated, layer for CPython C extensions. This is great, because the one restriction preventing the wider deployment of PyPy in several scenarios will hopefully be removed. However, we advocate not to use CPyExt, but rather to not write C code at all (let PyPy speed up your Python code) or use &lt;a href="https://cffi.readthedocs.io/en/latest/"&gt;cffi&lt;/a&gt;.&lt;br&gt;
&lt;br&gt;
The work done here to support vectorization helps&lt;i&gt; micronumpy &lt;/i&gt;(NumPyPy) to speed up operations for PPC and s390x. So why is PyPy supporting both NumPyPy and NumPy, do we actually need both? Yes, there are places where gcc can beat the JIT, and places where the tight integration between NumPyPy and PyPy is more performant. We do have plans to integrate both, hijacking the C-extension method calls to use NumPyPy where we know NumPyPy can be faster.&lt;br&gt;
&lt;br&gt;
Just to give you an idea why this is a benefit:&lt;br&gt;
&lt;br&gt;
NumPy arrays can carry custom dtypes and apply user defined python functions on the arrays. How could one optimize this kind of scenario? In a traditional setup, you cannot. But as soon as NumPyPy is turned on, you can suddenly JIT compile this code and vectorize it.&lt;br&gt;
&lt;br&gt;
Another example is element access that occurs frequently, or any other calls that cross between Python and the C level frequently.&lt;br&gt;
&lt;h3&gt;
&lt;span style="font-size: large;"&gt;Benchmarks&lt;/span&gt;&lt;/h3&gt;
Let's have a look at some benchmarks reusing &lt;a href="https://bitbucket.org/mikefc/numpy-benchmark/src" target="_blank"&gt;mikefc's numpy benchmark suite&lt;/a&gt; (find the forked version &lt;a href="https://bitbucket.org/plan_rich/numpy-benchmark" target="_blank"&gt;here&lt;/a&gt;). &lt;span style="white-space: pre-wrap;"&gt;I only ran a subset of microbenchmarks, showing that the core functionality is&lt;/span&gt;&lt;br&gt;
&lt;span style="white-space: pre-wrap;"&gt;functioning properly. &lt;/span&gt;Additionally it has been rewritten to use &lt;a href="https://perf.readthedocs.io/en/latest/"&gt;&lt;i&gt;perf&lt;/i&gt;&lt;/a&gt; instead of the &lt;i&gt;timeit&lt;/i&gt; stdlib module.&lt;br&gt;
&lt;br&gt;
&lt;h2&gt;
&lt;span style="font-size: small;"&gt;Setup&lt;/span&gt;&lt;/h2&gt;
x86 runs on a Intel i7-2600 clocked at 3.40GHz using 4 cores. PowerPC runs on the Power 8 clocked at 3.425GHz providing 160 cores. Last but not least the mainframe machine clocked up to 4 GHz, but fully virtualized (as it is common for such machines). Note that PowerPC is a non private remote machine. It is used by many users and it is crowded with processes. It is hard to extract a stable benchmark there.&lt;br&gt;
&lt;br&gt;
x86 ran on Fedora 24 (kernel version of 4.8.4), PPC ran on Fedora 21 (kernel version 3.17.4) and s390x ran on Redhat Linux 7.2 (kernel version 3.10.0). Respectivley, numpy on cpython had openblas available on x86, no blas implementation were present on s390x and PPC provided blas and lapack.&lt;br&gt;
&lt;br&gt;
As you can see all machines run very different configurations. It does not make sense to compare across platforms, but rather implementations on the same platform.&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;a href="https://4.bp.blogspot.com/-SLgW88U0Bek/WBhPjeuohdI/AAAAAAAAAkY/c5VgHxLjVaoWHIqf6zj65eBQeAefv5HPgCK4B/s1600/vecopt-x86-sse4.png"&gt;&lt;img border="0" height="388" src="https://4.bp.blogspot.com/-SLgW88U0Bek/WBhPjeuohdI/AAAAAAAAAkY/c5VgHxLjVaoWHIqf6zj65eBQeAefv5HPgCK4B/s640/vecopt-x86-sse4.png" width="640"&gt;&lt;/a&gt;&lt;br&gt;
&lt;br&gt;
&lt;a href="https://1.bp.blogspot.com/-z8V9bUPw_BY/WBhPoQvdZ2I/AAAAAAAAAkg/n5IoXwIRnIwaNvOcb8S4S7-Iw455_dFGgCK4B/s1600/vecopt-ppc64le.png"&gt;&lt;/a&gt;&lt;a href="https://4.bp.blogspot.com/-b8xDL8pO4q4/WBhPqZW0wRI/AAAAAAAAAko/ZRyZpD4GP9IF6fbT4ngUfWmEJcQ536uZQCK4B/s1600/vecopt-s390x.png"&gt;&lt;img border="0" height="390" src="https://4.bp.blogspot.com/-b8xDL8pO4q4/WBhPqZW0wRI/AAAAAAAAAko/ZRyZpD4GP9IF6fbT4ngUfWmEJcQ536uZQCK4B/s640/vecopt-s390x.png" width="640"&gt;&lt;/a&gt;&lt;img border="0" height="396" src="https://1.bp.blogspot.com/-z8V9bUPw_BY/WBhPoQvdZ2I/AAAAAAAAAkg/n5IoXwIRnIwaNvOcb8S4S7-Iw455_dFGgCK4B/s640/vecopt-ppc64le.png" width="640"&gt;&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
Blue shows CPython 2.7.10+ available on that platform using the latest NumPy (1.11). Micro NumPy is used for PyPy. PyPy+ indicates that the vectorization optimization is turned on.&lt;br&gt;
All bar charts show the median value of all runs (5 samples, 100 loops, 10 inner loops, for the operations on vectors (not matrices) the loops are set to 1000). PyPy additionally gets 3 extra executions to warmup the JIT.&lt;br&gt;
&lt;br&gt;
The comparison is really comparing speed of machine code. It compares the PyPy's JIT output vs GCC's output. It has little to do with the speed of the interpreter.&lt;br&gt;
&lt;br&gt;
Both new SIMD backends speedup the numeric kernels. Some times it is near to the speed of CPython, some times it is faster. The maximum parallelism very much depends on the extension emitted by the compiler. All three SIMD backends have the same vector register size (which is 128 bit). This means that all three behave similar but ppc and s390x gain more because they can load 128bit of memory from quadword aligned memory.&lt;br&gt;
&lt;br&gt;
&lt;h2&gt;
Future directions&lt;/h2&gt;
Python is achieving rapid adoption in data science. This is currently a trend emerging in Europe, and Python is already heavily used for data science in the USA many other places around the world.&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
PyPy can make a valuable contribution for data scientists, helping them to rapidly write scientific programs in Python and run them at near native speed. If you happen to be in that situation, we are eager to hear you feedback or resolve your issues and also work together to improve the performance of your,&lt;br&gt;
code. Just get in touch!&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
Richard Plangger (plan_rich) and the PyPy team&lt;/div&gt;
&lt;/div&gt;</description><guid>https://www.pypy.org/posts/2016/11/vectorization-extended-powerpc-and-s390x-4042433015460084057.html</guid><pubDate>Thu, 03 Nov 2016 08:35:00 GMT</pubDate></item><item><title>PyPy Tooling Upgrade: JitViewer and VMProf</title><link>https://www.pypy.org/posts/2016/08/pypy-tooling-upgrade-jitviewer-and-5107430577468391432.html</link><dc:creator>Richard Plangger</dc:creator><description>&lt;p&gt;We are happy to announce a major JitViewer (JV) update.&lt;br&gt;
JV allows you to inspect RPython's internal compiler representation (the language in which PyPy is implemented) including the generated machine code of your program. It can graphically show you details of the JIT compiled code and helps you pinpoint issues in your program.&lt;br&gt;
&lt;br&gt;
VMProf is a statistical CPU profiler for python imposing very little overhead at runtime.&lt;br&gt;
&lt;br&gt;
Both VMProf and JitViewer share a common goal: Present useful information for your python program.&lt;br&gt;
The combination of both can reveal more information than either alone.&lt;br&gt;
That is the reason why they are now both packaged together.&lt;br&gt;
We also updated &lt;a href="https://vmprof.com/" target="_blank"&gt;vmprof.com&lt;/a&gt; with various bug fixes and changes including an all new interface to JV.&lt;br&gt;
&lt;br&gt;
This work was done with the goal of improving tooling and libraries around the Python/PyPy/RPython ecosystem.&lt;br&gt;
Some of the tools we have developed:&lt;br&gt;
&lt;br&gt;
&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;b&gt;CFFI&lt;/b&gt; - Foreign Function Interface that avoids CPyExt (&lt;a href="https://cffi.readthedocs.io/en/latest/" target="_blank"&gt;CFFI docs&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;RevDB &lt;/b&gt;- A reverse debugger for python (&lt;a href="https://morepypy.blogspot.co.at/2016/07/reverse-debugging-for-python.html" target="_blank"&gt;RevDB blog post&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
and of course the tools we discuss here:&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;VMProf&lt;/b&gt; - A statistical CPU profiler (&lt;a href="https://vmprof.readthedocs.io/en/latest/" target="_blank"&gt;VMProf docs&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;JitViewer&lt;/b&gt; - Visualization of the log file produced by RPython (&lt;a href="https://vmprof.readthedocs.io/en/latest/"&gt;JitLog docs&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3&gt;
A "brand new" JitViewer&lt;/h3&gt;
&lt;br&gt;
JitViewer has two pieces: you create a log file when running your program, and then use a graphic tool to view what happened.&lt;br&gt;
&lt;br&gt;
The old logging format was a hard-to-maintain, plain-text-logging facility. Frequent changes often broke internal tools.&lt;br&gt;
Additionally, the logging output of a long running program required a lot of disk space.&lt;br&gt;
&lt;br&gt;
Our new binary format encodes data densely, makes use of some compression (gzip), and tries to remove repetition where possible.&lt;br&gt;
It also supports versioning for future proofing and can be extended easily.&lt;br&gt;
&lt;br&gt;
And *drumroll* you no longer need to install a tool to view the log yourself&lt;br&gt;
anymore! The whole system moved to vmprof.com and you can use it any time.&lt;br&gt;
&lt;br&gt;
Sounds great. But what can you do with it? Here are two examples for a PyPy user:&lt;br&gt;
&lt;h3&gt;
&lt;br&gt;PyPy crashed? Did you discover a bug?&lt;/h3&gt;
&lt;br&gt;
For some hard to find bugs it is often necessary to look at the compiled code. The old&lt;br&gt;
procedure often required you to upload a plain text file which was hard to parse and to look through.&lt;br&gt;
&lt;br&gt;
A better way to share a crash report is to install the ``vmprof`` module from PyPi and execute either of the two commands:&lt;br&gt;
&lt;span&gt;&lt;br&gt;&lt;/span&gt;
&lt;span&gt;# this program does not crash, but has some weird behaviour&lt;/span&gt;&lt;br&gt;
&lt;span&gt;$ pypy -m jitlog --web &amp;lt;your program args&amp;gt;&lt;/span&gt;&lt;br&gt;
&lt;span&gt;...&lt;/span&gt;&lt;br&gt;
&lt;span&gt;PyPy Jitlog: https://vmprof.com/#/&amp;lt;hash&amp;gt;/traces&lt;/span&gt;&lt;br&gt;
&lt;span&gt;# this program segfaults&lt;/span&gt;&lt;br&gt;
&lt;span&gt;$ pypy -m jitlog -o /tmp/log &amp;lt;your program args&amp;gt;&lt;/span&gt;&lt;br&gt;
&lt;span&gt;...&lt;/span&gt;&lt;br&gt;
&lt;span&gt;&amp;lt;Segfault&amp;gt;&lt;/span&gt;&lt;br&gt;
&lt;span&gt;$ pypy -m jitlog --upload /tmp/log&lt;/span&gt;&lt;br&gt;
&lt;span&gt;PyPy Jitlog: https://vmprof.com/#/&amp;lt;hash&amp;gt;/traces&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
Providing the link in the bug report allows PyPy developers to browse and identify potential issues.&lt;br&gt;
&lt;br&gt;
&lt;h3&gt;
Speed issues&lt;/h3&gt;
&lt;br&gt;
VMProf is a great tool to find hot spots that consume a lot of time in your program. As soon as you have identified code that runs slowly, you can switch to jitlog and maybe pinpoint certain aspects that do not behave as expected. You will find an overview, and are able to browse the generated code. If you cannot make sense of all that, you can just share the link with us and we can have a look too.&lt;br&gt;
&lt;h3&gt;
&lt;br&gt;Future direction&lt;/h3&gt;
&lt;br&gt;
We hope that the new release will help both PyPy developers and PyPy users resolve potential issues and easily point them out.&lt;br&gt;
&lt;br&gt;
Here are a few ideas what might come in the next few releases:&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt; Combination of CPU profiles and the JITLOG (sadly did not make it into the current release).&lt;/li&gt;
&lt;li&gt;Extend vmprof.com to be able to query vmprof/jitlog. &lt;br&gt;An example query for vmprof: 'methods.callsites() &amp;gt; 5' and&lt;br&gt;for the jitlog would be 'traces.contains('call_assembler').hasbridge('*my_func_name*')'.&lt;/li&gt;
&lt;li&gt;Extend the jitlog to capture the information of the optimization stage.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
Richard Plangger (plan_rich) and the PyPy team&lt;br&gt;
&lt;div&gt;
&lt;br&gt;&lt;/div&gt;</description><guid>https://www.pypy.org/posts/2016/08/pypy-tooling-upgrade-jitviewer-and-5107430577468391432.html</guid><pubDate>Thu, 11 Aug 2016 11:52:00 GMT</pubDate></item><item><title>PyPy Enterprise Edition</title><link>https://www.pypy.org/posts/2016/04/pypy-enterprise-edition-3688275697656890948.html</link><dc:creator>Richard Plangger</dc:creator><description>&lt;p&gt;With the latest additions, PyPy's JIT now supports the Z architecture on Linux. The newest architecture revision (also known as s390x, or colloquially referred to as "&lt;a href="https://en.wikipedia.org/wiki/Big_iron" title="Big iron"&gt;big iron&lt;/a&gt;") is the 64-bit extension for IBM mainframes. Currently only Linux 64 bit is supported (not z/OS nor TPF).&lt;br&gt;
This is the fourth assembler backend supported by PyPy in addition to x86 (32 and 64), ARM (32-bit only) and PPC64 (both little- and big-endian). It might seem that we kind of get a hang of new architectures. Thanks to IBM for funding this work!&lt;br&gt;
&lt;br&gt;
&lt;/p&gt;&lt;h2&gt;
&lt;span style="font-size: large;"&gt;History&lt;/span&gt; &lt;/h2&gt;
When I went to university one lecture covered the prediction of Thomas Watson in 1943. His famous quote "I think there is a world market for maybe five computers ...", turned out not to be true. &lt;br&gt;
&lt;br&gt;
However, even 70 years later, mainframes are used more often than you think. They back critical tasks requiring a high level of stability/security and offer high hardware and computational utilization rates by virtualization.&lt;br&gt;
&lt;br&gt;
With the new PyPy JIT backend we are happy to present a fast Python virtual machine for mainframes and contribute more free software running on s390x.&lt;br&gt;
&lt;br&gt;
&lt;h2&gt;
&lt;span style="font-size: large;"&gt;Meta tracing&lt;/span&gt;&lt;/h2&gt;
Even though the JIT backend has been tested on PyPy, it is not restricted to  the Python programming language. Do you have a great idea for a DSL, or another language that should run on mainframes? Go ahead and just implement your interpreter using RPython.&lt;br&gt;
&lt;br&gt;
&lt;h2&gt;
&lt;span style="font-size: large;"&gt;How do I get a copy?&lt;/span&gt;&lt;/h2&gt;
PyPy can be built using the usual instructions found &lt;a href="https://pypy.org/download.html#building-from-source" target="_blank"&gt;here&lt;/a&gt;. As soon as the next PyPy version has been released we will provide binaries. Until then you can just grab a nightly &lt;a href="https://buildbot.pypy.org/nightly/" target="_blank"&gt;here&lt;/a&gt;.We are currently busy to get the next version of PyPy ready, so an official release will be rolled out soon.&lt;br&gt;
&lt;br&gt;
&lt;h2&gt;
&lt;span style="font-size: large;"&gt;&lt;b&gt;Comparing s390x to x86&lt;/b&gt;&lt;/span&gt;&lt;/h2&gt;
The goal of this comparison is not to scientifically evaluate the benefits/disadvantages on s390x, but rather to see that PyPy's architecture delivers the same benefits as it does on other platforms. Similar to the comparison done for PPC I ran the benchmarks using the same setup. The first column is the speedup of the PyPy JIT VM compared to the speedup of a pure PyPy interpreter&lt;span style="font-size: xx-small;"&gt; 1)&lt;/span&gt;. Note that the s390x's OS was virtualized.&lt;br&gt;
&lt;span&gt;&lt;br&gt;&lt;/span&gt;
&lt;span&gt;  Label               x86     s390x      s390x (run 2)&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: small;"&gt;&lt;span&gt;  ai                 13.7      12.4       11.9&lt;br&gt;  bm_chameleon        8.5       6.3        6.8&lt;br&gt;  bm_dulwich_log      5.1       5.0        5.1&lt;br&gt;  bm_krakatau         5.5       2.0        2.0&lt;br&gt;  bm_mako             8.4       5.8        5.9&lt;br&gt;  bm_mdp              2.0       3.8        3.8&lt;br&gt;  chaos              56.9      52.6       53.4&lt;br&gt;  crypto_pyaes       62.5      64.2       64.2&lt;br&gt;  deltablue           3.3       3.9        3.6&lt;br&gt;  django             28.8      22.6       21.7&lt;br&gt;  eparse              2.3       2.5        2.6&lt;br&gt;  fannkuch            9.1       9.9       10.1&lt;br&gt;  float              13.8      12.8       13.8&lt;br&gt;  genshi_text        16.4      10.5       10.9&lt;br&gt;  genshi_xml          8.2       7.9        8.2&lt;br&gt;  go                  6.7       6.2       11.2&lt;br&gt;  hexiom2            24.3      23.8       23.5&lt;br&gt;  html5lib            5.4       5.8        5.7&lt;br&gt;  json_bench         28.8      27.8       28.1&lt;br&gt;  meteor-contest      5.1       4.2        4.4&lt;br&gt;  nbody_modified     20.6      19.3       19.4&lt;br&gt;  pidigits            1.0      -1.1       -1.0&lt;br&gt;  pyflate-fast        9.0       8.7        8.5&lt;br&gt;  pypy_interp         3.3       4.2        4.4&lt;br&gt;  raytrace-simple    69.0     100.9       93.4&lt;br&gt;  richards           94.1      96.6       84.3&lt;br&gt;  rietveld            3.2       2.5        2.7&lt;br&gt;  slowspitfire        2.8       3.3        4.2&lt;br&gt;  spambayes           5.0       4.8        4.8&lt;br&gt;  spectral-norm      41.9      39.8       42.6&lt;br&gt;  spitfire            3.8       3.9        4.3&lt;br&gt;  spitfire_cstringio  7.6       7.9        8.2&lt;br&gt;  sympy_expand        2.9       1.8        1.8&lt;br&gt;  sympy_integrate     4.3       3.9        4.0&lt;br&gt;  sympy_str           1.5       1.3        1.3&lt;br&gt;  sympy_sum           6.2       5.8        5.9&lt;br&gt;  telco              61.2      48.5       54.8&lt;br&gt;  twisted_iteration  55.5      41.9       43.8&lt;br&gt;  twisted_names       8.2       9.3        9.7&lt;br&gt;  twisted_pb         12.1      10.4       10.2&lt;br&gt;  twisted_tcp         4.9       4.8        5.2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: small;"&gt;&lt;span&gt;&lt;br&gt;&lt;b&gt;  Geometric mean:    9.31      9.10       9.43&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
As you can see the benefits are comparable on both platforms.&lt;br&gt;
Of course this is scientifically not good enough, but it shows a tendency. s390x can achieve the same results as you can get on x86. &lt;br&gt;
&lt;br&gt;
Are you running your business application on a mainframe? We would love to get some feedback. Join us in IRC tell us if PyPy made your application faster! &lt;br&gt;
&lt;br&gt;
plan_rich &amp;amp; the PyPy Team&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: xx-small;"&gt;1) PyPy revision for the benchmarks: 4b386bcfee54&lt;/span&gt;</description><guid>https://www.pypy.org/posts/2016/04/pypy-enterprise-edition-3688275697656890948.html</guid><pubDate>Mon, 18 Apr 2016 10:13:00 GMT</pubDate></item><item><title>Automatic SIMD vectorization support in PyPy</title><link>https://www.pypy.org/posts/2015/10/automatic-simd-vectorization-support-in-639063580401330508.html</link><dc:creator>Richard Plangger</dc:creator><description>&lt;div dir="ltr" style="text-align: left;"&gt;
Hi everyone,&lt;br&gt;
&lt;br&gt;
it took some time to catch up with the JIT refacrtorings merged in this summer. &lt;span style="font-size: small;"&gt;But, (drums) we are happy to announce that:&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2 style="text-align: center;"&gt;
&lt;span style="font-size: large;"&gt;The next release of PyPy,  "PyPy 4.0.0", will ship the new auto vectorizer&lt;/span&gt;&lt;/h2&gt;
&lt;span style="font-size: small;"&gt;The goal of this project was to increase the speed of numerical applications in both the NumPyPy library and for arbitrary Python programs. In PyPy we have focused a lot on improvements in the 'typical python workload',  which usually involves object and string manipulations, mostly for web development. We're hoping with this work that we'll continue improving the other very important Python use case - numerics.&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2&gt;
&lt;span style="font-size: small;"&gt;&lt;span style="font-size: large;"&gt;What it can do!&lt;/span&gt; &lt;/span&gt;&lt;/h2&gt;
&lt;span style="font-size: small;"&gt;It targets numerics only. It 
will not execute object manipulations faster, but it is capable of 
enhancing common vector and matrix operations.&lt;/span&gt;&lt;br&gt;
Good news is that it is not specifically targeted for the NumPy library and the PyPy 
virtual machine. Any interpreter (written in RPython) is able make use 
of the vectorization. For more information about that take a look &lt;a href="https://pypyvecopt.blogspot.co.at/"&gt;here&lt;/a&gt;, or consult the documentation. For the time being it is not turn on by default, so be sure to enable it by specifying &lt;span&gt;--jit vec=1&lt;span style="font-family: inherit;"&gt; &lt;/span&gt;&lt;/span&gt;before running your program.&lt;br&gt;
&lt;br&gt;
If your language (written in RPython) contains many array/matrix operations, you can easily integrate the optimization by adding the parameter 'vec=1' to the JitDriver.&lt;br&gt;
&lt;br&gt;
&lt;h2&gt;
&lt;span style="font-size: large;"&gt;NumPyPy Improvements&lt;/span&gt;&lt;/h2&gt;
&lt;span style="font-size: small;"&gt;&lt;/span&gt;
&lt;span style="font-size: small;"&gt;Let's take a look at the core functions of the NumPyPy library (*). &lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: small;"&gt;The following tests tests show the speedup of the core functions commonly used in Python code interfacing with NumPy, on CPython with NumPy, on the PyPy 2.6.1 relased several weeks ago, and on PyPy 15.11 to be released soon. Timeit was used to test the time needed to run the operation in the plot title on various vector (lower case) and square matrix (upper case) sizes displayed on the X axis. The Y axis shows the speedup compared to CPython 2.7.10. &lt;b&gt;This means that higher is better&lt;/b&gt;. &lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://3.bp.blogspot.com/-aqC2wMdVRaU/ViUZJYlUNoI/AAAAAAAAAXQ/FGa9DfdDZ-4/s1600/matrix-vector.png" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="353" src="https://3.bp.blogspot.com/-aqC2wMdVRaU/ViUZJYlUNoI/AAAAAAAAAXQ/FGa9DfdDZ-4/s640/matrix-vector.png" width="640"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;/div&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;/div&gt;
&lt;span style="font-size: small;"&gt;In comparison to PyPy 2.6.1, the speedup &lt;/span&gt;&lt;span style="font-size: small;"&gt;&lt;span style="font-size: small;"&gt;greatly&lt;/span&gt; improved. The hardware support really strips down the runtime of the vector and matrix operations. There is another operation we would like to highlight: the dot product.&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: small;"&gt;It is a very common operation in numerics and PyPy now (given a moderate sized matrix and vector) decreases the time spent in that operation. See for yourself:&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://3.bp.blogspot.com/-TMuz6OUEOXU/ViUZWEng4AI/AAAAAAAAAXY/dZOYp1LO1G0/s1600/dotproduct.png" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="353" src="https://3.bp.blogspot.com/-TMuz6OUEOXU/ViUZWEng4AI/AAAAAAAAAXY/dZOYp1LO1G0/s640/dotproduct.png" width="640"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;/div&gt;
These are nice improvements in the NumPyPy library and we got to a competitive level only making use of SSE4.1.&lt;br&gt;
&lt;br&gt;
&lt;h2&gt;
&lt;span style="font-size: large;"&gt;Future work   &lt;/span&gt;&lt;/h2&gt;
&lt;br&gt;
&lt;span style="font-size: small;"&gt;This is not the end of the road. The GSoC project showed that it is possible to implement this optimization in PyPy. There might be other improvements we can make to carry this further:&lt;/span&gt;&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style="font-size: small;"&gt;Check alignment at runtime to increase the memory throughput of the CPU&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style="font-size: small;"&gt;Support the AVX vector extension which (at least) doubles the size of the vector register&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style="font-size: small;"&gt;Handle each and every corner case in Python traces to enable it  globally&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style="font-size: small;"&gt;Do not rely only on loading operations to trigger the analysis, there might be cases where combination of floating point values could be done in parallel &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
Cheers,&lt;br&gt;
The PyPy Team&lt;br&gt;
&lt;h4&gt;
&lt;span style="font-size: x-small;"&gt;(*) The benchmark code can be found &lt;a href="https://bitbucket.org/plan_rich/numpy-benchmark"&gt;here&lt;/a&gt; it was run using this configuration: i7-2600 CPU @ 3.40GHz (4 cores). &lt;/span&gt;&lt;/h4&gt;
&lt;/div&gt;</description><guid>https://www.pypy.org/posts/2015/10/automatic-simd-vectorization-support-in-639063580401330508.html</guid><pubDate>Tue, 20 Oct 2015 14:38:00 GMT</pubDate></item></channel></rss>
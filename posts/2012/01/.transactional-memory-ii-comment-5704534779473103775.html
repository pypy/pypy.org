<html><body><p>@Charles: Indeed, I am suggesting VM-wide STM, with the resulting transactional overhead for every read and write.  I actually got such a VM yesterday (with no GC): it seems to be about 10x slower on a single thread.<br><br>Note that even 10x slower is a plus if it scales to dozens of processors.  But of course, a better point of view is that some years ago the regular pypy *was* 10x slower than CPython.  It was a lot of efforts but we managed to make it only 1.5-2x slower.  And this is all without counting the JIT.  If STM bogs down to a generally-not-triggered read barrier before every read, then the performance impact could be well under 2x.<br><br>Please note also that I don't care about Java-like performance where even loosing 10% of performance would be a disaster.  If we end up with a pypy-tm that is 2x slower than a regular pypy, I would be quite happy, and I believe that there is a non-negligible fraction of the Python users that would be, too.<br><br>On granularity: for now I'm going with the idea that the granularity is defined "naturally" in the source program as the amount of work done every time some central dispatch loop calls some code.  There might be several dispatch loops in total, too.  This is true in the cases I can think of: typical Twisted or Stackless programs, pypy's "translate.py", the richards benchmark, etc.<br><br>Please look at https://paste.pocoo.org/show/539822/ for an example of what I'm talking about.  It's a diff against the standard <a href="https://foss.heptapod.net/pypy/pypy/-/tree/branch//default/pypy/translator/goal/richards.py" rel="nofollow">richards.py</a>: it is a pure Python user program in which I added calls to the new 'transaction' module.  At this level there is no hint of Transactional Memory.</p></body></html>
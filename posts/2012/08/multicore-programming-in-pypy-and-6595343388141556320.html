<!DOCTYPE html>
<html \ prefix="
        og: http://ogp.me/ns# article: http://ogp.me/ns/article#
    " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Multicore Programming in PyPy and CPython | PyPy</title>
<link href="../../../assets/css/rst_base.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_rst.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/styles.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../../rss.xml">
<link rel="canonical" href="https://www.pypy.org/posts/2012/08/multicore-programming-in-pypy-and-6595343388141556320.html">
<link rel="icon" href="../../../favicon2.ico" sizes="16x16">
<link rel="icon" href="../../../favicon32x32.ico" sizes="32x32">
<!--[if lt IE 9]><script src="../../../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="../../../assets/css/tipuesearch.css">
<meta name="author" content="Armin Rigo">
<link rel="prev" href="hello-everyone-5492331040603503642.html" title="NumPyPy non-progress report" type="text/html">
<link rel="next" href="c-objects-in-cppyy-part-1-data-members-1105848719513737614.html" title="C++ objects in cppyy, part 1: Data Members" type="text/html">
<meta property="og:site_name" content="PyPy">
<meta property="og:title" content="Multicore Programming in PyPy and CPython">
<meta property="og:url" content="https://www.pypy.org/posts/2012/08/multicore-programming-in-pypy-and-6595343388141556320.html">
<meta property="og:description" content="Hi all,
This is a short &quot;position paper&quot; kind of post about my view (Armin
Rigo's) on the future of multicore programming in high-level languages.
It is a summary of the
keynote presentation at EuroPy">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2012-08-09T09:27:00Z">
<meta property="article:tag" content="stm">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="container">
             <header id="header"><!-- Adapted from https://www.taniarascia.com/responsive-dropdown-navigation-bar --><section class="navigation"><div class="nav-container">
            <div class="brand">
                <a href="../../../index.html">
                    <image id="toplogo" src="../../../images/pypy-logo.svg" width="75px;" alt="PyPy/"></image></a>
            </div>
            <nav><ul class="nav-list">
<li> 
                <a href="#!">Features</a>
                <ul class="nav-dropdown">
<li> <a href="../../../features.html">What is PyPy?</a> </li>  
                    <li> <a href="../../../compat.html">Compatibility</a> </li>  
                    <li> <a href="../../../performance.html">Performance</a> </li>  
                </ul>
</li>
          <li> <a href="../../../download.html">Download</a> </li>  
          <li> <a href="http://doc.pypy.org">Dev Docs</a> </li>  
            <li> 
                <a href="#!">Blog</a>
                <ul class="nav-dropdown">
<li> <a href="../../../blog/">Index</a> </li>  
                    <li> <a href="../../../categories/">Tags</a> </li>  
                    <li> <a href="../../../archive.html">Archive by year</a> </li>  
                    <li> <a href="../../../rss.xml">RSS feed</a> </li>  
                    <li> <a href="https://morepypy.blogspot.com/">Old site</a> </li>  
                </ul>
</li>
            <li> 
                <a href="#!">About</a>
                <ul class="nav-dropdown">
<li> <a href="https://bsky.app/profile/pypyproject.bsky.social">Bluesky</a> </li>  
                    <li> <a href="https://libera.irclog.whitequark.org/pypy">IRC logs</a> </li>  
                    <li> <a href="https://www.youtube.com/playlist?list=PLADqad94yVqDRQXuqxKrPS5QnVqbDLlRt">YouTube</a> </li>  
                    <li> <a href="https://www.twitch.tv/pypyproject">Twitch</a> </li>  
                    <li> <a href="../../../pypy-sponsors.html">Sponsors</a> </li>  
                    <li> <a href="../../../howtohelp.html">How To Help?</a> </li>  
                    <li> <a href="../../../contact.html">Contact</a> </li>  
                </ul>
</li>

                </ul></nav><div class="nav-mobile">
                <a id="nav-toggle" href="#!"> <span></span></a>
            </div>
        </div>
    </section><div class="searchform" role="search">
                
<form class="navbar-form navbar-left" action="../../../search.html" role="search">
    <div class="form-group">
        <input type="text" class="form-control" id="tipue_search_input" name="q" placeholder="Search…" autocomplete="off">
</div>
    <input type="submit" value="Local Search" style="visibility: hidden;">
</form>

            </div>
    </header><main id="content"><article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><div class="post">
          <header><h1 class="p-name entry-title" itemprop="headline name"><a href="#" class="u-url">Multicore Programming in PyPy and CPython</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    <a class="u-url" href="../../../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="#" rel="bookmark">
            <time class="published dt-published" datetime="2012-08-09T09:27:00Z" itemprop="datePublished" title="2012-08-09 09:27">2012-08-09 09:27</time></a>
            </p>
                <p class="commentline">24 comments</p>

                <p class="commentline">            <a href="multicore-programming-in-pypy-and-6595343388141556320.html#utterances-thread">Comments</a>


            
        </p>
</div>
        
    </header><div class="e-content entry-content" itemprop="articleBody text">
      <p>Hi all,</p>
<p>This is a short "position paper" kind of post about my view (Armin
Rigo's) on the future of multicore programming in high-level languages.
It is a summary of the
keynote presentation at EuroPython.  As I learned by talking with people
afterwards, I am not a good enough speaker to manage to convey a deeper
message in a 20-minutes talk.  I will try instead to convey it in a
250-lines post...</p>
<p>This is about three points:</p>
<ol class="arabic simple">
<li>We often hear about people wanting a version of Python running without
the Global Interpreter Lock (GIL): a "GIL-less Python".  But what we
programmers really need is not just a GIL-less Python --- we need a
higher-level way to write multithreaded programs than using directly
threads and locks.  One way is Automatic Mutual Exclusion (AME), which
would give us an "AME Python".</li>
<li>A good enough Software Transactional Memory (STM) system can be used
as an internal tool to do that.
This is what we are building into an "AME PyPy".</li>
<li>The picture is darker for CPython, though there is a way too.  The
problem is that when we say STM, we think about either GCC 4.7's STM
support, or Hardware Transactional Memory (HTM).  However, both
solutions are enough for a "GIL-less CPython", but not
for "AME CPython", due to capacity limitations.  For the latter, we
need somehow to add some large-scale STM into the compiler.</li>
</ol>
<p>Let me explain these points in more details.</p>
<div class="section">
<h3 id="gil-less-versus-ame"><a id="gil-less-versus-ame" name="gil-less-versus-ame">GIL-less versus AME</a><a href="#gil-less-versus-ame" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>The first point is in favor of the so-called Automatic Mutual Exclusion
approach.  The issue with using threads (in any language with or without
a GIL) is that threads are fundamentally non-deterministic.  In other
words, the programs' behaviors are not reproductible at all, and worse,
we cannot even reason about it --- it becomes quickly messy.  We would
have to consider all possible combinations of code paths and timings,
and we cannot hope to write tests that cover all combinations.  This
fact is often documented as one of the main blockers towards writing
successful multithreaded applications.</p>
<p>We need to solve this issue with a higher-level solution.  Such
solutions exist theoretically, and Automatic Mutual Exclusion (AME) is
one of them.  The idea of AME is that we divide the execution of each
thread into a number of "atomic blocks".  Each block is well-delimited
and typically large.  Each block runs atomically, as if it acquired a
GIL for its whole duration.  The trick is that internally we use
Transactional Memory, which is a technique that lets the system run the
atomic blocks from each thread in parallel, while giving the programmer
the illusion that the blocks have been run in some global serialized
order.</p>
<p>This doesn't magically solve all possible issues, but it helps a lot: it
is far easier to reason in terms of a random ordering of large atomic
blocks than in terms of a random ordering of lines of code --- not to
mention the mess that multithreaded C is, where even a random ordering
of instructions is not a sufficient model any more.</p>
<p>How do such atomic blocks look like?  For example, a program might
contain a loop over all keys of a dictionary, performing some
"mostly-independent" work on each value.  This is a typical example:
each atomic block is one iteration through the loop.  By using the
technique described here, we can run the iterations in parallel
(e.g. using a thread pool) but using AME to ensure that they appear to
run serially.</p>
<p>In Python, we don't care about the order in which the loop iterations
are done, because we are anyway iterating over the keys of a dictionary.
So we get exactly the same effect as before: the iterations still run in
some random order, but --- and that's the important point --- they
appear to run in a
global serialized order.  In other words, we introduced parallelism, but
only under the hood: from the programmer's point of view, his program
still appears to run completely serially.  Parallelisation as a
theoretically invisible optimization...  more about the "theoretically"
in the next paragraph.</p>
<p>Note that randomness of order is not fundamental: they are techniques
building on top of AME that can be used to force the order of the
atomic blocks, if needed.</p>
</div>
<div class="section">
<h3 id="pypy-and-stmame"><a id="pypy-and-stm-ame" name="pypy-and-stm-ame">PyPy and STM/AME</a><a href="#pypy-and-stmame" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>Talking more precisely about PyPy: the current prototype <tt class="docutils literal"><span class="pre">pypy-stm</span></tt> is
doing precisely this.  In <tt class="docutils literal"><span class="pre">pypy-stm</span></tt>, the length of the atomic blocks is
selected in one of two ways: either explicitly or automatically.</p>
<p>The automatic selection gives blocks corresponding to some small number
of bytecodes, in which case we have merely a GIL-less Python: multiple
threads will appear to run serially, with the execution randomly
switching from one thread to another at bytecode boundaries, just like
in CPython.</p>
<p>The explicit selection is closer to what was described in the previous
section: someone --- the programmer or the author of some library that
the programmer uses --- will explicitly put <tt class="docutils literal"><span class="pre">with</span> <span class="pre">thread.atomic:</span></tt> in
the source, which delimitates an atomic block.  For example, we can use
it to build a library that can be used to iterate over the keys of a
dictionary: instead of iterating over the dictionary directly, we would
use some custom utility which gives the elements "in parallel".  It
would give them by using internally a pool of threads, but enclosing
every handling of an element into such a <tt class="docutils literal"><span class="pre">with</span> <span class="pre">thread.atomic</span></tt> block.</p>
<p>This gives the nice illusion of a global serialized order, and thus
gives us a well-behaving model of the program's behavior.</p>
<p>Restating this differently,
the <em>only</em> semantical difference between <tt class="docutils literal"><span class="pre">pypy-stm</span></tt> and
a regular PyPy or CPython is that it has <tt class="docutils literal"><span class="pre">thread.atomic</span></tt>, which is a
context manager that gives the illusion of forcing the GIL to not be
released during the execution of the corresponding block of code.  Apart
from this addition, they are apparently identical.</p>
<p>Of course they are only semantically identical if we ignore performance:
<tt class="docutils literal"><span class="pre">pypy-stm</span></tt> uses multiple threads and can potentially benefit from that
on multicore machines.  The drawback is: when does it benefit, and how
much?  The answer to this question is not immediate.  The programmer
will usually have to detect and locate places that cause too many
"conflicts" in the Transactional Memory sense.  A conflict occurs when
two atomic blocks write to the same location, or when <tt class="docutils literal"><span class="pre">A</span></tt> reads it,
<tt class="docutils literal"><span class="pre">B</span></tt> writes it, but <tt class="docutils literal"><span class="pre">B</span></tt> finishes first and commits.  A conflict
causes the execution of one atomic block to be aborted and restarted,
due to another block committing.  Although the process is transparent,
if it occurs more than occasionally, then it has a negative impact on
performance.</p>
<p>There is no out-of-the-box perfect solution for solving all conflicts.
What we will need is more tools to detect them and deal with them, data
structures that are made aware of the risks of "internal" conflicts when
externally there shouldn't be one, and so on.  There is some work ahead.</p>
<p>The point here is that from the point of view of the final programmer,
we gets conflicts that we should resolve --- but at any point, our
program is <em>correct</em>, even if it may not be yet as efficient as it could
be.  This is the opposite of regular multithreading, where programs are
efficient but not as correct as they could be.  In other words, as we
all know, we only have resources to do the easy 80% of the work and not
the remaining hard 20%.  So in this model we get a program that has 80%
of the theoretical maximum of performance and it's fine.  In the regular
multithreading model we would instead only manage to remove 80% of the
bugs, and we are left with obscure rare crashes.</p>
</div>
<div class="section">
<h3 id="cpython-and-htm"><a id="cpython-and-htm" name="cpython-and-htm">CPython and HTM</a><a href="#cpython-and-htm" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>Couldn't we do the same for CPython?  The problem here is that
<tt class="docutils literal"><span class="pre">pypy-stm</span></tt> is implemented as a transformation step during translation,
which is not directly possible in CPython.  Here are our options:</p>
<ul class="simple">
<li>We could review and change the C code everywhere in CPython.</li>
<li>We use GCC 4.7, which supports some form of STM.</li>
<li>We wait until Intel's next generation of CPUs comes out ("Haswell")
and use HTM.</li>
<li>We write our own C code transformation within a compiler (e.g. LLVM).</li>
</ul>
<p>I will personally file the first solution in the "thanks but no thanks"
category.  If anything, it will give us another fork of CPython that
will painfully struggle to keep not more than 3-4 versions behind, and
then eventually die.  It is very unlikely to be ever merged into the
CPython trunk, because it would need changes <em>everywhere</em>.  Not to
mention that these changes would be very experimental: tomorrow we might
figure out that different changes would have been better, and have to
start from scratch again.</p>
<p>Let us turn instead to the next two solutions.  Both of these solutions
are geared toward small-scale transactions, but not long-running ones.
For example, I have no clue how to give GCC rules about performing I/O
in a transaction --- this seems not supported at all; and moreover
looking at the STM library that is available so far to be linked with
the compiled program, it assumes short transactions only.  By contrast,
when I say "long transaction" I mean transactions that can run for 0.1
seconds or more.  To give you an idea, in 0.1 seconds a PyPy program
allocates and frees on the order of ~50MB of memory.</p>
<p>Intel's Hardware Transactional Memory solution is both more flexible and
comes with a stricter limit.  In one word, the transaction boundaries
are given by a pair of special CPU instructions that make the CPU enter
or leave "transactional" mode.  If the transaction aborts, the CPU
cancels any change, rolls back to the "enter" instruction and causes
this instruction to return an error code instead of re-entering
transactional mode (a bit like a <tt class="docutils literal"><span class="pre">fork()</span></tt>).  The software then detects
the error code.  Typically, if transactions are rarely cancelled, it is
fine to fall back to a GIL-like solution just to redo these cancelled
transactions.</p>
<p>About the implementation: this is done by recording all the changes that
a transaction wants to do to the main memory, and keeping them invisible
to other CPUs.  This is "easily" achieved by keeping them inside this
CPU's local cache; rolling back is then just a matter of discarding a
part of this cache without committing it to memory.  From this point of
view, <a class="reference" href="https://arstechnica.com/business/2012/02/transactional-memory-going-mainstream-with-intel-haswell/">there is a lot to bet</a> that we are actually talking about the
regular per-core Level 1 and Level 2 caches --- so any transaction that
cannot fully store its read and written data in the 64+256KB of the L1+L2
caches will abort.</p>
<p>So what does it mean?  A Python interpreter overflows the L1 cache of
the CPU very quickly: just creating new Python function frames takes a
lot of memory (on the order of magnitude of 1/100 of the whole L1
cache).  Adding a 256KB L2 cache into the picture helps, particularly
because it is highly associative and thus avoids a lot of fake conflicts.
However, as long as the HTM support is limited to L1+L2 caches,
it is not going to be enough to run an "AME Python" with any sort of
medium-to-long transaction.  It can
run a "GIL-less Python", though: just running a few hundred or even
thousand bytecodes at a time should fit in the L1+L2 caches, for most
bytecodes.</p>
<p>I would vaguely guess that it will take on the order of 10 years until
CPU cache sizes grow enough for a CPU in HTM mode to actually be able to
run 0.1-second transactions.  (Of course in 10 years' time a lot of other
things may occur too, including the whole Transactional Memory model
being displaced by something else.)</p>
</div>
<div class="section">
<h3 id="write-your-own-stm-for-c"><a id="write-your-own-stm-for-c" name="write-your-own-stm-for-c">Write your own STM for C</a><a href="#write-your-own-stm-for-c" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>Let's discuss now the last option: if neither GCC 4.7 nor HTM are
sufficient for an "AME CPython", then we might want to
write our own C compiler patch (as either extra work on GCC 4.7, or an
extra pass to LLVM, for example).</p>
<p>We would have to deal with the fact that we get low-level information,
and somehow need to preserve interesting high-level bits through the
compiler up to the point at which our pass runs: for example, whether
the field we read is immutable or not.  (This is important because some
common objects are immutable, e.g. PyIntObject.  Immutable reads don't
need to be recorded, whereas reads of mutable data must be protected
against other threads modifying them.)  We can also have custom code to
handle the reference counters: e.g. not consider it a conflict if
multiple transactions have changed the same reference counter, but just
resolve it automatically at commit time.  We are also free to handle I/O
in the way we want.</p>
<p>More generally, the advantage of this approach over both the current GCC
4.7 and over HTM is that we control the whole process.  While this still
looks like a lot of work, it looks doable.  It would be possible to come
up with a minimal patch of CPython that can be accepted into core
without too much troubles (e.g. to mark immutable fields and tweak the
refcounting macros), and keep all the cleverness inside the compiler
extension.</p>
</div>
<div class="section">
<h3 id="conclusion"><a id="conclusion" name="conclusion">Conclusion</a><a href="#conclusion" class="headerlink" title="Permalink to this heading">¶</a></h3>
<p>I would assume that a programming model specific to PyPy and not
applicable to CPython has little chances to catch on, as long as PyPy is
not the main Python interpreter (which looks unlikely to change anytime
soon).  Thus as long as only PyPy has AME, it looks like it will not
become the main model of multicore usage in Python.  However, I can
conclude with a more positive note than during the EuroPython
conference: it is a lot of work, but there is a more-or-less reasonable
way forward to have an AME version of CPython too.</p>
<p>In the meantime, <tt class="docutils literal"><span class="pre">pypy-stm</span></tt> is around the corner, and together with
tools developed on top of it, it might become really useful and used.  I
hope that in the next few years this work will trigger enough motivation
for CPython to follow the ideas.</p>
</div>
      </div>
      <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../../categories/stm.html" rel="tag">stm</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="hello-everyone-5492331040603503642.html" rel="prev" title="NumPyPy non-progress report">Previous post</a>
            </li>
            <li class="next">
                <a href="c-objects-in-cppyy-part-1-data-members-1105848719513737614.html" rel="next" title="C++ objects in cppyy, part 1: Data Members">Next post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                <div class="comment-level comment-level-1">
      <div class="comment comment-4694823314449372690">
        <div class="comment-header">
          <a name="comment-4694823314449372690"></a>
            <span class="author">JohnLenton</span> wrote on <span class="date">2012-08-09 12:29</span>:
        </div>
        <div class="comment-content">
          <p>A question: does a “donate towards STM/AME in pypy” also count as a donation towards the CPython work? Getting the hooks in CPython to allow exploration and implementation of this seems at least as important as the pypy work. In fact, I think it’s quite a bit more important.</p>
        </div>
      </div>
      <div class="comment comment-4610585241136755871">
        <div class="comment-header">
          <a name="comment-4610585241136755871"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2012-08-09 12:55</span>:
        </div>
        <div class="comment-content">
          <p>@John: I didn't foresee this development at the start of the year, so I don't know.  It's a topic that would need to be discussed internally, likely with feedback from past donators.<br><br>Right now of course I'm finishing the basics of pypy-stm (working on the JIT now), and from there on there is a lot that can be done as pure Python, like libraries of better-suited data structures --- and generally gaining experience that would anyway be needed for CPython's work.</p>
        </div>
      </div>
      <div class="comment comment-4690384693403515977">
        <div class="comment-header">
          <a name="comment-4690384693403515977"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2012-08-09 15:53</span>:
        </div>
        <div class="comment-content">
          <p>With HTM you don't have to have a one-to-one mapping between your application transactions and the hardware interface. You can also have an STM, that is implemented using HTM. So you may do all the book-keeping yourself in software, but then at commit time use HTM.</p>
        </div>
      </div>
      <div class="comment comment-2311470550508976836">
        <div class="comment-header">
          <a name="comment-2311470550508976836"></a>
            <span class="author">Nat Tuck</span> wrote on <span class="date">2012-08-09 16:37</span>:
        </div>
        <div class="comment-content">
          <p>No. We really do want a GIL-free Python. Even if that means we sometimes need to deal with locks.<br><br>Right now a high end server can have 64 cores. That means that parallel python code could run faster than serial C code.<br><br>STM and other high level abstractions are neat, but they're no substitute for just killing the damn GIL.</p>
        </div>
      </div>
      <div class="comment comment-4260799307776080409">
        <div class="comment-header">
          <a name="comment-4260799307776080409"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2012-08-09 17:32</span>:
        </div>
        <div class="comment-content">
          <p>What does 'just killing the damn GIL' mean without something like STM? Do you consider it acceptable for Python primitives not to be threadsafe?<br><br>If you intend to run 64 cores, then what is the exact reason you need threading and can't use multiprocessing?</p>
        </div>
      </div>
      <div class="comment comment-6596702968197956257">
        <div class="comment-header">
          <a name="comment-6596702968197956257"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2012-08-09 19:54</span>:
        </div>
        <div class="comment-content">
          <p>Jesus Christ why don't we all just spend 5 min fiddling with the multiprocessing module and learn how to partition execution and queues like we partition sequences of statements into functions?  So sick of GIL articles and the obsession with not learning how to divide up the work and communicate.  In some ways the need to recognize narrow channels where relatively small amounts of data are being channeled through relatively intense blocks of execution and create readable, explicit structure around those blocks might actually improve the comprehensibility of some code I've seen.  Getting a little tired of seeing so much effort by excellent, essential, dedicated Python devs getting sucked up by users who won't get it.<br><br>I think users are driving this speed-for-free obsession way to far.  If anything bugs in a magical system are harder to find than understanding explicit structure and explicit structure that's elegant is neither crufty nor slow.  Eventually, no interpreter will save a bad programmer.  Are we next going to enable the novice "Pythonista" to forego any knowledge of algorithms?  <br><br>We -need- JIT on production systems to get response times down for template processing without micro-caching out the wazoo.  These types of services are already parallel by nature of the servers and usually I/O bound except for the few slow parts.  Cython already serves such an excellent roll for both C/C++ API's AND speed AND optimizing existing python code with minimal changes.  JIT PyPy playing well with Cython would make Python very generally uber.  Users who actually get multiprocessing and can divide up the workflow won't want a slower implementation of any other kind.  Getting a somewhat good solution for 'free' is not nearly as appealing as the additional headroom afforded by an incremental user cost (adding some strong typing or patching a function to work with pypy/py3k).</p>
        </div>
      </div>
      <div class="comment comment-8366653431368532441">
        <div class="comment-header">
          <a name="comment-8366653431368532441"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2012-08-09 19:59</span>:
        </div>
        <div class="comment-content">
          <p>template processing. lol.</p>
        </div>
      </div>
      <div class="comment comment-4712262515030557844">
        <div class="comment-header">
          <a name="comment-4712262515030557844"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2012-08-09 21:27</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous.<br><br>I welcome you to work out how to make pypy translation process parallel using any techniques you described.</p>
        </div>
      </div>
      <div class="comment comment-1592629036731341659">
        <div class="comment-header">
          <a name="comment-1592629036731341659"></a>
            <span class="author">Benjamin</span> wrote on <span class="date">2012-08-10 07:27</span>:
        </div>
        <div class="comment-content">
          <p>I get the overall goals and desires and I think they are fabulous. However, one notion that seems counterintuitive to me is the desire for large atomic operations.<br><br>Aside from the nomenclature (atomic generally means smallest possible), my intuition is that STM would generally operate more efficiently by having fewer roll-backs with small atomic operations and frequent commits. This leads me to assume there is some sort of significant overhead involved with the setup or teardown of the STM 'wrapper'.<br><br>From a broader perspective, I get that understanding interlacing is much easier with larger pieces, but larger pieces of code don't lend themselves to wide distribution across many cores like small pieces do.<br><br>It seems, to me, that you're focusing heavily on the idea of linearly written code magically functioning in parallel and neglecting the idea of simple, low-cost concurrency, which might have a much bigger short-term impact; and which, through use, may shed light on better frameworks for reducing the complexity inherent in concurrency.</p>
        </div>
      </div>
      <div class="comment comment-2306665026240692316">
        <div class="comment-header">
          <a name="comment-2306665026240692316"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2012-08-10 08:57</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous: "So you may do all the book-keeping yourself in software, but then at commit time use HTM.": I don't see how (or the point), can you be more explicit or post a link?<br><br>@Anonymous: I'm not saying that STM is the final solution to all problems.  Some classes of problems have other solutions that work well so far and I'm not proposing to change them.  Big servers can naturally handle big loads just by having enough processes.  What I'm describing instead is a pure language feature that may or may not help in particular cases --- and there are other cases than the one you describe where the situation is very different and multiprocessing doesn't help at all.  Also, you have to realise that any argument "we will never need feature X because we can work around it using hack Y" is bound to lose eventually: at least some people in some cases will need the clean feature X because the hack Y is too complicated to learn or use correctly.<br><br>@Benjamin: "atomic" actually means "not decomposable", not necessarily "as small as possible".  This focus on smallness of transaction IMO is an artefact of last decade's research focus.  In my posts I tend to focus on large transaction as a counterpoint: in the use cases I have in mind there is no guarantee that all transactions will be small.  Some of them may be, but others not, and this is a restriction.  In things like "one iteration through this loop = one transaction", some of these iterations go away and do a lot of stuff.</p>
        </div>
      </div>
      <div class="comment comment-4195395007552517214">
        <div class="comment-header">
          <a name="comment-4195395007552517214"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2012-08-10 18:15</span>:
        </div>
        <div class="comment-content">
          <p>Transactional programming is neat.  So are Goroutines and functional-style parallelism.  On the other hand, I think that C and C++ (or at least C1x and C++11) get one thing completely right: they don't try to enforce any particular threading model.  For some problems (like reference counts, as you mention), you really do want a different model.  As long as other languages force me to choose a single model, my big projects will stay in C/C++.</p>
        </div>
      </div>
      <div class="comment comment-3929821176571847171">
        <div class="comment-header">
          <a name="comment-3929821176571847171"></a>
            <span class="author">Benjamin</span> wrote on <span class="date">2012-08-10 21:17</span>:
        </div>
        <div class="comment-content">
          <p>@Armin I'd love to hear your thoughts (benefits, costs, entrenched ideas, etc.) on large vs small transactions at some point. Though I suspect that would be a post unto itself.</p>
        </div>
      </div>
      <div class="comment comment-5236835361567858920">
        <div class="comment-header">
          <a name="comment-5236835361567858920"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2012-08-10 22:04</span>:
        </div>
        <div class="comment-content">
          <p>@Benjamin: a user program might be optimized to reduce its memory usage, for example by carefully reusing objects instead of throwing them away, finding more memory-efficient constructs, and so on.  But in many cases in Python you don't care too much.  Similarly, I expect that it's possible to reduce the size of transactions by splitting them up carefully, hoping to get some extras in performance.  But most importantly I'd like a system where the programmer didn't have to care overmuch about that.  It should still work reasonably well for *any* size, just like a reasonable GC should work for any heap size.<br><br>If I had to describe the main issue I have against HTM, it is that beyond some transaction size we loose all parallelism because it has to fall back on the GIL.<br><br>Well, now that I think about it, it's the same in memory usage: if you grow past the RAM size, the program is suddenly swapping, and performance becomes terrible.  But RAM sizes are so far much more generous than maximum hardware transaction sizes.</p>
        </div>
      </div>
      <div class="comment comment-1098583861664057625">
        <div class="comment-header">
          <a name="comment-1098583861664057625"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2012-08-12 08:26</span>:
        </div>
        <div class="comment-content">
          <p>There are two key concurrency patterns to keep in mind when considering Armin's STM work:<br><br>1. Event-loop based applications that spend a lot of time idling waiting for events.<br><br>2. Map-reduce style applications where only the reduce step is particularly prone to resource contention, but the map step is read-heavy (and thus hard to split amongst multiple processes)<br><br>For both of those use cases, splitting out multiple processes often won't pay off due to either the serialisation overhead or the additional complexity needed to make serialisation possible at all.<br><br>Coarse-grained STM, however, should pay off handsomely in both of those scenarios: if the CPU bound parts of the application are touching different data structures, or are only *reading* any shared data, with any writes being batched for later application, then the STM interaction can be built in to the event loop or parallel execution framework.<br><br>Will STM help with threading use cases where multiple threads are simultaneously reading and writing the same data structure? No, it won't. However, such applications don't exploit multiple cores effectively even with free threading, because their *lock* contention will also be high.<br><br>As far as "just kill the GIL" goes, I've already written extensively on that topic: https://python-notes.boredomandlaziness.org/en/latest/python3/questions_and_answers.html#but-but-surely-fixing-the-gil-is-more-important-than-fixing-unicode</p>
        </div>
      </div>
      <div class="comment comment-7381272253006405337">
        <div class="comment-header">
          <a name="comment-7381272253006405337"></a>
            <span class="author">klaussfreire</span> wrote on <span class="date">2012-08-13 23:35</span>:
        </div>
        <div class="comment-content">
          <p>Option 5, implement STM on the operating system. Linux already has COW for processes, imagine COW-MERGE for threads.<br><br>When you start transactional mode, all pages are marked read-only, thread-private and COW. When you commit, dirty pages are merged with the processes' page maps, unless conflicts arise (the process already has dirty pages).<br><br>A simple versioning system and version checks would take care of conflict detection.<br><br>I just wonder how difficult it would be designing applications that can run on this model (conflicts at page level vs object level). <br><br>Thread-private allocation arenas are entirely possible to avoid new objects from creating conflicts all the time, so it would be a matter of making read-only use of objects really read-only, something I've done incrementally in patches already. Reference counts have to be externalized (taken out of PyObject), for instance.<br></p>
        </div>
      </div>
      <div class="comment comment-3255499729482906069">
        <div class="comment-header">
          <a name="comment-3255499729482906069"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2012-08-14 09:12</span>:
        </div>
        <div class="comment-content">
          <p>@klaussfreire: that approach is a cool hack but unlikely to work in practice in a language like Python, because the user doesn't control at all what objects are together with what other objects on the same pages.  Even with the reference counts moved out of the way I guess you'd have far too many spurious conflicts.</p>
        </div>
      </div>
      <div class="comment comment-5173870488381841812">
        <div class="comment-header">
          <a name="comment-5173870488381841812"></a>
            <span class="author">klaussfreire</span> wrote on <span class="date">2012-08-14 15:43</span>:
        </div>
        <div class="comment-content">
          <p>@Armin, well, Python itself does know.<br><br>In my half-formed idea in my head, python would use thread-local versions of the integer pool and the various free lists, and allocation of new objects would be served from an also thread-local arena (while in a transaction).<br><br>Read-write access to shared objects, yes, would be a little bit unpredictable. That's why I was wondering how good (if at all) it would work for Python.<br></p>
        </div>
      </div>
      <div class="comment comment-2508884162390677225">
        <div class="comment-header">
          <a name="comment-2508884162390677225"></a>
            <span class="author">Wim Lavrijsen</span> wrote on <span class="date">2012-08-14 20:18</span>:
        </div>
        <div class="comment-content">
          <p>@klaussfreire<br><br>is this perhaps what you are looking for: https://plasma.cs.umass.edu/emery/grace<br><br>Cheers,<br>Wim</p>
        </div>
      </div>
      <div class="comment comment-2467361979110362839">
        <div class="comment-header">
          <a name="comment-2467361979110362839"></a>
            <span class="author">klaussfreire</span> wrote on <span class="date">2012-08-14 21:50</span>:
        </div>
        <div class="comment-content">
          <p>Damn. And I thought I was being original. I can already spot a few key places where kernel-based support would be superior (not only raw performance, but also transparency), but in general, that's exactly what I was talking about, sans transaction retrials.<br></p>
        </div>
      </div>
      <div class="comment comment-6929153709225308058">
        <div class="comment-header">
          <a name="comment-6929153709225308058"></a>
            <span class="author">Mark D.</span> wrote on <span class="date">2012-08-16 04:23</span>:
        </div>
        <div class="comment-content">
          <p>0.1 second transactions?  With hardware transactional memory the general idea is transactions about ten thousand times smaller.  A dozen memory modifications maybe.  <br><br>It would be prohibitively expensive, hardware wise, to implement conflict detection for transactions much larger than that, to say nothing of the occurrence of conflicts requiring rollback and re-execution if such enormously large transactions were executed optimistically.</p>
        </div>
      </div>
      <div class="comment comment-2053451555187067864">
        <div class="comment-header">
          <a name="comment-2053451555187067864"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2012-08-19 11:58</span>:
        </div>
        <div class="comment-content">
          <p>@Mark D.: I don't know if "a dozen memory modification" comes from real work in the field or is just a guess.  My own guess would be that Intel Haswell supports easily hunderds of modifications, possibly thousands.  Moreover the built-in cache coherency mechanisms should be used here too, in a way that scales with the cache size; this means they should not be "prohibitively expensive".<br>Of course I know that in 0.1 seconds we do far more than thousands writes, but I think that nothing strictly limits the progression of future processors in that respect.<br><br>The occurrence of conflicts in large transactions depends on two factors.  First, "true conflicts", which is the hard problem, but which I think should be relatively deterministic and debuggable with new tools.  Second, "false conflicts", which is the HTM/STM mechanism detecting a conflict when there is none.  To handle large transactions this should occur with a probability very, very close to 0% for each memory access.  In pypy-stm it is 0%, but indeed, with HTM it depends on how close to 0% they can get.  I have no data on that.</p>
        </div>
      </div>
      <div class="comment comment-3318067672599099088">
        <div class="comment-header">
          <a name="comment-3318067672599099088"></a>
            <span class="author">Ole Laursen</span> wrote on <span class="date">2012-09-06 15:04</span>:
        </div>
        <div class="comment-content">
          <p>I'm a little late, but regarding the simple let's-do-the-loop-concurrently example, if pypy-stm ends up working out as hoped, would it be relatively easy for pypy to do it automatically without having to use parallel loop thing explicitly?<br><br>I have a hunch the answer would be yes, but that the hard part is figuring out when it makes sense and how to do the split (each thread needs a good chunk to work on).<br><br>On the other hand, GCC has OpenMP which does seem really convenient and also looks like it has (or rather an implementation of that would have to have) solved part of this problem.<br><br>Many years ago, I read about research in auto-parallellising compilers and it stroke me as a really hard problem. But if you can just do some magic with the loops, perhaps it's an attainable goal?</p>
        </div>
      </div>
      <div class="comment comment-3651916502813214832">
        <div class="comment-header">
          <a name="comment-3651916502813214832"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2012-09-06 21:02</span>:
        </div>
        <div class="comment-content">
          <p>I really believe that concurrency - like memory allocation, GC and safe arrays - should be done without the user thinking about it...<br><br>Languages like Erlang, ABCL and Concurrent Object Oriented C solves this quite elegant.<br><br>Just make every Object a "process" (thread/greenlet) and every return value a Future and your are done :-)</p>
        </div>
      </div>
      <div class="comment comment-5989118319024103187">
        <div class="comment-header">
          <a name="comment-5989118319024103187"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-09-22 07:53</span>:
        </div>
        <div class="comment-content">
          <p>Ammm... Jython 2.7.0 !<br><br>All pure Python syntax using threading instantly go MULTI-CORE!  All you need to do is replace the 'p' with a 'j' in your command and voila!<br><br>;)</p>
        </div>
      </div>
         </div>

          </section>
</div>
    <div class="sidebar">
<div>
  <h2>
    The PyPy blogposts
  </h2>
  <div>
    Create a guest post via a PR to the <a href="https://github.com/pypy/pypy.org">source repo</a>
  </div>
</div>
    <div id="global-recent-posts">
    <h2>
      Recent Posts
    </h2>
    <ul class="post-list">
      <li>
        <a href="/posts/2025/12/toy-load-store.html" class="listtitle">Load and store forwarding in the Toy Optimizer</a>
      </li>
      <li>
        <a href="/posts/2025/07/pypy-v7320-release.html" class="listtitle">PyPy v7.3.20 release</a>
      </li>
      <li>
        <a href="/posts/2025/06/rpython-gc-allocation-speed.html" class="listtitle">How fast can the RPython GC allocate?</a>
      </li>
      <li>
        <a href="/posts/2025/04/prospero-in-rpython.html" class="listtitle">Doing the Prospero-Challenge in RPython</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7319-release.html" class="listtitle">PyPy v7.3.19 release</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-gc-sampling.html" class="listtitle">Low Overhead Allocation Sampling with VMProf in PyPy's GC</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7318-release.html" class="listtitle">PyPy v7.3.18 release</a>
      </li>
      <li>
        <a href="/posts/2025/01/musings-tracing.html" class="listtitle">Musings on Tracing in PyPy</a>
      </li>
      <li>
        <a href="/posts/2025/01/towards-pypy311-an-update.html" class="listtitle">Towards PyPy3.11 - an update</a>
      </li>
      <li>
        <a href="/posts/2024/11/guest-post-final-encoding-in-rpython.html" class="listtitle">Guest Post: Final Encoding in RPython Interpreters</a>
      </li>
    </ul>
  </div>

          <div id="global-archive-list">
          <h2>
            Archives
          </h2>
          <ul class="archive-level archive-level-1">
            <li><a class="reference" href="/2007/">2007</a> (19)
            </li>
            <li><a class="reference" href="/2008/">2008</a> (62)
            </li>
            <li><a class="reference" href="/2009/">2009</a> (38)
            </li>
            <li><a class="reference" href="/2010/">2010</a> (44)
            </li>
            <li><a class="reference" href="/2011/">2011</a> (43)
            </li>
            <li><a class="reference" href="/2012/">2012</a> (44)
            </li>
            <li><a class="reference" href="/2013/">2013</a> (46)
            </li>
            <li><a class="reference" href="/2014/">2014</a> (22)
            </li>
            <li><a class="reference" href="/2015/">2015</a> (20)
            </li>
            <li><a class="reference" href="/2016/">2016</a> (20)
            </li>
            <li><a class="reference" href="/2017/">2017</a> (13)
            </li>
            <li><a class="reference" href="/2018/">2018</a> (12)
            </li>
            <li><a class="reference" href="/2019/">2019</a> (12)
            </li>
            <li><a class="reference" href="/2020/">2020</a> (9)
            </li>
            <li><a class="reference" href="/2021/">2021</a> (10)
            </li>
            <li><a class="reference" href="/2022/">2022</a> (13)
            </li>
            <li><a class="reference" href="/2023/">2023</a> (6)
            </li>
            <li><a class="reference" href="/2024/">2024</a> (13)
            </li>
            <li><a class="reference" href="/2025/">2025</a> (9)
            </li>
          </ul>
        </div>


          <div id="global-tag-list">
          <h2>
            Tags
          </h2>
          <ul>
            <li><a class="reference" href="/categories/arm.html">arm</a> (2)</li>
            <li><a class="reference" href="/categories/benchmarking.html">benchmarking</a> (1)</li>
            <li><a class="reference" href="/categories/casestudy.html">casestudy</a> (3)</li>
            <li><a class="reference" href="/categories/cli.html">cli</a> (1)</li>
            <li><a class="reference" href="/categories/compiler.html">compiler</a> (1)</li>
            <li><a class="reference" href="/categories/conda-forge.html">conda-forge</a> (1)</li>
            <li><a class="reference" href="/categories/cpyext.html">cpyext</a> (4)</li>
            <li><a class="reference" href="/categories/cpython.html">CPython</a> (3)</li>
            <li><a class="reference" href="/categories/ep2008.html">ep2008</a> (1)</li>
            <li><a class="reference" href="/categories/extension-modules.html">extension modules</a> (3)</li>
            <li><a class="reference" href="/categories/gc.html">gc</a> (3)</li>
            <li><a class="reference" href="/categories/guestpost.html">guestpost</a> (3)</li>
            <li><a class="reference" href="/categories/graalpython.html">GraalPython</a> (1)</li>
            <li><a class="reference" href="/categories/hpy.html">hpy</a> (1)</li>
            <li><a class="reference" href="/categories/heptapod.html">Heptapod</a> (1)</li>
            <li><a class="reference" href="/categories/jit.html">jit</a> (23)</li>
            <li><a class="reference" href="/categories/jython.html">jython</a> (1)</li>
            <li><a class="reference" href="/categories/kcachegrind.html">kcachegrind</a> (1)</li>
            <li><a class="reference" href="/categories/meta.html">meta</a> (1)</li>
            <li><a class="reference" href="/categories/numpy.html">numpy</a> (24)</li>
            <li><a class="reference" href="/categories/parser.html">parser</a> (1)</li>
            <li><a class="reference" href="/categories/performance.html">performance</a> (2)</li>
            <li><a class="reference" href="/categories/profiling.html">profiling</a> (7)</li>
            <li><a class="reference" href="/categories/pypy.html">pypy</a> (6)</li>
            <li><a class="reference" href="/categories/pypy3.html">pypy3</a> (16)</li>
            <li><a class="reference" href="/categories/pyqt4.html">PyQt4</a> (1)</li>
            <li><a class="reference" href="/categories/release.html">release</a> (66)</li>
            <li><a class="reference" href="/categories/releasecffi.html">releasecffi</a> (3)</li>
            <li><a class="reference" href="/categories/releaserevdb.html">releaserevdb</a> (1)</li>
            <li><a class="reference" href="/categories/releasestm.html">releasestm</a> (1)</li>
            <li><a class="reference" href="/categories/revdb.html">revdb</a> (1)</li>
            <li><a class="reference" href="/categories/roadmap.html">roadmap</a> (2)</li>
            <li><a class="reference" href="/categories/rpython.html">rpython</a> (1)</li>
            <li><a class="reference" href="/categories/rpyc.html">RPyC</a> (1)</li>
            <li><a class="reference" href="/categories/speed.html">speed</a> (6)</li>
            <li><a class="reference" href="/categories/sponsors.html">sponsors</a> (7)</li>
            <li><a class="reference" href="/categories/sprint.html">sprint</a> (3)</li>
            <li><a class="reference" href="/categories/sprints.html">sprints</a> (1)</li>
            <li><a class="reference" href="/categories/stm.html">stm</a> (14)</li>
            <li><a class="reference" href="/categories/sun.html">sun</a> (1)</li>
            <li><a class="reference" href="/categories/smalltalk.html">Smalltalk</a> (1)</li>
            <li><a class="reference" href="/categories/squeak.html">Squeak</a> (1)</li>
            <li><a class="reference" href="/categories/testing.html">testing</a> (1)</li>
            <li><a class="reference" href="/categories/toy-optimizer.html">toy-optimizer</a> (6)</li>
            <li><a class="reference" href="/categories/unicode.html">unicode</a> (1)</li>
            <li><a class="reference" href="/categories/valgrind.html">valgrind</a> (1)</li>
            <li><a class="reference" href="/categories/vmprof.html">vmprof</a> (3)</li>
            <li><a class="reference" href="/categories/z3.html">z3</a> (5)</li>
          </ul>
        </div>    </div>
</article></main><footer id="footer"><p>
</p>
<div class="myfooter">
  <div class="logotext">
    © 2026 <a href="mailto:pypy-dev@pypy.org">The PyPy Team</a>
     
    Built with <a href="https://getnikola.com" rel="nofollow">Nikola</a>
     
    Last built 2026-01-17T00:22
  </div>
  <div style="margin-left: auto">
  <a href="../../../rss.xml">RSS feed</a>
</div>

            
        

    </div>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js" crossorigin="anonymous"></script><script src="../../../assets/js/styles.js"></script></footer>
</div>
</body>
</html>
<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PyPy (Posts about vmprof)</title><link>https://www.pypy.org/</link><description></description><atom:link href="https://www.pypy.org/categories/vmprof.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2026 &lt;a href="mailto:pypy-dev@pypy.org"&gt;The PyPy Team&lt;/a&gt; </copyright><lastBuildDate>Sat, 17 Jan 2026 00:22:36 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Low Overhead Allocation Sampling with VMProf in PyPy's GC</title><link>https://www.pypy.org/posts/2025/02/pypy-gc-sampling.html</link><dc:creator>Christoph Jung</dc:creator><description>&lt;h3 id="introduction"&gt;Introduction&lt;/h3&gt;
&lt;p&gt;There are many time-based statistical profilers around (like VMProf or py-spy
just to name a few). They allow the user to pick a trade-off between profiling
precision and runtime overhead.&lt;/p&gt;
&lt;p&gt;On the other hand there are memory profilers
such as &lt;a href="https://github.com/bloomberg/memray"&gt;memray&lt;/a&gt;. They can be handy for
finding leaks or for discovering functions that allocate a lot of memory.
Memory profilers typlically save every single allocation a program does. This
results in precise profiling, but larger overhead.&lt;/p&gt;
&lt;p&gt;In this post we describe our experimental approach to low overhead statistical
memory profiling. Instead of saving every single allocation a program does, it
only saves every nth allocated byte. We have tightly integrated VMProf and the
PyPy Garbage Collector to achieve this. The main technical insight is that the
check whether an allocation should be sampled can be made free. This is done by
folding it into the bump pointer allocator check that the PyPy’s GC uses to
find out if it should start a minor collection. In this way the fast path with
and without memory sampling are exactly the same.&lt;/p&gt;
&lt;h3 id="background"&gt;Background&lt;/h3&gt;
&lt;p&gt;To get an insight how the profiler and GC interact, lets take a brief look at
both of them first.&lt;/p&gt;
&lt;h4 id="vmprof"&gt;VMProf&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/vmprof/vmprof-python"&gt;VMProf&lt;/a&gt; is a statistical time-based profiler for PyPy. VMProf samples the stack of currently running Python functions a certain user-configured number of times per second. By adjusting
this number, the overhead of profiling can be modified to pick the correct trade-off between overhead and precision of the profile. In the resulting profile, functions with huge runtime stand out the most, functions with shorter runtime less so. If you want to get a little more introduction to VMProf and how to use it with PyPy, you may look
at &lt;a href="https://pypy.org/posts/2024/05/vmprof-firefox-converter.html"&gt;this blog post&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="pypys-gc"&gt;PyPy’s GC&lt;/h4&gt;
&lt;p&gt;PyPy uses a generational incremental copying collector. That means there are two spaces for allocated objects, the nursery and the old-space. Freshly allocated objects will be allocated into the nursery. When the nursery is full at some point, it will be collected and all objects that survive will be tenured i.e. moved into the old-space. The old-space is much larger than the nursery and is collected less frequently and &lt;a href="https://www.pypy.org/posts/2024/03/fixing-bug-incremental-gc.html"&gt;incrementally&lt;/a&gt; (not completely
collected in one go, but step-by-step). The old space collection is not relevant for the rest of the post though. We will now take a look at nursery allocations and how the nursery is collected.&lt;/p&gt;
&lt;h4 id="bump-pointer-allocation-in-the-nursery"&gt;Bump Pointer Allocation in the Nursery&lt;/h4&gt;
&lt;p&gt;The nursery (a small continuous memory area) utilizes two pointers to keep track from where on the nursery is free and where it ends. They are called &lt;code&gt;nursery_free&lt;/code&gt; and &lt;code&gt;nursery_top&lt;/code&gt;. When memory is allocated, the GC checks if there is enough space in the nursery left. If there is enough space, the &lt;code&gt;nursery_free&lt;/code&gt; pointer will be returned as the start address for the newly allocated memory, and &lt;code&gt;nursery_free&lt;/code&gt; will be moved forward by the amount of allocated memory.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.pypy.org/images/2025_02_allocation_sampling_images/nursery_allocation.svg"&gt;&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;allocate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;totalsize&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="c1"&gt;# Save position, where the object will be allocated to as result&lt;/span&gt;
  &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_free&lt;/span&gt;
  &lt;span class="c1"&gt;# Move nursery_free pointer forward by totalsize&lt;/span&gt;
  &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_free&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;totalsize&lt;/span&gt;
  &lt;span class="c1"&gt;# Check if this allocation would exceed the nursery&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_free&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_top&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="c1"&gt;# If it does =&amp;gt; collect the nursery and allocate afterwards&lt;/span&gt;
      &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collect_and_reserve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;totalsize&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="c1"&gt;# result is a pointer into the nursery, obj will be allocated there&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;collect_and_reserve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size_of_allocation&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# do a minor collection and return the start of the nursery afterwards&lt;/span&gt;
    &lt;span class="n"&gt;minor_collection&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_free&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Understanding this is crucial for our allocation sampling approach, so let us go through this step-by-step.&lt;/p&gt;
&lt;p&gt;We already saw an example on how an allocation into a non-full nursery will look like. But what happens, if the nursery is (too) full?&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.pypy.org/images/2025_02_allocation_sampling_images/nursery_full.svg"&gt;&lt;/p&gt;
&lt;p&gt;As soon as an object doesn't fit into the nursery anymore, it will be collected. A nursery collection will move all surviving objects into the old-space, so that the nursery is free afterwards, and the requested allocation can be made.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.pypy.org/images/2025_02_allocation_sampling_images/nursery_collected.svg"&gt;&lt;/p&gt;
&lt;p&gt;(Note that this is still a bit of a simplification.)&lt;/p&gt;
&lt;h3 id="sampling-approach"&gt;Sampling Approach&lt;/h3&gt;
&lt;p&gt;The last section described how the nursery allocation works normally. Now we'll talk how we integrate the new allocation sampling approach into it.&lt;/p&gt;
&lt;p&gt;To decide whether the GC should trigger a sample, the sampling logic is integrated into the bump pointer allocation logic. Usually, when there is not enough space in the nursery left to fulfill an allocation request, the nursery will be collected and the allocation will be done afterwards. We reuse that mechanism for sampling, by introducing a new pointer called &lt;code&gt;sample_point&lt;/code&gt; that is calculated by &lt;code&gt;sample_point = nursery_free + sample_n_bytes&lt;/code&gt; where &lt;code&gt;sample_n_bytes&lt;/code&gt; is the number of bytes allocated before a sample is made (i.e. our sampling rate).&lt;/p&gt;
&lt;p&gt;Imagine we'd have a nursery of 2MB and want to sample every 512KB allocated, then you could imagine our nursery looking like that:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.pypy.org/images/2025_02_allocation_sampling_images/nursery_sampling.svg"&gt;&lt;/p&gt;
&lt;p&gt;We use the sample point as &lt;code&gt;nursery_top&lt;/code&gt;, so that allocating a chunk of 512KB would exceed the nursery top and start a nursery collection. But of course we don't want to do a minor collection just then, so before starting a collection, we need to check if the nursery is actually full or if that is just an exceeded sample point. The latter will then trigger a VMprof stack sample. Afterwards we don't actually do a minor collection, but change &lt;code&gt;nursery_top&lt;/code&gt; and immediately return to the caller.&lt;/p&gt;
&lt;p&gt;The last picture is a conceptual simplification. Only one sampling point exists at any given time. After we created the sampling point, it will be used as nursery top, if exceeded at some point, we will just add &lt;code&gt;sample_n_bytes&lt;/code&gt; to that sampling point, i.e. move it forward.&lt;/p&gt;
&lt;p&gt;Here's how the updated &lt;code&gt;collect_and_reserve&lt;/code&gt; function looks like:&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;collect_and_reserve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size_of_allocation&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Check if we exceeded a sample point or if we need to do a minor collection&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_top&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_point&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# One allocation could exceed multiple sample points&lt;/span&gt;
        &lt;span class="c1"&gt;# Sample, move sample_point forward&lt;/span&gt;
        &lt;span class="n"&gt;vmprof&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_point&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;sample_n_bytes&lt;/span&gt;

        &lt;span class="c1"&gt;# Set sample point as new nursery_top if it fits into the nursery&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;sample_point&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real_nursery_top&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_top&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample_point&lt;/span&gt;
        &lt;span class="c1"&gt;# Or use the real nursery top if it does not fit&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_top&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real_nursery_top&lt;/span&gt;

        &lt;span class="c1"&gt;# Is there enough memory left inside the nursery&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_free&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;size_of_allocation&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_top&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# Yes =&amp;gt; move nursery_free forward&lt;/span&gt;
            &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_free&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;size_of_allocation&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_free&lt;/span&gt;

    &lt;span class="c1"&gt;# We did not exceed a sampling point and must do a minor collection, or&lt;/span&gt;
    &lt;span class="c1"&gt;# we exceeded a sample point but we needed to do a minor collection anyway&lt;/span&gt;
    &lt;span class="n"&gt;minor_collection&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nursery_free&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="why-is-the-overhead-low"&gt;Why is the Overhead ‘low’&lt;/h3&gt;
&lt;p&gt;The most important property of our approach is that the bump-pointer fast path is not changed at all. If sampling is turned off, the slow path in &lt;code&gt;collect_and_reserve&lt;/code&gt; has three extra instructions for the if at the beginning, but are only a very small amount of overhead, compared to doing a minor collection.&lt;/p&gt;
&lt;p&gt;When sampling is on, the extra logic in &lt;code&gt;collect_and_reserve&lt;/code&gt; gets executed. Every time an allocation exceeds the &lt;code&gt;sample_point&lt;/code&gt;, &lt;code&gt;collect_and_reserve&lt;/code&gt; will sample the Python functions currently executing. The resulting overhead is directly controlled by &lt;code&gt;sample_n_bytes&lt;/code&gt;. After sampling, the &lt;code&gt;sample_point&lt;/code&gt; and &lt;code&gt;nursery_top&lt;/code&gt; must be set accordingly. This will be done once after sampling in &lt;code&gt;collect_and_reserve&lt;/code&gt;. At some point a nursery collection will free the nursery and set the new &lt;code&gt;sample_point&lt;/code&gt; afterwards.&lt;/p&gt;
&lt;p&gt;That means that the overhead mostly depends on the sampling rate and the rate at which the user program allocates memory, as the combination of those two factors determines the amount of samples.&lt;/p&gt;
&lt;p&gt;Since the sampling rate can be adjusted from as low as 64 Byte to a theoretical maximum of ~4 GB (at the moment), the tradeoff between number of samples (i.e. profiling precision) and overhead can be completely adjusted.&lt;/p&gt;
&lt;p&gt;We also suspect linkage between user program stack depth and overhead (a deeper stack takes longer to walk, leading to higher overhead), especially when walking the C call stack to.&lt;/p&gt;
&lt;h3 id="sampling-rates-bigger-than-the-nursery-size"&gt;Sampling rates bigger than the nursery size&lt;/h3&gt;
&lt;p&gt;The nursery usually has a size of a few megabytes, but profiling long-runningor larger applications with tons of allocations could result in very high number of samples per second (and thus overhead). To combat that it is possible to use sampling rates higher than the nursery size.&lt;/p&gt;
&lt;p&gt;The sampling point is not limited by the nursery size, but if it is 'outside' the nursery (e.g. because &lt;code&gt;sample_n_bytes&lt;/code&gt; is set to twice the nursery size) it won't be used as &lt;code&gt;nursery_top&lt;/code&gt; until it 'fits' into the nursery.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.pypy.org/images/2025_02_allocation_sampling_images/nursery_sampling_larger_than_nursery.svg"&gt;&lt;/p&gt;
&lt;p&gt;After every nursery collection, we'd usually set the &lt;code&gt;sample_point&lt;/code&gt; to &lt;code&gt;nursery_free + sample_n_bytes&lt;/code&gt;, but if it is larger than the nursery, then the amount of collected memory during the last nursery collection is subtracted from &lt;code&gt;sample_point&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.pypy.org/images/2025_02_allocation_sampling_images/nursery_sampling_larger_than_nursery_post_minor.svg"&gt;&lt;/p&gt;
&lt;p&gt;At some point the &lt;code&gt;sample_point&lt;/code&gt; will be smaller than the nursery size, then it will be used as &lt;code&gt;nursery_top&lt;/code&gt; again to trigger a sample when exceeded.&lt;/p&gt;
&lt;h3 id="differences-to-time-based-sampling"&gt;Differences to Time-Based Sampling&lt;/h3&gt;
&lt;p&gt;As mentioned in the introduction, time-based sampling ‘hits’ functions with high runtime, and allocation-sampling ‘hits’ functions allocating much memory. But are those always different functions? The answer is: sometimes. There can be functions allocating lots of memory, that do not have a (relative) high runtime.&lt;/p&gt;
&lt;p&gt;Another difference to time-based sampling is that the profiling overhead does not solely depend on the sampling rate (if we exclude a potential stack-depth - overhead correlation for now) but also on the amount of memory the user code allocates.&lt;/p&gt;
&lt;p&gt;Let us look at an example:&lt;/p&gt;
&lt;p&gt;If we’d sample every 1024 Byte and some program A allocates 3 MB and runs for 5 seconds, and program B allocates 6 MB but also runs for 5 seconds, there will be ~3000 samples when profiling A, but ~6000 samples when profiling B. That means we cannot give a ‘standard’ sampling rate like time-based profilers use to do (e.g. vmprof uses ~1000 samples/s for time sampling), as the number of resulting samples, and thus overhead, depends on sampling rate and amount of memory allocated by the program.&lt;/p&gt;
&lt;p&gt;For testing and benchmarking, we usually started with a sampling rate of 128Kb and then halved or doubled that (multiple times) depending on sample counts, our need for precision (and size of the profile).&lt;/p&gt;
&lt;h3 id="evaluation"&gt;Evaluation&lt;/h3&gt;
&lt;h4 id="overhead"&gt;Overhead&lt;/h4&gt;
&lt;p&gt;Now let us take a look at the allocation sampling overhead, by profiling some benchmarks. &lt;/p&gt;
&lt;p&gt;The x-axis shows the sampling rate, while the y-axis shows the overhead, which is computed as &lt;code&gt;runtime_with_sampling / runtime_without_sampling&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;All benchmarks were executed five times on a PyPy with JIT and native profiling enabled, so that every dot in the plot is one run of a benchmark.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.pypy.org/images/2025_02_allocation_sampling_images/as_overhead.png"&gt;&lt;/p&gt;
&lt;p&gt;As you probably expected, the Overhead drops with higher allocation sampling rates.
Reaching from as high as ~390% for 32kb allocation sampling to as low as &amp;lt; 10% for 32mb.&lt;/p&gt;
&lt;p&gt;Let me give one concrete example: One run of the microbenchmark at 32kb sampling took 15.596 seconds and triggered 822050 samples.
That makes a ridiculous amount of &lt;code&gt;822050 / 15.596 = ~52709&lt;/code&gt; samples per second. &lt;/p&gt;
&lt;p&gt;There is probably no need for that amount of samples per second, so that for 'real' application profiling a much higher sampling rate would be sufficient.&lt;/p&gt;
&lt;p&gt;Let us compare that to time sampling.&lt;/p&gt;
&lt;p&gt;This time we ran those benchmarks with 100, 1000 and 2000 samples per second.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.pypy.org/images/2025_02_allocation_sampling_images/ts_overhead.png"&gt;&lt;/p&gt;
&lt;p&gt;The overhead varies with the sampling rate. Both with allocation and time sampling, you can reach any amount of overhead and any level of profiling precision you want. The best approach probably is to just try out a sampling rate and choose what gives you the right tradeoff between precision and overhead (and disk usage).&lt;/p&gt;
&lt;p&gt;The benchmarks used are:&lt;/p&gt;
&lt;p&gt;microbenchmark &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Cskorpion/microbenchmark"&gt;https://github.com/Cskorpion/microbenchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pypy microbench.py 65536&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;gcbench &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/pypy/pypy/blob/main/rpython/translator/goal/gcbench.py"&gt;https://github.com/pypy/pypy/blob/main/rpython/translator/goal/gcbench.py&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;print statements removed&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pypy gcbench.py 1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;pypy translate step&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first step of the pypy translation (annotation step)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pypy path/to/rpython --opt=0 --cc=gcc --dont-write-c-files --gc=incminimark --annotate path/to/pypy/goal/targetpypystandalone.py&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;interpreter pystone&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pystone benchmark on top of an interpreted pypy on top of a translated pypy&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pypy path/to/pypy/bin/pyinteractive.py -c "import test.pystone; test.pystone.main(1)"&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All benchmarks executed on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubuntu 24.04&lt;/li&gt;
&lt;li&gt;AMD Ryzen 7 5700U&lt;/li&gt;
&lt;li&gt;24gb DDR4 3200MHz (dual channel)&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SSD benchmarking at read: 1965 MB/s, write: 227 MB/s&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sequential 1MB 1 Thread 8 Queues&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Self built PyPy with allocation sampling features&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Cskorpion/pypy/tree/gc_allocation_sampling_u_2.7"&gt;https://github.com/Cskorpion/pypy/tree/gc_allocation_sampling_u_2.7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Modified VMProf with allocation sampling support&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Cskorpion/vmprof-python/tree/pypy_gc_allocation_sampling"&gt;https://github.com/Cskorpion/vmprof-python/tree/pypy_gc_allocation_sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="example"&gt;Example&lt;/h4&gt;
&lt;p&gt;We have also modified &lt;a href="https://github.com/Cskorpion/vmprof-firefox-converter/tree/allocation_sampling"&gt;vmprof-firefox-converter&lt;/a&gt; to show the allocation samples in the Firefor Profiler UI. With the techniques from this post, the output looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://www.pypy.org/images/2025_02_allocation_sampling_images/allocation_sampling_call_tree.png"&gt;&lt;/p&gt;
&lt;p&gt;While this view is interesting, it would be even better if we could also see what types of objects are being allocated in these functions. We will take about how to do this in a future blog post.&lt;/p&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this blog post we introduced allocation sampling for PyPy by going through the technical aspects and the corresponding overhead. In a future blog post, we are going to dive into the actual usage of allocation sampling with VMProf, and show an example case study. That will be accompanied by some new improvements and additional features, like extracting the type of an object that triggered a sample.&lt;/p&gt;
&lt;p&gt;So far all this work is still experimental and happening on PyPy branches but
we hope to get the technique stable enough to merge it to main and ship it with
PyPy eventually.&lt;/p&gt;
&lt;p&gt;-- Christoph Jung and CF Bolz-Tereick&lt;/p&gt;</description><category>gc</category><category>profiling</category><category>vmprof</category><guid>https://www.pypy.org/posts/2025/02/pypy-gc-sampling.html</guid><pubDate>Tue, 25 Feb 2025 10:16:00 GMT</pubDate></item><item><title>Profiling PyPy using the Firefox profiler user interface</title><link>https://www.pypy.org/posts/2024/05/vmprof-firefox-converter.html</link><dc:creator>Christoph Jung</dc:creator><description>&lt;h3 id="introduction"&gt;Introduction&lt;/h3&gt;
&lt;p&gt;If you ever wanted to profile your Python code on PyPy, you probably came across &lt;a href="https://vmprof.readthedocs.io/en/latest/vmprof.html"&gt;VMProf&lt;/a&gt; — a statistical profiler for PyPy.&lt;/p&gt;
&lt;p&gt;VMProf's console output can already give some insights into where your code spends time, 
but it is far from showing all the information captured while profiling.&lt;/p&gt;
&lt;p&gt;There have been some tools around to visualize VMProf's output.
Unfortunately the vmprof.com user interface is no longer available and vmprof-server is not as easy to use, you may want to take a look at a local viewer or converter.
Those so far could give you some general visualizations of your profile, but do not show any PyPy related context like PyPy's log output (&lt;a href="https://rpython.readthedocs.io/en/latest/logging.html"&gt;PyPyLog&lt;/a&gt;, which is output when using the PYPYLOG environment variable to log JIT actions).&lt;/p&gt;
&lt;p&gt;To bring all of those features together in one tool, you may take a look at the vmprof-firefox-converter.&lt;/p&gt;
&lt;p&gt;Created in the context of my bachelor's thesis, the vmprof-firefox-converter is a tool for analyzing VMProf profiles with the &lt;a href="https://profiler.firefox.com/"&gt;Firefox profiler&lt;/a&gt; user interface. 
Instead of building a new user interface from scratch, this allows us to reuse the user interface work Mozilla put into the Firefox profiler.
The Firefox profiler offers a timeline where you can zoom into profiles and work with different visualizations like a flame graph or a stack chart.
To understand why there is time spent inside a function, you can revisit the source code and even dive into the intermediate representation of functions executed by PyPy's just-in-time compiler.
Additionally, there is a visualization for PyPy's log output, to keep track whether PyPy spent time inside the interpreter, JIT or GC throughout the profiling time.&lt;/p&gt;
&lt;h3 id="profiling-word-count"&gt;Profiling word count&lt;/h3&gt;
&lt;p&gt;In this blog post, I want to show an example of how to use the vmprof-firefox-converter for a simple Python program.
Based on Ben Hoyt's blog &lt;a href="https://benhoyt.com/writings/count-words/"&gt;Performance comparison: counting words in Python, Go, C++, C, AWK, Forth, and Rust&lt;/a&gt; we will profile two python versions of a word counter running on PyPy. One being a bit more optimized. For this, VMProf will be used, but instead of just going with the console output, we will use the Firefox profiler user interface.&lt;/p&gt;
&lt;p&gt;At first, we are going to look at a simple way of counting words with &lt;code&gt;Collections.Counter&lt;/code&gt;.
This will read one line from the standard input at a time and count the words with &lt;code&gt;counter.update()&lt;/code&gt;&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;counts = collections.Counter()
for line in sys.stdin:
    words = line.lower().split()
    counts.update(words)

for word, count in counts.most_common():
    print(word, count)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To start profiling, simply execute:
&lt;code&gt;pypy -m vmprofconvert -run simple.py &amp;lt;kjvbible_x10.txt&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This will run the above code with vmprof, automatically capture and convert the results and finally open the Firefox profiler. &lt;/p&gt;
&lt;p&gt;The input file is the king James version of the bible concatenated ten times.&lt;/p&gt;
&lt;p&gt;To get started, we take a look at the call stack.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/Cskorpion/vmprof-firefox-converter/blob/main/images/blog/simple_call_stack_crp.png?raw=true"&gt;
Here we see that most of the time is spent in native code (marked as blue) e.g., the &lt;code&gt;counter.update()&lt;/code&gt; or &lt;code&gt;split()&lt;/code&gt; C implementation.&lt;/p&gt;
&lt;p&gt;Now let's proceed with the more optimized version.
This time we read 64 Kb of data from the standard input and count the words with &lt;code&gt;counter.update()&lt;/code&gt;.&lt;/p&gt;
&lt;div class="code"&gt;&lt;pre class="code literal-block"&gt;counts = collections.Counter()
remaining = ''
while True:
    chunk = remaining + sys.stdin.read(64*1024)
    if not chunk:
        break
    last_lf = chunk.rfind('\n')  # process to last LF character
    if last_lf == -1:
        remaining = ''
    else:
        remaining = chunk[last_lf+1:]
        chunk = chunk[:last_lf]
    counts.update(chunk.lower().split())

for word, count in counts.most_common():
    print(word, count)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we did before, we are going to take a peek at the call stack.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/Cskorpion/vmprof-firefox-converter/blob/main/images/blog/optimized_call_stack_crp.png?raw=true"&gt; &lt;/p&gt;
&lt;p&gt;Now there is more time spent in native code, caused by larger chunks of text passed to  &lt;code&gt;counter.update()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This becomes even more clear by comparing the stack charts.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/Cskorpion/vmprof-firefox-converter/blob/main/images/blog/simple_stack_chart.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;Here, in the unoptimized case, we only read in one line at each loop iteration.
This results in small "spikes" in the stack chart. &lt;/p&gt;
&lt;p&gt;But let's take an even closer look.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/Cskorpion/vmprof-firefox-converter/blob/main/images/blog/simple_stack_chart_zoom.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;Zoomed in, we see the call stack alternating between &lt;code&gt;_count_elements()&lt;/code&gt; and (unfortunately unsymbolized) native calls coming from reading and splitting the input text (e.g., &lt;code&gt;decode()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let us now take a look at the optimized case.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/Cskorpion/vmprof-firefox-converter/blob/main/images/blog/optimized_stack_chart.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;And if we look closer at the same interval as before, we see some spikes, but slightly different.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/Cskorpion/vmprof-firefox-converter/blob/main/images/blog/optimized_stack_chart_zoom.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;Even though we do not want to compare the (amount of) milliseconds directly, we clearly see that the spikes are wider, i.e. the time spent in those function calls is longer.
You may already know where this comes from.
We read a 64 Kb chunk of data from std in and pass that to &lt;code&gt;counter.update()&lt;/code&gt;, so both these tasks do more work and take longer.
Bigger chunks mean there is less alternating between reading and counting, so there is more time spent doing work than "doing" loop iterations.&lt;/p&gt;
&lt;h3 id="getting-started"&gt;Getting started&lt;/h3&gt;
&lt;p&gt;You can get the converter from &lt;a href="https://github.com/Cskorpion/vmprof-firefox-converter"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Both VMProf and the vmprof-firefox-converter were created for profiling PyPy, but you can also use them with CPython. &lt;/p&gt;
&lt;p&gt;This project is still somewhat experimental, so if you want to try it out, please let us know whether it worked for you.&lt;/p&gt;</description><category>profiling</category><category>vmprof</category><guid>https://www.pypy.org/posts/2024/05/vmprof-firefox-converter.html</guid><pubDate>Fri, 26 Apr 2024 14:38:00 GMT</pubDate></item><item><title>Native profiling in VMProf</title><link>https://www.pypy.org/posts/2017/04/native-profiling-in-vmprof-6949065546884243105.html</link><dc:creator>Richard Plangger</dc:creator><description>&lt;p&gt;We are happy to announce a new release for the PyPI package &lt;span&gt;vmprof&lt;/span&gt;.&lt;br&gt;
It is now able to capture native stack frames on Linux and Mac OS X to show you bottle necks in compiled code (such as CFFI modules, Cython or C Python extensions). It supports PyPy, CPython versions 2.7, 3.4, 3.5 and 3.6. Special thanks to Jetbrains for funding the native profiling support.&lt;br&gt;
&lt;br&gt;
&lt;/p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://3.bp.blogspot.com/-94RAR1lkAP8/WNmQn-kpLhI/AAAAAAAAAqE/RXg6T4hptnQtH-8fdi87yh_BI37eN6COQCLcB/s1600/vmprof-logo.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="vmprof logo" border="0" src="https://3.bp.blogspot.com/-94RAR1lkAP8/WNmQn-kpLhI/AAAAAAAAAqE/RXg6T4hptnQtH-8fdi87yh_BI37eN6COQCLcB/s1600/vmprof-logo.png" title="vmprof logo"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;What is vmprof?&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;If you have already worked with vmprof you can skip the next two section. If not, here is a short introduction:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;The goal of vmprof package is to give you more insight into your program. It is a statistical profiler. Another prominent profiler you might already have worked with is cProfile. It is bundled with the Python standard library.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;vmprof's distinct feature (from most other profilers) is that it does not significantly slow down your program execution. The employed strategy is statistical, rather than deterministic. Not every function call is intercepted, but it samples stack traces and memory usage at a configured sample rate (usually around 100hz). You can imagine that this creates a lot less contention than doing work before and after each function call.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;As mentioned earlier cProfile gives you a complete profile, but it needs to intercept every function call (it is a deterministic profiler). Usually this means that you have to capture and record every function call, but this takes an significant amount time.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt; &lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;The overhead vmprof consumes is roughly 3-4% of your total program runtime or even less if you reduce the sampling frequency. Indeed it lets you sample and inspect much larger programs. If you failed to profile a large application with cProfile, please give vmprof a shot.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;span style="font-size: large;"&gt;vmprof.com or PyCharm&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;div&gt;
&lt;div&gt;
There are two major alternatives to the command-line tools shipped with vmprof:&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;A web service on &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyCharm Professional Edition &lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;
While the command line tool is only good for quick inspections, &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt;
 and PyCharm compliment each other providing deeper insight into your 
program. With PyCharm you can view the per-line profiling results inside
 the editor. With the &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt; you get a handy visualization of the profiling results as a flame chart and memory usage graph.&lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;br&gt;&lt;/div&gt;
&lt;div&gt;
&lt;div&gt;
Since the PyPy Team runs and maintains the service on &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt; (which is by the way free and open-source), I’ll explain some more details here. On &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt; you can inspect the generated profile interactively instead of looking at console output. What is sent to &lt;a href="https://vmprof.com/"&gt;vmprof.com&lt;/a&gt;? You can find details &lt;a href="https://vmprof.readthedocs.io/en/latest/data.html" target="_blank"&gt;here&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;b&gt;Flamegraph&lt;/b&gt;: &lt;/span&gt;&lt;/span&gt;Accumulates and displays the most frequent codepaths. It allows you to quickly and accurately identify hot spots in your code. The flame graph below is a very short run of richards.py (Thus it shows a lot of time spent in PyPy's JIT compiler).&lt;br&gt;
&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://4.bp.blogspot.com/-n5LoH2hf7qI/WNvtNvIAbsI/AAAAAAAAAqc/zn0AXv8fkzIMQXWUwMLtLFpjochspz5MwCLcB/s1600/flamegraph.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="231" src="https://4.bp.blogspot.com/-n5LoH2hf7qI/WNvtNvIAbsI/AAAAAAAAAqc/zn0AXv8fkzIMQXWUwMLtLFpjochspz5MwCLcB/s400/flamegraph.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;b&gt;List all functions (optionally sorted)&lt;/b&gt;: the equivalent of the vmprof command line output in the web.&lt;br&gt;
&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://3.bp.blogspot.com/-zzAmBuf-3KM/WNvtNze_sZI/AAAAAAAAAqg/9u4Kxv_OzMsTV7KgRx9PvXGHOAPdfXYUgCLcB/s1600/list-of-functions.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="215" src="https://3.bp.blogspot.com/-zzAmBuf-3KM/WNvtNze_sZI/AAAAAAAAAqg/9u4Kxv_OzMsTV7KgRx9PvXGHOAPdfXYUgCLcB/s400/list-of-functions.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
 &lt;b&gt;Memory curve&lt;/b&gt;: A line plot that shows how how many MBytes have been consumed over the lifetime of your program (see more info in the section below).&lt;br&gt;
&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://cloud.githubusercontent.com/assets/175722/17400119/70d43a84-5a46-11e6-974b-913cfa22a531.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="187" src="https://cloud.githubusercontent.com/assets/175722/17400119/70d43a84-5a46-11e6-974b-913cfa22a531.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;span style="font-size: large;"&gt;Native programs&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;The new feature introduced in vmprof 0.4.x allows you to look beyond the Python level. As you might know, Python maintains a stack of frames to save the execution. Up to now the vmprof profiles only contained that level of information. But what if you program jumps to native code (such as calling gzip compression on a large file)? Up to now you would not see that information.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;Many packages make use of the CPython C API (which we discurage, please lookup &lt;a href="https://cffi.readthedocs.org/" target="_blank"&gt;cffi&lt;/a&gt; for a better way to call C). Have you ever had the issue that you know that your performance problems reach down to, but you could not profile it properly?&lt;b&gt; Now you can!&lt;/b&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt; &lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;Let's inspect a very simple Python program to find out why a program is significantly slower on Linux than on Mac:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;&lt;span style="font-size: small;"&gt;&lt;span&gt;import numpy as np&lt;br&gt;
n = 1000&lt;br&gt;
a = np.random.random((n, n))&lt;br&gt;
b = np.random.random((n, n))&lt;br&gt;
c = np.dot(np.abs(a), b)&lt;/span&gt;&lt;br&gt;
&lt;/span&gt;&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
Take two NxN random matrix objects and create a dot product. The first argument to the dot product provides the absolute value of the random matrix.&lt;br&gt;
&lt;br&gt;
&lt;table border="1" style="border: 1px solid silver;"&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Run&lt;/td&gt;&lt;td&gt;Python&lt;/td&gt;&lt;td&gt;NumPy&lt;/td&gt;&lt;td&gt;OS&lt;/td&gt;&lt;td&gt;n=...&lt;/td&gt; &lt;td&gt;Took&lt;/td&gt; &lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;[1]&lt;/td&gt;&lt;td&gt;CPython 3.5.2&lt;/td&gt;&lt;td&gt;NumPy 1.12.1&lt;/td&gt;&lt;td&gt;Mac OS X, 10.12.3&lt;/td&gt;&lt;td&gt;n=5000&lt;/td&gt;&lt;td&gt;~9 sec&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt; &lt;td&gt;[2]&lt;/td&gt;&lt;td&gt;CPython 3.6.0&lt;/td&gt;&lt;td&gt;NumPy 1.12.1&lt;/td&gt;&lt;td&gt;Linux 64, Kernel 4.9.14&lt;/td&gt;&lt;td&gt;n=1000&lt;/td&gt;&lt;td&gt;~26 sec&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;br&gt;
Note that the Linux machine operates on a 5 times smaller matrix, still it takes much longer. What is wrong? Is Linux slow? CPython 3.6.0? Well no, lets inspect and &lt;a href="https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac" target="_blank"&gt;[1]&lt;/a&gt; and &lt;a href="https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c" target="_blank"&gt;[2]&lt;/a&gt; (shown below in that order).&lt;br&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://3.bp.blogspot.com/-WF-JpMQhJaI/WNvx8CPNpTI/AAAAAAAAAqw/ixZpWng6TDc4kIlEHu9zhqrNX4tx0S4rgCLcB/s1600/macosx-profile-blog.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="105" src="https://3.bp.blogspot.com/-WF-JpMQhJaI/WNvx8CPNpTI/AAAAAAAAAqw/ixZpWng6TDc4kIlEHu9zhqrNX4tx0S4rgCLcB/s400/macosx-profile-blog.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;
&lt;a href="https://1.bp.blogspot.com/-gjM2uj5Ko_E/WNvx73qcXEI/AAAAAAAAAqs/cMvDfcHQ2eAti4BRU0ldwGQ5M-1_TQ2FACEw/s1600/linux-blog.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="113" src="https://1.bp.blogspot.com/-gjM2uj5Ko_E/WNvx73qcXEI/AAAAAAAAAqs/cMvDfcHQ2eAti4BRU0ldwGQ5M-1_TQ2FACEw/s400/linux-blog.png" width="400"&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br&gt;
&lt;a href="https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c" target="_blank"&gt;[2]&lt;/a&gt; runs on Linux, spends nearly all of the time in PyArray_MatrixProduct2, if you compare to &lt;a href="https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac" target="_blank"&gt;[1]&lt;/a&gt; on Mac OS X, you'll see that a lot of time is spent in generating the random numbers and the rest in cblas_matrixproduct.&lt;br&gt;
&lt;br&gt;
Blas has a very efficient implementation so you can achieve the same on Linux if you install a blas implementation (such as openblas).&lt;br&gt;
&lt;br&gt;
Usually you can spot potential program source locations that take a lot of time and might be the first starting point to resolve performance issues.&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;Beyond Python programs &lt;/span&gt;&lt;br&gt;
&lt;br&gt;
It is not unthinkable that the strategy can be reused for native programs. Indeed this can already be done by creating a small cffi wrapper around an entry point of a compiled C program. It would even work for programs compiled from other languages (e.g. C++ or Fortran). The resulting function names are the full symbol name embedded into either the executable symboltable or extracted from the dwarf debugging information. Most of those will be compiler specific and contain some cryptic information.&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;Memory profiling&lt;/span&gt;&lt;br&gt;
We thankfully received a code contribution from the company Blue Yonder. They have built a memory profiler (for Linux and Mac OS X) on top of vmprof.com that displays the memory consumption for the runtime of your process.&lt;br&gt;
&lt;br&gt;
You can run it the following way:&lt;br&gt;
&lt;br&gt;
&lt;span&gt;$ python -m vmprof --mem --web script.py&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
By adding --mem, vmprof will capture memory information and display it in the dedicated view on vmprof.com. You can view it by by clicking the 'Memory' switch in the flamegraph view.&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;There is more&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
Some more minor highlights contained in 0.4.x:&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;VMProf support for Windows 64 bit (No native profiling)&lt;/li&gt;
&lt;li&gt;VMProf can read profiles generated by another host system&lt;/li&gt;
&lt;li&gt;VMProf is now bundled in several binary wheel for fast and easy installation (Mac OS X, Linux 32/64 for CPython 2.7, 3.4, 3.5, 3.6)&lt;/li&gt;
&lt;/ul&gt;
&lt;span style="font-size: large;"&gt;Future plans - Profile Streaming&lt;/span&gt;&lt;br&gt;
&lt;br&gt;
vmprof has not reached the end of development. There are many features we could implement. But there is one feature that could be a great asset to many Python developers.&lt;br&gt;
&lt;br&gt;
Continuous delivery of your statistical profile, or in short, profile streaming. One of the great strengths of vmprof is that is consumes very little overhead. It is not a crazy idea to run this in production.&lt;br&gt;
&lt;br&gt;
It would require a smart way to stream the profile in the background to vmprof.com and new visualizations to look at much more data your Python service produces.&lt;br&gt;
&lt;br&gt;
If that sounds like a solid vmprof improvement, don't hesitate to get in touch with us (e.g. IRC #pypy, mailing list pypy-dev, or comment below)&lt;br&gt;
&lt;br&gt;
&lt;span style="font-size: large;"&gt;You can help! &lt;/span&gt;&lt;br&gt;
&lt;br&gt;
There are some immediate things other people could help with. Either by donating time or money (yes we have occasional contributors which is great)!&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;We gladly received code contribution for the memory profiler. But it was not enough time to finish the migration completely. Sadly it is a bit brittle right now.&lt;/li&gt;
&lt;li&gt;We would like to spend more time on other visualizations. This should include to give a much better user experience on vmprof.com (like a tutorial that explains the visualization that we already have). &lt;/li&gt;
&lt;li&gt;Build Windows 32/64 bit wheels (for all CPython versions we currently support)&lt;/li&gt;
&lt;/ul&gt;
We are also happy to accept google summer of code projects on vmprof for new visualizations and other improvements. If you qualify and are interested, don't hesitate to ask!&lt;br&gt;
&lt;br&gt;
Richard Plangger (plan_rich) and the PyPy Team&lt;br&gt;
&lt;br&gt;
[1] Mac OS X &lt;a href="https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac"&gt;https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac&lt;/a&gt;&lt;br&gt;
[2] Linux64 &lt;a href="https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c"&gt;https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c&lt;/a&gt;</description><category>profiling</category><category>vmprof</category><guid>https://www.pypy.org/posts/2017/04/native-profiling-in-vmprof-6949065546884243105.html</guid><pubDate>Sat, 01 Apr 2017 14:17:00 GMT</pubDate></item></channel></rss>
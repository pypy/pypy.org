<!DOCTYPE html>
<html \ prefix="
        og: http://ogp.me/ns# article: http://ogp.me/ns/article#
    " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A Faster Python">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PyPy (old posts, page 31) | PyPy</title>
<link href="../assets/css/rst_base.css" rel="stylesheet" type="text/css">
<link href="../assets/css/nikola_rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../assets/css/styles.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://www.pypy.org/blog/index-31.html">
<link rel="icon" href="../favicon2.ico" sizes="16x16">
<link rel="icon" href="../favicon32x32.ico" sizes="32x32">
<link rel="prev" href="index-32.html" type="text/html">
<link rel="next" href="index-30.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="../assets/css/tipuesearch.css">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="container">
             <header id="header"><!-- Adapted from https://www.taniarascia.com/responsive-dropdown-navigation-bar --><section class="navigation"><div class="nav-container">
            <div class="brand">
                <a href="../index.html">
                    <image id="toplogo" src="../images/pypy-logo.svg" width="75px;" alt="PyPy/"></image></a>
            </div>
            <nav><ul class="nav-list">
<li> 
                <a href="#!">Features</a>
                <ul class="nav-dropdown">
<li> <a href="../features.html">What is PyPy?</a> </li>  
                    <li> <a href="../compat.html">Compatibility</a> </li>  
                    <li> <a href="../performance.html">Performance</a> </li>  
                </ul>
</li>
          <li> <a href="../download.html">Download</a> </li>  
          <li> <a href="http://doc.pypy.org">Dev Docs</a> </li>  
            <li> 
                <a href="#!">Blog</a>
                <ul class="nav-dropdown">
<li> <a href=".">Index</a> </li>  
                    <li> <a href="../categories/">Tags</a> </li>  
                    <li> <a href="../archive.html">Archive by year</a> </li>  
                    <li> <a href="../rss.xml">RSS feed</a> </li>  
                    <li> <a href="https://morepypy.blogspot.com/">Old site</a> </li>  
                </ul>
</li>
            <li> 
                <a href="#!">About</a>
                <ul class="nav-dropdown">
<li> <a href="https://bsky.app/profile/pypyproject.bsky.social">Bluesky</a> </li>  
                    <li> <a href="https://libera.irclog.whitequark.org/pypy">IRC logs</a> </li>  
                    <li> <a href="https://www.youtube.com/playlist?list=PLADqad94yVqDRQXuqxKrPS5QnVqbDLlRt">YouTube</a> </li>  
                    <li> <a href="https://www.twitch.tv/pypyproject">Twitch</a> </li>  
                    <li> <a href="../pypy-sponsors.html">Sponsors</a> </li>  
                    <li> <a href="../howtohelp.html">How To Help?</a> </li>  
                    <li> <a href="../contact.html">Contact</a> </li>  
                </ul>
</li>

                </ul></nav><div class="nav-mobile">
                <a id="nav-toggle" href="#!"> <span></span></a>
            </div>
        </div>
    </section><div class="searchform" role="search">
                
<form class="navbar-form navbar-left" action="../search.html" role="search">
    <div class="form-group">
        <input type="text" class="form-control" id="tipue_search_input" name="q" placeholder="Search…" autocomplete="off">
</div>
    <input type="submit" value="Local Search" style="visibility: hidden;">
</form>

            </div>
    </header><main id="content"><div class="post">
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/08/a-field-test-of-software-transactional-5659022209916605798.html" class="u-url">A Field Test of Software Transactional Memory Using the RSqueak Smalltalk VM</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/08/a-field-test-of-software-transactional-5659022209916605798.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-08-09T13:15:00Z" itemprop="datePublished" title="2014-08-09 13:15">2014-08-09 13:15</time></a>
            </p>
                <p class="commentline">5 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <h2>
Extending the Smalltalk RSqueakVM with STM</h2>
<p>by Conrad Calmez, Hubert Hesse, Patrick Rein and Malte Swart supervised by Tim Felgentreff and Tobias Pape</p>
<h2>
Introduction</h2>
<p>After pypy-stm we can announce that through the <a href="https://bitbucket.org/pypy/lang-smalltalk">RSqueakVM</a> (which used to be called <em>SPyVM</em>) a second VM implementation supports software transactional memory. RSqueakVM is a Smalltalk implementation based on the RPython toolchain. We have added STM support based on the <a href="../posts/2014/07/pypy-stm-first-interesting-release-8684276541915333814.html">STM tools from RPython (rstm)</a>. The benchmarks indicate that linear scale up is possible, however in some situations the STM overhead limits speedup.</p>
<p>The work was done as a master's project at the <a href="https://www.hpi.uni-potsdam.de/hirschfeld/">Software Architechture Group</a> of Professor Robert Hirschfeld at at the <a href="https://hpi.de/">Hasso Plattner Institut</a> at the <a href="https://www.uni-potsdam.de/">University of Potsdam</a>. We - four students - worked about one and a half days per week for four months on the topic. The RSqueakVM was <a href="https://pypysqueak.blogspot.de/2007/10/first-day-discussions.html">originally developped during a sprint at the University of Bern</a>. When we started the project we were new to the topic of building VMs / interpreters.</p>
<p>We would like to thank  Armin, Remi and the #pypy IRC channel who supported us over the course of our project. We also like to thank Toni Mattis and Eric Seckler, who have provided us with an <a href="https://bitbucket.org/amintos/lang-smalltalk">initial code base</a>.</p>
<h2 id="introduction-to-rsqueakvm">
Introduction to RSqueakVM</h2>
<p>As the original Smalltalk implementation, the RSqueakVM executes a given Squeak Smalltalk image, containing the Smalltalk code and a snapshot of formerly created objects and active execution contexts. These execution contexts are scheduled inside the image (greenlets) and not mapped to OS threads. Thereby the non-STM RSqueakVM runs on only one OS thread.</p>
<h2 id="changes-to-rsqueakvm">
Changes to RSqueakVM</h2>
<p>The core adjustments to support STM were inside the VM and transparent from the view of a Smalltalk user. Additionally we added Smalltalk code to influence the behavior of the STM. As the RSqueakVM has run in one OS thread so far, we added the capability to start OS threads. Essentially, we added an additional way to launch a new Smalltalk execution context (thread). But in contrast to the original one this one creates a new native OS thread, not a Smalltalk internal green thread.</p>

<p>STM (with automatic transaction boundaries) already solves the problem of concurrent access on one value as this is protected by the STM transactions (to be more precise one instruction). But there are cases were the application relies on the fact that a bigger group of changes is executed either completely or not at all (atomic). Without further information transaction borders could be in the middle of such a set of atomic statements. rstm allows to aggregate multiple statements into one higher level transaction. To let the application mark the beginning and the end of these atomic blocks (high-level transactions), we added two more STM specific extensions to Smalltalk.</p>

<h2 id="benchmarks">
Benchmarks</h2>
<p>RSqueak was executed in a single OS thread so far. rstm enables us to execute the VM using several OS threads. Using OS threads we expected a speed-up in benchmarks which use multiple threads. We measured this speed-up by using two benchmarks: a simple parallel summation where each thread sums up a predefined interval and an implementation of Mandelbrot where each thread computes a range of predefined lines.</p>

<p>To assess the speed-up, we used one RSqueakVM compiled with rstm enabled, but once running the benchmarks with OS threads and once with Smalltalk green threads. The workload always remained the same and only the number of threads increased. To assess the overhead imposed by the STM transformation we also ran the green threads version on an unmodified RSqueakVM. All VMs were translated with the JIT optimization and all benchmarks were run once before the measurement to warm up the JIT. As the JIT optimization is working it is likely to be adoped by VM creators (the baseline RSqueakVM did that) so that results with this optimization are more relevant in practice than those without it. We measured the execution time by getting the system time in Squeak. The results are:</p>
<h4>
Parallel Sum Ten Million</h4>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;">

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-7J05whp07m8/U-iEdb3Ce0I/AAAAAAAAAVw/91sD_1KEiGc/s1600/parallelSum10MioChart.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://1.bp.blogspot.com/-7J05whp07m8/U-iEdb3Ce0I/AAAAAAAAAVw/91sD_1KEiGc/s320/parallelSum10MioChart.png"></a></div>

</td></tr>
<tr><td class="tr-caption" style="text-align: center;"><span style="font-size: small; text-align: start;">Benchmark Parallel Sum 10,000,000</span></td></tr>
</tbody></table>
<table>
<thead><tr>
<th>Thread Count</th> <th>RSqueak green threads</th> <th>RSqueak/STM green threads</th> <th>RSqueak/STM OS threads</th> <th>Slow down from  RSqueak green threads to RSqueak/STM green threads</th> <th>Speed up from RSqueak/STM green threads to RSQueak/STM OS Threads</th> </tr></thead>
<tbody>
<tr>
<td>1</td>   <td>168.0 ms</td>   <td>240.0 ms</td>   <td>290.9 ms</td>   <td>0.70</td>   <td>0.83</td>  </tr>
<tr>
<td>2</td>   <td>167.0 ms</td>   <td>244.0 ms</td>   <td>246.1 ms</td>   <td>0.68</td>   <td>0.99</td>  </tr>
<tr>
<td>4</td>   <td>167.8 ms</td>   <td>240.7 ms</td>   <td>366.7 ms</td>   <td>0.70</td>   <td>0.66</td>  </tr>
<tr>
<td>8</td>   <td>168.1 ms</td>   <td>241.1 ms</td>   <td>757.0 ms</td>   <td>0.70</td>   <td>0.32</td>  </tr>
<tr>
<td>16</td>   <td>168.5 ms</td>   <td>244.5 ms</td>   <td>1460.0 ms</td>   <td>0.69</td>   <td>0.17</td>  </tr>
</tbody>
</table>
<br><h4>
Parallel Sum One Billion</h4>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;">

<div class="separator" style="clear: both; text-align: center;"><a href="https://3.bp.blogspot.com/-wN-Bad8Pnd8/U-iE43ZtHcI/AAAAAAAAAV4/dii8NU0rseE/s1600/parallelSum1BioChart.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://3.bp.blogspot.com/-wN-Bad8Pnd8/U-iE43ZtHcI/AAAAAAAAAV4/dii8NU0rseE/s320/parallelSum1BioChart.png"></a></div>

</td></tr>
<tr><td class="tr-caption" style="text-align: center;">Benchmark Parallel Sum 1,000,000,000</td></tr>
</tbody></table>
<br><table>
<thead><tr>
<th>Thread Count</th>
<th>RSqueak green threads</th>
<th>RSqueak/STM green threads</th>
<th>RSqueak/STM OS threads</th>
<th>Slow down from  RSqueak green threads to RSqueak/STM green threads</th>
<th>Speed up from RSqueak/STM green threads to RSQueak/STM OS Threads</th>
</tr></thead>
<tbody>
<tr>
<td>1</td>   <td>16831.0 ms</td>   <td>24111.0 ms</td>   <td>23346.0 ms</td>   <td>0.70</td>   <td>1.03</td>  </tr>
<tr>
<td>2</td>   <td>17059.9 ms</td>   <td>24229.4 ms</td>   <td>16102.1 ms</td>   <td>0.70</td>   <td>1.50</td>  </tr>
<tr>
<td>4</td>   <td>16959.9 ms</td>   <td>24365.6 ms</td>   <td>12099.5 ms</td>   <td>0.70</td>   <td>2.01</td>  </tr>
<tr>
<td>8</td>   <td>16758.4 ms</td>   <td>24228.1 ms</td>   <td>14076.9 ms</td>   <td>0.69</td>   <td>1.72</td>  </tr>
<tr>
<td>16</td>   <td>16748.7 ms</td>   <td>24266.6 ms</td>   <td>55502.9 ms</td>   <td>0.69</td>   <td>0.44</td>  </tr>
</tbody>
</table>
<br><h4>
Mandelbrot Iterative</h4>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;">

<div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/-_wLcNRFGkQc/U-iFOB3wDmI/AAAAAAAAAWA/He1oxb0hEpc/s1600/mandelbrotChart.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://2.bp.blogspot.com/-_wLcNRFGkQc/U-iFOB3wDmI/AAAAAAAAAWA/He1oxb0hEpc/s320/mandelbrotChart.png"></a></div>

</td></tr>
<tr><td class="tr-caption" style="text-align: center;">Benchmark Mandelbrot</td></tr>
</tbody></table>
<table>
<thead><tr>
<th>Thread Count</th> <th>RSqueak green threads</th> <th>RSqueak/STM green threads</th> <th>RSqueak/STM OS threads</th> <th>Slow down from  RSqueak green threads to RSqueak/STM green threads</th> <th>Speed up from RSqueak/STM green threads to RSqueak/STM OS Threads</th> </tr></thead>
<tbody>
<tr>
<td>1</td>   <td>724.0 ms</td>   <td>983.0 ms</td>   <td>1565.5 ms</td>   <td>0.74</td>   <td>0.63</td>  </tr>
<tr>
<td>2</td>   <td>780.5 ms</td>   <td>973.5 ms</td>   <td>5555.0 ms</td>   <td>0.80</td>   <td>0.18</td>  </tr>
<tr>
<td>4</td>   <td>781.0 ms</td>   <td>982.5 ms</td>   <td>20107.5 ms</td>   <td>0.79</td>   <td>0.05</td>  </tr>
<tr>
<td>8</td>   <td>779.5 ms</td>   <td>980.0 ms</td>   <td>113067.0 ms</td>   <td>0.80</td>   <td>0.01</td>
</tr>
</tbody>
</table>
<br><h2>
Discussion of benchmark results</h2>
<p>First of all, the ParallelSum benchmarks show that the parallelism is actually paying off, at least for sufficiently large embarrassingly parallel problems. Thus RSqueak can also benefit from rstm.</p>
<p>On the other hand, our Mandelbrot implementation shows the limits of our current rstm integration. We implemented two versions of the algorithm one using one low-level array and one using two nested collections. In both versions, one job only calculates a distinct range of rows and both lead to a slowdown. The summary of the state of rstm transactions shows that there are a lot of inevitable transactions (transactions which must be completed). One reason might be the interactions between the VM and its low-level extensions, so called plugins. We have to investigate this further.</p>
<h2 id="limitations">
Limitations</h2>
<p>Although the current VM setup is working well enough to support our benchmarks, the VM still has limitations. First of all, as it is based on rstm, it has the current limitation of only running on 64-bit Linux.</p>
<p>Besides this, we also have two major limitations regarding the VM itself. First, the atomic interface exposed in Smalltalk is currently not working, when the VM is compiled using the just-in-time compiler transformation. Simple examples such as concurrent parallel sum work fine while more complex benchmarks such as <a href="https://benchmarksgame.alioth.debian.org/u32/performance.php?test=chameneosredux#about">chameneos</a> fail. The reasons for this are currently beyond our understanding. Second, Smalltalk supports green threads, which are threads which are managed by the VM and are not mapped to OS threads. We currently support starting new Smalltalk threads as OS threads instead of starting them as green threads. However, existing threads in a Smalltalk image are not migrated to OS threads, but remain running as green threads.</p>
<h2 id="future-work-for-stm-in-rsqueak">
Future work for STM in RSqueak</h2>
The work we presented showed interesting problems, we propose the following problem statements for further analysis:<br><ul>
<li>
<strong>Inevitable transactions</strong> in benchmarks. This looks like it could limit other applications too so it should be solved.</li>
<li>
<strong>Collection implementation aware of STM</strong>: The current implementation of collections can cause a lot of STM collisions due to their internal memory structure. We believe it could bear potential for performance improvements,  if we replace these collections in an STM enabled interpreter with implementations with less STM collisions. As already proposed by Remi Meier, bags, sets and lists are of particular interest.</li>
<li>Finally, we exposed <strong>STM through languages features</strong> such as the atomic method, which is provided through the VM. Originally, it was possible to model STM transactions barriers implicitly by using clever locks, now its exposed via the atomic keyword. From a language design point of view, the question arises whether this is a good solution and what features an stm-enabled interpreter must provide to the user in general? Of particular interest are for example, access to the transaction length and hints for transaction borders to and their  performance impact.</li>
</ul>
<ul></ul>
<h2 id="details-for-the-technically-inclined">
Details for the technically inclined</h2>
<ul>
<li>
<a href="https://bitbucket.org/pypy/lang-smalltalk/diff/spyvm/interpreter.py?diff1=7a217be69118&amp;diff2=a772ee2447d96041e7db6550e160e90251d0dd85&amp;at=stmgc-c7#Lspyvm/interpreter.pyT233">Adjustments to the interpreter loop were minimal</a>.</li>
<li>STM works on bytecode granularity that means, there is a implicit transaction border after every bytecode executed. Possible alternatives: only break transactions after certain  bytecodes, break transactions on one abstraction layer above, e.g. object methods (setter, getter).</li>
<li>rstm calls were exposed using primtives (a way to expose native code in Smalltalk), this was mainly used for atomic.</li>
<li>Starting and stopping OS threads is exposed via primitives as well. Threads are started from within the interpreter.</li>
<li>For Smalltalk enabled STM code we currently have different image versions. However another way to add, load and replace code to the Smalltalk code base is required to make a switch between STM and non-STM code simple.</li>
</ul>
<ul></ul>
<h2 id="details-on-the-project-setup">
Details on the project setup</h2>
<p>From a non-technical perspective, a problem we encountered was the huge roundtrip times (on our machines up to 600s, 900s with JIT enabled). This led to a tendency of bigger code changes ("Before we compile, let's also add this"), lost flow ("What where we doing before?") and different compiled interpreters in parallel testing ("How is this version different from the others?") As a consequence it was harder to test and correct errors. While this is not as much of a problem for other RPython VMs, RSqueakVM needs to execute the entire image, which makes running it untranslated even slower.</p>
<h2 id="summary">
Summary</h2>
<p>The benchmarks show that speed up is possible, but also that the STM overhead in some situations can eat up the speedup. The  resulting STM-enabled VM still has some limitations: As rstm is  currently only running on 64-bit Linux the RSqueakVM is doing so as  well. Eventhough it is possible for us now to create new threads that  map to OS threads within the VM, the migration of exiting Smalltalk threads keeps being problematic.</p>
<p>We showed that an existing VM code base can benefit of STM in terms of scaling up. Further it was relatively easy to enable STM support. This may also be valuable to VM developers considering to get STM support for their VMs.</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-1134013810206557754">
        <div class="comment-header">
          <a name="comment-1134013810206557754"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-08-09 14:10</span>:
        </div>
        <div class="comment-content">
          <p>"We showed that an existing VM code base can benefit of STM in terms of scaling up."  I dispute this conclusion: in the benchmarks, it seems that the non-STM version is scaling up well, even better than the STM+OS-threads version.  But how can the non-STM version scale at all?  It shouldn't: that's a property of RPython.  And why is the STM+OS-threads version faster even with just 1 thread?  I think you need to answer these questions first.  Right now it screams "you are running buggy benchmarks" to me.</p>
        </div>
      </div>
      <div class="comment comment-5924716782062654109">
        <div class="comment-header">
          <a name="comment-5924716782062654109"></a>
            <span class="author">Stefan Marr</span> wrote on <span class="date">2014-08-10 09:09</span>:
        </div>
        <div class="comment-content">
          <p>I concur with Armin, the conclusions are problematic in the light of the current numbers.<br><br>Could you give some more details on the benchmarks? Can I find the Smalltalk code somewhere?<br><br>Things that come to mind are details about the scheduler. In the RoarVM, that was also one of the issues (which we did not solve). The standard Squeak scheduling data structure remains unchanged I suppose? How does that interact with the STM, is it problematic that each STM thread updates this shared data structure during every scheduling operation?<br><br>Also, more basic, are you making sure that the benchmark processes are running with highest priority (80, IIRC), to avoid interference with other processes in the image?<br><br>On the language level, something that could also have an impact on the results is closures. How are they implemented? I suppose similar to the way the CogVM implements them? I suppose, you make sure that closures are not shared between processes?<br><br>And finally, what kind of benchmark harness are you using? Did you have a look at SMark? (https://smalltalkhub.com/#!/~StefanMarr/SMark)<br>We used that one for the RoarVM, and it provides various options to do different kind of benchmarks, including weak-scaling benchmarks, which I would find more appropriate for scalability tests. Weak-scaling means, you increase the problem size with the number of cores. That replicates the scenario where the problem itself is not really parallelizable, but you can solve more problems at the same time in parallel. It also makes sure that each process/thread does the identical operations (if setup correctly).<br><br>Well, all those questions aside, interesting work :) Hope to read more soon ;)</p>
        </div>
      </div>
      <div class="comment comment-5483563096834838698">
        <div class="comment-header">
          <a name="comment-5483563096834838698"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-08-10 20:13</span>:
        </div>
        <div class="comment-content">
          <p>You definitely hit a really weak spot in our report... Today we investigated the ParallelSum benchmark again. So far, we've found out that it was indeed partially a problem with the priority of the benchmark process. The preliminary benchmark results make more sense now and as soon as we have stable ones we will update them.<br><br>I'll still try to address some of your questions right now. :)<br><br>1. Benchmark code<br>I've just wrapped up the current version of our benchmarks and put them in our repository. You can find the two Squeak4.5 images at the <a href="https://bitbucket.org/pypy/lang-smalltalk/src/627db53859875ea0120a4dbf3f21e244174f1618/images/benchmark-images/?at=stmgc-c7" rel="nofollow">stmgc-c7 branch of the RSqueak Repository</a> . You can find the benchmarks in the CPB package. The Squeak4.5stm image needs the RSqueak/STM VM.<br><br>2. Scheduler data structures<br>Yes, the  scheduling data structure is completely unchanged. We have only added a new subclass of Process which overwrites fork and calls a different primitive. However, these Processes are not managed by the Smalltalk scheduler, so there should be no synchronization issues here.<br><br>3. Interference of other processes:<br>This is probably the source of the "speed-up" we observe on the normal RSqueakVM. With more threads we might get a bigger portion of the total runtime. So far, the benchmarks already ran in a VM mode which disables the Smalltalk GUI thread, however in the traces we found that the event handler is still scheduled every now and then. We've done it as you suggested, Stefan, and set the priority to 80 (or 79 to not mess up the timer interrupt handler).<br><br>4. Benchmark harness<br>We actually use SMark and also made sure the timing operations of RSqueak do their job correctly. However we are probably not using SMark at its full potential.</p>
        </div>
      </div>
      <div class="comment comment-5673200995509471022">
        <div class="comment-header">
          <a name="comment-5673200995509471022"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-08-11 10:12</span>:
        </div>
        <div class="comment-content">
          <p>I've just updated the benchmarks. All benchmark processes are now running with the Smalltalk process priority of 79 (80 is the highest). The single-threaded VMs now show the expected behavior.</p>
        </div>
      </div>
      <div class="comment comment-3527177116588518292">
        <div class="comment-header">
          <a name="comment-3527177116588518292"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-08-11 14:11</span>:
        </div>
        <div class="comment-content">
          <p>To further clarify on the Mandelbrot benchmarks: After a discussion with Stefan, I have changed the Mandelbrot implementation. Each job now only has private data and does not read or write in any shared data structure. Still the benchmark results remain the same and we can still observe a high proportion of inevitable transactions.<br><br>As Armin pointed out, and which would be a next step, we would need to figure out which parts of the interpreter might cause systematic conflicts.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/07/pypy-stm-first-interesting-release-8684276541915333814.html" class="u-url">PyPy-STM: first "interesting" release</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/07/pypy-stm-first-interesting-release-8684276541915333814.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-07-05T09:37:00Z" itemprop="datePublished" title="2014-07-05 09:37">2014-07-05 09:37</time></a>
            </p>
                <p class="commentline">20 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi all,</p>

<p>PyPy-STM is now reaching a point where we can say it's good enough to be
a GIL-less Python.  (We don't guarantee there are no more bugs, so please
report them :-)  The first official STM release:</p>

<ul>
<li>
<a href="https://bitbucket.org/pypy/pypy/downloads/pypy-stm-2.3-r2-linux64.tar.bz2">pypy-stm-2.3-r2-linux64</a>
<br><i>(UPDATE: this is release r2, fixing a systematic segfault at start-up on some systems)</i>
</li>
</ul>
<p>This corresponds roughly to PyPy 2.3 (not 2.3.1).  It requires 64-bit
Linux.  More precisely, this release is built for Ubuntu 12.04 to 14.04;
you can also <a href="https://pypy.org/download.html#building-from-source">rebuild it
from source</a> by getting the branch <strong>stmgc-c7</strong>.  You need
clang to compile, and you need a <a href="https://bitbucket.org/pypy/stmgc/src/default/c7/llvmfix/">patched
version of llvm</a>.</p>

<p>This version's performance can reasonably be compared with a regular
PyPy, where both include the JIT.  Thanks for following the meandering progress of PyPy-STM over the past three years --- we're finally getting somewhere really interesting!  We cannot thank enough all contributors to the <a href="https://pypy.org/tmdonate.html">previous PyPy-STM money pot</a> that made this possible.  And, although this blog post is focused on the results from that period of time, I have of course to remind you that we're running a <a href="https://pypy.org/tmdonate2.html">second call for donation</a> for future work, which I will briefly mention again later.</p>

<p>A recap of what we did to get there: <a href="../posts/2014/02/rewrites-of-stm-core-model-again-633249729751034512.html">around the start of the year</a> we found a new model, a "redo-log"-based STM which uses a couple of hardware tricks to not require chasing pointers, giving it (in this context) exceptionally cheap read barriers.  This idea <a href="../posts/2014/03/hi-all-here-is-one-of-first-full-pypys-8725931424559481728.html">was developed</a> over the following months and (relatively) easily <a href="../posts/2014/04/stm-results-and-second-call-for-1767845182888902777.html">integrated with the JIT compiler</a>.  The most recent improvements on the Garbage Collection side are closing the gap with a regular PyPy (there is still a bit more to do there).  There is some <a href="https://pypy.readthedocs.org/en/latest/stm.html">preliminary user documentation</a>.</p>

<p>Today, the result of this is a PyPy-STM that is capable of running pure Python code on multiple threads in parallel, as we will show in the benchmarks that follow.  A quick warning: this is only about pure Python code.  We didn't try so far to optimize the case where most of the time is spent in external libraries, or even manipulating "raw" memory like <code>array.array</code> or numpy arrays.  To some extent there is no point because the approach of CPython works well for this case, i.e. releasing the GIL around the long-running operations in C.  Of course it would be nice if such cases worked as well in PyPy-STM --- which they do to some extent; but checking and optimizing that is future work.</p>

<p>As a starting point for our benchmarks, when running code that
only uses one thread, we get a slow-down between 1.2 and 3: at worst,
three times as slow; at best only 20% slower than a regular
PyPy.  This worst case has been brought down --it used to be 10x-- by
recent work on "card marking", a useful GC technique that is also
present in the regular PyPy (and about which I don't find any blog post;
maybe we should write one :-)  The main remaining issue is fork(), or
any function that creates subprocesses: it works, but is very slow.  To
remind you of this fact, it prints a line to stderr when used.</p>

<p>Now the real main part: when you run multithreaded code, it scales very nicely with two
threads, and less-than-linearly but still not badly with three or four
threads.  Here is an artificial example:</p>

<pre>    total = 0
    lst1 = ["foo"]
    for i in range(100000000):
        lst1.append(i)
        total += lst1.pop()</pre>

<p>We run this code N times, once in each of N threads
(<a href="https://bitbucket.org/pypy/benchmarks/raw/default/multithread/minibench1.py">full
benchmark</a>).  Run times, best of three:</p>

<table border="1" cellpadding="5"><tbody>
<tr>
<td>Number of threads</td>
    <td>Regular PyPy (head)</td>
    <td>PyPy-STM</td>
</tr>
<tr>
<td>N = 1</td>
    <td>real <strong>0.92s</strong> <br>
user+sys 0.92s</td>
    <td>real <strong>1.34s</strong> <br>
user+sys 1.34s</td>
</tr>
<tr>
<td>N = 2</td>
    <td>real <strong>1.77s</strong> <br>
user+sys 1.74s</td>
    <td>real <strong>1.39s</strong> <br>
user+sys 2.47s</td>
</tr>
<tr>
<td>N = 3</td>
    <td>real <strong>2.57s</strong> <br>
user+sys 2.56s</td>
    <td>real <strong>1.58s</strong> <br>
user+sys 4.106s</td>
</tr>
<tr>
<td>N = 4</td>
    <td>real <strong>3.38s</strong> <br>
user+sys 3.38s</td>
    <td>real <strong>1.64s</strong> <br>
user+sys 5.35s</td>
</tr>
</tbody></table>
<p>(The "real" time is the wall clock time.  The "user+sys" time is the
recorded CPU time, which can be larger than the wall clock time if
multiple CPUs run in parallel.  This was run on a 4x2 cores machine.
For direct comparison, avoid loops that are so trivial
that the JIT can remove <b>all</b> allocations from them: right now
PyPy-STM does not handle this case well.  It has to force a dummy allocation
in such loops, which makes minor collections occur much more frequently.)</p>

<p>Four threads is the limit so far: only four threads can be executed in
parallel.  Similarly, the memory usage is limited to 2.5 GB of GC
objects.  These two limitations are not hard to increase, but at least
increasing the memory limit requires fighting against more LLVM bugs.
(Include here snark remarks about LLVM.)</p>

<p>Here are some measurements from more real-world benchmarks.  This time,
the amount of work is fixed and we parallelize it on T threads.  The first benchmark is just running <a href="https://pypy.org/download.html#building-from-source">translate.py</a> on a trunk PyPy.  The last
three benchmarks are <a href="https://bitbucket.org/pypy/benchmarks/src/default/multithread/">here</a>.</p>

<table border="1" cellpadding="5"><tbody>
<tr>
<td>Benchmark</td>
    <td>PyPy 2.3</td>
    <td bgcolor="#A0A0A0">(PyPy head)</td>
    <td>PyPy-STM, T=1</td>
    <td>T=2</td>
    <td>T=3</td>
    <td>T=4</td>
</tr>
<tr>
<td>
<code>translate.py --no-allworkingmodules</code><br>
(annotation step)</td>
    <td>184s</td>
    <td bgcolor="#A0A0A0">(170s)</td>
    <td>386s (2.10x)</td>
    <td colspan="3">n/a</td>
</tr>
<tr>
<td>multithread-richards<br>
5000 iterations</td>
    <td>24.2s</td>
    <td bgcolor="#A0A0A0">(16.8s)</td>
    <td>52.5s (2.17x)</td>
    <td>37.4s (1.55x)</td>
    <td>25.9s (1.07x)</td>
    <td>32.7s (1.35x)</td>
</tr>
<tr>
<td>mandelbrot<br>
divided in 16-18 bands</td>
    <td>22.9s</td>
    <td bgcolor="#A0A0A0">(18.2s)</td>
    <td>27.5s (1.20x)</td>
    <td>14.4s (0.63x)</td>
    <td>10.3s (0.45x)</td>
    <td>8.71s (0.38x)</td>
</tr>
<tr>
<td>btree</td>
    <td>2.26s</td>
    <td bgcolor="#A0A0A0">(2.00s)</td>
    <td>2.01s (0.89x)</td>
    <td>2.22s (0.98x)</td>
    <td>2.14s (0.95x)</td>
    <td>2.42s (1.07x)</td>
</tr>
</tbody></table>
<p>This shows various cases that can occur:</p>

<ul>
<li>The mandelbrot example runs with minimal overhead and very good parallelization.
It's dividing the plane to compute in bands, and each of the T threads receives the
same number of bands.

</li>
<li>Richards, a classical benchmark for PyPy (tweaked to run the iterations
in multiple threads), is hard to beat on regular PyPy:
we suspect that the difference is due to the fact that a lot of
paths through the loops don't allocate, triggering the issue already
explained above.  Moreover, the speed of Richards was again improved
dramatically recently, in trunk.

</li>
<li>The translation benchmark measures the time <code>translate.py</code>
takes to run the first phase only, "annotation" (for now it consumes too much memory
to run <code>translate.py</code> to the end).  Moreover the timing starts only after the large number of
subprocesses spawned at the beginning (mostly gcc).  This benchmark is not parallel, but we
include it for reference here.  The slow-down factor of 2.1x is still too much, but
we have some idea about the reasons: most likely, again the Garbage Collector, missing the regular PyPy's
very fast small-object allocator for old objects.  Also, <code>translate.py</code>
is an example of application that could, with
reasonable efforts, be made largely parallel in the future using <i>atomic blocks.</i>

</li>
<li>Atomic blocks are also present in the btree benchmark.  I'm not completely sure
but it seems that, in this case, the atomic blocks create too many
conflicts between the threads for actual parallization: the base time is very good,
but running more threads does not help at all.
</li>
</ul>
<p>As a summary, PyPy-STM looks already useful to run CPU-bound multithreaded
applications.  We are certainly still going to fight slow-downs, but it
seems that there are cases where 2 threads are enough to outperform a regular
PyPy, by a large margin.  Please try it out on your own small examples!</p>

<p>And, at the same time, please don't attempt to retrofit threads inside
an existing large program just to benefit from PyPy-STM!
Our goal is not to send everyone down the obscure route of multithreaded
programming and its dark traps.  We are going finally to shift our main
focus on the <a href="https://pypy.org/tmdonate2.html">phase 2 of our
research</a> (donations welcome): how to enable a better way of writing multi-core programs.
The starting point is to fix and test atomic blocks.  Then we will have to
debug common causes of conflicts and fix them or work around them; and
try to see how common frameworks like Twisted can be adapted.</p>

<p>Lots of work ahead, but lots of work behind too :-)</p>

<p>Armin (thanks Remi as well for the work).</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-847304889041641602">
        <div class="comment-header">
          <a name="comment-847304889041641602"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-07-05 16:22</span>:
        </div>
        <div class="comment-content">
          <p>You're just extracting and running the "bin/pypy"?  It works for me on a very close configuration, Ubuntu 14.04 too...</p>
        </div>
      </div>
      <div class="comment comment-6129373448166719029">
        <div class="comment-header">
          <a name="comment-6129373448166719029"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-07-05 20:13</span>:
        </div>
        <div class="comment-content">
          <p>Yes.  Sorry, it doesn't make sense to me.  You need to debug with gdb, probably with an executable that has got the debugging symbols.  You need to either build it yourself, or recompile the pregenerated sources from: https://cobra.cs.uni-duesseldorf.de/~buildmaster/misc/pypy-c-r72356-stm-jit-SOURCE.txz</p>
        </div>
      </div>
      <div class="comment comment-1718242961576777973">
        <div class="comment-header">
          <a name="comment-1718242961576777973"></a>
            <span class="author">Ernst Sjöstrand</span> wrote on <span class="date">2014-07-05 23:40</span>:
        </div>
        <div class="comment-content">
          <p>If I try virtualenv I get:<br>virtualenv stmtest -p Projekt/pypy-stm-2.3-linux64/bin/pypy <br>Running virtualenv with interpreter Projekt/pypy-stm-2.3-linux64/bin/pypy<br>[forking: for now, this operation can take some time]<br>[forking: for now, this operation can take some time]<br>New pypy executable in stmtest/bin/pypy<br>[forking: for now, this operation can take some time]<br>ERROR: The executable stmtest/bin/pypy is not functioning<br>ERROR: It thinks sys.prefix is u'/home/ernst' (should be u'/home/ernst/stmtest')<br>ERROR: virtualenv is not compatible with this system or executable</p>
        </div>
      </div>
      <div class="comment comment-7417758447888372077">
        <div class="comment-header">
          <a name="comment-7417758447888372077"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-07-06 08:48</span>:
        </div>
        <div class="comment-content">
          <p>@Ernst: sorry, it works fine for me as well.  I tried the pypy-stm provided here, both on a Ubuntu 12.04 and a Ubuntu 14.04 machine.  Maybe you have a too old virtualenv?  Does it work with regular PyPy?</p>
        </div>
      </div>
      <div class="comment comment-316466071949263542">
        <div class="comment-header">
          <a name="comment-316466071949263542"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-07-07 12:37</span>:
        </div>
        <div class="comment-content">
          <p>Thanks to the author of the now-deleted comments, we could track and fix a bug that only shows up on some Linux systems.  If pypy-stm systematically segfaults at start-up for you too, try the "2.3-r2" release (see update in the post itself).</p>
        </div>
      </div>
      <div class="comment comment-9180922439710098283">
        <div class="comment-header">
          <a name="comment-9180922439710098283"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-07-07 20:00</span>:
        </div>
        <div class="comment-content">
          <p>This is exciting! One minor bug in the actual post: you can describe slowdown / speedup in two different ways, with <i>total time</i> as a percentage of original time, or with <i>time difference</i> as a percentage of original time. You mention a 20% slowdown (clearly using the latter standard) and then a 300% slowdown, which you describe as 3x (suggesting that you use the former standard). To be consistent , you should either describe them as 120% and 300%, respectively (using the former standard), or 20% and 200%, respectively (using the latter standard).<br><br>Thanks!</p>
        </div>
      </div>
      <div class="comment comment-3518498281907978124">
        <div class="comment-header">
          <a name="comment-3518498281907978124"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-07-07 21:35</span>:
        </div>
        <div class="comment-content">
          <p>Hi again,<br><br>just to play around a little I've put together https://github.com/Tinche/stm-playground for myself.<br><br>I picked a generic CPU-bound problem (primality testing) and tried comparing multithreaded implementations in CPython 2.7, ordinary PyPy and PyPy-STM.<br><br>I figured this would be easily parallelizable (low conflicts) but it doesn't seem to be the case - I don't get all my cores pegged using the STM.<br><br>bench-threadpool.py, on my machine, gives about the same time for CPython and PyPy-STM, while ordinary PyPy totally smokes them both (even with the GIL :), one order of magnitude difference (20 sec vs 2 sec).<br><br>bench-threadpool-naive will crash the STM interpreter on my system. :)<br><br>Getting away from threads, CPython will actually beat PyPy in a multi-process scenario by a factor of 2, which I found surprising. CPython does indeed use up all my cores 100% while dealing with a process pool, while PyPy has won't even come close.<br><br>For the same workload, PyPy is actually faster running multithreaded with the GIL than multi-process, and fastest running with only 1 thread (expected, with the GIL only being overhead in this scenario).</p>
        </div>
      </div>
      <div class="comment comment-2112283081867579797">
        <div class="comment-header">
          <a name="comment-2112283081867579797"></a>
            <span class="author">Pim</span> wrote on <span class="date">2014-07-07 21:40</span>:
        </div>
        <div class="comment-content">
          <p>This is good news.  For many of my applications, an important feature in the next phase will be the optimization for <i>[..] the built-in dictionary type, for which we would like accesses and writes using independent keys to be truly independent [..]</i>.  My applications are mostly server applications (Twisted-based and others) that store state information on sessions/transactions in a small number of dictionaries that can have hundreds or thousands of entries concurrently, and would be accessed constantly.<br><br>I'm glad I donated and plan do so again in the future :-)</p>
        </div>
      </div>
      <div class="comment comment-5209811594865201707">
        <div class="comment-header">
          <a name="comment-5209811594865201707"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-07-08 10:47</span>:
        </div>
        <div class="comment-content">
          <p>@Tin: I would tweak bench-queue.py to avoid a million inter-thread communications via the queue.  For example, run 1000 check_primes instead of just 1 for every number received from the queue.</p>
        </div>
      </div>
      <div class="comment comment-655623208312316426">
        <div class="comment-header">
          <a name="comment-655623208312316426"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-07-08 11:17</span>:
        </div>
        <div class="comment-content">
          <p>@Tin: ...no, I tried too and it doesn't seem to help.  We'll need to look into this in more details....</p>
        </div>
      </div>
      <div class="comment comment-6685757942855237486">
        <div class="comment-header">
          <a name="comment-6685757942855237486"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-07-08 21:04</span>:
        </div>
        <div class="comment-content">
          <p>@Armin I've pushed a version of bench-queue with a tweakable batch size and concurrency level. Doing the work in batches of, say, 1000 does indeed make it go faster with all implementations.<br><br>I've noticed pypy-stm runs have a large variance. It's not like I'm doing scientific measurements here, but for the queue test I'm getting runtimes from ~15 sec to ~27 sec, whereas for example ordinary PyPy is in the range 4.6 sec - 4.9 sec, and CPython ~22.5 - ~24.7, again, relatively close. Again, this is just something I noticed along the way and not the result of serious benchmarking in isolation.</p>
        </div>
      </div>
      <div class="comment comment-2934799691602993962">
        <div class="comment-header">
          <a name="comment-2934799691602993962"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-07-10 08:44</span>:
        </div>
        <div class="comment-content">
          <p>Ooooof.  Ok, I found out what is wrong in bench-queue.  The issue is pretty technical, but basically if you add "with __pypy__.thread.atomic:" in the main top-level loop in worker(), then it gets vastly faster.  On my machine it beats the real-time speed of a regular pypy.  See https://bpaste.net/show/450553/<br><br>It clearly needs to be fixed...</p>
        </div>
      </div>
      <div class="comment comment-212615962092206453">
        <div class="comment-header">
          <a name="comment-212615962092206453"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-07-10 09:31</span>:
        </div>
        <div class="comment-content">
          <p>Added an answer to the question "what about PyPy3?": https://pypy.readthedocs.org/en/latest/stm.html#python-3</p>
        </div>
      </div>
      <div class="comment comment-331088964102585652">
        <div class="comment-header">
          <a name="comment-331088964102585652"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-07-12 01:03</span>:
        </div>
        <div class="comment-content">
          <p>@Armin, cool! I've found that the thread pool version can be sped up ~2-3x by wrapping the contents of check_prime with 'atomic' too.<br><br>One more observation: with the atomic context manager, on PyPy-STM the queue implementation will beat the thread pool implementation (slightly), which isn't the case for CPython or ordinary PyPy.</p>
        </div>
      </div>
      <div class="comment comment-971976868841362148">
        <div class="comment-header">
          <a name="comment-971976868841362148"></a>
            <span class="author">geerk</span> wrote on <span class="date">2014-07-16 08:16</span>:
        </div>
        <div class="comment-content">
          <p>This is exciting news! I think pypy is the future of python.</p>
        </div>
      </div>
      <div class="comment comment-1750480159387933541">
        <div class="comment-header">
          <a name="comment-1750480159387933541"></a>
            <span class="author">Canesin</span> wrote on <span class="date">2014-07-19 15:40</span>:
        </div>
        <div class="comment-content">
          <p>If you guys did a facelift on the website like yours HippyVM I believe the project would gain a lot of momentum, it is unfortunate but true that most company managers would visit it and think it is not industrial quality if an employ comes saying that they should sponsor developing something in PyPy.</p>
        </div>
      </div>
      <div class="comment comment-232182151094407831">
        <div class="comment-header">
          <a name="comment-232182151094407831"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-07-20 11:26</span>:
        </div>
        <div class="comment-content">
          <p>r2 still doesn't work for me (ubuntu 14.04, intel  Core2 CPU T7400)<br>bash: ./pypy: cannot execute binary file: Exec format error</p>
        </div>
      </div>
      <div class="comment comment-4569742965503234967">
        <div class="comment-header">
          <a name="comment-4569742965503234967"></a>
            <span class="author">isomorph</span> wrote on <span class="date">2014-07-31 05:46</span>:
        </div>
        <div class="comment-content">
          <p>this is a question for the guys developing PyPy... i am completely new to Python so please bear with me.<br><br>here is what i don't understand:  it seems to me that you are reinventing the wheel because doesn't the Oracle or Azul Systems JVM already provide a super performant GC and JIT? even STM is becoming available.  and since Jython can run on the JVM, why do PyPy at all?  <br><br>wouldn't a JVM compliant implementation of Python be more performant than PyPy or CPython?<br><br>or am i missing something here?<br><br>any pointers greatly appreciated. thanks.</p>
        </div>
      </div>
      <div class="comment comment-5455837696108207648">
        <div class="comment-header">
          <a name="comment-5455837696108207648"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-08-04 08:04</span>:
        </div>
        <div class="comment-content">
          <p>Having a JIT in the JVM is very different from having a JIT that can understand Python.  For proof, the best (and only) implementation of Python on the JVM, Jython, is running at around CPython speed (generally a bit slower).  I suspect that STM is similarly not designed for the purposes to which Jython would put it and would thus perform poorly.  The only part that would probably work out of the box would be the GC.  A more subtle argument against starting from the JVM is that of semantic mismatch.  See for example https://www.stups.uni-duesseldorf.de/mediawiki/images/5/51/Pypy.pdf</p>
        </div>
      </div>
      <div class="comment comment-9038957033110235106">
        <div class="comment-header">
          <a name="comment-9038957033110235106"></a>
            <span class="author">isomorph</span> wrote on <span class="date">2014-08-04 14:44</span>:
        </div>
        <div class="comment-content">
          <p>awesome!  thanks a lot armin.  :D</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/06/pypy3-231-fulcrum-3765964217640322884.html" class="u-url">PyPy3 2.3.1 - Fulcrum</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/philip-jenvey.html">Philip Jenvey</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/06/pypy3-231-fulcrum-3765964217640322884.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-06-20T21:31:00Z" itemprop="datePublished" title="2014-06-20 21:31">2014-06-20 21:31</time></a>
            </p>
                <p class="commentline">6 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>We're pleased to announce the first stable release of PyPy3. PyPy3<br>
targets Python 3 (3.2.5) compatibility.</p>
<p>We would like to thank all of the people who <a class="reference external" href="../posts/2012/01/py3k-and-numpy-first-stage-thanks-to-3008917396290059758.html">donated</a> to the <a class="reference external" href="https://pypy.org/py3donate.html">py3k proposal</a><br>
for supporting the work that went into this.</p>
<p>You can download the PyPy3 2.3.1 release here:</p>
<blockquote><a class="reference external" href="https://pypy.org/download.html#pypy3-2-3-1">https://pypy.org/download.html#pypy3-2-3-1</a></blockquote>
<div class="section" id="highlights">
<h1>Highlights</h1>
<ul class="simple">
<li>The first stable release of PyPy3: support for Python 3!</li>
<li>The stdlib has been updated to Python 3.2.5</li>
<li>Additional support for the u'unicode' syntax (<a class="reference external" href="https://legacy.python.org/dev/peps/pep-0414/">PEP 414</a>) from Python 3.3</li>
<li>Updates from the default branch, such as incremental GC and various JIT<br>
improvements</li>
<li>Resolved some notable JIT performance regressions from PyPy2:</li>
</ul>
<blockquote><ul class="simple">
<li>Re-enabled the previously disabled collection (list/dict/set) strategies</li>
<li>Resolved performance of iteration over range objects</li>
<li>Resolved handling of Python 3's exception __context__ unnecessarily forcing<br>
frame object overhead</li>
</ul></blockquote>
</div>
<div class="section" id="what-is-pypy">
<h1>What is PyPy?</h1>
<p>PyPy is a very compliant Python interpreter, almost a drop-in replacement for<br>
CPython 2.7.6 or 3.2.5. It's fast due to its integrated tracing JIT compiler.</p>
<p>This release supports x86 machines running Linux 32/64, Mac OS X 64, Windows,<br>
and OpenBSD,<br>
as well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.</p>
<p>While we support 32 bit python on Windows, work on the native Windows 64<br>
bit python is still stalling, we would welcome a volunteer<br>
to <a class="reference external" href="https://doc.pypy.org/en/latest/windows.html#what-is-missing-for-a-full-64-bit-translation">handle that</a>.</p>
</div>
<div class="section" id="how-to-use-pypy">
<h1>How to use PyPy?</h1>
<p>We suggest using PyPy from a <a class="reference external" href="https://www.virtualenv.org/en/latest/">virtualenv</a>. Once you have a virtualenv<br>
installed, you can follow instructions from <a class="reference external" href="https://doc.pypy.org/en/latest/getting-started.html#installing-using-virtualenv">pypy documentation</a> on how<br>
to proceed. This document also covers other <a class="reference external" href="https://doc.pypy.org/en/latest/getting-started.html#installing-pypy">installation schemes</a>.</p>
<p>Cheers,<br>
the PyPy team</p>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-8337188602615024814">
        <div class="comment-header">
          <a name="comment-8337188602615024814"></a>
            <span class="author">Omer Katz</span> wrote on <span class="date">2014-06-24 08:26</span>:
        </div>
        <div class="comment-content">
          <p>Can we get some benchmarks much like we have for PyPY and CPython 2.7?</p>
        </div>
      </div>
      <div class="comment comment-319334287815414588">
        <div class="comment-header">
          <a name="comment-319334287815414588"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-06-24 09:06</span>:
        </div>
        <div class="comment-content">
          <p>As far as I know, a majority of the benchmarks we use have never been ported to Python 3.  So it's far more complicated than just push a switch.</p>
        </div>
      </div>
      <div class="comment comment-8004592542250333529">
        <div class="comment-header">
          <a name="comment-8004592542250333529"></a>
            <span class="author">jusic</span> wrote on <span class="date">2014-06-25 08:25</span>:
        </div>
        <div class="comment-content">
          <p>Awesome, congrats on the new release! Finally some stable PyPy goodness for Python 3 as well :)</p>
        </div>
      </div>
      <div class="comment comment-2087876253124726985">
        <div class="comment-header">
          <a name="comment-2087876253124726985"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-06-27 05:37</span>:
        </div>
        <div class="comment-content">
          <p>Woo! This is exciting! (Now we just need to upgrade to 3.4... : ) )</p>
        </div>
      </div>
      <div class="comment comment-708279398688696672">
        <div class="comment-header">
          <a name="comment-708279398688696672"></a>
            <span class="author">geerk</span> wrote on <span class="date">2014-06-28 09:06</span>:
        </div>
        <div class="comment-content">
          <p>Glad to hear that PyPy is now for python 3. Great work!</p>
        </div>
      </div>
      <div class="comment comment-731294035922898289">
        <div class="comment-header">
          <a name="comment-731294035922898289"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-07-03 15:04</span>:
        </div>
        <div class="comment-content">
          <p>This is great!<br><br>Now I can finally test PyPy on some code for which I wanted to test it for years!<br><br>(backporting to py2 was too painful)<br><br>Thank you very much!</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/06/pypy-231-terrestrial-arthropod-trap-5076300474324870908.html" class="u-url">PyPy 2.3.1 - Terrestrial Arthropod Trap Revisited</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/mattip.html">mattip</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/06/pypy-231-terrestrial-arthropod-trap-5076300474324870908.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-06-07T23:14:00Z" itemprop="datePublished" title="2014-06-07 23:14">2014-06-07 23:14</time></a>
            </p>
            
        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">We're pleased to announce PyPy 2.3.1, a feature-and-bugfix improvement over our recent 2.3 release last month.<br><br>
This release contains several bugfixes and enhancements among the user-facing improvements:<br><ul style="text-align: left;">
<li>The built-in <tt class="docutils literal"><span class="pre">struct</span></tt> module was renamed to <tt class="docutils literal"><span class="pre">_struct</span></tt>, solving issues with IDLE and other modules</li>
<li>Support for compilation with gcc-4.9</li>
<li>A CFFI-based version of the gdbm module is now included in our binary bundle</li>
<li>Many issues were <a class="reference external" href="https://foss.heptapod.net/pypy/pypy/-/issues?status=resolved">resolved</a> since the 2.3 release on May 8 </li>
</ul>
<br>
You can download the PyPy 2.3.1 release here:<br><br><a href="https://pypy.org/download.html">https://pypy.org/download.html</a><br><br><a href="https://www.pypy.org/">PyPy</a> is a very compliant Python interpreter, almost a drop-in replacement for <a href="https://www.python.org/">CPython</a> 2.7. It's fast (<a href="https://speed.pypy.org/">pypy 2.3.1 and cpython 2.7.x performance comparison</a>) due to its integrated tracing JIT compiler.<br><br>
This release supports x86 machines running Linux 32/64, Mac OS X 64,  Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7,  with VFPv3) running Linux.  <br>
We would like to thank our donors for the continued support of the PyPy project.<br><br>
The complete release notice is <a href="https://doc.pypy.org/en/latest/release-2.3.1.html">here.</a><br><br>
Please try it out and let us know what you think. We especially welcome success stories, please tell us about how it has helped you!<br><br>
Cheers, The PyPy Team</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/05/pypy-23-terrestrial-arthropod-trap-9057496904945555741.html" class="u-url">PyPy 2.3 - Terrestrial Arthropod Trap</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/mattip.html">mattip</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/05/pypy-23-terrestrial-arthropod-trap-9057496904945555741.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-05-09T08:38:00Z" itemprop="datePublished" title="2014-05-09 08:38">2014-05-09 08:38</time></a>
            </p>
                <p class="commentline">6 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">
<div dir="ltr" style="text-align: left;">
We’re pleased to announce PyPy 2.3, which targets version 2.7.6 of the Python language. This release updates the stdlib from 2.7.3, jumping directly to 2.7.6.<br><br>
This release also contains several bugfixes and performance improvements, many generated by real users finding corner cases. <a href="https://cffi.readthedocs.org/">CFFI</a> has made it easier than ever to use existing C code with both cpython and PyPy, easing the transition for packages like <a href="https://cryptography.io/">cryptography</a>, <a href="https://pypi.python.org/pypi/Pillow/2.4.0">Pillow</a>(Python Imaging Library [Fork]), a basic port of <a href="https://github.com/CTPUG/pygame_cffi">pygame-cffi</a>, and others.<br><br>
PyPy can now be embedded in a hosting application, for instance inside <a href="https://uwsgi-docs.readthedocs.org/en/latest/PyPy.html">uWSGI</a><br><br>
You can download the PyPy 2.3 release here:<br><br><a href="https://pypy.org/download.html">https://pypy.org/download.html</a><br><br>
PyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (<a href="https://speed.pypy.org/">pypy 2.3 and cpython 2.7.x performance comparison</a>; note that cpython's speed has not changed since 2.7.2) due to its integrated tracing JIT compiler.<br><br>
This release supports x86 machines running Linux 32/64, Mac OS X 64, Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux. </div>
<br>
We would like to thank our donors for the continued support of the PyPy project.<br><br>
The complete release notice is <a href="https://doc.pypy.org/en/latest/release-2.3.0.html">here</a><br><br>
Cheers, The PyPy Team</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-8455486395054686439">
        <div class="comment-header">
          <a name="comment-8455486395054686439"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-05-10 05:20</span>:
        </div>
        <div class="comment-content">
          <p>Hi Why don't you accept Bitcoin as one of donation methods? Bitcoin makes it easier to donate your project<br><br>I believe that you add it and announce it here, there will be several posts in Reddit and others sources that help you to collect funds</p>
        </div>
      </div>
      <div class="comment comment-7924036830614203707">
        <div class="comment-header">
          <a name="comment-7924036830614203707"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-05-10 06:40</span>:
        </div>
        <div class="comment-content">
          <p>right, i think so</p>
        </div>
      </div>
      <div class="comment comment-1778224702437464822">
        <div class="comment-header">
          <a name="comment-1778224702437464822"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-05-10 22:21</span>:
        </div>
        <div class="comment-content">
          <p>Hey,<br>Just wondering, does v2.3 contains the fix for issue 1683 titled "BytesIO leaks like hell"?<br><br>https://bugs.pypy.org/issue1683</p>
        </div>
      </div>
      <div class="comment comment-6419600948366627976">
        <div class="comment-header">
          <a name="comment-6419600948366627976"></a>
            <span class="author">Eric van Riet Paap</span> wrote on <span class="date">2014-05-12 21:40</span>:
        </div>
        <div class="comment-content">
          <p>The bug status is set to resolved so one would expect it to be fixed. Please reopen the bug report if  you think differently.</p>
        </div>
      </div>
      <div class="comment comment-9181582466378701023">
        <div class="comment-header">
          <a name="comment-9181582466378701023"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-05-14 10:59</span>:
        </div>
        <div class="comment-content">
          <p>There is no info about what what exactly made CFFI easier in this release.</p>
        </div>
      </div>
      <div class="comment comment-6855401007321299849">
        <div class="comment-header">
          <a name="comment-6855401007321299849"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-05-14 20:21</span>:
        </div>
        <div class="comment-content">
          <p>Hello pypy team!  If you have not have not seen this post... https://www.rfk.id.au/blog/entry/pypy-js-faster-than-cpython/ , I think you will find it to be quite interesting!</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/04/numpy-on-pypy-status-update-1103134247318103282.html" class="u-url">NumPy on PyPy - Status Update</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/brian-kearns.html">Brian Kearns</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/04/numpy-on-pypy-status-update-1103134247318103282.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-04-15T20:08:00Z" itemprop="datePublished" title="2014-04-15 20:08">2014-04-15 20:08</time></a>
            </p>
                <p class="commentline">8 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Work on NumPy on PyPy continued in March, though at a lighter pace than the previous few months. Progress was made on both compatibility and speed fronts. Several behavioral issues reported to the bug tracker were resolved. The most significant of these was probably the correction of casting to built-in Python types. Previously, int/long conversions of numpy scalars such as inf/nan/1e100 would return bogus results. Now, they raise or return values, as appropriate.<br><br>
On the speed front, enhancements to the PyPy JIT were made to support virtualizing the raw_store/raw_load memory operations used in numpy arrays. Further work remains here in virtualizing the alloc_raw_storage when possible. This will allow scalars to have storages but still be virtualized when possible in loops.<br><br>
Aside from continued work on compatibility/speed of existing code, we also hope to begin implementing the C-level components of other numpy modules such as mtrand, nditer, linalg, and so on. Several approaches could be taken to get C-level code in these modules working, ranging from reimplementing in RPython to interfacing with existing code with CFFI, if possible. The appropriate approach depends on many factors and will probably vary from module to module.<br><br>To try out PyPy + NumPy, grab a <a href="https://buildbot.pypy.org/nightly/trunk/">nightly PyPy</a> and install our <a href="https://bitbucket.org/pypy/numpy">NumPy fork</a>. Feel free to report comments/issues to IRC, our mailing list, or bug tracker. Thanks to the contributors to the <a href="https://pypy.org/numpydonate.html">NumPy on PyPy</a> proposal for supporting this work.</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-6400248038590907144">
        <div class="comment-header">
          <a name="comment-6400248038590907144"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-24 23:22</span>:
        </div>
        <div class="comment-content">
          <p>Trying to install scipy on top gives me an error while compiling scipy/cluster/src/vq_module.c; isn't scipy yet supported?</p>
        </div>
      </div>
      <div class="comment comment-5521763149430155242">
        <div class="comment-header">
          <a name="comment-5521763149430155242"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-04-30 12:38</span>:
        </div>
        <div class="comment-content">
          <p>scipy is not supported. Sometimes scipy functions are in fact in numpy in which case you can just copy the code. Otherwise you need to start learning cffi.</p>
        </div>
      </div>
      <div class="comment comment-6791929198131342129">
        <div class="comment-header">
          <a name="comment-6791929198131342129"></a>
            <span class="author">Yichao Yu</span> wrote on <span class="date">2014-05-18 02:07</span>:
        </div>
        <div class="comment-content">
          <p>You mentioned storage and scalar types. Is it related to <a href="https://bitbucket.org/pypy/numpy/issue/10/scalar-types-should-not-be-iterable" rel="nofollow">this bug</a></p>
        </div>
      </div>
      <div class="comment comment-1163556554411187648">
        <div class="comment-header">
          <a name="comment-1163556554411187648"></a>
            <span class="author">vak</span> wrote on <span class="date">2014-08-14 09:19</span>:
        </div>
        <div class="comment-content">
          <p>what is the status about incorporating BLAS library?</p>
        </div>
      </div>
      <div class="comment comment-3524602750550973532">
        <div class="comment-header">
          <a name="comment-3524602750550973532"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-09-22 21:52</span>:
        </div>
        <div class="comment-content">
          <p>How far is running Pandas on Pypy? Will it be just a recompile when Numpy is ported, or is it heavy work to port Pandas to Pypy after Numpy is done? Should I look after another solution than plan to run Pandas on Pypy?</p>
        </div>
      </div>
      <div class="comment comment-5294670401307346999">
        <div class="comment-header">
          <a name="comment-5294670401307346999"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-11-13 10:07</span>:
        </div>
        <div class="comment-content">
          <p>Pandas on PyPy would indeed be very interesting for huge analysis runs.</p>
        </div>
      </div>
      <div class="comment comment-2300428569909267206">
        <div class="comment-header">
          <a name="comment-2300428569909267206"></a>
            <span class="author">Jami</span> wrote on <span class="date">2014-11-18 17:14</span>:
        </div>
        <div class="comment-content">
          <p>Any news on the NumPy front? I check this blog for such stuff every week and also contributed to the funding drive.<br><br>I fully understand that developers skilled enough to work on such a project are hard to come by even with money, and NumPy support isn't probably the most technologically exciting aspect of PyPy.<br><br>Just even a few lines on the latest development or some milestones would show that the project is alive (although I fully understand that writing blog posts isn't everybody's favorite thing). And some kind of summary that in what shape the developers think the code is in. If you prefer coding to blogging, maybe implementing some kind of time-series graph for the numpypy-status page could be nice also (I keep checking it out but can never remember what was the state last time I checked). Maybe I can see if I can do a quick hack via eg archive.org for this.<br><br>I think also a huge boost would be to have even a hacky temporary way to interface with Matplotlib and/or SciPy, as it's quite hard to do many practical analyses without these. I'd probably try to do my analyses in such an environment and perhaps even implement/fix at least things that are my own itches. There was the 2011 hack, but it doesn't seem to be elaborated anywhere. I could live with (or even prefer, so it definitely won't become the permanent version) a ugly, slow, memory-hungry and unstable hack that would spam the stderr with insulting messages. But without any way of interfacing the existing stuff it's just too much work for the more complicated analyses.<br><br>I'm trying to track the https://bitbucket.org/pypy/numpy branch but it's a bit hard to see the bigger picture just from the commits. Even just some tags and/or meta-issues could be helpful. I'm also a bit confused on where (repo-wise) the development is actually happening. There are some sort of fresh NumPy-branches in the numpy tree. The micronumpy-project is probably dead or merged into the pypy/numpy-branch?<br><br>PS. Please don't take this as too strong criticism. I prefer to just silently code away myself too. Just what would be nice to see as somebody eagerly waiting to use Pypy in numerical stuff.</p>
        </div>
      </div>
      <div class="comment comment-6511786739763976032">
        <div class="comment-header">
          <a name="comment-6511786739763976032"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2014-11-24 12:00</span>:
        </div>
        <div class="comment-content">
          <p>Hey Jami<br><br>We'll try to write a blog post shortly</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/04/stm-results-and-second-call-for-1767845182888902777.html" class="u-url">STM results and Second Call for Donations</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/04/stm-results-and-second-call-for-1767845182888902777.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-04-09T09:33:00Z" itemprop="datePublished" title="2014-04-09 09:33">2014-04-09 09:33</time></a>
            </p>
                <p class="commentline">2 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi all,</p>

<p>We now have a preliminary version of <a href="https://pypy.readthedocs.org/en/latest/stm.html#current-status">PyPy-STM
with the JIT</a>, from the new <a href="https://pypy.readthedocs.org/en/latest/stm.html">STM documentation
page.</a>  This PyPy-STM is still not quite useful, failing to top the
performance of a regular PyPy by a small margin on most benchmarks, but
it's definitely getting there :-)  The overheads with the JIT are still
a bit too high.  (I've been tracking an obscure bug since days.
It turned out to be a simple buffer overflow.  But if anybody has
a clue about why a hardware watchpoint in gdb, set on one of the garbled
memory locations, fails to trigger but the memory ends up being modified
anyway... and, it turns out, by just a regular pointer write... ideas
welcome.)</p>

<p>But I go off-topic :-)  The main point of this post is to announce the
<a href="https://pypy.org/tmdonate2.html">2nd Call for Donation about
STM</a>.  We achieved most of the goals laid out in the first call.  We
even largely overachieved them in terms of raw performance, even if
there are many cases that are unreasonably slow for now.  So, after the
successful research, we are launching a second proposal about the
development part of the project:</p>

<ol>
<li>
<p>Polish PyPy-STM to get a consistently reasonable speed, 25%-40%
slower than a regular JITted PyPy when running single-threaded code.  Of
course it is supposed to scale nicely as long as there are no
user-visible conflicts.</p>

</li>
<li>
<p>Focus on developing the Python-facing interface: both internal things
(e.g. do dictionaries need to be more TM-friendly in general?) as well
as directly visible things (e.g. some profiler-like interface to explore
common conflicts in a program).</p>

</li>
<li><p>Regular multithreaded code should benefit out of the box, but the
final goal is to explore and tweak some existing non-multithreaded
frameworks and improve their TM-friendliness.  So existing programs
using Twisted or Stackless, for example, should run on multiple cores
without any major change.</p></li>
</ol>
<p>See the <a href="https://pypy.org/tmdonate2.html">full call</a> for more
details!  I'd like to thank Remi Meier for getting involved.  And a big
thank you to everybody who contributed money on the first call.  It
took more time than anticipated, but it's there in good but rough shape.
Now it needs a lot of polishing :-)</p>

<p>Armin</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-3580155145909585133">
        <div class="comment-header">
          <a name="comment-3580155145909585133"></a>
            <span class="author">Dmitrey</span> wrote on <span class="date">2014-05-03 19:48</span>:
        </div>
        <div class="comment-content">
          <p>it would be good to have compiled stm version for something more recent than Ubuntu 12.04, e.g. 14.04, preferably with numpy included, to simplify numpy installation. Or, maybe, that version for 12.04 works with 14.04?</p>
        </div>
      </div>
      <div class="comment comment-4509013790359331231">
        <div class="comment-header">
          <a name="comment-4509013790359331231"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-05-10 14:56</span>:
        </div>
        <div class="comment-content">
          <p>Yes, Ubuntu 14.04 seems to run fine any PyPy compiled for Ubuntu 12.04.  Numpy probably works in pypy-stm, but being a module that accesses matrix data as "external" raw memory, it does not support multi-core execution.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/03/pygamecffi-pygame-on-pypy-8679802461301121984.html" class="u-url">pygame_cffi: pygame on PyPy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/03/pygamecffi-pygame-on-pypy-8679802461301121984.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-03-26T16:28:00Z" itemprop="datePublished" title="2014-03-26 16:28">2014-03-26 16:28</time></a>
            </p>
                <p class="commentline">40 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">
<p>The Raspberry Pi aims to be a low-cost educational tool that anyone can use to learn about electronics and programming. Python and <a class="reference external" href="https://pygame.org/news.html">pygame</a> are included in the Pi's programming toolkit. And since last year, thanks in part to sponsorship from the <a class="reference external" href="https://www.raspberrypi.org/">Raspberry Pi Foundation</a>, PyPy also works on the Pi (read more <a class="reference external" href="../posts/2013/05/pypy-20-alpha-for-arm-2318299473927531503.html">here</a>).</p>
<p>With PyPy working on the Pi, game logic written in Python stands to gain an awesome performance boost. However, the original pygame is a Python C extension. This means it performs poorly on PyPy and negates any speedup in the Python parts of the game code.</p>
<p>One solution to making pygame games run faster on PyPy, and eventually on the Raspberry Pi, comes in the form of <a class="reference external" href="https://github.com/CTPUG/pygame_cffi">pygame_cffi</a>. pygame_cffi uses <a class="reference external" href="https://cffi.readthedocs.org/">CFFI</a> to wrap the underlying SDL library instead of a C extension. A few months ago, the Raspberry Pi Foundation sponsored a <a class="reference external" href="../posts/2013/12/pygame-cffi-8991437796535033699.html">Cape Town Python User Group hackathon</a> to build a proof-of-concept pygame using CFFI. This hackathon was a success and it produced an early working version of pygame_cffi.</p>
<p>So for the last 5 weeks Raspberry Pi has been funding work on pygame_cffi. The goal was a complete implementation of the core modules. We also wanted benchmarks to illuminate performance differences between pygame_cffi on PyPy and pygame on CPython. We are happy to report that those goals were met. So without further ado, here's a rundown of what works.</p>
<div class="section" id="current-functionality">
<h3>Current functionality</h3>
<ul class="simple">
<li>
<a class="reference external" href="https://www.pygame.org/docs/ref/surface.html">Surfaces</a> support all the usual flags for SDL and OpenGL rendering (more about OpenGL <a class="reference internal" href="../posts/2014/03/pygamecffi-pygame-on-pypy-8679802461301121984.html#pyopenglperformance">below</a>).</li>
<li>The graphics-related modules <a class="reference external" href="https://www.pygame.org/docs/ref/color.html">color</a>, <a class="reference external" href="https://www.pygame.org/docs/ref/display.html">display</a>, <a class="reference external" href="https://www.pygame.org/docs/ref/font.html">font</a> and <a class="reference external" href="https://www.pygame.org/docs/ref/image.html">image</a>, and parts of <a class="reference external" href="https://www.pygame.org/docs/ref/draw.html">draw</a> and <a class="reference external" href="https://www.pygame.org/docs/ref/transform.html">transform</a> are mostly complete.</li>
<li>
<a class="reference external" href="https://www.pygame.org/docs/ref/event.html">Events</a>! No <a class="reference external" href="https://www.pygame.org/docs/ref/fastevent.html">fastevent</a> module yet, though.</li>
<li>Mouse and keyboard functionality, as provided by the <a class="reference external" href="https://www.pygame.org/docs/ref/mouse.html">mouse</a> and <a class="reference external" href="https://www.pygame.org/docs/ref/key.html">key</a> modules, is complete.</li>
<li>Sound functionality, as provided by the <a class="reference external" href="https://www.pygame.org/docs/ref/mixer.html">mixer</a> and <a class="reference external" href="https://www.pygame.org/docs/ref/music.html">music</a> modules, is complete.</li>
<li>Miscellaneous modules, <a class="reference external" href="https://www.pygame.org/docs/ref/cursors.html">cursors</a>, <a class="reference external" href="https://www.pygame.org/docs/ref/rect.html">rect</a>, <a class="reference external" href="https://www.pygame.org/docs/ref/sprite.html">sprite</a> and <a class="reference external" href="https://www.pygame.org/docs/ref/time.html">time</a> are also complete.</li>
</ul>

Invention screenshot:

<div class="separator" style="clear: both; text-align: center;"><a href="https://3.bp.blogspot.com/-1ZVah86dW3s/UzL9ZhiDiKI/AAAAAAAABvI/kMO9Pnmq9FY/s1600/invention_screenshot.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://3.bp.blogspot.com/-1ZVah86dW3s/UzL9ZhiDiKI/AAAAAAAABvI/kMO9Pnmq9FY/s320/invention_screenshot.png"></a></div>

Mutable mamba screenshot:

<div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/-JZzDhMwp43s/UzL9g4lktwI/AAAAAAAABvQ/WuCvtbCA3Lc/s1600/mutable_mamba_screenshot.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://2.bp.blogspot.com/-JZzDhMwp43s/UzL9g4lktwI/AAAAAAAABvQ/WuCvtbCA3Lc/s320/mutable_mamba_screenshot.png"></a></div>

<p>With the above-mentioned functionality in place we could get 10+ of the pygame examples to work, and a number of <a class="reference external" href="https://pyweek.org/">PyWeek</a> games. At the time of writing, if a game doesn't work it is most likely due to an unimplemented <a class="reference external" href="https://www.pygame.org/docs/ref/transform.html">transform</a> or <a class="reference external" href="https://www.pygame.org/docs/ref/draw.html">draw</a> function. That will be remedied soon.</p>
</div>
<div class="section" id="performance">
<h3>Performance</h3>
<p>In terms of performance, pygame_cffi on PyPy is showing a lot of promise. It beats pygame on CPython by a significant margin in our events processing and collision detection benchmarks, while blit and fill benchmarks perform similarly. The pygame examples we checked also perform better.</p>

<div class="separator" style="clear: both; text-align: center;"><a href="https://3.bp.blogspot.com/-tSV6v3J5rwc/UzL-4CbkqCI/AAAAAAAABwQ/NFDuq4biNqY/s1600/collision_increase.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://3.bp.blogspot.com/-tSV6v3J5rwc/UzL-4CbkqCI/AAAAAAAABwQ/NFDuq4biNqY/s400/collision_increase.png"></a></div>

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-HJCdpeVHbj0/UzL-0e5eGMI/AAAAAAAABwI/3eKRVRpP45s/s1600/examples_bench.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://1.bp.blogspot.com/-HJCdpeVHbj0/UzL-0e5eGMI/AAAAAAAABwI/3eKRVRpP45s/s400/examples_bench.png"></a></div>

<p>However, there is still work to be done to identify and eliminate bottlenecks. On the Raspberry Pi performance is markedly worse compared to pygame (barring collision detection). The PyWeek games we tested also performed slightly worse. Fortunately there is room for improvement in various places.</p>

Invention &amp; Mutable Mamba (x86)

<div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-jYdr73oj154/UzL-u4aAwWI/AAAAAAAABwA/cv_vNSFtb0Q/s1600/pyweek_games_bench.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://4.bp.blogspot.com/-jYdr73oj154/UzL-u4aAwWI/AAAAAAAABwA/cv_vNSFtb0Q/s400/pyweek_games_bench.png"></a></div>

Standard pygame examples (Raspberry Pi)

<div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/-gd9KEHANb_I/UzL-oKCx5BI/AAAAAAAABv4/frssbcGhI9A/s1600/examples_bench_rasp.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://2.bp.blogspot.com/-gd9KEHANb_I/UzL-oKCx5BI/AAAAAAAABv4/frssbcGhI9A/s400/examples_bench_rasp.png"></a></div>

<p>Here's a summary of some of the benchmarks. Relative speed refers to the frame rate obtained in pygame_cffi on PyPy relative to pygame on CPython.</p>
<table border="1" class="docutils">
<colgroup>
<col width="76%">
<col width="24%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Benchmark</th>
<th class="head">Relative speed (pypy speedup)</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>Events (x86)</td>
<td>1.41</td>
</tr>
<tr>
<td>Events (Pi)</td>
<td>0.58</td>
</tr>
<tr>
<td>N<sup>2</sup> collision detection on 100 sprites (x86)</td>
<td>4.14</td>
</tr>
<tr>
<td>N<sup>2</sup> collision detection on 100 sprites (Pi)</td>
<td>1.01</td>
</tr>
<tr>
<td>Blit 100 surfaces (x86)</td>
<td>1.06</td>
</tr>
<tr>
<td>Blit 100 surfaces (Pi)</td>
<td>0.60</td>
</tr>
<tr>
<td>Invention (x86)</td>
<td>0.95</td>
</tr>
<tr>
<td>Mutable Mamba (x86)</td>
<td>0.72</td>
</tr>
<tr>
<td>stars example (x86)</td>
<td>1.95</td>
</tr>
<tr>
<td>stars example (Pi)</td>
<td>0.84</td>
</tr>
</tbody>
</table>
<div class="section" id="opengl">
<h2>OpenGL</h2>
<p id="pyopenglperformance">Some not-so-great news is that <a class="reference external" href="https://pyopengl.sourceforge.net/">PyOpenGL</a> performs poorly on PyPy since PyOpenGL uses ctypes. This translates into a nasty reduction in frame rate for games that use OpenGL surfaces. It might be worthwhile creating a CFFI-powered version of PyOpenGL as well.</p>
</div>
</div>
<div class="section" id="where-to-now">
<h3>Where to now?</h3>
<p>Work on pygame_cffi is ongoing. Here are some things that are in the pipeline:</p>
<ul class="simple">
<li>Get pygame_cffi on PyPy to a place where it is consistently faster than pygame on CPython.</li>
<li>Implement the remaining modules and functions, starting with <a class="reference external" href="https://www.pygame.org/docs/ref/draw.html">draw</a> and <a class="reference external" href="https://www.pygame.org/docs/ref/transform.html">transform</a>.</li>
<li>Improve test coverage.</li>
<li>Reduce the time it takes for CFFI to parse the cdef. This makes the initial pygame import slow.</li>
</ul>
<p>If you want to contribute you can find pygame_cffi <a class="reference external" href="https://github.com/CTPUG/pygame_cffi">on Github</a>.
Feel free to find us on #pypy on freenode or post issues on github.</p>
<p>Cheers,<br>
Rizmari Versfeld</p>
</div>
<br>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-1655733976004847288">
        <div class="comment-header">
          <a name="comment-1655733976004847288"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-28 01:04</span>:
        </div>
        <div class="comment-content">
          <p>Pygame should be an excellent way to benchmark the performance of pypy, so this is great!  I wanted to let you fellas know of another project that is using pypy that looks really neat as well... https://github.com/rfk/pypyjs</p>
        </div>
      </div>
      <div class="comment comment-8249132770798691487">
        <div class="comment-header">
          <a name="comment-8249132770798691487"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-28 12:46</span>:
        </div>
        <div class="comment-content">
          <p>pygame seems outdated, because it is based on first SDL version.<br><br>It will be interesting to see CFFI comparison for newer, SDL2 bindings, such as PySDL2, which is ctypes based at the moment.<br><br>https://pypi.python.org/pypi/PySDL2</p>
        </div>
      </div>
      <div class="comment comment-2197101148120095862">
        <div class="comment-header">
          <a name="comment-2197101148120095862"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2014-03-28 15:02</span>:
        </div>
        <div class="comment-content">
          <p>Anatoly, pygame is outdated but have no clear replacement. PySDL2 is nice, but it's only a low level binding, it does not really help in the case of writing games.</p>
        </div>
      </div>
      <div class="comment comment-3756951900884338415">
        <div class="comment-header">
          <a name="comment-3756951900884338415"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-28 18:31</span>:
        </div>
        <div class="comment-content">
          <p>Is it not wrapping the current SDL?  I thought that it was...  On github it says it's a pygame based wrapper(copies the api) for SDL, would that not make it the current SDL?</p>
        </div>
      </div>
      <div class="comment comment-7209139772900464696">
        <div class="comment-header">
          <a name="comment-7209139772900464696"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-03-29 00:37</span>:
        </div>
        <div class="comment-content">
          <p>I looked into PyOpenGL's code to see if there is an easy way to upgrade to CFFI.<br><br>It's a bag of cats EVERYWHERE. <br><br>ctypes are defined all over the place, unlike most ctypes-&gt;cffi projects, where there is a single source file (api.py) that is easy to convert due to it being the raw interface to the C library.<br><br></p>
        </div>
      </div>
      <div class="comment comment-805625527457221648">
        <div class="comment-header">
          <a name="comment-805625527457221648"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-29 06:41</span>:
        </div>
        <div class="comment-content">
          <p>@Maciej, pygame includes a lot of helpers and good documentation, but it is not perspective technology to play with. I'd say there are more interesting libs out there that gain more interesting results and speeding up dynamic binding for them would be very cool to make things like these - https://devart.withgoogle.com/ - possible.<br><br><br>@Anonymous, if I were to provide OpenGL bindings, I'd start with looking at https://github.com/p3/regal project and binding generator in scripts/</p>
        </div>
      </div>
      <div class="comment comment-2013483921445808712">
        <div class="comment-header">
          <a name="comment-2013483921445808712"></a>
            <span class="author">Temia Eszteri</span> wrote on <span class="date">2014-03-29 18:42</span>:
        </div>
        <div class="comment-content">
          <p>I've actually been working to see if I can get my own Pygame release, Sky Eraser, optimised enough to work on a Raspberry Pi -- it'd be worth seeing how implementing it under this configuration would work on top of the optimisations I've been working on in the background (boy are there a lot to make).<br><br>I might also be rewriting the APIs for Allegro 5.1 as an experiment though, to test under both CPython and PyPy.</p>
        </div>
      </div>
      <div class="comment comment-6204412962320323650">
        <div class="comment-header">
          <a name="comment-6204412962320323650"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-29 21:15</span>:
        </div>
        <div class="comment-content">
          <p>I started to work on a newer and experimental OpenGL wrapper for Python,  proudly blessed PyOpenGLng.<br><br>In comparison to PyOpenGL, it generates the requested OpenGL API from the OpenGL XML Registry and use an automatic translator to map the C API to Python. The translator is quite light weight in comparison to PyOpenGL source code. And it is already able to run a couple of examples for OpenGL V3 and V4.<br><br>Actually the wrapper use ctypes. But I am looking for tips to do the same for cffi, as well as feedbacks on performance and comments.<br><br>The project is hosted on https://github.com/FabriceSalvaire/PyOpenGLng.</p>
        </div>
      </div>
      <div class="comment comment-1437180940869577437">
        <div class="comment-header">
          <a name="comment-1437180940869577437"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-30 08:16</span>:
        </div>
        <div class="comment-content">
          <p>@Fabrice, how is your newer and experimental OpenGL wrapper generator is better than existing ones? I am not saying that there is a NIH effect - probably some omission from documentation.</p>
        </div>
      </div>
      <div class="comment comment-8926413427958821430">
        <div class="comment-header">
          <a name="comment-8926413427958821430"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-30 08:19</span>:
        </div>
        <div class="comment-content">
          <p>I mean that if PyOpenGL doesn't use wrapper generator then there are a couple around not limiting themselves to Python. I am especially interested to know the comparison with regal.</p>
        </div>
      </div>
      <div class="comment comment-342599703653596616">
        <div class="comment-header">
          <a name="comment-342599703653596616"></a>
            <span class="author">Alecks Gates</span> wrote on <span class="date">2014-03-30 22:20</span>:
        </div>
        <div class="comment-content">
          <p>It was my impression that OpenGL isn't hardware accelerated on the pi anyway... or am I incorrect?</p>
        </div>
      </div>
      <div class="comment comment-3915197357064046080">
        <div class="comment-header">
          <a name="comment-3915197357064046080"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-31 10:17</span>:
        </div>
        <div class="comment-content">
          <p>@anatoly: The only real replacement for pygame which I know is pyglet. It is not quite as game-optimized as pygame, but very versatile and a joy to use.<br><br>https://pyglet.org</p>
        </div>
      </div>
      <div class="comment comment-4800815379467947206">
        <div class="comment-header">
          <a name="comment-4800815379467947206"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-01 20:05</span>:
        </div>
        <div class="comment-content">
          <p>I've actually made a CFFI OpenGL binding, as part of my successor to my old PyGL3Display project. It's not hosted anywhere yet, but I'll see about getting up somewhere soon.</p>
        </div>
      </div>
      <div class="comment comment-1569339313098984299">
        <div class="comment-header">
          <a name="comment-1569339313098984299"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-02 14:32</span>:
        </div>
        <div class="comment-content">
          <p>And... done. A mostly drop-in replacement for PyOpenGL on CFFI, or at least for OpenGL 3.2 core spec.<br><br>https://www.dropbox.com/s/rd44asge17xjbn2/gl32.zip</p>
        </div>
      </div>
      <div class="comment comment-2979420898382012559">
        <div class="comment-header">
          <a name="comment-2979420898382012559"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-02 14:35</span>:
        </div>
        <div class="comment-content">
          <p>@Arne, pyglet rocks, because it is just `clone and run` unlike all other engines. But it looks a little outdated, that's why I started to look for alternatives.</p>
        </div>
      </div>
      <div class="comment comment-1334842746653621045">
        <div class="comment-header">
          <a name="comment-1334842746653621045"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-02 14:38</span>:
        </div>
        <div class="comment-content">
          <p>@David, if you want people to comment on this, Bitbucket would be a better way to share sources than Dropbox.</p>
        </div>
      </div>
      <div class="comment comment-5844610699095504300">
        <div class="comment-header">
          <a name="comment-5844610699095504300"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-02 14:57</span>:
        </div>
        <div class="comment-content">
          <p>@anatoly techtonick:<br>Actually, it'll end up on Launchpad in the near future (probably within 2 weeks?). However, it's the output of a wrapper generator and the wrapper generator is in pretty poor shape at the moment, in terms of packaging it's output. I just figured people might be able to use it in the near future, even if it is in 'source-code-dump' form. If there's a better temporary home for it somewhere, I'm all ears.</p>
        </div>
      </div>
      <div class="comment comment-907211568521745319">
        <div class="comment-header">
          <a name="comment-907211568521745319"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-02 15:08</span>:
        </div>
        <div class="comment-content">
          <p>@David, why reinvent the wheel? There are many wrapper generators around. Also, you project is not a replacement for PyOpenGL, because of GPL restrictions.</p>
        </div>
      </div>
      <div class="comment comment-392554119337358909">
        <div class="comment-header">
          <a name="comment-392554119337358909"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-02 15:39</span>:
        </div>
        <div class="comment-content">
          <p>@anatoly<br><br>I never claimed my project is a replacement for PyOpenGL - it's not API compatible, for a start. Regarding license, it'll probably get changed for the bindings at some point, probably to 3-clause BSD.<br><br>On the wrapper generator: Really, the only actively maintained wrapper generator for Python that I'm aware of (which isn't project specific) is SWIG, which is not appropriate (at the very least, googling for 'python wrapper generator -swig' doesn't seem to give many results). In any case, the wrapper generator isn't a lot of code.</p>
        </div>
      </div>
      <div class="comment comment-3573871793579930149">
        <div class="comment-header">
          <a name="comment-3573871793579930149"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-03 07:28</span>:
        </div>
        <div class="comment-content">
          <p>@anatoly: pyglet seems to be in maintenance mode right now. There are commits every few days, but only small stuff.<br><br>On the other hand I understand that: pyglet supplies everything a backend for a game-engine needs (I use it¹), so the next step should be to use it for many games and see whether shared needs arise.<br><br>¹: See https://1w6.org/deutsch/anhang/programme/hexbattle-mit-zombies and https://bitbucket.org/ArneBab/hexbattle/</p>
        </div>
      </div>
      <div class="comment comment-1490435339316396207">
        <div class="comment-header">
          <a name="comment-1490435339316396207"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-03 10:35</span>:
        </div>
        <div class="comment-content">
          <p>@David, I am speaking about OpenGL specific wrapper generators. I've added information to this page - https://www.opengl.org/wiki/Related_toolkits_and_APIs#OpenGL_loading_libraries<br><br>The OpenGL generator in Python is included in regal project here https://github.com/p3/regal/scripts<br><br>pyglet also has one.</p>
        </div>
      </div>
      <div class="comment comment-1118721639684635338">
        <div class="comment-header">
          <a name="comment-1118721639684635338"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-03 10:36</span>:
        </div>
        <div class="comment-content">
          <p>Sorry, the correct link is https://github.com/p3/regal/tree/master/scripts</p>
        </div>
      </div>
      <div class="comment comment-2780282584915669970">
        <div class="comment-header">
          <a name="comment-2780282584915669970"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-03 10:39</span>:
        </div>
        <div class="comment-content">
          <p>@Arne, kissing elves trick is low. =) Otherwise looks wesnothy and 2D. I don't see why it should use OpenGL. 3D models would be cool.<br><br>I'd try to make it run on PySDL2 with "from sdl2.ext.api import pyglet". There is no pyglet API there, but would be interesting to see if it is possible to provide one.</p>
        </div>
      </div>
      <div class="comment comment-6207023902483125812">
        <div class="comment-header">
          <a name="comment-6207023902483125812"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-03 15:58</span>:
        </div>
        <div class="comment-content">
          <p>@anatoly<br><br>Pyglet's GL wrapper generator creates a lot of chained functions (fairly slow in cPython). I'm also not sure if there's enough development activity in Pyglet to allow modifying core code, and given the size of the Pyglet project I'm not going to fork it. PyOpenGL has more or less the same issues.<br><br>Regal appears to be a very large project (a 68MB checkout), which has a scope much greater than just its wrapper generator - the sheer scope of the project does cause some barriers to entry. I'm still looking through, but I am fairly certain that it would take more effort to adapt Regals binding generator than I have expended on my own.</p>
        </div>
      </div>
      <div class="comment comment-9071520511276958069">
        <div class="comment-header">
          <a name="comment-9071520511276958069"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-03 21:06</span>:
        </div>
        <div class="comment-content">
          <p>@anatoly: I like kissing elves ☺ (and when I get to write the next part of the story, I intend to keep them as player characters: That someone starts out in an intimate moment does not mean he or she is watchmeat).<br><br>@David: I guess modifying core-code in pyglet is not that big of a problem, especially *because* it is mostly being maintained right now: Little danger of breaking the in-progress work of someone else.</p>
        </div>
      </div>
      <div class="comment comment-2366334588211182049">
        <div class="comment-header">
          <a name="comment-2366334588211182049"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-03 21:09</span>:
        </div>
        <div class="comment-content">
          <p>@anatoly: more specifically, I do not consider intimate moments as cheap (and WTactics has the image, so I could pull this off). Instead I try to rid myself of baseless inhibitions, though that’s not always easy: Killing off no longer needed societal conditioning is among the hardest battles…</p>
        </div>
      </div>
      <div class="comment comment-8901268274616702486">
        <div class="comment-header">
          <a name="comment-8901268274616702486"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-04 01:23</span>:
        </div>
        <div class="comment-content">
          <p>@Arne: Maybe it'd be worth looking at integrating it then; however, it really is a completely different approach - gl32 is a source code writer, whereas Pyglet uses Pythons inbuilt metaprogramming capabilities - and so it would be completely rewriting a large chunk of Pyglets core. Once I've got the binding generator finalised, it might be worth seeing if it's possible to replace Pyglet's OpenGL bindings with these ones.<br><br>That said, in the interest of full disclosure: I'm not a fan of Pyglets per object draw method, again in the interests of speed. The per object draw method that Pyglet encourages with its API is not very scalable and eliminates a large number of the advantages of using OpenGL. So whilst I might see if gl32 can be plugged in for interesting benchmarks/proof-of-concept, I probably wouldn't try to get it bug-free and integrated into upstream Pyglet.</p>
        </div>
      </div>
      <div class="comment comment-3616231432826004421">
        <div class="comment-header">
          <a name="comment-3616231432826004421"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-04 15:26</span>:
        </div>
        <div class="comment-content">
          <p>@Arne: Regarding Pyglet integration - it seems it would require a lot of work. There's two major issues - firstly, Pyglet only has raw OpenGL bindings, which are used everywhere and hence the "more pythonic" bindings of gl32 would be hard to integrate without editing every file using GL in Pyglet. Secondly, Pyglet uses GL functions which were removed in 3.2, and hence are not in gl32, so the API generator would have to be extended to handle any special cases on these functions.</p>
        </div>
      </div>
      <div class="comment comment-1356665352638378268">
        <div class="comment-header">
          <a name="comment-1356665352638378268"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-07 17:23</span>:
        </div>
        <div class="comment-content">
          <p>@David: The per-object draw-method is very convenient for programming. As soon as you need more performance, most of the objects are grouped into batches, though. That way only the draw method of the batch is called and the batch can do all kinds of optimizations.</p>
        </div>
      </div>
      <div class="comment comment-1417474657930851444">
        <div class="comment-header">
          <a name="comment-1417474657930851444"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-07 17:25</span>:
        </div>
        <div class="comment-content">
          <p>For Python 3.2 you might find useful stuff in the python-3 port of pyglet, though that hasn’t been released, yet, IIRC.</p>
        </div>
      </div>
      <div class="comment comment-6538215610814316240">
        <div class="comment-header">
          <a name="comment-6538215610814316240"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-07 21:26</span>:
        </div>
        <div class="comment-content">
          <p>@Arne:<br><br>I'd argue that objects with Z-order would be more convenient programmatically, but frankly that's a matter of opinion. (Incidentally, this is something I'm working on as well, and I think I'm mostly done on it). <br><br>However, per-object-draw is only one concern I have on Pyglets speed credentials, as I do not believe Pyglet was written with speed as a design goal. For a different example, see pyglet.graphics.vertexbuffer; copying a ctypes object into a list in order to get slices to work is not a smart thing to do, performance wise!<br><br>I'm not sure where you got Python 3.2 from, but what I meant was that currently I'm restricting myself to OpenGL 3.2, which means that certain older OpenGL functions do not exist. Pyglet uses some of these removed functions (e.g. glPushClientAttrib), and hence the bindings I'm generating at the moment do not provide all the features Pyglet uses.</p>
        </div>
      </div>
      <div class="comment comment-8340463375497750517">
        <div class="comment-header">
          <a name="comment-8340463375497750517"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-04-08 04:47</span>:
        </div>
        <div class="comment-content">
          <p>I'd like to remind readers of these comments that this thread has gone farther and farther from both the original post and the whole blog -- which is supposed to be related to PyPy.  I'm rather sure that you're now discussing performance on CPython, which in this case is very different from performance on PyPy (or would be if it supported all packages involved).  Maybe move this discussion somewhere more appropriate?</p>
        </div>
      </div>
      <div class="comment comment-1918749471660495806">
        <div class="comment-header">
          <a name="comment-1918749471660495806"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-09 11:47</span>:
        </div>
        <div class="comment-content">
          <p>@Armin: You’re right… actually I would be pretty interested, though, whether pypy also has a performance issue with pyglet's chained functions.</p>
        </div>
      </div>
      <div class="comment comment-8870180920478767155">
        <div class="comment-header">
          <a name="comment-8870180920478767155"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-09 14:30</span>:
        </div>
        <div class="comment-content">
          <p>@Arne: In principal, PyPy seems to handle Pyglets chained functions relatively well (non-scientifically running the Astraea examples title screen sees CPU usage start very high, but eventually drops to about 80% of cPythons after the JIT warms up). There is one caveat preventing better testing: the moment keyboard input is given to Astraea on PyPy, PyPy segfaults.</p>
        </div>
      </div>
      <div class="comment comment-4514310958986189673">
        <div class="comment-header">
          <a name="comment-4514310958986189673"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-10 09:05</span>:
        </div>
        <div class="comment-content">
          <p>@David: That is a really important feedback to Armin and and Anatoly, I think.</p>
        </div>
      </div>
      <div class="comment comment-2897705282695531406">
        <div class="comment-header">
          <a name="comment-2897705282695531406"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-04-10 09:06</span>:
        </div>
        <div class="comment-content">
          <p>@David: Can you give some more background on the error (how to get the code, how to reproduce the segfault)?</p>
        </div>
      </div>
      <div class="comment comment-7159537636839667973">
        <div class="comment-header">
          <a name="comment-7159537636839667973"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-15 11:35</span>:
        </div>
        <div class="comment-content">
          <p>@Arne: It's as simple as running the Astraea example in Pyglet and pressing a key (under PyPy 2.2, Pyglet 1.2-beta, Ubuntu 14.04). As far as I remember, this has been the case for some time (at least as far back as Ubuntu 12.10/PyPy 2.0 beta - although back then the major issue was PyPy using a lot more CPU; I didn't report this then due to a blog post at the time saying how cTypes would be rewritten). The error reported by Apport is "Cannot access memory at address 0x20"<br><br>Doing a cursory scan through other examples, the noisy and text_input examples also have problems. noisy segfaults when a spawned ball collides with a boundary (occasionally giving a partial rpython traceback); text_input appears to have a random chance of any of the input boxes being selectable.<br><br>Maybe it's time to file a proper bug report on this...</p>
        </div>
      </div>
      <div class="comment comment-6821195969382759213">
        <div class="comment-header">
          <a name="comment-6821195969382759213"></a>
            <span class="author">David</span> wrote on <span class="date">2014-04-15 14:09</span>:
        </div>
        <div class="comment-content">
          <p>@Arne: I've now submitted a bug on the PyPy Bug tracker (Issue 1736), with more detail etc. Probably best to move conversation on any Pyglet related issues over there.</p>
        </div>
      </div>
      <div class="comment comment-3498460290294515473">
        <div class="comment-header">
          <a name="comment-3498460290294515473"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-04-16 11:50</span>:
        </div>
        <div class="comment-content">
          <p>Maybe indeed :-)</p>
        </div>
      </div>
      <div class="comment comment-1241565184074256430">
        <div class="comment-header">
          <a name="comment-1241565184074256430"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-01-24 15:22</span>:
        </div>
        <div class="comment-content">
          <p>I came up with a funny idea about why not making emscripten generates code targeted on RPython, then now we can use C/C++ in PyPy directly? A LLVM to RPython compiler, how about this?</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/03/hi-all-here-is-one-of-first-full-pypys-8725931424559481728.html" class="u-url">STMGC-C7 with PyPy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/03/hi-all-here-is-one-of-first-full-pypys-8725931424559481728.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-03-15T17:00:00Z" itemprop="datePublished" title="2014-03-15 17:00">2014-03-15 17:00</time></a>
            </p>
                <p class="commentline">11 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi all,</p>

<p>Here is one of the first full PyPy's
(edit: it was r69967+, but the general list of versions is currently <a href="https://cobra.cs.uni-duesseldorf.de/~buildmaster/misc/">here</a>)
compiled with the new <a href="../posts/2014/02/rewrites-of-stm-core-model-again-633249729751034512.html">StmGC-c7
library</a>.  It has no JIT so far, but it runs some small
single-threaded benchmarks by taking around 40% more time than a
corresponding non-STM, no-JIT version of PyPy.  It scales --- up to two
threads only, which is the hard-coded maximum so far in the c7 code.
But the scaling looks perfect in these small benchmarks without
conflict: starting two threads each running a copy of the benchmark
takes almost exactly the same amount of total time, simply using two
cores.</p>

<p>Feel free to try it!  It is not actually useful so far, because it is
limited to two cores and CPython is something like 2.5x faster.  One of
the important next steps is to re-enable the JIT.  Based on our <a href="https://foss.heptapod.net/pypy/pypy/-/tree/branch//stmgc-c7/TODO">current
understanding</a> of the "40%" figure, we can probably reduce it with
enough efforts; but also, the JIT should be able to easily produce
machine code that suffers a bit less than the interpreter from these
effects.  This seems to mean that we're looking at 20%-ish slow-downs
for the future PyPy-STM-JIT.</p>

<p>Interesting times :-)</p>

<p>For reference, this is what you get by downloading <a href="https://cobra.cs.uni-duesseldorf.de/~buildmaster/misc/pypy-c-r69967+-stm-1d0b870195e7.tbz2">the
PyPy binary linked above</a>: a Linux 64 binary (Ubuntu 12.04) that
should behave mostly like a regular PyPy.  (One main missing feature is
that destructors are never called.)  It uses two cores, but obviously
only if the Python program you run is multithreaded.  The only new
built-in feature is <code>with __pypy__.thread.atomic:</code> this gives
you a way to enforce that a block of code runs "atomically", which means
without any operation from any other thread randomly interleaved.</p>

<p>If you want to translate it yourself, you need a trunk version of clang
with <a href="https://bitbucket.org/pypy/stmgc/raw/default/c7/llvmfix">three patches</a> applied.  That's the number of bugs that we couldn't
find workarounds for, not the total number of bugs we found by (ab)using
the <a href="https://clang.llvm.org/docs/LanguageExtensions.html#target-specific-extensions">address_space</a> feature...</p>

<p>Stay tuned for more!</p>

<p>Armin &amp; Remi</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-7745971750818505261">
        <div class="comment-header">
          <a name="comment-7745971750818505261"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-03-16 20:32</span>:
        </div>
        <div class="comment-content">
          <p>The provided pypy-c crashes when calling fork().  Sadly fork() is indirectly called by a lot of things, including the subprocess module --- which can be executed just by importing random modules...</p>
        </div>
      </div>
      <div class="comment comment-8004082123995338012">
        <div class="comment-header">
          <a name="comment-8004082123995338012"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-17 08:39</span>:
        </div>
        <div class="comment-content">
          <p>That sounds pretty huge!<br><br>Do you require clang for that? (why is it named on https://foss.heptapod.net/pypy/pypy/-/tree/branch//stmgc-c7/TODO )</p>
        </div>
      </div>
      <div class="comment comment-3358716380887223056">
        <div class="comment-header">
          <a name="comment-3358716380887223056"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-03-17 20:42</span>:
        </div>
        <div class="comment-content">
          <p>Only clang has the address_space extension mention in the blog post; gcc does not.</p>
        </div>
      </div>
      <div class="comment comment-4440813128986138402">
        <div class="comment-header">
          <a name="comment-4440813128986138402"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-19 13:51</span>:
        </div>
        <div class="comment-content">
          <p>I want to hear more talks on this.  When is your next talk... pycon 2014?  It would be hilarious if the pypy group were able to create naive concurrency in python, no one would have seen that coming!  Many would have thought, "surely Haskell", or some other immutable, static language would get us there first.  But no, it might just be that pypy allows any language that targets it to be concurrent, kiss style...amazing!  Anyway, enough gushing, time for a random question.  Mainstream vms like the JVM have added ways of speeding up dynamic languages, what advantages does pypy have over these traditional vms(other than the concurrency one that might come to fruition)?  I think this would be a good question to answer at the next talk for pypy.</p>
        </div>
      </div>
      <div class="comment comment-1830911613584343784">
        <div class="comment-header">
          <a name="comment-1830911613584343784"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-03-20 06:54</span>:
        </div>
        <div class="comment-content">
          <p>As it turns out there will be no PyPy talk at PyCon 2014.<br><br>The JVM runs Jython at a speed that is around that of CPython.  PyPy runs substantially faster than this.  One difference is that PyPy contains a small number of annotations targeted specifically towards RPython's JIT generator, whereas the JVM has no support for this.</p>
        </div>
      </div>
      <div class="comment comment-7516979993707582760">
        <div class="comment-header">
          <a name="comment-7516979993707582760"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-03-20 07:37</span>:
        </div>
        <div class="comment-content">
          <p>Update containing the most obvious fixes: https://cobra.cs.uni-duesseldorf.de/~buildmaster/misc/pypy-c-r70103-70091-stm.tbz2 (Ubuntu 12.04 Linux 64-bit)</p>
        </div>
      </div>
      <div class="comment comment-5887891256441897929">
        <div class="comment-header">
          <a name="comment-5887891256441897929"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-03-20 15:45</span>:
        </div>
        <div class="comment-content">
          <p>Oh, I do not want to know personally about the superiority of pypy vs the jvm.  I was just suggesting a talking point; basically, show others that pypy is a better alternative(for dynamic languages, possibly all languages with naive concurrency working!) then llvm, jvm, etc...  I do have a question though, would you suppose that performance of pypy-stm would be better than that of something like the approach clojure has?  I have heard that immutable data structures are nice for correctness but that they are bad for performance.</p>
        </div>
      </div>
      <div class="comment comment-6049705667489488220">
        <div class="comment-header">
          <a name="comment-6049705667489488220"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-03-21 17:21</span>:
        </div>
        <div class="comment-content">
          <p>So PyPy-STM is Python without GIL? And it's possible to make it only 20% slower than "regular" PyPy? That would be quite an achievement.<br><br>Could you publish a build of PyPy-STM for Debian Stable?</p>
        </div>
      </div>
      <div class="comment comment-2785048928132293428">
        <div class="comment-header">
          <a name="comment-2785048928132293428"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-03-22 12:24</span>:
        </div>
        <div class="comment-content">
          <p>The PyPy-STM we have so far doesn't include any JIT.  If you want to try it out anyway on other Linux platforms than Ubuntu, you need to translate it yourself, or possibly hack around with symlinks and LD_LIBRARY_PATH.</p>
        </div>
      </div>
      <div class="comment comment-8162726717866983881">
        <div class="comment-header">
          <a name="comment-8162726717866983881"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2014-03-22 12:44</span>:
        </div>
        <div class="comment-content">
          <p>&gt; The PyPy-STM we have so far doesn't include any JIT<br><br>Yep, that's what blog post said :) But also PyPy-STM doesn't include GIL, does it?</p>
        </div>
      </div>
      <div class="comment comment-5761349177337727614">
        <div class="comment-header">
          <a name="comment-5761349177337727614"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2014-03-23 07:44</span>:
        </div>
        <div class="comment-content">
          <p>Indeed, which is the point :-)  You're welcome to try it out, but I'm just saying that I don't want to go to great lengths to provide precompiled binaries that work on Linux XYZ when I could basically release an updated version every couple of days...  It's still experimental and in-progress.  Early versions are limited to two cores; later versions to 4 cores.  We still have to determine the optimal number for this limit; maybe around 8? (higher numbers imply a bit of extra overheads)  It's an example of in-progress work.  Another example is that so far you don't get feedback from cross-transaction conflicts; you used to in previous versions, but we didn't port it yet.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2014/03/hello-everyone-there-is-interview-with-7561523711224053700.html" class="u-url">PyPy on uWSGI</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2014/03/hello-everyone-there-is-interview-with-7561523711224053700.html" rel="bookmark">
            <time class="published dt-published" datetime="2014-03-10T15:07:00Z" itemprop="datePublished" title="2014-03-10 15:07">2014-03-10 15:07</time></a>
            </p>
            
        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">
Hello everyone
<p>There is an interview with Roberto De Ioris (from <a href="https://uwsgi-docs.readthedocs.org">uWSGI</a> fame) about embedding PyPy in uWSGI that covers recent addition of a <a href="https://doc.pypy.org/en/latest/embedding.html">PyPy embedding interface</a> using cffi and the experience with using it. Read <a href="https://baroquesoftware.com/blog#interview-with-roberto_de_ioris">The full interview</a></p>
Cheers<br>
fijal
<br>
</div>
    </div>
    </article>
</div>
</div>
<div class="sidebar">
<div>
  <h2>
    The PyPy blogposts
  </h2>
  <div>
    Create a guest post via a PR to the <a href="https://github.com/pypy/pypy.org">source repo</a>
  </div>
</div>
    <div id="global-recent-posts">
    <h2>
      Recent Posts
    </h2>
    <ul class="post-list">
      <li>
        <a href="/posts/2025/12/toy-load-store.html" class="listtitle">Load and store forwarding in the Toy Optimizer</a>
      </li>
      <li>
        <a href="/posts/2025/07/pypy-v7320-release.html" class="listtitle">PyPy v7.3.20 release</a>
      </li>
      <li>
        <a href="/posts/2025/06/rpython-gc-allocation-speed.html" class="listtitle">How fast can the RPython GC allocate?</a>
      </li>
      <li>
        <a href="/posts/2025/04/prospero-in-rpython.html" class="listtitle">Doing the Prospero-Challenge in RPython</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7319-release.html" class="listtitle">PyPy v7.3.19 release</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-gc-sampling.html" class="listtitle">Low Overhead Allocation Sampling with VMProf in PyPy's GC</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7318-release.html" class="listtitle">PyPy v7.3.18 release</a>
      </li>
      <li>
        <a href="/posts/2025/01/musings-tracing.html" class="listtitle">Musings on Tracing in PyPy</a>
      </li>
      <li>
        <a href="/posts/2025/01/towards-pypy311-an-update.html" class="listtitle">Towards PyPy3.11 - an update</a>
      </li>
      <li>
        <a href="/posts/2024/11/guest-post-final-encoding-in-rpython.html" class="listtitle">Guest Post: Final Encoding in RPython Interpreters</a>
      </li>
    </ul>
  </div>

          <div id="global-archive-list">
          <h2>
            Archives
          </h2>
          <ul class="archive-level archive-level-1">
            <li><a class="reference" href="/2007/">2007</a> (19)
            </li>
            <li><a class="reference" href="/2008/">2008</a> (62)
            </li>
            <li><a class="reference" href="/2009/">2009</a> (38)
            </li>
            <li><a class="reference" href="/2010/">2010</a> (44)
            </li>
            <li><a class="reference" href="/2011/">2011</a> (43)
            </li>
            <li><a class="reference" href="/2012/">2012</a> (44)
            </li>
            <li><a class="reference" href="/2013/">2013</a> (46)
            </li>
            <li><a class="reference" href="/2014/">2014</a> (22)
            </li>
            <li><a class="reference" href="/2015/">2015</a> (20)
            </li>
            <li><a class="reference" href="/2016/">2016</a> (20)
            </li>
            <li><a class="reference" href="/2017/">2017</a> (13)
            </li>
            <li><a class="reference" href="/2018/">2018</a> (12)
            </li>
            <li><a class="reference" href="/2019/">2019</a> (12)
            </li>
            <li><a class="reference" href="/2020/">2020</a> (9)
            </li>
            <li><a class="reference" href="/2021/">2021</a> (10)
            </li>
            <li><a class="reference" href="/2022/">2022</a> (13)
            </li>
            <li><a class="reference" href="/2023/">2023</a> (6)
            </li>
            <li><a class="reference" href="/2024/">2024</a> (13)
            </li>
            <li><a class="reference" href="/2025/">2025</a> (9)
            </li>
          </ul>
        </div>


          <div id="global-tag-list">
          <h2>
            Tags
          </h2>
          <ul>
            <li><a class="reference" href="/categories/arm.html">arm</a> (2)</li>
            <li><a class="reference" href="/categories/benchmarking.html">benchmarking</a> (1)</li>
            <li><a class="reference" href="/categories/casestudy.html">casestudy</a> (3)</li>
            <li><a class="reference" href="/categories/cli.html">cli</a> (1)</li>
            <li><a class="reference" href="/categories/compiler.html">compiler</a> (1)</li>
            <li><a class="reference" href="/categories/conda-forge.html">conda-forge</a> (1)</li>
            <li><a class="reference" href="/categories/cpyext.html">cpyext</a> (4)</li>
            <li><a class="reference" href="/categories/cpython.html">CPython</a> (3)</li>
            <li><a class="reference" href="/categories/ep2008.html">ep2008</a> (1)</li>
            <li><a class="reference" href="/categories/extension-modules.html">extension modules</a> (3)</li>
            <li><a class="reference" href="/categories/gc.html">gc</a> (3)</li>
            <li><a class="reference" href="/categories/guestpost.html">guestpost</a> (3)</li>
            <li><a class="reference" href="/categories/graalpython.html">GraalPython</a> (1)</li>
            <li><a class="reference" href="/categories/hpy.html">hpy</a> (1)</li>
            <li><a class="reference" href="/categories/heptapod.html">Heptapod</a> (1)</li>
            <li><a class="reference" href="/categories/jit.html">jit</a> (23)</li>
            <li><a class="reference" href="/categories/jython.html">jython</a> (1)</li>
            <li><a class="reference" href="/categories/kcachegrind.html">kcachegrind</a> (1)</li>
            <li><a class="reference" href="/categories/meta.html">meta</a> (1)</li>
            <li><a class="reference" href="/categories/numpy.html">numpy</a> (24)</li>
            <li><a class="reference" href="/categories/parser.html">parser</a> (1)</li>
            <li><a class="reference" href="/categories/performance.html">performance</a> (2)</li>
            <li><a class="reference" href="/categories/profiling.html">profiling</a> (7)</li>
            <li><a class="reference" href="/categories/pypy.html">pypy</a> (6)</li>
            <li><a class="reference" href="/categories/pypy3.html">pypy3</a> (16)</li>
            <li><a class="reference" href="/categories/pyqt4.html">PyQt4</a> (1)</li>
            <li><a class="reference" href="/categories/release.html">release</a> (66)</li>
            <li><a class="reference" href="/categories/releasecffi.html">releasecffi</a> (3)</li>
            <li><a class="reference" href="/categories/releaserevdb.html">releaserevdb</a> (1)</li>
            <li><a class="reference" href="/categories/releasestm.html">releasestm</a> (1)</li>
            <li><a class="reference" href="/categories/revdb.html">revdb</a> (1)</li>
            <li><a class="reference" href="/categories/roadmap.html">roadmap</a> (2)</li>
            <li><a class="reference" href="/categories/rpython.html">rpython</a> (1)</li>
            <li><a class="reference" href="/categories/rpyc.html">RPyC</a> (1)</li>
            <li><a class="reference" href="/categories/speed.html">speed</a> (6)</li>
            <li><a class="reference" href="/categories/sponsors.html">sponsors</a> (7)</li>
            <li><a class="reference" href="/categories/sprint.html">sprint</a> (3)</li>
            <li><a class="reference" href="/categories/sprints.html">sprints</a> (1)</li>
            <li><a class="reference" href="/categories/stm.html">stm</a> (14)</li>
            <li><a class="reference" href="/categories/sun.html">sun</a> (1)</li>
            <li><a class="reference" href="/categories/smalltalk.html">Smalltalk</a> (1)</li>
            <li><a class="reference" href="/categories/squeak.html">Squeak</a> (1)</li>
            <li><a class="reference" href="/categories/testing.html">testing</a> (1)</li>
            <li><a class="reference" href="/categories/toy-optimizer.html">toy-optimizer</a> (6)</li>
            <li><a class="reference" href="/categories/unicode.html">unicode</a> (1)</li>
            <li><a class="reference" href="/categories/valgrind.html">valgrind</a> (1)</li>
            <li><a class="reference" href="/categories/vmprof.html">vmprof</a> (3)</li>
            <li><a class="reference" href="/categories/z3.html">z3</a> (5)</li>
          </ul>
        </div></div>
</main>
</div>
<div style="clear: both; width: 75%; margin: 1em auto;">
        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-32.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-30.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
         
                 <footer id="footer"><p>
</p>
<div class="myfooter">
  <div class="logotext">
    © 2026 <a href="mailto:pypy-dev@pypy.org">The PyPy Team</a>
     
    Built with <a href="https://getnikola.com" rel="nofollow">Nikola</a>
     
    Last built 2026-01-17T00:22
  </div>
  <div style="margin-left: auto">
  <a href="../rss.xml">RSS feed</a>
</div>

            
        

    </div>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js" crossorigin="anonymous"></script><script src="../assets/js/styles.js"></script></footer>
</body>
</html>
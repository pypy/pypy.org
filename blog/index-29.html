<!DOCTYPE html>
<html \ prefix="
        og: http://ogp.me/ns# article: http://ogp.me/ns/article#
    " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A Faster Python">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PyPy (old posts, page 29) | PyPy</title>
<link href="../assets/css/rst_base.css" rel="stylesheet" type="text/css">
<link href="../assets/css/nikola_rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../assets/css/styles.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://www.pypy.org/blog/index-29.html">
<link rel="icon" href="../favicon2.ico" sizes="16x16">
<link rel="icon" href="../favicon32x32.ico" sizes="32x32">
<link rel="prev" href="index-30.html" type="text/html">
<link rel="next" href="index-28.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="../assets/css/tipuesearch.css">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="container">
             <header id="header"><!-- Adapted from https://www.taniarascia.com/responsive-dropdown-navigation-bar --><section class="navigation"><div class="nav-container">
            <div class="brand">
                <a href="../index.html">
                    <image id="toplogo" src="../images/pypy-logo.svg" width="75px;" alt="PyPy/"></image></a>
            </div>
            <nav><ul class="nav-list">
<li> 
                <a href="#!">Features</a>
                <ul class="nav-dropdown">
<li> <a href="../features.html">What is PyPy?</a> </li>  
                    <li> <a href="../compat.html">Compatibility</a> </li>  
                    <li> <a href="../performance.html">Performance</a> </li>  
                </ul>
</li>
          <li> <a href="../download.html">Download</a> </li>  
          <li> <a href="http://doc.pypy.org">Dev Docs</a> </li>  
            <li> 
                <a href="#!">Blog</a>
                <ul class="nav-dropdown">
<li> <a href=".">Index</a> </li>  
                    <li> <a href="../categories/">Tags</a> </li>  
                    <li> <a href="../archive.html">Archive by year</a> </li>  
                    <li> <a href="../rss.xml">RSS feed</a> </li>  
                    <li> <a href="https://morepypy.blogspot.com/">Old site</a> </li>  
                </ul>
</li>
            <li> 
                <a href="#!">About</a>
                <ul class="nav-dropdown">
<li> <a href="https://bsky.app/profile/pypyproject.bsky.social">Bluesky</a> </li>  
                    <li> <a href="https://libera.irclog.whitequark.org/pypy">IRC logs</a> </li>  
                    <li> <a href="https://www.youtube.com/playlist?list=PLADqad94yVqDRQXuqxKrPS5QnVqbDLlRt">YouTube</a> </li>  
                    <li> <a href="https://www.twitch.tv/pypyproject">Twitch</a> </li>  
                    <li> <a href="../pypy-sponsors.html">Sponsors</a> </li>  
                    <li> <a href="../howtohelp.html">How To Help?</a> </li>  
                    <li> <a href="../contact.html">Contact</a> </li>  
                </ul>
</li>

                </ul></nav><div class="nav-mobile">
                <a id="nav-toggle" href="#!"> <span></span></a>
            </div>
        </div>
    </section><div class="searchform" role="search">
                
<form class="navbar-form navbar-left" action="../search.html" role="search">
    <div class="form-group">
        <input type="text" class="form-control" id="tipue_search_input" name="q" placeholder="Search…" autocomplete="off">
</div>
    <input type="submit" value="Local Search" style="visibility: hidden;">
</form>

            </div>
    </header><main id="content"><div class="post">
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/11/pypy-22-incrementalism-4723643710897639332.html" class="u-url">PyPy 2.2 - Incrementalism</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/11/pypy-22-incrementalism-4723643710897639332.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-11-14T09:33:00Z" itemprop="datePublished" title="2013-11-14 09:33">2013-11-14 09:33</time></a>
            </p>
                <p class="commentline">6 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>We're pleased to announce PyPy 2.2, which targets version 2.7.3 of the Python language. This release main highlight is the introduction of the incremental garbage collector, sponsored by the <a class="reference external" href="https://www.raspberrypi.org/">Raspberry Pi Foundation</a>.<br>
This release also contains several bugfixes and performance improvements.<br>
You can download the PyPy 2.2 release here:<br></p>
<blockquote><a class="reference external" href="https://pypy.org/download.html">https://pypy.org/download.html</a></blockquote>We would like to thank our donors for the continued support of the PyPy project. We showed quite a bit of progress on all three projects (see below) and we're slowly running out of funds. Please consider donating more so we can finish those projects!  The three projects are:<br><ul class="simple">
<li>Py3k (supporting Python 3.x): the release PyPy3 2.2 is imminent.</li>
<li>STM (software transactional memory): a preview will be released very soon, as soon as we fix a few bugs</li>
<li>NumPy: the work done is included in the PyPy 2.2 release. More details below.</li>
</ul>
<br><div class="section" id="what-is-pypy">
<h2>What is PyPy?</h2>PyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (<a class="reference external" href="https://speed.pypy.org/">pypy 2.2 and cpython 2.7.2</a> performance comparison) due to its integrated tracing JIT compiler.<br>
This release supports x86 machines running Linux 32/64, Mac OS X 64, Windows 32, or ARM (ARMv6 or ARMv7, with VFPv3).<br>
Work on the native Windows 64 is still stalling, we would welcome a volunteer to handle that.</div>
<br><div class="section" id="highlights">
<h2>Highlights</h2>
<ul class="simple">
<li>Our Garbage Collector is now "incremental".  It should avoid almost all pauses due to a major collection taking place.  Previously, it would pause the program (rarely) to walk all live objects, which could take arbitrarily long if your process is using a whole lot of RAM.  Now the same work is done in steps.  This should make PyPy more responsive, e.g. in games.  There are still other pauses, from the GC and the JIT, but they should be on the order of 5 milliseconds each.</li>
<li>The JIT counters for hot code were never reset, which meant that a process running for long enough would eventually JIT-compile more and more rarely executed code.  Not only is it useless to compile such code, but as more compiled code means more memory used, this gives the impression of a memory leak.  This has been tentatively fixed by decreasing the counters from time to time.</li>
<li>NumPy has been split: now PyPy only contains the core module, called <tt class="docutils literal">_numpypy</tt>.  The <tt class="docutils literal">numpy</tt> module itself has been moved to <tt class="docutils literal"><span class="pre">https://bitbucket.org/pypy/numpy</span></tt> and <tt class="docutils literal">numpypy</tt> disappeared. You need to install NumPy separately with a virtualenv: <tt class="docutils literal">pip install <span class="pre">git+https://bitbucket.org/pypy/numpy.git</span></tt>; or directly: <tt class="docutils literal">git clone <span class="pre">https://bitbucket.org/pypy/numpy.git;</span> cd numpy; pypy setup.py install</tt>.</li>
<li>non-inlined calls have less overhead</li>
<li>Things that use <tt class="docutils literal">sys.set_trace</tt> are now JITted (like coverage)</li>
<li>JSON decoding is now very fast (JSON encoding was already very fast)</li>
<li>various buffer copying methods experience speedups (like list-of-ints to <tt class="docutils literal">int[]</tt> buffer from cffi)</li>
<li>We finally wrote (hopefully) all the missing <tt class="docutils literal">os.xxx()</tt> functions, including <tt class="docutils literal">os.startfile()</tt> on Windows and a handful of rare ones on Posix.</li>
<li>numpy has a rudimentary C API that cooperates with <tt class="docutils literal">cpyext</tt>
</li>
</ul>Cheers,<br>
Armin Rigo and Maciej Fijalkowski</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-8988943787144936479">
        <div class="comment-header">
          <a name="comment-8988943787144936479"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2013-11-14 11:52</span>:
        </div>
        <div class="comment-content">
          <p>The Win32 build is here, thanks Matti!  https://bitbucket.org/pypy/pypy/downloads/pypy-2.2-win32.zip</p>
        </div>
      </div>
      <div class="comment comment-930599783004306577">
        <div class="comment-header">
          <a name="comment-930599783004306577"></a>
            <span class="author">foobie42</span> wrote on <span class="date">2013-11-14 14:23</span>:
        </div>
        <div class="comment-content">
          <p>Congrats! adb push pypypypy /sdcard/!</p>
        </div>
      </div>
      <div class="comment comment-4335600935028680640">
        <div class="comment-header">
          <a name="comment-4335600935028680640"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-11-14 19:38</span>:
        </div>
        <div class="comment-content">
          <p>@foobie42 that's what I've done just a second ago! Gotta unpack raspbian chroot zip now...</p>
        </div>
      </div>
      <div class="comment comment-4866495252975446523">
        <div class="comment-header">
          <a name="comment-4866495252975446523"></a>
            <span class="author">Wilfred</span> wrote on <span class="date">2013-11-16 10:55</span>:
        </div>
        <div class="comment-content">
          <p>Is speed.pypy.org still updated? The second graph on https://speed.pypy.org/ only shows 2.0 beta and trunk, and https://speed.pypy.org/comparison/ doesn't offer 2.1 or 2.2 either.</p>
        </div>
      </div>
      <div class="comment comment-8292317073327241334">
        <div class="comment-header">
          <a name="comment-8292317073327241334"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2013-11-16 11:56</span>:
        </div>
        <div class="comment-content">
          <p>I managed to update it, check it out now</p>
        </div>
      </div>
      <div class="comment comment-527859283085526983">
        <div class="comment-header">
          <a name="comment-527859283085526983"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2014-02-24 21:39</span>:
        </div>
        <div class="comment-content">
          <p>Do you have plans to support python 3.3 features?</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/11/py3k-status-update-12-5307085693947812769.html" class="u-url">Py3k status update #12</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/philip-jenvey.html">Philip Jenvey</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/11/py3k-status-update-12-5307085693947812769.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-11-12T23:16:00Z" itemprop="datePublished" title="2013-11-12 23:16">2013-11-12 23:16</time></a>
            </p>
                <p class="commentline">3 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>This is the 12th status update about our work on the <a class="reference external" href="https://bitbucket.org/pypy/pypy/commits/all/tip/branch%28%22py3k%22%29">py3k branch</a>, which we<br>
can work on thanks to all of the people who <a class="reference external" href="../posts/2012/01/py3k-and-numpy-first-stage-thanks-to-3008917396290059758.html">donated</a> to the <a class="reference external" href="https://pypy.org/py3donate.html">py3k proposal</a>.</p>
<p>Here's an update on the recent progress:</p>
<ul class="simple">
<li>Thank you to everyone who has provided initial feedback on the PyPy3 2.1 beta<br>
1 release. We've gotten a number of bug reports, most of which have been<br>
fixed.</li>
<li>As usual, we're continually keeping up with changes from the default<br>
branch. Oftentimes these merges come at a cost (conflicts and or<br>
reintegration of py3k changes) but occasionally we get goodies for free, such<br>
as the <a class="reference external" href="../posts/2013/10/making-coveragepy-faster-under-pypy-935409618297062344.html">recent JIT optimizations</a> and <a class="reference external" href="../posts/2013/10/incremental-garbage-collector-in-pypy-8956893523842234676.html">incremental garbage collection</a>.</li>
<li>We've been focusing on re-optimizing Python 2 int sized (machine sized)<br>
integers:</li>
</ul>
<p>We have a couple of known, notable speed regressions in the PyPy3 beta release<br>
vs regular PyPy. The major one being with Python 2.x int sized (or machine<br>
sized) integers.</p>
<p>Python 3 drops the distinction between int and long types. CPython 3.x<br>
accomplishes this by removing the old int type entirely and renaming the long<br>
type to int. Initially, we've done the same for PyPy3 for the sake of<br>
simplicity and getting everything working.</p>
<p>However PyPy's JIT is capable of heavily optimizing these machine sized integer<br>
operations, so this came with a regression in performance in this area.</p>
<p>We're now in the process of solving this. Part of this work also involves some<br>
house cleaning on these numeric types which also benefits the default branch.</p>
<p>cheers,<br>
Phil</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-7526549533947916836">
        <div class="comment-header">
          <a name="comment-7526549533947916836"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2013-11-13 08:33</span>:
        </div>
        <div class="comment-content">
          <p>We should note that the re-optimization is different than CPython's.  In the latter they use a "long" implementation which they heavily optimized for the common case of small integers.  In PyPy instead we use two really different implementations (like "int" and "long" on Python 2); they just happen to be exposed at the user level with the same Python type in Python 3.</p>
        </div>
      </div>
      <div class="comment comment-3451705027680482638">
        <div class="comment-header">
          <a name="comment-3451705027680482638"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-11-13 21:58</span>:
        </div>
        <div class="comment-content">
          <p>I just have to say, the PyPy team is doing a great job.<br><br>Well done guys!</p>
        </div>
      </div>
      <div class="comment comment-7225449905939123101">
        <div class="comment-header">
          <a name="comment-7225449905939123101"></a>
            <span class="author">Alessandro</span> wrote on <span class="date">2014-01-04 05:47</span>:
        </div>
        <div class="comment-content">
          <p>I know nothing on pypy, but I'm interested. I have a doubt: Will the PyPy version with python 3 support leverage all of the progress of the python 2 pypy version?<br><br>Like for example, will current numpypy be able to work on PyPy3k ?</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/10/making-coveragepy-faster-under-pypy-935409618297062344.html" class="u-url">Making coverage.py faster under PyPy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/alex.html">Alex</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/10/making-coveragepy-faster-under-pypy-935409618297062344.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-10-26T00:48:00Z" itemprop="datePublished" title="2013-10-26 00:48">2013-10-26 00:48</time></a>
            </p>
                <p class="commentline">5 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>If you've ever tried to run your programs with <tt class="docutils literal">coverage.py</tt> under PyPy,<br>
you've probably experienced some incredible slowness. Take this simple<br>
program:</p>
<pre class="code python literal-block"><span class="keyword">def</span> <span class="name function">f</span><span class="punctuation">():</span>
    <span class="keyword">return</span> <span class="literal number integer">1</span>


<span class="keyword">def</span> <span class="name function">main</span><span class="punctuation">():</span>
    <span class="name">i</span> <span class="operator">=</span> <span class="literal number integer">10000000</span>
    <span class="keyword">while</span> <span class="name">i</span><span class="punctuation">:</span>
        <span class="name">i</span> <span class="operator">-=</span> <span class="name">f</span><span class="punctuation">()</span>

<span class="name">main</span><span class="punctuation">()</span>
</pre>
<p>Running <tt class="docutils literal">time coverage.py run test.py</tt> five times, and looking at the best<br>
run, here's how PyPy 2.1 stacks up against CPython 2.7.5:</p>
<table border="1" class="docutils">
<colgroup>
<col width="32%">
<col width="19%">
<col width="49%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Python</th>
<th class="head">Time</th>
<th class="head">Normalized to CPython</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>CPython 2.7.5</td>
<td>3.879s</td>
<td>1.0x</td>
</tr>
<tr>
<td>PyPy 2.1</td>
<td>53.330s</td>
<td>13.7x slower</td>
</tr>
</tbody>
</table>
<p>Totally ridiculous. I got turned onto this problem because on one of my<br>
projects CPython takes about 1.5 minutes to run our test suite on the build<br>
bot, but PyPy takes 8-10 minutes.</p>
<p>So I sat down to address it. And the results:</p>
<table border="1" class="docutils">
<colgroup>
<col width="32%">
<col width="19%">
<col width="49%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Python</th>
<th class="head">Time</th>
<th class="head">Normalized to CPython</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>CPython 2.7.5</td>
<td>3.879s</td>
<td>1.0x</td>
</tr>
<tr>
<td>PyPy 2.1</td>
<td>53.330s</td>
<td>13.7x slower</td>
</tr>
<tr>
<td>PyPy head</td>
<td>1.433s</td>
<td>2.7x faster</td>
</tr>
</tbody>
</table>
<p>Not bad.</p>
<div class="section" id="technical-details">
<h1>Technical details</h1>
<p>So how'd we do it? Previously, using <tt class="docutils literal">sys.settrace()</tt> (which <tt class="docutils literal">coverage.py</tt><br>
uses under the hood) disabled the JIT. Except it didn't just disable the JIT,<br>
it did it in a particularly insidious way — the JIT had no idea it was being<br>
disabled!</p>
<p>Instead, every time PyPy discovered that one of your functions was a hotspot,<br>
it would start tracing to observe what the program was doing, and right when it<br>
was about to finish, <tt class="docutils literal">coverage</tt> would run and cause the JIT to abort. Tracing<br>
is a slow process, it makes up for it by generating fast machine code at the<br>
end, but tracing is still incredibly slow. But we never actually got to the<br>
"generate fast machine code" stage. Instead we'd pay all the cost of tracing,<br>
but then we'd abort, and reap none of the benefits.</p>
<p>To fix this, we adjusted some of the heuristics in the JIT, to better show it<br>
how <tt class="docutils literal"><span class="pre">sys.settrace(&lt;tracefunc&gt;)</span></tt> works. Previously the JIT saw it as an opaque<br>
function which gets the frame object, and couldn't tell whether or not it<br>
messed with the frame object. Now we let the JIT look inside the<br><tt class="docutils literal">&lt;tracefunc&gt;</tt> function, so it's able to see that <tt class="docutils literal">coverage.py</tt> isn't<br>
messing with the frame in any weird ways, it's just reading the line number and<br>
file path out of it.</p>
<p>I asked several friends in the VM implementation and research field if they<br>
were aware of any other research into making VMs stay fast when debugging tools<br>
like <tt class="docutils literal">coverage.py</tt> are running. No one I spoke to was aware of any (but I<br>
didn't do a particularly exhaustive review of the literature, I just tweeted at<br>
a few people), so I'm pleased to say that PyPy is quite possibly the first VM<br>
to work on optimizing code in debugging mode! This is possible because of our<br>
years spent investing in meta-tracing research.</p>
</div>
<p>Happy testing,<br>
Alex</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-7519221966276194932">
        <div class="comment-header">
          <a name="comment-7519221966276194932"></a>
            <span class="author">John Doe</span> wrote on <span class="date">2013-10-26 20:40</span>:
        </div>
        <div class="comment-content">
          <p>No, you're not the first to make this pretentious mistake.<br><br>What's the report for code that was actually eliminated by optimizations? Was it covered? Was it not?</p>
        </div>
      </div>
      <div class="comment comment-2643323908086188088">
        <div class="comment-header">
          <a name="comment-2643323908086188088"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-10-27 17:53</span>:
        </div>
        <div class="comment-content">
          <p>You misunderstand John Doe.  The coverage report is for the user's Python code, which isn't optimized, eliminated, or otherwise modified.  The PyPy speedups come from a clever reimplementation of the interpreter that runs the user's Python code, and this article was explaining how they found and fixed a big slowdown that happens to be triggered by a common test-related library.</p>
        </div>
      </div>
      <div class="comment comment-8025009020854068964">
        <div class="comment-header">
          <a name="comment-8025009020854068964"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2013-10-27 19:37</span>:
        </div>
        <div class="comment-content">
          <p>@John Doe: sadly, we fail to understand exactly what part of the blog post you're answering to in your sentence "No, you're not the first to make this pretentious mistake".  Can you please give more context and elaborate a bit?</p>
        </div>
      </div>
      <div class="comment comment-749813283935744890">
        <div class="comment-header">
          <a name="comment-749813283935744890"></a>
            <span class="author">John M. Camara</span> wrote on <span class="date">2013-10-27 21:09</span>:
        </div>
        <div class="comment-content">
          <p>@Armin: I believe John Doe is talking about the last paragraph as I believe the JVM also does not disable optimizations when using debug tools.<br><br>If this is the case than his comment is silly as Alex clearly stated he didn't do an exhaustive search.</p>
        </div>
      </div>
      <div class="comment comment-7651274891160099053">
        <div class="comment-header">
          <a name="comment-7651274891160099053"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2013-10-30 07:17</span>:
        </div>
        <div class="comment-content">
          <p>This sounds similar to the -Og setting of GCC, which enables all optimizations which do not interfere with debugging.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/10/update-on-stm-7145890443443707910.html" class="u-url">Update on STM</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/10/update-on-stm-7145890443443707910.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-10-16T17:01:00Z" itemprop="datePublished" title="2013-10-16 17:01">2013-10-16 17:01</time></a>
            </p>
                <p class="commentline">5 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi all,</p>
<p>The sprint in London was a lot of fun and very fruitful. In the last
update on STM, Armin was working on improving and specializing the
automatic barrier placement. There is still a lot to do in that area,
but that work is merged now. Specializing and improving barrier placement
is still to be done for the JIT.</p>
<p>But that is not all. Right after the sprint, we were able to squeeze
the last obvious bugs in the STM-JIT combination. However, the performance
was nowhere near to what we want. So until now, we fixed some of the most
obvious issues. Many come from RPython erring on the side of caution
and e.g. making a transaction inevitable even if that is not strictly
necessary, thereby limiting parallelism. Another problem came from
increasing counters everytime a guard fails, which caused transactions
to conflict on these counter updates. Since these counters do not have
to be completely accurate, we update them non-transactionally now with
a chance of small errors.</p>
<p>There are still many such performance issues of various complexity left
to tackle: we are nowhere near done. So stay tuned or contribute :)</p>

<h2>Performance</h2>
<p>Now, since the JIT is all about performance, we want to at least
show you some numbers that are indicative of things to come.
Our set of STM benchmarks is very small unfortunately
(something you can help us out with), so this is
not representative of real-world performance. We tried to
minimize the effect of JIT warm-up in the benchmark results.</p>
<p>The machine these benchmarks were executed on has 4 physical
cores with Hyper-Threading (8 hardware threads).</p>
<p><strong>Raytracer</strong> from <a class="reference external" href="https://bitbucket.org/Raemi/stm-benchmarks/src">stm-benchmarks</a>:
Render times in seconds for a 1024x1024 image:</p>
<table border="1" class="docutils">
<colgroup>
<col width="23%">
<col width="39%">
<col width="38%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Interpreter</th>
<th class="head">Base time: 1 thread</th>
<th class="head">8 threads (speedup)</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>PyPy-2.1</td>
<td>2.47</td>
<td>2.56 (0.96x)</td>
</tr>
<tr>
<td>CPython</td>
<td>81.1</td>
<td>73.4 (1.1x)</td>
</tr>
<tr>
<td>PyPy-STM</td>
<td>50.2</td>
<td>10.8 (4.6x)</td>
</tr>
</tbody>
</table>
<p>For comparison, disabling the JIT gives 148s on PyPy-2.1 and 87s on
PyPy-STM (with 8 threads).</p>
<p><strong>Richards</strong> from <a class="reference external" href="https://bitbucket.org/pypy/pypy/commits/branch/stmgc-c4">PyPy repository on the stmgc-c4
branch</a>:
Average time per iteration in milliseconds:</p>
<table border="1" class="docutils">
<colgroup>
<col width="23%">
<col width="39%">
<col width="38%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">Interpreter</th>
<th class="head">Base time: 1 thread</th>
<th class="head">8 threads (speedup)</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>PyPy-2.1</td>
<td>15.6</td>
<td>15.4 (1.01x)</td>
</tr>
<tr>
<td>CPython</td>
<td>239</td>
<td>237 (1.01x)</td>
</tr>
<tr>
<td>PyPy-STM</td>
<td>371</td>
<td>116 (3.2x)</td>
</tr>
</tbody>
</table>
<p>For comparison, disabling the JIT gives 492ms on PyPy-2.1 and 538ms on
PyPy-STM.</p>

<h2>Try it!</h2>
<p>All this can be found in the <a class="reference external" href="https://bitbucket.org/pypy/pypy/commits/branch/stmgc-c4">PyPy repository on the stmgc-c4
branch</a>.
Try it for yourself, but keep in mind that this is still experimental
with a lot of things yet to come. Only Linux x64 is supported right
now, but contributions are welcome.</p>
<p>You can download a prebuilt binary from here:
<a class="reference external" href="https://bitbucket.org/pypy/pypy/downloads/pypy-oct13-stm.tar.bz2">https://bitbucket.org/pypy/pypy/downloads/pypy-oct13-stm.tar.bz2</a>
(Linux x64 Ubuntu &gt;= 12.04).  This was made at revision bafcb0cdff48.</p>

<h2>Summary</h2>
<p>What the numbers tell us is that PyPy-STM is, as expected,
the only of the three interpreters where multithreading gives a large
improvement in speed.  What they also tell us is that, obviously, the
result is not good enough <em>yet:</em> it still takes longer on a 8-threaded
PyPy-STM than on a regular single-threaded PyPy-2.1.  However, as you
should know by now, we are good at promising speed and delivering it...
years later <tt class="docutils literal"><span class="pre">:-)</span></tt></p>
<p>But it has been two years already since PyPy-STM started, and this is
our first preview of the JIT integration.  Expect major improvements
soon: with STM, the JIT generates code that is completely suboptimal in
many cases (barriers, allocation, and more).  Once we improve this, the
performance of the STM-JITted code should come much closer to PyPy 2.1.</p>
<p>Cheers</p>
<p>Remi &amp; Armin</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-5138907301933282975">
        <div class="comment-header">
          <a name="comment-5138907301933282975"></a>
            <span class="author">tobami</span> wrote on <span class="date">2013-10-16 21:14</span>:
        </div>
        <div class="comment-content">
          <p>To see a multithreading speed up in a python interpreter is awesome!<br><br>For next update, I would suggest to do the benchmarking turning off hyperthreading and measuring 1, 2 and 4 threads. That would give a better picture of how the STM implementation scales with threads/cores.<br></p>
        </div>
      </div>
      <div class="comment comment-1483015218034105">
        <div class="comment-header">
          <a name="comment-1483015218034105"></a>
            <span class="author">Mak Sim</span> wrote on <span class="date">2013-10-17 09:22</span>:
        </div>
        <div class="comment-content">
          <p>Guys you are doing great job!</p>
        </div>
      </div>
      <div class="comment comment-5736323322757721880">
        <div class="comment-header">
          <a name="comment-5736323322757721880"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-10-17 13:34</span>:
        </div>
        <div class="comment-content">
          <p>STM | Société de transport de Montréal ?</p>
        </div>
      </div>
      <div class="comment comment-170543446392052847">
        <div class="comment-header">
          <a name="comment-170543446392052847"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-10-17 17:07</span>:
        </div>
        <div class="comment-content">
          <p>STM stands for Software Transactional Memory and is a way to run multiple non-conflicting tasks at the same time and make it appear as if they had run in sequence.</p>
        </div>
      </div>
      <div class="comment comment-457723894493037643">
        <div class="comment-header">
          <a name="comment-457723894493037643"></a>
            <span class="author">LKRaider</span> wrote on <span class="date">2013-10-23 21:45</span>:
        </div>
        <div class="comment-content">
          <p>A bit off-topic, but just came across this paper:<br><br>"Speculative Staging for Interpreter Optimization<br>(...)<br>-- we report that our optimization makes the CPython interpreter up to more than four times faster, where our interpreter closes the gap between and sometimes even outperforms PyPy's just-in-time compiler."<br>https://arxiv.org/abs/1310.2300</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/10/incremental-garbage-collector-in-pypy-8956893523842234676.html" class="u-url">Incremental Garbage Collector in PyPy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/10/incremental-garbage-collector-in-pypy-8956893523842234676.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-10-15T11:20:00Z" itemprop="datePublished" title="2013-10-15 11:20">2013-10-15 11:20</time></a>
            </p>
                <p class="commentline">16 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">
<div dir="ltr" style="text-align: left;">

<p>Hello everyone.</p>
<p>We're pleased to announce that as of today,
the default PyPy comes with a GC that has much smaller pauses than yesterday.</p>
<p>Let's start with explaining roughly what GC pauses are. In CPython each
object has a reference count, which is incremented each time we create
references and decremented each time we forget them. This means that objects
are freed each time they become unreachable. That is only half of the story
though. First note that when the last reference to a large tree of
objects goes away, you have a pause: all the objects are freed. Your
program is not progressing at all during this pause, and this pause's
duration can be arbitrarily large. This occurs at deterministic times,
though. But consider code like this:</p>
<pre class="literal-block">
class A(object):
     pass

a = A()
b = A()
a.item = b
b.item = a
del a
del b
</pre>
<p>This creates a reference cycle. It means that while we deleted references to
<tt class="docutils literal">a</tt> and <tt class="docutils literal">b</tt> from the current scope, they still have a reference count of 1,
because they point to each other, even though the whole group has no references
from the outside. CPython employs a cyclic garbage collector which is used to
find such cycles. It walks over all objects in memory, starting from some known
roots, such as <tt class="docutils literal">type</tt> objects, variables on the stack, etc. This solves the
problem, but can create noticeable, nondeterministic GC pauses as the heap
becomes large and convoluted.</p>
<p>PyPy essentially has only the cycle finder - it does not bother with reference
counting, instead it walks alive objects every now and then (this is a big
simplification, PyPy's GC is much more complex than this). Although this might
sound like a missing feature, it is really one of the reasons why PyPy is so
fast, because at the end of the day the total time spent in managing the
memory is lower in PyPy than CPython. However, as a result, PyPy also has the
problem of GC pauses.</p>
<p>To alleviate this problem, which is essential for
applications like games, we started to work on incremental GC, which spreads
the walking of objects and cleaning them across the execution time in smaller
intervals. The work was sponsored by the Raspberry Pi foundation, started
by Andrew Chambers and finished by Armin Rigo and Maciej Fijałkowski.</p>
</div>
<div class="section" id="benchmarks">
<h3>Benchmarks</h3>
<p>Everyone loves benchmarks. We did not measure any significant speed difference
on our quite extensive benchmark suite on speed.pypy.org. The main
benchmark that we used for other comparisons was translating the <a class="reference external" href="https://docs.topazruby.com/en/latest/">topaz</a>
ruby interpreter using various versions of PyPy and CPython. The exact
command was <tt class="docutils literal">python <span class="pre">&lt;pypy-checkout&gt;/bin/rpython</span> <span class="pre">-O2</span> <span class="pre">--rtype</span> targettopaz.py</tt>.
Versions:</p>
<ul class="simple">
<li>topaz - dce3eef7b1910fc5600a4cd0afd6220543104823</li>
<li>pypy source - defb5119e3c6</li>
<li>pypy compiled with minimark (non-incremental GC) - d1a0c07b6586</li>
<li>pypy compiled with incminimark (new, incremental GC) - 417a7117f8d7</li>
<li>CPython - 2.7.3</li>
</ul>
<p>The memory usage of CPython, PyPy with minimark and PyPy with incminimark is
shown here. Note that this benchmark is quite bad for PyPy in general, the
memory usage is higher and the amount of time taken is longer. This is due
to the JIT warmup being both memory hungry and inefficient (see below).
But first, the new GC is not worse than the old one.</p>

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-Ty4LVNMb8OQ/Ul0VmNvlCiI/AAAAAAAABo0/7HzP9D0tOFI/s1600/memusage.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://1.bp.blogspot.com/-Ty4LVNMb8OQ/Ul0VmNvlCiI/AAAAAAAABo0/7HzP9D0tOFI/s400/memusage.png"></a></div>
<p><b>EDIT:</b>Red line is CPython, blue is incminimark (new), green is minimark (old)</p>

<p>The image was obtained by graphing the output of <a class="reference external" href="https://foss.heptapod.net/pypy/pypy/-/tree/branch/default/pypy/tool/memusage/memusage.py?at=default">memusage.py</a>.</p>
<p>However, the GC pauses are significantly smaller. For PyPy the way to
get GC pauses is to measure time between start and stop while running stuff
with <tt class="docutils literal"><span class="pre">PYPYLOG=gc-collect:log</span> pypy program.py</tt>, for CPython, the magic
incantation is <tt class="docutils literal">gc.set_debug(gc.DEBUG_STATS)</tt> and parsing the output.
For what is worth, the average and total for CPython, as well as the total
number of events are not directly comparable since it only shows the cyclic
collector, not the reference counts. The only comparable thing is the
amount of long pauses and their duration. In the table below, pause duration
is sorted into 8 buckets, each meaning "below that or equal to the threshold".
The output is generated using the <a class="reference external" href="https://foss.heptapod.net/pypy/pypy/-/tree/branch/default/rpython/tool/gcanalyze.py?at=default">gcanalyze</a> tool.</p>
<p>CPython:</p>
<table border="1" class="docutils">
<colgroup>
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="14%">
<col width="14%">
</colgroup>
<tbody valign="top">
<tr>
<td>150.1ms</td>
<td>300.2ms</td>
<td>450.3ms</td>
<td>600.5ms</td>
<td>750.6ms</td>
<td>900.7ms</td>
<td>1050.8ms</td>
<td>1200.9ms</td>
</tr>
<tr>
<td>5417</td>
<td>5</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>PyPy minimark (non-incremental GC):</p>
<table border="1" class="docutils">
<colgroup>
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="13%">
<col width="13%">
<col width="13%">
<col width="13%">
</colgroup>
<tbody valign="top">
<tr>
<td>216.4ms</td>
<td>432.8ms</td>
<td>649.2ms</td>
<td>865.6ms</td>
<td>1082.0ms</td>
<td>1298.4ms</td>
<td>1514.8ms</td>
<td>1731.2ms</td>
</tr>
<tr>
<td>27</td>
<td>14</td>
<td>6</td>
<td>4</td>
<td>6</td>
<td>5</td>
<td>3</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>PyPy incminimark (new incremental GC):</p>
<table border="1" class="docutils">
<colgroup>
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="12%">
<col width="14%">
<col width="14%">
</colgroup>
<tbody valign="top">
<tr>
<td>15.7ms</td>
<td>31.4ms</td>
<td>47.1ms</td>
<td>62.8ms</td>
<td>78.6ms</td>
<td>94.3ms</td>
<td>110.0ms</td>
<td>125.7ms</td>
</tr>
<tr>
<td>25512</td>
<td>122</td>
<td>4</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>As we can see, while there is still work to be done (the 100ms ones could
be split among several steps), we did improve the situation quite drastically
without any actual performance difference.</p>
<p>Note about the benchmark - we know it's a pretty extreme case of JIT
warmup, we know we suck on it, we're working on it and we're not afraid of
showing PyPy is not always the best <tt class="docutils literal"><span class="pre">;-)</span></tt></p>
</div>
<div class="section" id="nitty-gritty-details">
<h3>Nitty gritty details</h3>
<p>Here are some nitty gritty details for people really interested in
Garbage Collection.  This was done as a patch to "minimark", our current
GC, and called "incminimark" for now.  The former is a generational
stop-the-world GC.  New objects are allocated "young", which means that
they initially live in the "nursery", a special zone of a few MB of
memory.  When the nursery is full, a "minor collection" step moves the
surviving objects out of the nursery.  This can be done quickly (a few
millisecond) because we only need to walk through the young objects that
survive --- usually a small fraction of all young objects; and also by
far not <em>all</em> objects that are alive at this point, but only the young
ones.  However, from time to time this minor collection is followed by a
"major collection": in that step, we really need to walk all objects to
classify which ones are still alive and which ones are now dead
("marking") and free the memory occupied by the dead ones ("sweeping").
You can read more details <a class="reference external" href="https://doc.pypy.org/en/latest/garbage_collection.html#minimark-gc">here</a>.</p>
<p>This "major collection" is what gives the long GC pauses.  To fix this
problem we made the GC incremental: instead of running one complete
major collection, we split its work into a variable number of pieces and
run each piece after every minor collection for a while, until there are
no more pieces.  The pieces are each doing a fraction of marking, or a
fraction of sweeping.  It adds some few milliseconds after each of these
minor collections, rather than requiring hundreds of milliseconds in one
go.</p>
<p>The main issue is that splitting the major collections means that the
main program is actually running between the pieces, and so it can
change the pointers in the objects to point to other objects.  This is
not a problem for sweeping: dead objects will remain dead whatever the
main program does.  However, it is a problem for marking.  Let us see
why.</p>
<p>In terms of the incremental GC literature, objects are either "white",
"gray" or "black".  This is called <em>tri-color marking.</em>  See for example
this <a class="reference external" href="https://rubini.us/2013/06/22/concurrent-garbage-collection/">blog post about Rubinius</a>, or this <a class="reference external" href="https://wiki.luajit.org/New-Garbage-Collector/01fd5e5ca4f95d45e0c4b8a98b49f2b656cc23dd">page about LuaJIT</a> or the <a href="https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29#Tri-color_marking">wikipedia description</a>.  The
objects start as "white" at the beginning of marking; become "gray" when
they are found to be alive; and become "black" when they have been fully
traversed.  Marking proceeds by scanning grey objects for pointers to
white objects.  The white objects found are turned grey, and the grey
objects scanned are turned black.  When there are no more grey objects,
the marking phase is complete: all remaining white objects are truly
unreachable and can be freed (by the following sweeping phase).</p>
<p>In this model, the important part is that a black object can never point
to a white object: if the latter remains white until the end, it will be
freed, which is incorrect because the black object itself can still be
reached.  How do we ensure that the main program, running in the middle
of marking, will not try to write a pointer to white object into a black
object?  This requires a "write barrier", i.e. a piece of code that runs
every time we set a pointer into an object or array.  This piece of code
checks if some (hopefully rare) condition is met, and calls a function
if that is the case.</p>
<p>The trick we used in PyPy is to consider minor collections as part of
the whole, rather than focus only on major collections.  The existing
minimark GC had always used a write barrier of its own to do its job,
like any generational GC.  This existing write barrier is used to detect
when an old object (outside the nursery) is modified to point to a young
object (inside the nursery), which is essential information for minor
collections.  Actually, although this was the goal, the actual write
barrier code is simpler: it just records all old objects into which we
write <em>any</em> pointer --- to a young or old object.  As we found out over
time, doing so is not actually slower, and might actually be a
performance improvement: for example, if the main program does a lot of
writes into the same old object, we don't need to check over and over
again if the written pointer points to a young object or not.  We just
record the old object in some list the first time, and that's it.</p>
<p>The trick is that this <em>unmodified</em> write barrier works for incminimark
too.  Imagine that we are in the middle of the marking phase, running
the main program.  The write barrier will record all old objects that
are being modified.  Then at the next minor collection, all surviving
young objects will be moved out of the nursery.  At this point, as we're
about to continue running the major collection's marking phase, we
simply add to the list of pending gray objects all the objects that we
just considered --- both the objects listed as "old objects that are
being modified", and the objects that we just moved out of the nursery.
A fraction from the former list were black object; so this mean that
they are turned back from the black to the gray color.  This technique
implements nicely, if indirectly, what is called a "backward write
barrier" in the literature.  The backwardness is about the color that
needs to be changed in the opposite of the usual direction "white -&gt;
gray -&gt; black", thus making more work for the GC.  (This is as opposed
to "forward write barrier", where we would also detect "black -&gt; white"
writes but turn the white object gray.)</p>
<p>In summary, I realize that this description is less about how we turned
minimark into incminimark, and more about how we differ from the
standard way of making a GC incremental.  What we really had to do to
make incminimark was to write logic that says "if the major collection
is in the middle of the marking phase, then add this object to the list
of gray objects", and put it at a few places throughout minor
collection.  Then we simply split a major collection into increments,
doing marking or sweeping of some (relatively arbitrary) number of
objects before returning.  That's why, after we found that the existing
write barrier would do, it was not much actual work, and could be done
without major changes.  For example, not a single line from the JIT
needed adaptation.  All in all it was relatively painless work. ;-)
</p>
<p>Cheers,<br>armin and fijal</p>
</div>
<br>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-2640666352609164748">
        <div class="comment-header">
          <a name="comment-2640666352609164748"></a>
            <span class="author">H*</span> wrote on <span class="date">2013-10-15 14:24</span>:
        </div>
        <div class="comment-content">
          <p>Nice work! :)</p>
        </div>
      </div>
      <div class="comment comment-4108635523366025462">
        <div class="comment-header">
          <a name="comment-4108635523366025462"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2013-10-15 19:10</span>:
        </div>
        <div class="comment-content">
          <p>Thank you for this nice explanation. <br><br>Which mechanism do you use for not adding twice an old object in the list of modified old objects?</p>
        </div>
      </div>
      <div class="comment comment-8135947294171177813">
        <div class="comment-header">
          <a name="comment-8135947294171177813"></a>
            <span class="author">René Dudfield</span> wrote on <span class="date">2013-10-15 21:56</span>:
        </div>
        <div class="comment-content">
          <p>Thank you! thank you! thank you!  Game dev on pypy just leveled up!</p>
        </div>
      </div>
      <div class="comment comment-8324069479613356368">
        <div class="comment-header">
          <a name="comment-8324069479613356368"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-10-15 22:08</span>:
        </div>
        <div class="comment-content">
          <p>Very clever! But eh, your graphs show that your program is using 2-3x the memory of CPython. How much faster is your program overall in exchange for this hugely larger memory usage?</p>
        </div>
      </div>
      <div class="comment comment-6274954072377772487">
        <div class="comment-header">
          <a name="comment-6274954072377772487"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2013-10-16 07:00</span>:
        </div>
        <div class="comment-content">
          <p>@François: a flag on the object.  All old objects have this flag initially, and we use it to detect if the write barrier must trigger.  We remove it when the write barrier has triggered once.  We re-add it during the following minor collection.<br><br>@Anonymous: this program is slower on PyPy too.  The point of the benchmark is to show that incminimark gives the same results as minimark, and to show that the JIT has bad cases.  Running the same program for a much longer time (5-10x) lets PyPy slowly catch up and eventually beat CPython by a factor 2.  The memory usage is evening out at around around 4 or 4.5GB (and I'd expect even larger examples to show lower consumption on PyPy, but that's mostly a guess).</p>
        </div>
      </div>
      <div class="comment comment-8884258461163233219">
        <div class="comment-header">
          <a name="comment-8884258461163233219"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-10-16 10:03</span>:
        </div>
        <div class="comment-content">
          <p>Thanks for moving Python forward!<br><br>How does the incminimarc compares to Azul C4 JVM GC and Hotspots G1 GC?<br>In other words are there strong guarantees that for big heap sizes e.g. 12 GB the GC pauses will not exceed some value e.g. 100ms?</p>
        </div>
      </div>
      <div class="comment comment-277139980756026923">
        <div class="comment-header">
          <a name="comment-277139980756026923"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-10-16 10:21</span>:
        </div>
        <div class="comment-content">
          <p>Sounds like great progress, but I hope you understand that even 15-30ms is <b>way</b> too much for games. That's 1-2 frames. It needs to be an order of magnitude less to ensure smooth FPS.<br><br>Do you have plans to give the program any say in whether the GC should strive for low latency vs. high throughput?</p>
        </div>
      </div>
      <div class="comment comment-2572947594762788530">
        <div class="comment-header">
          <a name="comment-2572947594762788530"></a>
            <span class="author">vdp</span> wrote on <span class="date">2013-10-16 11:13</span>:
        </div>
        <div class="comment-content">
          <p>Great writeup, explaining that kind of concept in a clear way is not easy. And well done on the unequivocal improvements :)<br><br>@annonymous Yes you'll still miss some frames, but compared to a 1 second pause, pypy suddenly became usable for games. 55fps (over what duration did those 25K collections happen ?) is not perfect, but most users won't notice. That said, it *would* be nice to be able to tune latency vs throughput.<br></p>
        </div>
      </div>
      <div class="comment comment-4490422238173482691">
        <div class="comment-header">
          <a name="comment-4490422238173482691"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2013-10-16 12:36</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous: our incminimark comes with no serious strong guarantee.  I still think it's enough for most games, say, if "almost all" the pauses are around 10ms.  It's also tweakable (see the PYPY_GC_* environment variables documented in rpython/memory/gc/incminimark.py, and try to call something like gc.collect(1) at the end of each frame).<br><br>Anyway, at around the same time scale is the time spent JITting, which also causes apparent pauses in the program.  I think that fixing it all with really strong guarantees is a much, much harder problem.  CPython doesn't gives any guarantee either, as explained at the start of the blog post.</p>
        </div>
      </div>
      <div class="comment comment-6075820676392197742">
        <div class="comment-header">
          <a name="comment-6075820676392197742"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-10-16 15:02</span>:
        </div>
        <div class="comment-content">
          <p>@Armin Rigo: Yeah, it's no use pushing GC pauses much lower than other pauses, but that just means other things need improving as well. ;) If I had to draw an arbitrary line, I'd say half a frame (i.e. 8ms for 60fps, 4ms for 120fps 3D) is probably a good target for the maximum.<br><br>The thing with CPython is that you can turn the GC off and still have everything non-cyclic collected. So with enough attention to detail you can avoid GC pauses completely.<br><br>BTW, is any work ongoing with regard to fully concurrent GC?</p>
        </div>
      </div>
      <div class="comment comment-1532535437129838539">
        <div class="comment-header">
          <a name="comment-1532535437129838539"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2013-10-16 15:14</span>:
        </div>
        <div class="comment-content">
          <p>You *cannot* avoid GC pauses in CPython: see the first paragraph of the blog post.  You can only make the GC pauses deterministic, by disabling the cyclic collector.  Then you can hack the program as needed to reduce GC pauses if there are some.</p>
        </div>
      </div>
      <div class="comment comment-5683772165414593188">
        <div class="comment-header">
          <a name="comment-5683772165414593188"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2013-10-16 22:14</span>:
        </div>
        <div class="comment-content">
          <p>Thanks for this really educational and accessible explanation - it's rare to find such a concise and clear piece of writing that a non expert can understand on the subject of GC.</p>
        </div>
      </div>
      <div class="comment comment-5629636211265567577">
        <div class="comment-header">
          <a name="comment-5629636211265567577"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2013-10-21 06:17</span>:
        </div>
        <div class="comment-content">
          <p>This needs visualization of processes to win Wikipedia article of the month.</p>
        </div>
      </div>
      <div class="comment comment-6870856759657872418">
        <div class="comment-header">
          <a name="comment-6870856759657872418"></a>
            <span class="author">Michael Hudson-Doyle</span> wrote on <span class="date">2013-10-22 02:29</span>:
        </div>
        <div class="comment-content">
          <p>You can also get arbitrarily long "gc" pauses in CPython by removing the last reference to some deeply nested data structure...</p>
        </div>
      </div>
      <div class="comment comment-71976144077756629">
        <div class="comment-header">
          <a name="comment-71976144077756629"></a>
            <span class="author">Dima Q</span> wrote on <span class="date">2013-11-10 13:07</span>:
        </div>
        <div class="comment-content">
          <p>Wow PyPy keeps paying off!<br>I am so glad you guys have time (and hopefully funding) push dynamic language world forward!</p>
        </div>
      </div>
      <div class="comment comment-1693418319997410540">
        <div class="comment-header">
          <a name="comment-1693418319997410540"></a>
            <span class="author">Franck</span> wrote on <span class="date">2013-11-21 12:19</span>:
        </div>
        <div class="comment-content">
          <p>If you fork() the whole process and do the marking on the frozen forked copy (on write) then you can be fully incremental without pauses, as long as you've got enough spare system memory compared to process size (as the main process keeps growing while you're marking and the pathological case of copy on write is 2x, however unlikely).</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/09/numpy-status-update-5160363918470470887.html" class="u-url">Numpy Status Update</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/romain-guillebert.html">Romain Guillebert</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/09/numpy-status-update-5160363918470470887.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-09-25T17:49:00Z" itemprop="datePublished" title="2013-09-25 17:49">2013-09-25 17:49</time></a>
            </p>
            
        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi everyone<br><br>
Thanks to the people who donated money to the <a href="https://pypy.org/numpydonate.html" target="_blank">numpy proposal</a>, here is what I've been working on recently :<br><br>
- Fixed conversion from a numpy complex number to a python complex number<br>
- Implement the rint ufunc<br>
- Make numpy.character usable as a dtype<br>
- Fix ndarray(dtype=str).fill()<br>
- Various fixes on boolean and fancy indexing<br><br>
Cheers<br>
Romain</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/09/pycon-south-africa-sprint-6630788654105016762.html" class="u-url">PyCon South Africa &amp; sprint</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/09/pycon-south-africa-sprint-6630788654105016762.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-09-23T11:27:00Z" itemprop="datePublished" title="2013-09-23 11:27">2013-09-23 11:27</time></a>
            </p>
                <p class="commentline">4 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi all,</p>

<p>For those of you that happen to be from South Africa: don't miss
<a href="https://za.pycon.org">PyCon ZA 2013</a>, next October 3rd and 4th!
Like last year, a few of us will be there.  There will be the first talk
about <a href="https://za.pycon.org/talks/15/">STM getting ready</a> (a
blog post about that should follow soon).</p>

<p>Moreover, general sprints will continue on the weekend (5th and 6th).
Afterwards, Fijal will host a longer PyPy sprint (marathon?) with me
until around the 21th.  You are welcome to it as well!  Write to the <a href="https://mail.python.org/mailman/listinfo/pypy-dev">mailing list</a> or to fijal directly (fijall
at gmail.com), or simply in comments of <a href="../posts/2013/09/pycon-south-africa-sprint-6630788654105016762.html">this post.</a></p>

<p>--- Armin</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-1138735253904672672">
        <div class="comment-header">
          <a name="comment-1138735253904672672"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-10-09 21:49</span>:
        </div>
        <div class="comment-content">
          <p>Hey lads, any change of 64-bit arm pypy build?<br><br>now that hardware is finally generally available...<br><br>I'm sure someone at the conference has the hw, perhaps already rooted?</p>
        </div>
      </div>
      <div class="comment comment-885394282168248458">
        <div class="comment-header">
          <a name="comment-885394282168248458"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2013-10-09 21:52</span>:
        </div>
        <div class="comment-content">
          <p>we don't have access to 64bit ARM. feel free to help us. also it's quite a bit of work</p>
        </div>
      </div>
      <div class="comment comment-7371116568733007092">
        <div class="comment-header">
          <a name="comment-7371116568733007092"></a>
            <span class="author">Nickolas</span> wrote on <span class="date">2013-10-10 06:17</span>:
        </div>
        <div class="comment-content">
          <p>Hey Armin<br><br>Thanks for the awesome presentations :-)<br><br>I'm very excited to try it out soon. I was wondering, would it not be useful to try and get the "with atomic" statement at the very least working on regular CPython? (just operating on the GIL, or simulated with a lock). This could smooth over migration somewhat?<br><br>Also, thanks for your live demo of cffi, It is so much simpler than ctypes :-)</p>
        </div>
      </div>
      <div class="comment comment-3910843687340268716">
        <div class="comment-header">
          <a name="comment-3910843687340268716"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2013-10-10 07:06</span>:
        </div>
        <div class="comment-content">
          <p>Hi Nikolas.<br><br>with atomic can be trivially available on CPython (and not do anything beyond have a lock)</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/08/slides-of-pypy-london-demo-evening-5157052112396009739.html" class="u-url">Slides of the PyPy London Demo Evening</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/08/slides-of-pypy-london-demo-evening-5157052112396009739.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-08-30T17:16:00Z" itemprop="datePublished" title="2013-08-30 17:16">2013-08-30 17:16</time></a>
            </p>
                <p class="commentline">4 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>The slides of the <a href="../posts/2013/08/preliminary-london-demo-evening-agenda-5254002451136674320.html">London demo evening</a> are now online:</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-9197983936667466791">
        <div class="comment-header">
          <a name="comment-9197983936667466791"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-08-30 20:41</span>:
        </div>
        <div class="comment-content">
          <p>Is there a better look ink to the slides? Watching them on the blog is difficult</p>
        </div>
      </div>
      <div class="comment comment-6797402227705529705">
        <div class="comment-header">
          <a name="comment-6797402227705529705"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-08-30 22:28</span>:
        </div>
        <div class="comment-content">
          <p>Clicking the full screen button makes them easy to read for me. Maybe try that?</p>
        </div>
      </div>
      <div class="comment comment-2478889803279609359">
        <div class="comment-header">
          <a name="comment-2478889803279609359"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-09-03 16:53</span>:
        </div>
        <div class="comment-content">
          <p>Could there perhaps be videos from the presentation?<br><br>big up for good work!</p>
        </div>
      </div>
      <div class="comment comment-8464907323189333568">
        <div class="comment-header">
          <a name="comment-8464907323189333568"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2013-09-10 14:22</span>:
        </div>
        <div class="comment-content">
          <p>i know, such questions probably get on your nerves, but do you think you will every reach a 10x average on speed.pypy.org? :)</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/08/numpy-road-forward-4210065750776753500.html" class="u-url">NumPy road forward</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/08/numpy-road-forward-4210065750776753500.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-08-27T11:20:00Z" itemprop="datePublished" title="2013-08-27 11:20">2013-08-27 11:20</time></a>
            </p>
                <p class="commentline">5 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">
<p>Hello everyone.</p>
<p>This is the roadmap for numpy effort in PyPy as discussed on the London sprint.
First, the highest on our priority list is to finish the low-level part
of the numpy module. What
we'll do is to finish the RPython part of numpy and provide a pip installable
numpypy repository that includes the pure python part of Numpy. This would
contain the original Numpy with a few minor changes.</p>
<p>Second, we need to work on the JIT support that will make NumPy on PyPy
faster. In detail:</p>
<ul class="simple">
<li>reenable the lazy loop evaluation</li>
<li>optimize bridges, which is depending on optimizer refactorings</li>
<li>SSE support</li>
</ul>
<p>On the compatibility front, there were some independent attempts into
making the following stuff working:</p>
<ul class="simple">
<li>f2py</li>
<li>C API (in fact, PyArray_* API is partly present in the nightly builds of
PyPy)</li>
<li>matplotlib (both using PyArray_* API and embedding CPython runtime in PyPy)</li>
<li>scipy</li>
</ul>
<p>In order to make all of the above happen faster, it would be helpful to raise
more funds. You can donate to <a class="reference external" href="https://pypy.org/numpydonate.html">PyPy's NumPy project</a> on our website. Note
that PyPy is a member of SFC which is a 501(c)(3) US non-profit, so donations
from US companies can be tax-deducted.</p>
<p>Cheers,<br>
fijal, arigo, ronan, rguillebert, anto and others</p>
<br>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-4015436111913908160">
        <div class="comment-header">
          <a name="comment-4015436111913908160"></a>
            <span class="author">Pim</span> wrote on <span class="date">2013-08-27 16:41</span>:
        </div>
        <div class="comment-content">
          <p>Thanks for the update.  I'm hoping the other presentations can also be summarized here for those who couldn't attend this (very interesting) mini-conference.</p>
        </div>
      </div>
      <div class="comment comment-1425642441777416396">
        <div class="comment-header">
          <a name="comment-1425642441777416396"></a>
            <span class="author">Dan</span> wrote on <span class="date">2013-08-28 20:11</span>:
        </div>
        <div class="comment-content">
          <p>Thanks for the info! I can't wait to play with it. <br>I only have a very rudimentary understanding of numpypy and pypy, so please forgive if this is a stupid question:<br><br>Will there be a way to do additional high level optimization steps before the JIT level? <br><br>I.e. elimination of temporaries for matrices, expression optimization and so on?<br><br>Basically check if the expression should be handled by the pypy JIT, or if if should be passed on to something like numexpr<br>https://code.google.com/p/numexpr/<br>that will itself hand over the code to optimized vendor libraries?<br><br>I am a bit concerned that while the pypy JIT optimizations are without question very impressive and probably close to optimal to what can be done for generic code, the performance issues with numerical code are very different.<br><br>Any JIT will (please correct me if I am wrong, this would be a significant breakthrough) never be able to even come close to what a vendor library like the MKL can do.<br><br>The comparison will be even more to the disadvantage of the JIT if one uses a library like Theano that runs the code on the GPU.<br><br>For my work, beating c for speed is not enough anymore, the challenges are how to run the computation in parallel, how to call optimized libraries without pain and how to use a GPU without re-writing the entire program and learning about a completely new system.<br><br>Will libraries like numexpr, numba and theano be able to run under pypy, and will it eventually be possible to automatically hand over numerical expressions automatically to these libraries?</p>
        </div>
      </div>
      <div class="comment comment-6865491046922416479">
        <div class="comment-header">
          <a name="comment-6865491046922416479"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2013-09-11 14:26</span>:
        </div>
        <div class="comment-content">
          <p>Hi Dan.<br><br>Yes, pypy will do the removal of temporary matrices, this is a very basic optimization that we had, but disabled for a while to simplify development.<br><br>I don't think numba, numexpr or theano would ever work on PyPy (I would ask their authors though), but I personally think we can match their performance or even exceed it, time will tell though.<br><br>Cheers,<br>fijal<br></p>
        </div>
      </div>
      <div class="comment comment-667046472293725993">
        <div class="comment-header">
          <a name="comment-667046472293725993"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-09-14 00:40</span>:
        </div>
        <div class="comment-content">
          <p>Hi Maciej,<br><br>Thanks for the answer. <br><br>A pypy that matches what numba or theano can do, all without doing any extra annotation, would not only be a huge breakthrough for pypy, it will be a gigantic step forward for the entire numerics community.<br><br>Thank you and keep up the good work,<br><br><br>Dan</p>
        </div>
      </div>
      <div class="comment comment-779071970416045114">
        <div class="comment-header">
          <a name="comment-779071970416045114"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2013-09-14 13:31</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous: I'd like to point out again that all this NumPy work would get more traction and faster development within PyPy if we could manage to interest (and get contributions from) anyone that comes from the scientific community.  Ourselves, we are  looking at this topic as a smallish part of the whole Python world, so we disagree (to a point) with your comment "a huge breakthrough for pypy". :-)</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2013/08/preliminary-london-demo-evening-agenda-5254002451136674320.html" class="u-url">Preliminary London Demo Evening Agenda</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2013/08/preliminary-london-demo-evening-agenda-5254002451136674320.html" rel="bookmark">
            <time class="published dt-published" datetime="2013-08-20T10:44:00Z" itemprop="datePublished" title="2013-08-20 10:44">2013-08-20 10:44</time></a>
            </p>
                <p class="commentline">3 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>We now have a preliminary agenda for the <a href="../posts/2013/07/pypy-demo-evening-in-london-august-27-3640213278969666664.html">demo evening in London next week</a>. It takes place on Tuesday, August 27 2013, 18:30-19:30 (BST) at King's College London, Strand. The preliminary agenda is as follows:</p>

<ul>
<li>
<a href="https://tratt.net/laurie/">Laurence Tratt</a>: Welcome from the <a href="https://soft-dev.org/">Software Development Team</a>
</li>
<li>
<a href="https://cfbolz.de">Carl Friedrich Bolz</a>: A Short Introduction to PyPy</li>
<li>
<a href="https://baroquesoftware.com/">Maciej Fijałkowski</a>: Numpy on PyPy, Present State and Outlook</li>
<li>
<a href="https://lukasdiekmann.com/">Lukas Diekmann</a>: Collection Strategies for Fast Containers in PyPy</li>
<li>Armin Rigo: Software Transactional Memory for PyPy</li>
<li>
<a href="https://eddbarrett.co.uk/">Edd Barrett</a>: Unipycation: Combining Prolog and Python</li>
</ul>
<p>All the talks are lightning talks. Afterwards there will be plenty of time for discussion.</p>

<p>There's still free spots, if you want to come, please register on the <a href="https://www.eventbrite.co.uk/event/7590686949">Eventbrite page</a>. Hope to see you there!</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-2503351468188171675">
        <div class="comment-header">
          <a name="comment-2503351468188171675"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2013-08-20 13:35</span>:
        </div>
        <div class="comment-content">
          <p>Will the video of the talks be available online?</p>
        </div>
      </div>
      <div class="comment comment-169036505194476317">
        <div class="comment-header">
          <a name="comment-169036505194476317"></a>
            <span class="author">Carl Friedrich Bolz-Tereick</span> wrote on <span class="date">2013-08-20 13:36</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous: unfortunately there are no plans to film the event, no :-(</p>
        </div>
      </div>
      <div class="comment comment-5543896657259500475">
        <div class="comment-header">
          <a name="comment-5543896657259500475"></a>
            <span class="author">Paddy3118</span> wrote on <span class="date">2013-08-21 15:17</span>:
        </div>
        <div class="comment-content">
          <p>Another request for cideos of the event to be made available. Please.</p>
        </div>
      </div>
         </div>

</div>
</div>
<div class="sidebar">
<div>
  <h2>
    The PyPy blogposts
  </h2>
  <div>
    Create a guest post via a PR to the <a href="https://github.com/pypy/pypy.org">source repo</a>
  </div>
</div>
    <div id="global-recent-posts">
    <h2>
      Recent Posts
    </h2>
    <ul class="post-list">
      <li>
        <a href="/posts/2025/07/pypy-v7320-release.html" class="listtitle">PyPy v7.3.20 release</a>
      </li>
      <li>
        <a href="/posts/2025/06/rpython-gc-allocation-speed.html" class="listtitle">How fast can the RPython GC allocate?</a>
      </li>
      <li>
        <a href="/posts/2025/04/prospero-in-rpython.html" class="listtitle">Doing the Prospero-Challenge in RPython</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7319-release.html" class="listtitle">PyPy v7.3.19 release</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-gc-sampling.html" class="listtitle">Low Overhead Allocation Sampling with VMProf in PyPy's GC</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7318-release.html" class="listtitle">PyPy v7.3.18 release</a>
      </li>
      <li>
        <a href="/posts/2025/01/musings-tracing.html" class="listtitle">Musings on Tracing in PyPy</a>
      </li>
      <li>
        <a href="/posts/2025/01/towards-pypy311-an-update.html" class="listtitle">Towards PyPy3.11 - an update</a>
      </li>
      <li>
        <a href="/posts/2024/11/guest-post-final-encoding-in-rpython.html" class="listtitle">Guest Post: Final Encoding in RPython Interpreters</a>
      </li>
      <li>
        <a href="/posts/2024/10/jit-peephole-dsl.html" class="listtitle">A DSL for Peephole Transformation Rules of Integer Operations in the PyPy JIT</a>
      </li>
    </ul>
  </div>

          <div id="global-archive-list">
          <h2>
            Archives
          </h2>
          <ul class="archive-level archive-level-1">
            <li><a class="reference" href="/2007/">2007</a> (19)
            </li>
            <li><a class="reference" href="/2008/">2008</a> (62)
            </li>
            <li><a class="reference" href="/2009/">2009</a> (38)
            </li>
            <li><a class="reference" href="/2010/">2010</a> (44)
            </li>
            <li><a class="reference" href="/2011/">2011</a> (43)
            </li>
            <li><a class="reference" href="/2012/">2012</a> (44)
            </li>
            <li><a class="reference" href="/2013/">2013</a> (46)
            </li>
            <li><a class="reference" href="/2014/">2014</a> (22)
            </li>
            <li><a class="reference" href="/2015/">2015</a> (20)
            </li>
            <li><a class="reference" href="/2016/">2016</a> (20)
            </li>
            <li><a class="reference" href="/2017/">2017</a> (13)
            </li>
            <li><a class="reference" href="/2018/">2018</a> (12)
            </li>
            <li><a class="reference" href="/2019/">2019</a> (12)
            </li>
            <li><a class="reference" href="/2020/">2020</a> (9)
            </li>
            <li><a class="reference" href="/2021/">2021</a> (10)
            </li>
            <li><a class="reference" href="/2022/">2022</a> (13)
            </li>
            <li><a class="reference" href="/2023/">2023</a> (6)
            </li>
            <li><a class="reference" href="/2024/">2024</a> (13)
            </li>
            <li><a class="reference" href="/2025/">2025</a> (8)
            </li>
          </ul>
        </div>


          <div id="global-tag-list">
          <h2>
            Tags
          </h2>
          <ul>
            <li><a class="reference" href="/categories/arm.html">arm</a> (2)</li>
            <li><a class="reference" href="/categories/benchmarking.html">benchmarking</a> (1)</li>
            <li><a class="reference" href="/categories/casestudy.html">casestudy</a> (3)</li>
            <li><a class="reference" href="/categories/cli.html">cli</a> (1)</li>
            <li><a class="reference" href="/categories/compiler.html">compiler</a> (1)</li>
            <li><a class="reference" href="/categories/conda-forge.html">conda-forge</a> (1)</li>
            <li><a class="reference" href="/categories/cpyext.html">cpyext</a> (4)</li>
            <li><a class="reference" href="/categories/cpython.html">CPython</a> (3)</li>
            <li><a class="reference" href="/categories/ep2008.html">ep2008</a> (1)</li>
            <li><a class="reference" href="/categories/extension-modules.html">extension modules</a> (3)</li>
            <li><a class="reference" href="/categories/gc.html">gc</a> (3)</li>
            <li><a class="reference" href="/categories/guestpost.html">guestpost</a> (3)</li>
            <li><a class="reference" href="/categories/graalpython.html">GraalPython</a> (1)</li>
            <li><a class="reference" href="/categories/hpy.html">hpy</a> (1)</li>
            <li><a class="reference" href="/categories/heptapod.html">Heptapod</a> (1)</li>
            <li><a class="reference" href="/categories/jit.html">jit</a> (23)</li>
            <li><a class="reference" href="/categories/jython.html">jython</a> (1)</li>
            <li><a class="reference" href="/categories/kcachegrind.html">kcachegrind</a> (1)</li>
            <li><a class="reference" href="/categories/meta.html">meta</a> (1)</li>
            <li><a class="reference" href="/categories/numpy.html">numpy</a> (24)</li>
            <li><a class="reference" href="/categories/parser.html">parser</a> (1)</li>
            <li><a class="reference" href="/categories/performance.html">performance</a> (2)</li>
            <li><a class="reference" href="/categories/profiling.html">profiling</a> (7)</li>
            <li><a class="reference" href="/categories/pypy.html">pypy</a> (6)</li>
            <li><a class="reference" href="/categories/pypy3.html">pypy3</a> (16)</li>
            <li><a class="reference" href="/categories/pyqt4.html">PyQt4</a> (1)</li>
            <li><a class="reference" href="/categories/release.html">release</a> (66)</li>
            <li><a class="reference" href="/categories/releasecffi.html">releasecffi</a> (3)</li>
            <li><a class="reference" href="/categories/releaserevdb.html">releaserevdb</a> (1)</li>
            <li><a class="reference" href="/categories/releasestm.html">releasestm</a> (1)</li>
            <li><a class="reference" href="/categories/revdb.html">revdb</a> (1)</li>
            <li><a class="reference" href="/categories/roadmap.html">roadmap</a> (2)</li>
            <li><a class="reference" href="/categories/rpython.html">rpython</a> (1)</li>
            <li><a class="reference" href="/categories/rpyc.html">RPyC</a> (1)</li>
            <li><a class="reference" href="/categories/speed.html">speed</a> (6)</li>
            <li><a class="reference" href="/categories/sponsors.html">sponsors</a> (7)</li>
            <li><a class="reference" href="/categories/sprint.html">sprint</a> (3)</li>
            <li><a class="reference" href="/categories/sprints.html">sprints</a> (1)</li>
            <li><a class="reference" href="/categories/stm.html">stm</a> (14)</li>
            <li><a class="reference" href="/categories/sun.html">sun</a> (1)</li>
            <li><a class="reference" href="/categories/smalltalk.html">Smalltalk</a> (1)</li>
            <li><a class="reference" href="/categories/squeak.html">Squeak</a> (1)</li>
            <li><a class="reference" href="/categories/testing.html">testing</a> (1)</li>
            <li><a class="reference" href="/categories/toy-optimizer.html">toy-optimizer</a> (5)</li>
            <li><a class="reference" href="/categories/unicode.html">unicode</a> (1)</li>
            <li><a class="reference" href="/categories/valgrind.html">valgrind</a> (1)</li>
            <li><a class="reference" href="/categories/vmprof.html">vmprof</a> (3)</li>
            <li><a class="reference" href="/categories/z3.html">z3</a> (5)</li>
          </ul>
        </div></div>
</main>
</div>
<div style="clear: both; width: 75%; margin: 1em auto;">
        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-30.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-28.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
         
                 <footer id="footer"><p>
</p>
<div class="myfooter">
  <div class="logotext">
    © 2025 <a href="mailto:pypy-dev@pypy.org">The PyPy Team</a>
     
    Built with <a href="https://getnikola.com" rel="nofollow">Nikola</a>
     
    Last built 2025-07-07T11:01
  </div>
  <div style="margin-left: auto">
  <a href="../rss.xml">RSS feed</a>
</div>

            
        

    </div>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js" crossorigin="anonymous"></script><script src="../assets/js/styles.js"></script></footer>
</body>
</html>
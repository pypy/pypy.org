<!DOCTYPE html>
<html \ prefix="
        og: http://ogp.me/ns# article: http://ogp.me/ns/article#
    " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A Faster Python">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PyPy (old posts, page 20) | PyPy</title>
<link href="../assets/css/rst_base.css" rel="stylesheet" type="text/css">
<link href="../assets/css/nikola_rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../assets/css/styles.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://www.pypy.org/blog/index-20.html">
<link rel="icon" href="../favicon2.ico" sizes="16x16">
<link rel="icon" href="../favicon32x32.ico" sizes="32x32">
<link rel="prev" href="index-21.html" type="text/html">
<link rel="next" href="index-19.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="../assets/css/tipuesearch.css">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="container">
             <header id="header"><!-- Adapted from https://www.taniarascia.com/responsive-dropdown-navigation-bar --><section class="navigation"><div class="nav-container">
            <div class="brand">
                <a href="../index.html">
                    <image id="toplogo" src="../images/pypy-logo.svg" width="75px;" alt="PyPy/"></image></a>
            </div>
            <nav><ul class="nav-list">
<li> 
                <a href="#!">Features</a>
                <ul class="nav-dropdown">
<li> <a href="../features.html">What is PyPy?</a> </li>  
                    <li> <a href="../compat.html">Compatibility</a> </li>  
                    <li> <a href="../performance.html">Performance</a> </li>  
                </ul>
</li>
          <li> <a href="../download.html">Download</a> </li>  
          <li> <a href="http://doc.pypy.org">Dev Docs</a> </li>  
            <li> 
                <a href="#!">Blog</a>
                <ul class="nav-dropdown">
<li> <a href=".">Index</a> </li>  
                    <li> <a href="../categories/">Tags</a> </li>  
                    <li> <a href="../archive.html">Archive by year</a> </li>  
                    <li> <a href="../rss.xml">RSS feed</a> </li>  
                    <li> <a href="https://morepypy.blogspot.com/">Old site</a> </li>  
                </ul>
</li>
            <li> 
                <a href="#!">About</a>
                <ul class="nav-dropdown">
<li> <a href="https://bsky.app/profile/pypyproject.bsky.social">Bluesky</a> </li>  
                    <li> <a href="https://libera.irclog.whitequark.org/pypy">IRC logs</a> </li>  
                    <li> <a href="https://www.youtube.com/playlist?list=PLADqad94yVqDRQXuqxKrPS5QnVqbDLlRt">YouTube</a> </li>  
                    <li> <a href="https://www.twitch.tv/pypyproject">Twitch</a> </li>  
                    <li> <a href="../pypy-sponsors.html">Sponsors</a> </li>  
                    <li> <a href="../howtohelp.html">How To Help?</a> </li>  
                    <li> <a href="../contact.html">Contact</a> </li>  
                </ul>
</li>

                </ul></nav><div class="nav-mobile">
                <a id="nav-toggle" href="#!"> <span></span></a>
            </div>
        </div>
    </section><div class="searchform" role="search">
                
<form class="navbar-form navbar-left" action="../search.html" role="search">
    <div class="form-group">
        <input type="text" class="form-control" id="tipue_search_input" name="q" placeholder="Search…" autocomplete="off">
</div>
    <input type="submit" value="Local Search" style="visibility: hidden;">
</form>

            </div>
    </header><main id="content"><div class="post">
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/10/speeding-up-json-encoding-in-pypy-8937643890263223898.html" class="u-url">Speeding up JSON encoding in PyPy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/10/speeding-up-json-encoding-in-pypy-8937643890263223898.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-10-27T15:48:00Z" itemprop="datePublished" title="2011-10-27 15:48">2011-10-27 15:48</time></a>
            </p>
                <p class="commentline">13 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi</p>
<p>Recently I spent a bit of effort into speeding up JSON in PyPy. I started with
writing a <a class="reference external" href="https://bitbucket.org/pypy/benchmarks/src/f04d6d63ba60/own/json_bench.py">benchmark</a>, which is admittedly not a very good one, but it's
better than nothing (suggestions on how to improve it are welcome!).</p>
<p>For this particular benchmark, the numbers are as follow. <strong>Note that CPython by
default uses the optimized C extension, while PyPy uses the pure Python one</strong>.
PyPy trunk contains another pure Python version which has been optimized
specifically for the PyPy JIT. Detailed optimizations are described later in
this post.</p>
<p>The number reported is the time taken for the third run, when things are
warmed up. Full session <a class="reference external" href="https://paste.pocoo.org/show/498988/">here</a>.</p>
<table border="1" class="docutils">
<colgroup>
<col width="68%">
<col width="32%">
</colgroup>
<tbody valign="top">
<tr>
<td>CPython 2.6</td>
<td>22s</td>
</tr>
<tr>
<td>CPython 2.7</td>
<td><strong>3.7s</strong></td>
</tr>
<tr>
<td>CPython 2.7 no C extension</td>
<td>44s</td>
</tr>
<tr>
<td>PyPy 1.5</td>
<td>34s</td>
</tr>
<tr>
<td>PyPy 1.6</td>
<td>22s</td>
</tr>
<tr>
<td>PyPy trunk</td>
<td><strong>3.3s</strong></td>
</tr>
</tbody>
</table>
<p>Lessons learned:</p>
<div class="section" id="expectations-are-high">
<h3>Expectations are high</h3>
<p>A lot of performance critical stuff in Python world is already written in a hand
optimized C. Writing C (especially when you interface with CPython C API) is
ugly and takes significant effort. This approach does not scale well when
there is a lot of code to be written or when there is a very tight coupling
between the part to be rewritten and the rest of the code. Still, people would
expect PyPy to be better at "tasks" and not precisely at running equivalent
code, hence a comparison between the C extension and the pure python version
is sound. Fortunately it's possible to outperform the C extension, but requires
a bit of effort on the programmer side as well.</p>
</div>
<div class="section" id="often-interface-between-the-c-and-python-part-is-ugly">
<h3>Often interface between the C and Python part is ugly</h3>
<p>This is very clear if you look at json module as implemented in CPython's
standard library. Not everything is in C (it would probably be just too
much effort) and the interface to what is in C is guided via profiling not
by what kind of interface makes sense. This especially is evident comparing CPython 2.6 to 2.7.
Just adapting the code to an interface with C made the Python version slower.
Removing this clutter improves the readability a lot and improves PyPy's version
a bit, although I don't have hard numbers.</p>
</div>
<div class="section" id="jitviewer-is-crucial">
<h3>JitViewer is crucial</h3>
<p>In case you're fighting with PyPy's performance, <a class="reference external" href="https://bitbucket.org/pypy/jitviewer">jitviewer</a> is worth a shot.
While it's not completely trivial to understand what's going on, it'll
definitely show you what kind of loops got compiled and how.</p>
</div>
<div class="section" id="no-nice-and-fast-way-to-build-strings-in-python">
<h3>No nice and fast way to build strings in Python</h3>
<p>PyPy has a custom thing called <tt class="docutils literal">__pypy__.builders.StringBuilder</tt>. It has
a few a features that make it much easier to optimize than other ways like
<tt class="docutils literal">str.join()</tt> or <tt class="docutils literal">cStringIO</tt>.</p>
<ul class="simple">
<li>You can specify the start size, which helps a lot if you can even provide
a rough estimate on the size of the string (less copying)</li>
<li>Only append and build are allowed. While  the string is being built you
can't seek or do anything else. After it's built you can never append any more.</li>
<li>Unicode version available as well as <tt class="docutils literal">__pypy__.builders.UnicodeBuilder</tt>.</li>
</ul>
</div>
<div class="section" id="method-calls-are-ok-immutable-globals-are-ok">
<h3>Method calls are ok, immutable globals are ok</h3>
<p>PyPy's JIT seems to be good enough for at least the simple cases. Calling
methods for common infrastructure or loading globals (instead of rebinding as
locals) is fast enough and improves code readability.</p>
</div>
<div class="section" id="string-copying-is-expensive">
<h3>String copying is expensive</h3>
<p><b>Edit:</b> see the comment at the end</p>
<p>If you use <tt class="docutils literal">re.sub</tt>, the current implementation will always create a copy
of the string even if there was no match to replace.
If you know your regexp is simple, first try to check if there is
anything to replace. This is a pretty hard optimization to
do automatically -- simply matching the regular expression can be too costly
for it to make sense. In our particular example however, the regexp is really
simple, checking ranges of characters. It also seems that this is by far the
fastest way to escape characters as of now.</p>
</div>
<div class="section" id="generators-are-slower-than-they-should-be">
<h3>Generators are slower than they should be</h3>
<p>I changed the entire thing to simply call <tt class="docutils literal">builder.append</tt> instead of
yielding to the main loop where it would be gathered. This is kind of a PyPy
bug that using generators extensively is slower, but a bit hard to fix.
Especially in cases where there is relatively little data being passed around
(few bytes), it makes sense to gather it first. If I were to implement an
efficient version of <tt class="docutils literal">iterencode</tt>, I would probably handle chunks of
predetermined size, about 1000 bytes instead of yielding data every few bytes.</p>
</div>
<div class="section" id="i-must-admit-i-worked-around-pypy-s-performance-bug">
<h3>I must admit I worked around PyPy's performance bug</h3>
<p>For obscure (although eventually fixable) reasons, this:</p>
<pre class="literal-block">
for c in s: # s is string
  del c
</pre>
<p>is faster than:</p>
<pre class="literal-block">
for c in s:
  pass
</pre>
<p>This is a PyPy performance bug and should be fixed, but on a different branch ;-)</p>
</div>
<div class="section" id="pypy-s-jit-is-good">
<h3>PyPy's JIT is good</h3>
<p>I was pretty surprised, but the JIT actually did make stuff work nicely.
The changes that were done were relatively minor and straightforward, once
the module was cleaned to the normal "pythonic" state.
It is worth noting that it's possible to write code in Python and make it
run really fast, but you have to be a bit careful. Again, jitviewer is your
friend when determining why things are slow. I hope we can write more tools
in the future that would more automatically guide people through potential
performance pitfals.</p>
<p>Cheers,
fijal</p>
<p><b>Edit:</b> I was wrong about re.sub. It just seems to be that the JIT is figuring match better than sub, will be fixed soon</p>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-7574559417447673978">
        <div class="comment-header">
          <a name="comment-7574559417447673978"></a>
            <span class="author">Ian McKellar</span> wrote on <span class="date">2011-10-27 17:20</span>:
        </div>
        <div class="comment-content">
          <p>It would be neat to get UnicodeBuilder and StringBuilder in to mainline Python. They'd be more efficient in CPython than existing string construction methods and it would be easier to write more performant portable Python.</p>
        </div>
      </div>
      <div class="comment comment-8806854308915764944">
        <div class="comment-header">
          <a name="comment-8806854308915764944"></a>
            <span class="author">Yury S</span> wrote on <span class="date">2011-10-27 17:32</span>:
        </div>
        <div class="comment-content">
          <p>Can you elaborate a bit on the slowness of generators?</p>
        </div>
      </div>
      <div class="comment comment-6992109462985946837">
        <div class="comment-header">
          <a name="comment-6992109462985946837"></a>
            <span class="author">Alex</span> wrote on <span class="date">2011-10-27 17:52</span>:
        </div>
        <div class="comment-content">
          <p>Ian: yes it would, python-ideas/dev has had this discussion many times, if you want to convince them of the merit of this idea, feel free to try, but I've gotten weary of this discussion</p>
        </div>
      </div>
      <div class="comment comment-6177106367663044899">
        <div class="comment-header">
          <a name="comment-6177106367663044899"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-27 23:27</span>:
        </div>
        <div class="comment-content">
          <p>This is not meant to derail the rather nice performance numbers, but I wouldn't call the json/simplejson code "pythonic" in the first place.</p>
        </div>
      </div>
      <div class="comment comment-1129306586176255834">
        <div class="comment-header">
          <a name="comment-1129306586176255834"></a>
            <span class="author">Gaëtan de Menten</span> wrote on <span class="date">2011-10-28 07:06</span>:
        </div>
        <div class="comment-content">
          <p>I wonder if using a constant object to dump in each iteration doesn't skew the benchmark in favor of pypy, whereas the jit couldn't optimize as much with a varying object (which is what usually happens in real-life scenarios).</p>
        </div>
      </div>
      <div class="comment comment-1199696093246607701">
        <div class="comment-header">
          <a name="comment-1199696093246607701"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-10-28 07:27</span>:
        </div>
        <div class="comment-content">
          <p>@Gaetan it certainly could in theory. In practice it does not occur here, but I only know that from looking at traces. However, creating a new object each time would make the benchmark more of an object creation one (probably GC related)</p>
        </div>
      </div>
      <div class="comment comment-6674892147338886083">
        <div class="comment-header">
          <a name="comment-6674892147338886083"></a>
            <span class="author">Gaëtan de Menten</span> wrote on <span class="date">2011-10-28 07:42</span>:
        </div>
        <div class="comment-content">
          <p>@Maciej: not if you build the list of objects to dump out of the timed loop, or did I miss something?</p>
        </div>
      </div>
      <div class="comment comment-7242058997833825567">
        <div class="comment-header">
          <a name="comment-7242058997833825567"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-10-28 07:47</span>:
        </div>
        <div class="comment-content">
          <p>True, that might be a bit biggish though. Anyway as I said, it's good enough, JIT does not assume such things are constant. In fact it would execute exactly the same code for similarily shaped objects (different if all objects slightly differ in shape though)</p>
        </div>
      </div>
      <div class="comment comment-7836719841169051889">
        <div class="comment-header">
          <a name="comment-7836719841169051889"></a>
            <span class="author">James Thiele</span> wrote on <span class="date">2011-10-28 16:11</span>:
        </div>
        <div class="comment-content">
          <p>Interfacing Python to C isn't ugly if you use Cython.</p>
        </div>
      </div>
      <div class="comment comment-5978054637190232838">
        <div class="comment-header">
          <a name="comment-5978054637190232838"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-10-28 16:31</span>:
        </div>
        <div class="comment-content">
          <p>That is probably a matter of taste which we should not discuss among gentleman, I however find pure python better than Python-Cython-C combination. Also parsing JSON in C is not fun at all.</p>
        </div>
      </div>
      <div class="comment comment-1870317140367580967">
        <div class="comment-header">
          <a name="comment-1870317140367580967"></a>
            <span class="author">Leonardo Santagada</span> wrote on <span class="date">2011-10-31 19:15</span>:
        </div>
        <div class="comment-content">
          <p>The guys from ultrajson have a benchmark here https://github.com/esnme/ultrajson/blob/master/python/benchmark.py<br><br>and the results are in the README https://github.com/esnme/ultrajson/blob/master/README<br><br>would be interesting to run those benchmarks (of course, first warming up the jit), and comparing the results to ultrajson.</p>
        </div>
      </div>
      <div class="comment comment-1940767933922280029">
        <div class="comment-header">
          <a name="comment-1940767933922280029"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-10-31 20:43</span>:
        </div>
        <div class="comment-content">
          <p>feel free leonardo :)</p>
        </div>
      </div>
      <div class="comment comment-3417137455281533285">
        <div class="comment-header">
          <a name="comment-3417137455281533285"></a>
            <span class="author">Leonardo Santagada</span> wrote on <span class="date">2011-11-01 14:42</span>:
        </div>
        <div class="comment-content">
          <p>It was just a suggestion on how to improve it, like you asked. If it was just going to be ignored I would not have bothered.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/10/pypy-goteborg-post-halloween-sprint-nov-7335004338996313725.html" class="u-url">PyPy Göteborg Post-Hallowe'en Sprint Nov 2nd - Nov 9th</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/10/pypy-goteborg-post-halloween-sprint-nov-7335004338996313725.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-10-17T19:26:00Z" itemprop="datePublished" title="2011-10-17 19:26">2011-10-17 19:26</time></a>
            </p>
            
        </div>
    </header><div class="p-summary entry-summary">
    <p>The next PyPy sprint will be in Gothenburg, Sweden. It is a public sprint,
suitable for newcomers.  We'll focus on making a public kickoff for
both the <a class="reference external" href="https://pypy.org/numpydonate.html">numpy/pypy integration project</a>
and the <a class="reference external" href="https://pypy.org/py3donate.html">Py3k support project</a>,
as well as whatever interests the Sprint attendees.  Since both of these
projects are very new, there will be plenty of work suitable for newcomers
to PyPy.</p>
<p>Other topics might include:</p>
<ul class="simple">
<li>Helping people get their code running with PyPy</li>
<li>work on a FSCons talk?</li>
<li>state of the STM Vinnova project (We most likely, but not for certain will
know whether or not we are approved by this date.)</li>
</ul>
<div class="section" id="other-useful-dates">
<h3>Other Useful dates</h3>
<p><a class="reference external" href="https://www.meetup.com/GothPy/events/32864862/">GothPyCon</a> - Saturday Oct 29.</p>
<p><a class="reference external" href="https://fscons.org/">FSCONS</a> Friday Nov 11 - Sunday Nov 12.</p>
</div>
<div class="section" id="location">
<h3>Location</h3>
<p>The sprint will be held in the apartment of Laura Creighton and Jacob Hallén
which is at Götabergsgatan 22 in Gothenburg, Sweden.  Here is a <a class="reference external" href="https://bit.ly/grRuQe">map</a>.  This is
in central Gothenburg.  It is between the <a class="reference external" href="https://www.vasttrafik.se/en/">tram</a> stops of Vasaplatsen and
Valand, (a distance of 4 blocks) where many lines call -- the 2, 3, 4, 5,
7, 10 and 13.</p>
<p>Probably cheapest and not too far away is to book accomodation at <a class="reference external" href="https://www.sgsveckobostader.se/en">SGS
Veckobostader</a>. The  <a class="reference external" href="https://www.elite.se/hotell/goteborg/park/">Elite Park Avenyn Hotel</a> is a luxury hotel just a
few blocks away. There are scores of hotels a short walk away from the
sprint location, suitable for every budget, desire for luxury, and desire
for the unusual.  You could, for instance, stay on a <a class="reference external" href="https://www.liseberg.se/en/home/Accommodation/Hotel/Hotel-Barken-Viking/">boat</a>.  Options are
too numerous to go into here. Just ask in the mailing list or on the blog.</p>
<p>Hours will be
from 10:00 until people have had enough.  It's a good idea to arrive a
day before the sprint starts and leave a day later.  In the middle of
the sprint there usually is a break day and it's usually ok to take
half-days off if you feel like it.  Of course, many of you may be interested
in sticking around for FSCons, held the weekend after the sprint.</p>
</div>
<div class="section" id="good-to-know">
<h3>Good to Know</h3>
<p>Sweden is not part of the Euro zone. One SEK (krona in singular, kronor
in plural) is roughly 1/10th of a Euro (9.36 SEK to 1 Euro).</p>
<p>The venue is central in Gothenburg.  There is a large selection of
places to get food nearby, from edible-and-cheap to outstanding.  We
often cook meals together, so let us know if you have any food allergies,
dislikes, or special requirements.</p>
<p>Sweden uses the same kind of plugs as Germany. 230V AC.</p>
</div>
<div class="section" id="getting-here">
<h3>Getting Here</h3>
<p>If are coming train, you will arrive at the <a class="reference external" href="https://bit.ly/fON43p">Central Station</a>.  It is
about 12 blocks to the site from there, or you can take a <a class="reference external" href="https://www.vasttrafik.se/en/">tram</a>.</p>
<p>There are two airports which are local to Göteborg, <a class="reference external" href="https://swedavia.se/en/Goteborg/Traveller-information/Traffic-information/">Landvetter</a> (the main
one) and <a class="reference external" href="https://www.goteborgairport.se/eng.asp">Gothenburg City Airport</a> (where some budget airlines fly).
If you arrive at <a class="reference external" href="https://swedavia.se/en/Goteborg/Traveller-information/Traffic-information/">Landvetter</a>  the airport bus stops right downtown at
<a class="reference external" href="https://www.elite.se/hotell/goteborg/park/">Elite Park Avenyn Hotel</a> which is the second stop, 4 blocks from the
Sprint site, as well as the end of the line, which is the <a class="reference external" href="https://bit.ly/fON43p">Central Station</a>.
If you arrive at <a class="reference external" href="https://www.goteborgairport.se/eng.asp">Gothenburg City Airport</a> take the bus to the end of the
line.  You will be at the  <a class="reference external" href="https://bit.ly/fON43p">Central Station</a>.</p>
<p>You can also arrive by <a class="reference external" href="https://www.stenaline.nl/en/ferry/">ferry</a>, from either Kiel in Germany or Frederikshavn
in Denmark.</p>
</div>
<div class="section" id="who-s-coming">
<h3>Who's Coming?</h3>
<p>If you'd like to come, please let us know when you will be arriving and
leaving, as well as letting us know your interests  We'll keep a list
of <a class="reference external" href="https://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/tip/sprintinfo/gothenburg-2011-2/people.txt">people</a> which we'll update (which you can do so yourself if you
have bitbucket pypy commit rights).</p>
</div>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/10/numpy-funding-and-status-update-2380711174693638392.html" class="u-url">Numpy funding and status update</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/10/numpy-funding-and-status-update-2380711174693638392.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-10-12T21:02:00Z" itemprop="datePublished" title="2011-10-12 21:02">2011-10-12 21:02</time></a>
            </p>
                <p class="commentline">7 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi everyone,</p>
<p>It's been a little while since we wrote about NumPy on PyPy, so we wanted to
give everyone an update on what we've been up to, and what's up next for us.</p>
<p>We would also like to note that we're launching a <strong>funding campaign</strong>
for NumPy support in PyPy. Details can be found on the <a class="reference external" href="https://pypy.org/numpydonate.html">donation page</a>.</p>
<p>Some of the things that have happened since last we wrote are:</p>
<ul class="simple">
<li>We added <tt class="docutils literal">dtype</tt> support, meaning you can now create arrays of a bunch of
different types, including bools, ints of a various sizes, and floats.</li>
<li>More array methods and ufuncs, including things like comparison methods
(<tt class="docutils literal">==</tt>, <tt class="docutils literal">&gt;</tt>, etc.)</li>
<li>Support for more and more argument types, for example you can index by a
tuple now (only works with tuples of length one, since we only have
single-dimension arrays thus far).</li>
</ul>
<p>Some of the things we're working on at the moment:</p>
<ul class="simple">
<li>More dtypes, including complex values and user-defined dtypes.</li>
<li>Subscripting arrays by other array as indices, and by bool arrays as masks.</li>
<li>Starting to reuse Python code from the original numpy.</li>
</ul>
<p>Some of the things on the near horizon are:</p>
<ul class="simple">
<li>Better support for scalar data, for example did you know that
<tt class="docutils literal"><span class="pre">numpy.array([True],</span> <span class="pre">dtype=bool)[0]</span></tt> doesn't return a <tt class="docutils literal">bool</tt> object?
Instead it returns a <tt class="docutils literal">numpy.bool_</tt>.</li>
<li>Multi-dimensional array support.</li>
</ul>
<p>If you're interested in helping out, we always love more contributors,
Alex, Maciej, Justin, and the whole PyPy team</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-1486602057604415039">
        <div class="comment-header">
          <a name="comment-1486602057604415039"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-12 23:34</span>:
        </div>
        <div class="comment-content">
          <p>What is the best way to contact people about this? Our company has some interest in sponsporing this work, but it wasn't clear from this or the donations page how to actually talk to anyone about it. Maybe I'm missing the obvious.</p>
        </div>
      </div>
      <div class="comment comment-6327341407093374062">
        <div class="comment-header">
          <a name="comment-6327341407093374062"></a>
            <span class="author">Alex</span> wrote on <span class="date">2011-10-12 23:53</span>:
        </div>
        <div class="comment-content">
          <p>Anonymous: The address to contact is "pypy at sfconservancy.org". Thanks!</p>
        </div>
      </div>
      <div class="comment comment-6071959358283494890">
        <div class="comment-header">
          <a name="comment-6071959358283494890"></a>
            <span class="author">stan</span> wrote on <span class="date">2011-10-13 00:14</span>:
        </div>
        <div class="comment-content">
          <p>Yay!  Time to put my money where my mouth is.  :)</p>
        </div>
      </div>
      <div class="comment comment-8000386125780470171">
        <div class="comment-header">
          <a name="comment-8000386125780470171"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-14 07:31</span>:
        </div>
        <div class="comment-content">
          <p>What does it mean "Starting to reuse Python code from the original numpy"? If it is copy-paste and something will be changed in numpy git trunc, will it be automatically taken into account by your numpy for PyPy?</p>
        </div>
      </div>
      <div class="comment comment-5531987022607322289">
        <div class="comment-header">
          <a name="comment-5531987022607322289"></a>
            <span class="author">Luis</span> wrote on <span class="date">2011-10-15 04:05</span>:
        </div>
        <div class="comment-content">
          <p>This is off topic but, congratulations! You already achieved Unladen Swallow's performance goal of 5x faster than cpython on average.<br><br>https://code.google.com/p/unladen-swallow/wiki/ProjectPlan#Performance<br><br>https://speed.pypy.org/</p>
        </div>
      </div>
      <div class="comment comment-169207815832234648">
        <div class="comment-header">
          <a name="comment-169207815832234648"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-17 08:51</span>:
        </div>
        <div class="comment-content">
          <p>You probably have already seen that, but there is an interesting comment from Travis Oliphant about the porting of numpy to pypy :<br><br>https://technicaldiscovery.blogspot.com/2011/10/thoughts-on-porting-numpy-to-pypy.html</p>
        </div>
      </div>
      <div class="comment comment-6227233178167802200">
        <div class="comment-header">
          <a name="comment-6227233178167802200"></a>
            <span class="author">D</span> wrote on <span class="date">2011-10-17 10:50</span>:
        </div>
        <div class="comment-content">
          <p>You haven't answered my question about reuse numpy code for 3 days, I guess because you don't know it overall. I'm not 100% agree with neither Travis opinion nor Stefan M comment from https://morepypy.blogspot.com/2011/09/py3k-for-pypy-fundraiser.html , but in answer to Stefan M you say "Since this is open source, people either work on what they like, because it's fun or scratches their itch" and "Improving the [C extensions] support is boring and frustrating". Guys, AFAIK you received FP7 support for developing some soft for users, not for fun. You should spend some efforts for boring yet important work toward the mentioned things, if you would like to obtain further increase of users number and finance support. Also, clarification about reusing CPython numpy code is also highly appreciated.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/10/more-compact-lists-with-list-strategies-8229304944653956829.html" class="u-url">More Compact Lists with List Strategies</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/10/more-compact-lists-with-list-strategies-8229304944653956829.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-10-11T11:25:00Z" itemprop="datePublished" title="2011-10-11 11:25">2011-10-11 11:25</time></a>
            </p>
                <p class="commentline">16 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Since we come closer to merging the list-strategy branch I want to try to explain this memory optimization today. </p>
<p>Datatypes in PyPy are stored as <tt>W_&lt;type&gt;Objects</tt> (e.g. <tt>W_StringObject to represent strings, W_IntObject</tt> to represent ints). This is necessary due to the dynamic nature of Python. So the actual value (e.g. string, integer) is stored inside that box, resulting in an indirection. When having a large amount of such boxed objects, for example in a list, the wasted memory can become quite large. </p>
<a href="https://3.bp.blogspot.com/-0-qMMmAxRro/TpQoKo728xI/AAAAAAAAAYA/Z5nNNEYYquk/s1600/overhead_before.png"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5662194794763842322" src="https://3.bp.blogspot.com/-0-qMMmAxRro/TpQoKo728xI/AAAAAAAAAYA/Z5nNNEYYquk/s400/overhead_before.png" style="display: block; margin: 0px auto 10px; text-align: center; cursor: pointer; cursor: hand; width: 400px; height: 183px;"></a>  <p>If you have a closer look at such lists, you will see that in many of them only one type of data is stored and only few (and smaller) lists store mixed types. Another thing to observe is that those lists often won't change the types of the objects they contain at runtime very often. For instance a list of a million integers is very unlikely to suddenly get a string appended to it. </p>
<h2>List Strategies</h2>
<p>The goal of this work is to write an optimization that exploits this behaviour. Instead of wrapping all items in a list, we implement lists in a way that they are optimized for storing certain (primitive) datatypes. These implementations store the content of the list in unwrapped form, getting rid of the extra indirection and wrapper objects. </p>
<p>One approach would be to add a level of indirection, making each <tt>W_ListObject</tt> instance point to another object that stores the actual content. For this other object, several implementations would exist, for every datatype we want to store without wrapping it (as well as a general one that deals with arbitrary content). The data layout would look something like this:</p>
<a href="https://3.bp.blogspot.com/-LWtyy4ORb00/TpQohbOjg2I/AAAAAAAAAYM/kgpVemG8-9o/s1600/with_special_impl.png"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5662195186221155170" src="https://3.bp.blogspot.com/-LWtyy4ORb00/TpQohbOjg2I/AAAAAAAAAYM/kgpVemG8-9o/s400/with_special_impl.png" style="display: block; margin: 0px auto 10px; text-align: center; cursor: pointer; cursor: hand; width: 400px; height: 193px;"></a>  <p>This approach has the problem that we need two indirections to get to the data and that the implementation instances need memory themselves.</p>
<p>What we would like to do is to make the <tt>W_ListObject</tt> point to an RPython list directly, that contains either wrapped or unwrapped data. This plan has the problem that storing different unwrapped data is not directly possible in RPython.  </p>
<p>To solve the problem, we use the <tt>rerased</tt> RPython library module. It allows us to erase the type of an object, in this case lists, and returns something similar to <tt>void-star</tt> in C, or <tt>Object</tt> in Java. This object is then stored on the <tt>W_ListObject</tt> in the field <tt>storage</tt>. If we want to work with the list, for example to append or delete items, we need to unerase the storage again.</p>
<p>Example for rerase: </p>
<pre>storage = erase([1 ,2 ,3 ,4])
# storage is an opaque object that you can do nothing with
....
l = unerase(storage)
l.clear()
</pre>
<p>Now that we know how to make the <tt>W_ListObject</tt> point directly to wrapped or unwrapped data, we need to find out how to actually do any operations on this data. This can be accomplished by adding another field to our <tt>W_ListObject</tt>. This field points to a <tt>ListStrategy</tt> object. The actual implementation of <tt>W_ListObject</tt> is now deferred to those <tt>ListStrategy</tt> classes. For instance, a <tt>W_ListObject</tt> which holds only integers will use the <tt>IntegerListStrategy</tt>.</p>
<p>When the type of content is being changed, we need to change the used strategy as well as the storage in compatible ways. For example when we add a string to the list of integers we need to switch to the <tt>ObjectListStrategy</tt> and change the storage to be a list of wrapped objects. Thus the currently used strategy always knows what to do with what is currently in the storage.</p>
<a href="https://4.bp.blogspot.com/-hFXLNQ0Ry0I/TpQohnZHRpI/AAAAAAAAAYY/-AUuRfoFYqA/s1600/with_strategies.png"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5662195189486667410" src="https://4.bp.blogspot.com/-hFXLNQ0Ry0I/TpQohnZHRpI/AAAAAAAAAYY/-AUuRfoFYqA/s400/with_strategies.png" style="display: block; margin: 0px auto 10px; text-align: center; cursor: pointer; cursor: hand; width: 400px; height: 130px;"></a>  <p>As you can see, we now save one level of indirections by storing some of the data unwrapped. Of course each operation on a list needs to go via the strategy, but since we save one indirection for each element stored in that list and the <tt>Strategy</tt> classes are singletons, the benefits outweigh the costs.</p>
<p>Currently there are only strategies for integers and strings since many lists seem to have these datatypes. Other strategies i.e for floats and unicode strings are planned. We also implemented two special strategies for empty lists and range-lists. The <tt>EmptyListStrategy</tt>'s storage is <tt>None</tt>. If objects are added to the list we just switch to the appropriate strategy (determined by the item's type). <tt>RangeListsStrategies</tt> do not store any items at all. Instead they only store values describing the range of the list, i.e. start, step and length. On any operations that changes the data of the list we switch to the <tt>IntegerStrategy</tt>.</p>
<p>A nice side-effect of storing unwrapped datatypes is that we can implement optimized methods for certain cases. For instance, since comparison of unwrapped integers is now much faster than comparison between arbitrary objects, we can rewrite the sorting methods for lists containing integers.</p>
<h2>Microbenchmarks</h2>
<p>Finally here is an early overview of the memory consumption of different Python implementations: <tt>CPython, PyPy</tt> and <tt>PyPy-list</tt> which uses list-strategies. To demonstrate how powerful list-strategies can be in the best case, we wrote benchmarks that create a list of integers, a list of strings and a range-list each with one million elements each and then reads out the heap size of the process as reported by the OS. </p>
<p>The results are as follows: </p>
<a href="https://2.bp.blogspot.com/-FG6r9y8tXF4/TpQohzPkeNI/AAAAAAAAAYk/h-oZpthkFEQ/s1600/osheapsize.png"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5662195192667863250" src="https://2.bp.blogspot.com/-FG6r9y8tXF4/TpQohzPkeNI/AAAAAAAAAYk/h-oZpthkFEQ/s400/osheapsize.png" style="display: block; margin: 0px auto 10px; text-align: center; cursor: pointer; cursor: hand; width: 400px; height: 252px;"></a>   <p>The savings on integers and strings in this ideal case are quite big.</p>
<p>The benchmark for range-lists is a little unfair, since in <tt>CPython</tt> one could accomplish the same memory behaviour using <tt>xrange</tt>. However, in PyPy users won't notice that internally the list does not store all items, making it still possible to use all list methods, such as <tt>append</tt> or <tt>delete</tt>.</p>
<h2>Conclusion</h2>
<p>We hope that list strategies bring memory savings for applications that use homogeneous lists of primitive types. Furthermore, operations on such lists tend to be somewhat faster as well. This also integrates well with the JIT. The list strategies optimizations will be merged to the PyPy's default branch at some point in the next months. An equivalent optimization for dictionaries has already been merged (and is part of PyPy 1.6), one for sets is coming in the future.</p>
<p>Lukas Diekmann and Carl Friedrich Bolz</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-2876446533628197895">
        <div class="comment-header">
          <a name="comment-2876446533628197895"></a>
            <span class="author">Winston Ewert</span> wrote on <span class="date">2011-10-11 13:10</span>:
        </div>
        <div class="comment-content">
          <p>Nice.<br><br>But isn't there a small change in semantics to do that? If a push a python int object onto a list and then pop it back off I'll have the exact same object. But if you unwrap the object and store it as a plain int and then repop it I don't have the exact same object. I've a got a new object.</p>
        </div>
      </div>
      <div class="comment comment-8682412757584252996">
        <div class="comment-header">
          <a name="comment-8682412757584252996"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-11 13:20</span>:
        </div>
        <div class="comment-content">
          <p>It seems to be very nice.<br><br>By the way, are object attributes optimized the same way? Objects of the same class can be expected to frequently store data of the same type in the same attribute.<br>I've found a nearly-year-old post on maps ( https://morepypy.blogspot.com/2010/11/efficiently-implementing-python-objects.html ), but it does not mention attribute value types... has this idea been considered?</p>
        </div>
      </div>
      <div class="comment comment-4052963990889114210">
        <div class="comment-header">
          <a name="comment-4052963990889114210"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2011-10-11 13:25</span>:
        </div>
        <div class="comment-content">
          <p>I can see float support presenting some interesting challenges being emblematic of a wider issue.  It would be very easy for someone to have a list of "floats" but if they populated it with any literals, most likely they'll be integer literals, missing any of the float optimization.<br><br>For most apps this won't be a problem but if someone is trying to optimize their application they might see this as a performance heisenbug.  For example they write a hard coded list and it is slow, read it from a file and it is fast.<br><br>One approach is for there to be a document on some website (that gets out of date) that lists PyPy micro-optimizations.  Someone would then need continually audit their code against that list.  This doesn't seem practical.<br><br>I've seen posted some low level visualization tools.  I'd be curious how practical it would be to have a higher level profiler tool integrate with the JIT to detect patterns like the list of mixed float/int situation to flag these micro-optimizations in a more automated fashion.</p>
        </div>
      </div>
      <div class="comment comment-6391255579021902201">
        <div class="comment-header">
          <a name="comment-6391255579021902201"></a>
            <span class="author">Alex</span> wrote on <span class="date">2011-10-11 13:27</span>:
        </div>
        <div class="comment-content">
          <p>Winston: Indeed, very clever of you to notice :)  However, we noticed as well, going forward integers (and other primitives) identity will be a function of their value, not the identity of their box.  This means that for all ints `i is x` if and only if `i == x`.  This also means that `id()` is now a function of value for primitives.  Don't rely on that though!  Just like we don't want people relying on `i is x` if `i == x and -100 &lt; i &lt; 200`, we don't want people relying on this either.<br><br>Anonymous:<br><br>Yes, this is definitely a consideration, I keep meaning to make time to work on this.</p>
        </div>
      </div>
      <div class="comment comment-3503256972437840441">
        <div class="comment-header">
          <a name="comment-3503256972437840441"></a>
            <span class="author">evilpies</span> wrote on <span class="date">2011-10-11 14:08</span>:
        </div>
        <div class="comment-content">
          <p>Well interesting, SpiderMonkey is considering to implement something like this, because NaN-boxing usually wastes a lot of memory.</p>
        </div>
      </div>
      <div class="comment comment-9065215242356577912">
        <div class="comment-header">
          <a name="comment-9065215242356577912"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-10-11 19:52</span>:
        </div>
        <div class="comment-content">
          <p>@Ed I think float list can accomodate a limited set of integer values (those that can be represented correctly when interpreted as float) without any issue. You would then however need to tag which one is integer and which one is float, having to keep a bitmap. That's certainly possible, but a bit of a mess.</p>
        </div>
      </div>
      <div class="comment comment-3153874564188090101">
        <div class="comment-header">
          <a name="comment-3153874564188090101"></a>
            <span class="author">Alex</span> wrote on <span class="date">2011-10-11 20:23</span>:
        </div>
        <div class="comment-content">
          <p>fijal: I think better than obscure hacks like a bitmap allowing integers as floats, perhaps it would be better just to eventually have logging of when you get fallbacks like that. For eventual integration with the jitviewer of course :)</p>
        </div>
      </div>
      <div class="comment comment-5501196602760962078">
        <div class="comment-header">
          <a name="comment-5501196602760962078"></a>
            <span class="author">Winston Ewert</span> wrote on <span class="date">2011-10-11 20:54</span>:
        </div>
        <div class="comment-content">
          <p>A general runtime warning system that could say things like: "list of floats decaying to list of objects because of adding int", "two ints being compared via is", etc. might be useful. That could handle any number of situations with surprising semantics or performance.</p>
        </div>
      </div>
      <div class="comment comment-8789804036786317191">
        <div class="comment-header">
          <a name="comment-8789804036786317191"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-11 21:47</span>:
        </div>
        <div class="comment-content">
          <p>This is very interesting. I have been thinking along somewhat similar lines for a while (but for performance reasons, rather than memory size), and so have already reviewed how I use lists in my own code. In my own programs, having non-uniform data types in a list is extremely rare. However, some lists are lists of lists (or tuples or dictionaries). The most common large lists however tend to be lists of strings.<br><br>1) If I correctly understand your explanation of what you are doing, your "list strategies" are effectively marking uniform lists as being either of one of a few known basic types (e.g. IntegerListStrategy), or just a traditional list of objects. Is that correct?<br><br>2) Do you think there are any meaningful performance optimsations which could be gained when the list type is known in advance? <br><br>3) What about built-in functions such as all(), any(), len(), min(), max(), etc? Would they be able to make use of this to improve their performance? <br><br>4) Would the underlying array data format be exposed for people who want to write extensions making direct use of it (e.g. for things like SIMD libraries)? <br><br>5) Could this allow a list to be in shared memory and directly accessed by another program?<br><br>6) Would the new list format be compatible with a "memoryview" (as str and bytearray are)?<br><br>7) My own earlier thoughts had involved marking a list as being of a uniform or non-uniform data when the list is created or altered, and using optimised code for the expected type for uniform lists. One sticky point however was threading, as a different type could be appended in another thread, which means that the consuming function would have to somehow be aware of this. Would your concept have a problem with threading if appending a string to an integer list suddenly required changing the underlying list strategy while another thread was accessing the same list?<br><br>8) Python 3.x seems to favour iterators over creating lists (e.g. map, filter, range are replaced by what used to be imap, ifilter, and xrange), and generators were introduced to complement list comprehensions in order to save memory. Does this have any implications for what you are doing?<br><br>9) Could your list concept be applied by the CPython developers to CPython? This might help ensure that any subtle semantic issues which arise as a result apply equally to CPython, rather than having people call them "Pypy bugs". <br><br>10) What about changing the Python language semantics to allow a user to specify that a list must be of a specific uniform type, and raising a type error if an element(s) of an unexpected type is added to the list? This is actually a language feature that I would like to have in order to catch errors without having to write code to examine each individual data element (as that can be slow and error prone in itself). <br><br>11) Finally, why is there such a large start-up memory use in your micro-benchmarks when comparing Pypy-list to CPython? Is this just general overhead from Pypy itself, or is that due to something related to converting the list format to a particular "list strategy"?</p>
        </div>
      </div>
      <div class="comment comment-4327769528149421078">
        <div class="comment-header">
          <a name="comment-4327769528149421078"></a>
            <span class="author">Alex</span> wrote on <span class="date">2011-10-11 23:14</span>:
        </div>
        <div class="comment-content">
          <p>Anonymous: Wow a lot of questions, I'll try to answer them :)<br><br>1) Yes.<br><br>2) Probably not, you get the most performance gains when you have a large list, and if it's large the very-very-very-small initial transition is amortized over many elements.<br><br>3) Many of those are pure-python and will thus automatically gain these benefits, max() and min() unfortunately are not.<br><br>4) Probably not, we don't expose this data in any other place nor do we have any APIs for it.<br><br>5) I suppose in theory, again we have no API for it.<br><br>6) No, it wouldn't be, since that's not a part of the list API.  We don't define the language, we just implement it (faster).<br><br>7) No, there's no problem with this, you simply need to lock (or whatever the equivilant in STM) is the list and do the modifications.<br><br>8) No, I don't think it does.<br><br>9) Yes, it could be applied to CPython with slightly more difficulty, and it would see the memory gains.  However, it would see performance losses (as you do with teh array module on CPython) because it would need to box/unbox at every iteraction, whereas teh JIT is able to remove that.<br><br>10) Propose it to python-ideas, we don't define the language.<br><br>11) I can't understand the charts, so I can't answer this one.</p>
        </div>
      </div>
      <div class="comment comment-4149801701855589450">
        <div class="comment-header">
          <a name="comment-4149801701855589450"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-12 03:15</span>:
        </div>
        <div class="comment-content">
          <p>Alex: I'm the anonymous with all the questions. Thank you for your detailed answers. I completely understand that there are side issues that you don't want to deal with at this time. <br><br>As for the possible performance effects of the proposed new list data format if applied to CPython, doing the operation: "y = reduce(operator.add, x, 0)" where x is either a list or array of 1,000,000 integers does not seem to produce a measurable difference in speed for me (Python 2.6 on 64 bit Ubuntu). Any differences seem to go either way when the test is repeated, so they seem equivalent within the margin of error. An equivalent for loop yields the same result (except for being slower, of course). <br><br>When extracting or replacing slices for lists and arrays (e.g. "y = x[i:i + 50]" and "x[i:i + 50] = y") within a for loop, the array version seems to be significantly *faster* than the list version for large slices (e.g. 50), and approximately the same for small slices (e.g. 5). <br><br>Theoretically, yes the implementation with array should always be slower, but I can't seem to get that result when I attempt to measure it. Perhaps I'm doing something wrong, but it appears from the (admittedly minimal) testing that I have done that significant speed penalties for CPython cannot simply be assumed. <br><br>I realize that ultimately this isn't a matter for the Pypy developers to concern themselves with, but should the question ever arise I don't think it can be dismissed out of hand.</p>
        </div>
      </div>
      <div class="comment comment-8827061726108938667">
        <div class="comment-header">
          <a name="comment-8827061726108938667"></a>
            <span class="author">Carl Friedrich Bolz-Tereick</span> wrote on <span class="date">2011-10-12 08:18</span>:
        </div>
        <div class="comment-content">
          <p>Some additional thoughts to @Anonymous questions:<br><br><i>3) What about built-in functions such as all(), any(), len(), min(), max(), etc? Would they be able to make use of this to improve their performance?</i><br><br>len does not depend on the content of the list, so it does not win. all, any, min and max could be improved, yes.<br><br><i>7) My own earlier thoughts had involved marking a list as being of a uniform or non-uniform data when the list is created or altered, and using optimised code for the expected type for uniform lists. One sticky point however was threading, as a different type could be appended in another thread, which means that the consuming function would have to somehow be aware of this. Would your concept have a problem with threading if appending a string to an integer list suddenly required changing the underlying list strategy while another thread was accessing the same list?</i><br><br>The JIT does indeed produce special optimized code for the type of list it is currently observing, making operations faster. The fact that another thread could change the type of the list is not a problem, because we have a GIL and thus the JIT knows at which points another thread can run.<br><br><i>10) What about changing the Python language semantics to allow a user to specify that a list must be of a specific uniform type, and raising a type error if an element(s) of an unexpected type is added to the list? This is actually a language feature that I would like to have in order to catch errors without having to write code to examine each individual data element (as that can be slow and error prone in itself).</i><br><br>this already exists. it's called the array module.<br><br><i>11) Finally, why is there such a large start-up memory use in your micro-benchmarks when comparing Pypy-list to CPython? Is this just general overhead from Pypy itself, or is that due to something related to converting the list format to a particular "list strategy"?</i><br><br>The higher startup memory is also there in the PyPy without list strategies, so those have nothing to do with it.</p>
        </div>
      </div>
      <div class="comment comment-571856896041855629">
        <div class="comment-header">
          <a name="comment-571856896041855629"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-13 09:54</span>:
        </div>
        <div class="comment-content">
          <p>I too had trouble understanding the chart. The vertical axis doesn't have negative numbers to represent a delta, just ignore the signs.<br><br>The blue area is an algebraically positive area, representing the startup memory use. The yellow area represents the memory use delta after doing the 1e6 items list operations.</p>
        </div>
      </div>
      <div class="comment comment-5862200887140172974">
        <div class="comment-header">
          <a name="comment-5862200887140172974"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-10-19 13:26</span>:
        </div>
        <div class="comment-content">
          <p>Re list of floats-and-ints: a fully compatible way is to use the NaN-tagging idea from SpiderMonkey, i.e. have a special encoding of NaN that is normally not used, and that still leaves 32 bits of extra information.  We would then represent ints in the list as such a NaN-encoded float.  (At least it works as long as the integer is not too large, on 64-bit platforms.)</p>
        </div>
      </div>
      <div class="comment comment-2022265183145067816">
        <div class="comment-header">
          <a name="comment-2022265183145067816"></a>
            <span class="author">Ole Laursen</span> wrote on <span class="date">2011-11-18 14:55</span>:
        </div>
        <div class="comment-content">
          <p>Neat!<br><br>Nice work people. I'm amazed it's so simple do to afterall, just switch type based on what the first element is. It must be a big boon for garbage collection, too?</p>
        </div>
      </div>
      <div class="comment comment-7316745693596613762">
        <div class="comment-header">
          <a name="comment-7316745693596613762"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-12-16 14:07</span>:
        </div>
        <div class="comment-content">
          <p>The benchmark measures virtual memory (don't know on which architecture); measuring RSS would be more representative of the actual amount of RAM spent storing the data. Presumably it would also be more favourable to PyPy, since moving garbage collection doubles the amount of virtual memory.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/09/py3k-for-pypy-fundraiser-8139653689520709617.html" class="u-url">Py3k for PyPy fundraiser</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/alex.html">Alex</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/09/py3k-for-pypy-fundraiser-8139653689520709617.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-09-21T17:44:00Z" itemprop="datePublished" title="2011-09-21 17:44">2011-09-21 17:44</time></a>
            </p>
                <p class="commentline">21 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi,</p>
<p>We would like to announce a donation campaign for implementing Python 3 in PyPy.<br>
Please read our <a class="reference external" href="https://pypy.org/py3donate.html">detailed plan</a> for all the details and donate using the<br>
button on that page!</p>
<p>Thanks,<br>
The PyPy Team</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-3089193310633939817">
        <div class="comment-header">
          <a name="comment-3089193310633939817"></a>
            <span class="author">stan</span> wrote on <span class="date">2011-09-21 18:21</span>:
        </div>
        <div class="comment-content">
          <p>Two comments:<br><br>1. It would be really nice to see a semi-frequently updated progress bar (live is best) with # of dollars and # of contributions for the fundraising.<br><br>Part of the excitement created by sites like Kickstarter (and the Humble Indie Bundle and so on) is seeing how your small contribution adds to the whole.  A donate button feels more like throwing your money into a dark hole (a very reasonable and worthwhile hole, but a hole nonetheless).  Take advantage of some video game psychology and give us that "level up" feedback when we contribute!  :)<br><br>2.  I know you don't want to oversubscribe yourselves, but would you consider doing a similar funding drive for Numpy support?  PLEASE???</p>
        </div>
      </div>
      <div class="comment comment-8464946697175791797">
        <div class="comment-header">
          <a name="comment-8464946697175791797"></a>
            <span class="author">Konstantine Rybnikov</span> wrote on <span class="date">2011-09-21 18:55</span>:
        </div>
        <div class="comment-content">
          <p>Totally agree with stan about progress bar. Recent novacut's donation campaign showed importance of that a lot (since people saw that they need to hurry up with fundings and did lots of them in last couple of days).</p>
        </div>
      </div>
      <div class="comment comment-4877998611475320449">
        <div class="comment-header">
          <a name="comment-4877998611475320449"></a>
            <span class="author">Carl Friedrich Bolz-Tereick</span> wrote on <span class="date">2011-09-21 19:30</span>:
        </div>
        <div class="comment-content">
          <p>@stan: 1. progress bar will be coming soon<br><br>2. we are actively working on putting up an equivalent page for Numpy support.</p>
        </div>
      </div>
      <div class="comment comment-8432905225916651537">
        <div class="comment-header">
          <a name="comment-8432905225916651537"></a>
            <span class="author">stan</span> wrote on <span class="date">2011-09-21 19:45</span>:
        </div>
        <div class="comment-content">
          <p>Awesome!  I want to be first in line to pitch $50 into the Numpy jar.</p>
        </div>
      </div>
      <div class="comment comment-8490701406282192859">
        <div class="comment-header">
          <a name="comment-8490701406282192859"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-09-21 21:03</span>:
        </div>
        <div class="comment-content">
          <p>Awesome! Infact I regard Python 3 as much more important as any other features you could add now. 10% more performance is not nearly in the same league as Python3 support. Will happily spend some money on this.</p>
        </div>
      </div>
      <div class="comment comment-2478100986808283531">
        <div class="comment-header">
          <a name="comment-2478100986808283531"></a>
            <span class="author">João Bernardo</span> wrote on <span class="date">2011-09-22 00:01</span>:
        </div>
        <div class="comment-content">
          <p>Great!! I was waiting for that</p>
        </div>
      </div>
      <div class="comment comment-6731633771045470768">
        <div class="comment-header">
          <a name="comment-6731633771045470768"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-09-22 01:03</span>:
        </div>
        <div class="comment-content">
          <p>For complete support thats like 200,000$. I understand it's a willing feature, but I don't think the pypy community and followers are that huge.<br><br>Btw, nice getting all the benchmarks above CPython 2.6 :)</p>
        </div>
      </div>
      <div class="comment comment-8893976926227350737">
        <div class="comment-header">
          <a name="comment-8893976926227350737"></a>
            <span class="author">Sojin</span> wrote on <span class="date">2011-09-22 05:41</span>:
        </div>
        <div class="comment-content">
          <p>Great work guys! I think keeping this amazing project alive is important for the Python eco-system... Here comes my $$.</p>
        </div>
      </div>
      <div class="comment comment-6018471169718133969">
        <div class="comment-header">
          <a name="comment-6018471169718133969"></a>
            <span class="author">Laurent</span> wrote on <span class="date">2011-09-22 14:56</span>:
        </div>
        <div class="comment-content">
          <p>I've heard that Py3K support for PyPy will be implemented in Python 2.X anyway. Is that true?</p>
        </div>
      </div>
      <div class="comment comment-2614291405405734774">
        <div class="comment-header">
          <a name="comment-2614291405405734774"></a>
            <span class="author">Antonio Cuni</span> wrote on <span class="date">2011-09-22 16:49</span>:
        </div>
        <div class="comment-content">
          <p>@Laurent: to be more precise, py3k will be implemented in RPython, which is indeed a subset of Python 2.<br><br>Right now we don't have any plan to port RPython to Python 3: it's not a priority and it won't give any advantage to the PyPy end users.</p>
        </div>
      </div>
      <div class="comment comment-3805180719367964919">
        <div class="comment-header">
          <a name="comment-3805180719367964919"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-09-23 00:31</span>:
        </div>
        <div class="comment-content">
          <p>cpython3 is a fork of cpython2, but here you intend to support both versions with the same codebase. Does not this make the task much harder, and peeking into cpython3 code for guidance less useful? Also, isn't it possible that the resulting large set of switch (PYVERSION) {} statements will make the code less readable and maintainable?<br><br>Anyway, I have full faith in your assessment of the best approach, but I am still interested in your explanation. :)</p>
        </div>
      </div>
      <div class="comment comment-8855990968340010007">
        <div class="comment-header">
          <a name="comment-8855990968340010007"></a>
            <span class="author">Zinahe</span> wrote on <span class="date">2011-09-30 16:25</span>:
        </div>
        <div class="comment-content">
          <p>Just made my donation. GOD SPEED.<br><br>I second stan's idea of providing a progress bar showing the overall status of the fundraising.</p>
        </div>
      </div>
      <div class="comment comment-3274650683691001173">
        <div class="comment-header">
          <a name="comment-3274650683691001173"></a>
            <span class="author">Harald Armin Massa</span> wrote on <span class="date">2011-09-30 21:28</span>:
        </div>
        <div class="comment-content">
          <p>a) please, please get the pages lac showed in her lightning talk at pycon.uk online.<br>   - There are pictures of people in it, and it is easier to donate to people then to something abstract<br>   - there is text what happened<br>   - there is text that anonymous donation is possible<br><br>b) please, work on the feedback. It is CRUCIAL to show the actual state. Giving 5€ and nothing happens is dull. Giving 5€ and a number goes up - good. Giving 500€ and a rendered bar moves a pixel - awesome!<br><br>c) I found the Python3PyPy fundraiser easily. I did not find the numpypy fundraiser. Please, put lacs pages up :) if I can vote for them somewhere, please let me know.</p>
        </div>
      </div>
      <div class="comment comment-37089018487832099">
        <div class="comment-header">
          <a name="comment-37089018487832099"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-09-30 21:36</span>:
        </div>
        <div class="comment-content">
          <p>@Harald poke lac harder so she deploys it :)</p>
        </div>
      </div>
      <div class="comment comment-6252769116955241797">
        <div class="comment-header">
          <a name="comment-6252769116955241797"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-05 01:04</span>:
        </div>
        <div class="comment-content">
          <p>It's been a couple of weeks and the progress bar still isn't there. Although there is a link for it that doesn't work.<br>Please fix this and make it visible without having to click anything.</p>
        </div>
      </div>
      <div class="comment comment-4089283027769679528">
        <div class="comment-header">
          <a name="comment-4089283027769679528"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-06 23:45</span>:
        </div>
        <div class="comment-content">
          <p>Hi! Please create the same kind of bucket for numpy support. I'm a big fan of Py3k, but I'm an even bigger fan of numpy - and I need it for my work. I'll donate to Py3k now, but I'll donate a bigger sum to both when I see the new bucket.</p>
        </div>
      </div>
      <div class="comment comment-5450762748390376121">
        <div class="comment-header">
          <a name="comment-5450762748390376121"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-10-07 00:10</span>:
        </div>
        <div class="comment-content">
          <p>We're waiting for the final ok of the proposal so it can be said it benefits the public good. Any day now :)</p>
        </div>
      </div>
      <div class="comment comment-3612110361107424076">
        <div class="comment-header">
          <a name="comment-3612110361107424076"></a>
            <span class="author">Stefan M</span> wrote on <span class="date">2011-10-08 19:48</span>:
        </div>
        <div class="comment-content">
          <p>* Who needs Python 3 support??? *<br><br>It looks like the PyPy project is adding things just to improve something and keep doing something but for who's sake?<br><br>I really need is proper support for C extensions. Without it, people who use Python professionally like myself, cannot switch to PyPy and we are stuck with Cython and/or Psyco.<br><br>Who steers the development of Pypy and why would these people refuse to realize what hinders thousands of developers, who would love to use Pypy to make the switch from CPython ???<br><br>Please tell me which real software projects use PyPy and for what reason they would need Py3K support!<br><br><br>Go ahead and add more language constructs that you can use to run academic programs even faster and keep ignoring what is really necessary to push Pypy into day-to-day usability<br><br>(* frustrated *)</p>
        </div>
      </div>
      <div class="comment comment-507810899964569215">
        <div class="comment-header">
          <a name="comment-507810899964569215"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-10-08 20:11</span>:
        </div>
        <div class="comment-content">
          <p>Hi Stefan. There is noone who steers direction in PyPy. Since this is open source, people either work on what they like, because it's fun or scratches their itch. Note that Python 3 work is something that people expressed interest in funding -- if they fund it enough, why wouldn't developers work on it? It's more interesting than most jobs.<br><br>With regard to C extensions - it's good enough for many people, like quora to run on PyPy. Improving the support is boring and frustrating, so I don't think anyone would be willing to invest significant amount of his *free time* into that. However, feel free to speak with your money, you know how to find me.<br><br>Cheers,<br>fijal</p>
        </div>
      </div>
      <div class="comment comment-3257059391253095898">
        <div class="comment-header">
          <a name="comment-3257059391253095898"></a>
            <span class="author">Stefan M</span> wrote on <span class="date">2011-10-13 03:46</span>:
        </div>
        <div class="comment-content">
          <p>Hi Maciej,<br><br>I realize that I came across in a somewhat obnoxious way. Sorry for that - I simply did not realize that PyPy is a true hobbyist project (at least currently).<br>I wish I could contribute funding but though I am using Python a lot at work, we are a National Lab and struggling to keep our government funding ourselves.<br><br>I hope a deep-pocket corporate will fund the Numpy development<br><br>Cheers, Stefan</p>
        </div>
      </div>
      <div class="comment comment-5667694554660248515">
        <div class="comment-header">
          <a name="comment-5667694554660248515"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-10-26 11:09</span>:
        </div>
        <div class="comment-content">
          <p>i wonder why you don't get (more) funding from google?<br><br>you seem to have reached the goal of unladen swallow now and there still is room for improvement.<br><br>and it would be peanuts for them anyway. :)</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/08/wrapping-c-libraries-with-reflection-3916959558080483711.html" class="u-url">Wrapping C++ Libraries with Reflection — Status Report One Year Later</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/08/wrapping-c-libraries-with-reflection-3916959558080483711.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-08-30T13:08:00Z" itemprop="datePublished" title="2011-08-30 13:08">2011-08-30 13:08</time></a>
            </p>
                <p class="commentline">5 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Well over a year ago, <a class="reference external" href="../posts/2010/07/cern-sprint-report-wrapping-c-libraries-6547377950791793143.html">work was started</a> on the <tt class="docutils literal">cppyy</tt> module which lives in the
<tt class="docutils literal">reflex-support</tt> branch.
Since then, work has progressed at a varying pace and has included a recent
sprint in Düsseldorf, last July.</p>
<p>Let's first take a step back and recap why we're interested in doing this,
given that it is perfectly possible to use C++ through generated bindings and
<tt class="docutils literal">cpyext</tt>.
<tt class="docutils literal">cppyy</tt> makes use of reflection information generated for the C++ classes of
interest, and has that reflection information available at run time.
Therefore, it is able to open up complex C++ types to the JIT in a
conceptually similar manner as simple types are open to it.
This means that it is possible to get rid of a lot of the marshalling layers
when making cross-language calls, resulting in much lower call overhead than
is possible when going through the CPython API, or other methods of wrapping.</p>
<p>There are two problems that need to be solved: C++ language constructs need to
be presented on the Python side in a natural way; and cross-language impedance
mismatches need to be minimized, with some hints of the user if need be.
For the former, the list of mapped features has grown to a set that is
sufficient to do real work.
There is now support for:</p>
<blockquote>
<ul class="simple">
<li>builtin, pointer, and array types</li>
<li>namespaces, classes, and inner classes</li>
<li>global functions, global data</li>
<li>static/instance data members and methods</li>
<li>default variables, object return by value</li>
<li>single and multiple (virtual) inheritance</li>
<li>templated classes</li>
<li>basic STL support and pythonizations</li>
<li>basic (non-global) operator mapping</li>
</ul>
</blockquote>
<p>The second problem is harder and will always be an on-going process.
But one of the more important issues has been solved at the recent Düsseldorf
sprint, namely, that of reclaiming C++ objects instantiated from the Python
side by the garbage collector.</p>
<p>Performance has also improved, especially that of the nicer "pythonized"
interface that the user actually sees, although it still misses out on
about a factor of 2.5 in comparison to the lower-level interface (which has
gotten uglier, so you really don't want to use that).
Most of this improvement is due to restructuring so that it plays nicer with
the JIT and libffi, both of which themselves have seen improvements.</p>
<p>Work is currently concentrated on the back-ends: a <a class="reference external" href="https://root.cern.ch/drupal/content/cint">CINT</a> back-end is underway
and a LLVM/CLang pre-compiled headers (PCH) back-end is planned.
The latter is needed for this code to be released in the wild, rather than
just used in high energy physics (HEP), as that would be easier to support.
Also, within HEP, CLang's PCH are foreseen to be the future format of
reflection information.</p>
<p>At the end of the Düsseldorf sprint, we tried a little code that did something
actually "useful," namely the filling of a histogram with some random values.
We did get it to work, but trying <tt class="docutils literal">cppyy</tt> on a large class library showed
that a good warning system for such things like missing classes was sorely
needed.
That has been added since, and revisiting the histogram example later, here is
an interesting note: the <tt class="docutils literal"><span class="pre">pypy-c</span></tt> run takes 1.5x the amount of time of that
of the compiled, optimized, C++ code.
The run was timed start to finish, including the reflection library loading
and JIT warm-up that is needed in the case of Python, but not for the compiled
C++ code.
However, in HEP, scientists run many short jobs while developing their
analysis codes, before submitting larger jobs on the GRID to run during lunch
time or overnight.
Thus, a more realistic comparison is to include the compilation time needed
for the C++ code and with that, the Python code needs only 55% of the time
required by C++.</p>
<p>The choice of a programming language is often a personal one, and such
arguments like the idea that C++ is hard to use typically do not carry much
weight with the in-crowd that studies quantum field dynamics for fun.
However, getting the prompt with your analysis results back faster is a sure
winner. We hope that <tt class="docutils literal">cppyy</tt> will soon have progressed far enough to make it
useful first to particle physicists and then other uses for wrapping C++
libraries.</p>

Wim Lavrijsen, Carl Friedrich Bolz, Armin Rigo
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-706163982137704503">
        <div class="comment-header">
          <a name="comment-706163982137704503"></a>
            <span class="author">René Dudfield</span> wrote on <span class="date">2011-08-31 10:15</span>:
        </div>
        <div class="comment-content">
          <p>Hi,<br><br>nice result.  Wrapping C++ code can be even more tiresome than C, especially with large code bases.  This will be a very welcome tool.<br><br><br>This question has probably been answered before... but I ask anyway since I couldn't find the answer.<br><br>Can the jit information be saved, so it does not need to be worked out again?  Assuming all of the dependencies have not changed (.py files, pypy itself, .so files etc).  Maybe if location independent code can not be saved, then trace hints or some higher level structure could be saved to inform the jit about what traces to jit?  That sounds like a solution to jit warm up for code that is used repeatedly.<br><br>cu.</p>
        </div>
      </div>
      <div class="comment comment-8935743431797418938">
        <div class="comment-header">
          <a name="comment-8935743431797418938"></a>
            <span class="author">Wim Lavrijsen</span> wrote on <span class="date">2011-08-31 18:53</span>:
        </div>
        <div class="comment-content">
          <p>Hi,<br><br>thanks! :)<br><br>There was a recent thread on saving JIT information on pypy-dev:<br><br>https://mail.python.org/pipermail/pypy-dev/2011-August/008073.html<br><br>and the conclusion there was that it is too hard to be of benefit because too many parts contain addresses or calculated variables that were turned into constants.<br><br>For our (HEP) purposes, it would be of limited benefit: in the development cycle, the .py's would change all the time, and it is a safe assumption that the user codes that are being developed are the most "hot." If there is anything in the supporting code that is "hot" (most likely in the framework) it'd be in C/C++ at that point anyway.<br><br>Rather, I'd like to have an easy way of letting the user determine which portions of the code will be hot. Saving not having to run a hot loop 1000x in interpreted mode before the JIT kicks in, is going to be more valuable in scientific codes where the hot loops tend to be blatantly obvious.<br><br>Cheers,<br>Wim</p>
        </div>
      </div>
      <div class="comment comment-6839677665053891703">
        <div class="comment-header">
          <a name="comment-6839677665053891703"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-31 19:49</span>:
        </div>
        <div class="comment-content">
          <p>This is great. I have been looking for just such a tool to wrap C++ numerical code.<br><br>I guess I have two questions:<br>1. Is there any documentation on how to use it?<br>2. It is very important to be able to translate between NumPy data structure and C++ data structure for me, so is there any plan to make this easy?<br><br>Thanks for great work.</p>
        </div>
      </div>
      <div class="comment comment-8813250548870848312">
        <div class="comment-header">
          <a name="comment-8813250548870848312"></a>
            <span class="author">Wim Lavrijsen</span> wrote on <span class="date">2011-08-31 22:18</span>:
        </div>
        <div class="comment-content">
          <p>Hi,<br><br>thanks! :)<br><br>ad 1) it's not at the level of being usable in a production environment. I have two known issues to resolve and probably some more unknowns. I've posted a description on pypy-dev, and I'm helping a few patient, very friendly users along. But actual documentation suggest a level of support that currently can't be offered, because all the current (and soon to disappear) caveats would need documenting as well.<br><br>ad 2) not sure what data translation you're thinking of, but in the CPython equivalent, support was added for the buffer interface and MemoryView. Those, or something similar, will be there so that numpy array's etc. can be build from return values, from public data members, and passed into function calls as arguments. Those are not translations, but rather extraction of the data pointers (which is typically intended and the most efficient, to be sure).<br><br>Cheers,<br>Wim</p>
        </div>
      </div>
      <div class="comment comment-329959907252462344">
        <div class="comment-header">
          <a name="comment-329959907252462344"></a>
            <span class="author">wholesale electronics</span> wrote on <span class="date">2011-12-17 01:23</span>:
        </div>
        <div class="comment-content">
          <p>Maybe if location independent code can not be saved, then trace hints or some higher level structure could be saved to inform the jit about what traces to jit?</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/08/we-need-software-transactional-memory-6513983438425039230.html" class="u-url">We need Software Transactional Memory</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/08/we-need-software-transactional-memory-6513983438425039230.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-08-23T12:53:00Z" itemprop="datePublished" title="2011-08-23 12:53">2011-08-23 12:53</time></a>
            </p>
                <p class="commentline">38 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi all.  Here is (an extract of) a short summary paper about my current position on
Software Transactional Memory as a general tool in the implementation
of Python or Python-like languages.  Thanks to people on IRC for discussion on making
this blog post better (lucian, Alex Gaynor, rguillebert, timonator, Da_Blitz).
For the purpose of the present discussion, we are comparing Java with Python
when it comes to multi-threading.</p>

<h2>The problem in complex high-level languages</h2>
<p>Like Java, the Python language gives guarantees: it is not acceptable
for the Python virtual machine to crash due to incorrect usage of
threads.  A primitive operation in Java is something like reading or
writing a field of an object; the corresponding guarantees are along the
lines of: if the program reads a field of an object, and another thread
writes to the same field of the same object, then the program will see
either the old value, or the new value, but not something else entirely,
and the virtual machine will not crash.</p>
<p>Higher-level languages like Python differ from Java by the fact that a
"primitive operation" is far more complex.  It may for example involve
looking in several hash maps, perhaps doing updates.  In general, it is
completely impossible to map every operation that must be atomic to a
single processor instruction.</p>

<h2>Jython: fine-grained locking</h2>
<p>This problem has been solved "explicitly" in the Jython interpreter that
runs on top of Java.  The solution is explicit in the following sense:
throughout the Jython interpreter, every single operation makes careful
use of Java-level locking mechanisms.  This is an application of
"fine-grained locking".  For example, operations like attribute lookup,
which need to perform look-ups in a number of hash maps, are protected
by acquiring and releasing locks (in __getattribute__).</p>
<p>A draw-back of this solution is the attention to detail required.
If even one place misses a lock, then there is either a
bug --- and such bugs occur in cases that are increasingly rare and hard
to debug as the previous bugs are fixed --- or we just file it under "differences
from CPython".  There is however the risk of
deadlock, if two threads attempt to lock the same objects in different
order.</p>

<p>In practice, the situation is actually not as bad as
I may paint it: the number of locks in Jython is reasonable, and allows for
all the "common cases" to work as expected.
(For the uncommon cases, see below.)</p>

<p>Performance-wise, the Java virtual machine itself comes with locks that
have been heavily optimized over a long period of time, so the
performance is acceptable.  However if this solution were coded in C, it
would need a lot of extra work to optimize the locks manually (possibly
introducing more of the subtle bugs).</p>

<h2>CPython: coarse-grained locking</h2>
<p>CPython, the standard implementation of Python in C, took a different
and simpler approach: it has a single global lock, called the Global
Interpreter Lock (GIL).  It uses "coarse-grained locking": the lock is
acquired and released around the whole execution of one bytecode (or
actually a small number of bytecodes, like 100).  This solution is
enough to ensure that no two operations can conflict with each other,
because the two bytecodes that invoke them are themselves
serialized by the GIL.  It is a solution which avoids --- unlike Jython
--- writing careful lock-acquiring code all over the interpreter.  It
also offers even stronger guarantees: every bytecode runs entirely
atomically.</p>
<p>Nowadays, the draw-back of the GIL approach is obvious on multi-core
machines: by serializing the execution of bytecodes, starting multiple
threads does not actually let the interpreter use of more than one core.</p>
<p>PyPy, the Python implementation in Python, takes the same approach so
far.</p>

<h2>Existing usage</h2>
<p>As we have seen, we have the following situation: the existing Python
language, as CPython implements it, offers very strong guarantees about
multi-threaded usage.  It is important to emphasize that most existing
multi-threaded Python programs actually rely on such strong guarantees.
This can be seen for example in a problem that takes a populated list
and does in several threads:</p>
<pre class="literal-block">
next_item = global_list.pop()
</pre>
<p>This implicitly relies on the fact that pop() will perform atomic
removal from the list.  If two threads try to pop() from the same list
at the same time, then the two operations will occur in one order or the
other; but they will not e.g. return the same object to both threads or
mess up the internal state of the list object.</p>
<p>With such an example in mind, it should be clear that we do not want a
solution to the multi-core issue that involves dropping these strong
guarantees.  It is ok however to lower the barrier, as Jython does; but
any Python implementation must offer <i>some</i> guarantees, or not offer
multi-threading at all.  This includes the fact that a lot of methods on
built-in types are supposed to be atomic.</p>

<p>(It should be noted that not offering multi-threading at all is actually
also a (partial) solution to the problem.  Recently, several "hacks"
have appeared that give a programmer more-or-less transparent access to
multiple independent processes (e.g. <a href="https://docs.python.org/library/multiprocessing.html">multiprocessing</a>).  While these provide appropriate
solutions in some context, they are not as widely applicable as
multi-threading.  As a typical example, they fail to apply when the
mutiple cores need to process information that cannot be serialized at
all --- a requirement for any data exchange between several processes.)</p>

<p>Here is an example of how Jython's consistency is weaker than CPython's GIL.
It takes uncommon examples to show it, and the fact that it does not work
like a CPython programmer expect them to is generally considered as an
implementation detail.  Consider:</p>
<pre>Thread 1:  set1.update(set2)
Thread 2:  set2.update(set3)
Thread 3:  set3.update(set1)</pre>
<p>Each operation is atomic in the case of CPython, but decomposed in two steps
(which can each be considered atomic) in the case of Jython: reading from the
argument, and then updating the target set.  Suppose that initially
set1 = {1}, set2 = {2}, set3 = {3}.  On CPython, independently on
the order in which the threads run, we will end up with at least one of the
sets being {1, 2, 3}.  On Jython, it is possible that all
three sets end up as containing two items only.  The example is a bit
far-fetched but should show that CPython's consistency is strictly stronger
than Jython's.</p>

<h2>PyPy</h2>
<p>PyPy is a Python interpreter much like CPython or Jython, but the way it
is produced is particular.  It is an interpreter written in RPython, a
subset of Python, which gets turned into a complete virtual machine (as
generated C code) automatically by a step called the "translation".  In
this context, the trade-offs are different from the ones in CPython and
in Jython: it is possible in PyPy, and even easy, to apply arbitrary
whole-program transformations to the interpreter at "translation-time".</p>
<p>With this in mind, it is possible to imagine a whole-program
transformation that would add locking on every object manipulated in
RPython by the interpreter.  This would end up in a situation similar to
Jython.  However, it would not automatically solve the issue of
deadlocks, which is avoided in the case of Jython by careful manual
placement of the locks.  (In fact, being deadlock-free is a global
program property that cannot be automatically ensured or verified; any
change to Jython can in theory break this property, and thus introduce
subtle deadlocks.  The same applies to non-atomicity.)</p>
<p>In fact, we can easily check that if the interpreter accesses (for
both reading and writing)
objects A and B in a bytecode of thread 1, and objects B and A (in the
opposite order) in a bytecode of thread 2 --- and moreover if you need to
have accessed the first object before you can decide that you will need
to access the second object --- then there is no way (apart from the GIL) to avoid
a deadlock while keeping the strong guarantee of atomicity.  Indeed, if
both threads have progressed to the middle of the execution of their
bytecode, then A has already been mutated by thread 1 and similarly B
has already been mutated by thread 2.  It is not possible to
successfully continue running the threads in that case.</p>

<h2>Using Software Transactional Memory</h2>
<p>Software Transactional Memory (STM) is an approach that gives a solution
to precisely the above problem.  If a thread ended up in a situation
where continuing to run it would be wrong, then we can <i>abort and
rollback.</i>  This is similar to the notion of transaction on databases.
In the above example, one or both threads would notice that they are
about to run into troubles and abort.  This means more concretely that
they need to have a way to restart execution at the start of the
bytecode, with all the side-effects of what they did so far being either
cancelled or just not committed yet.</p>
<p>We think that this capacity to abort and rollback is the missing piece
of the puzzle of multi-threaded implementations of Python.
Actually, according to the presentation of the problem given
above, it is unavoidable that any solution that wants to offer the
same level of consistency and atomicity as CPython would involve
the capacity of aborting and rolling back --- <i>which means precisely
that STM cannot be avoided.</i></p>

<p>Ok, but why not settle down with Jython's
approach and put careful locks left and right throughout the interpreter?
Because (1) we would have to consider every operation's atomicity and make decisions
(or steal Jython's) and document them
<a href="https://doc.pypy.org/en/latest/cpython_differences.html">here</a>;
(2) it would also be really a lot of work, to optimize these locks e.g. with the
JIT as well as the JVM does; and (3) it is not the PyPy way to require manually
tweaking your code everywhere for a feature that should be orthogonal.  Point
(3) is probably the most important here: you need to redo the work for every
language you implement in PyPy.
It also implies my own point (4): <i>it is not fun :-)</i></p>

<p>In more details, the process would work as follows.  (This gives an
overview of one possible model; it is possible that a different model
will end up being better.)  In every thread:</p>
<ul>
<li>At the start of a bytecode, we start a "transaction".  This means
setting up a thread-local data structure to record a log of what
occurs in the transaction.</li>
<li>We record in the log all objects that are read, as well as the
modifications that we would like to make.</li>
<li>During this time, we detect "read" inconsistencies, shown by the
object's "last-modified" timestamp being later than the start time
of the current transaction, and abort.  This prevents the rest of
the code from running with inconsistent values.</li>
<li>If we reach the end of the bytecode without a "read" inconsistency,
then we atomically check for "write" inconsistencies.  These are
inconsistencies which arise from concurrent updates to objects
in the other threads --- either our "write" objects, or our "read"
objects.</li>
<li>If no inconsistency is found, we "commit" the transaction by copying
the delayed writes from the log into main memory.</li>
</ul>
<p>The points at which a transaction starts or ends are exactly the
points at which, in CPython, the Global Interpreter Lock is
respectively acquired and released.  If we ignore the fact that (purely for
performance) CPython acquires and releases the GIL only every N bytecodes,
then this means:</p>
<ol>
<li>Before any bytecode we acquire the GIL (start a transaction), and after
the bytecode we release it (ends the transaction); and
</li>
<li>Before doing an external call to the C library or the OS we release the GIL
(ends the transaction) and afterwards re-acquire it (start the next transaction).
</li>
</ol>
So in particular this model is well suited to the STM condition that we cannot
do anything in a transaction that cannot be rolled back, like --- precisely ---
system calls.  Indeed, by construction, these system calls occur outside a
transaction, because in CPython they occur with the GIL released.

<h2>Performance</h2>
<p>A large number of implementation details are still open for now.
From a user's point of view (i.e. the programmer using Python),
the most relevant one is the overall performance impact.  We
cannot give precise numbers so far, and we expect the initial
performance to be abysmally bad (maybe 10x slower); however, with
successive improvements to the locking mechanism, to the global
program transformation inserting the locks, to the garbage 
collector (GC), and to the Just-in-Time (JIT) compiler, we
believe that it should be possible to get a roughly reasonable
performance (up to maybe 2x slower).  For example, the GC can
maintain flags on the objects to know that they did not escape
their creation thread, and do not need any logging; and the JIT
compiler can aggregate several reads or writes to an object into
one.  We believe that these are the kind of optimizations that
can give back a lot of the performance lost.</p>

<h2>The state of STM</h2>
<p>Transactional Memory is itself a relatively old idea, originating
from a 1986 paper by Tom Knight.  At first based on hardware
support, the idea of software-only transactional memory (STM) was
popularized in 1995 and has recently been the focus of intense 
research.</p>
<p>The approach outlined above --- using STM to form the core of the
implementation of a language --- is new, as far as we know.  So
far, most implementations provide STM as a library feature.  It
requires explicit usage, often in the form of explicitly
declaring which objects must be protected by STM (object-based
STMs).  It is only recently that native STM support has started
to appear, notably in the Clojure language.</p>
<p>STM is described on Wikipedia as an approach that "greatly
simplifies conceptual understanding of multithreaded programs and
helps make programs more maintainable by working in harmony with
existing high-level abstractions such as objects and modules."
We actually think that these benefits are important enough to
warrant being exposed to the Python programmer as well, instead
of being used only internally.  This would give the Python
programmer a very simple interface:</p>
<pre class="literal-block">
with atomic:
    &lt;these operations are executed atomically&gt;
</pre>
<p>(This is <a href="https://mail.python.org/pipermail/python-dev/2003-February/033259.html">an old idea.</a>  Funny how back in 2003 people, including me, thought that this was a hack.  Now I'm writing a blog post to say "it was not a hack; it's explicitly using locks that is a hack."  I'm buying the idea of <a href="https://en.wikipedia.org/wiki/Software_transactional_memory#Composable_operations">composability.</a>)</p>

<p>From a practical point of view, I started looking seriously at
the University of Rochester STM (RSTM), a C++ library that has
been a focus of --- and a collection of results from --- recent
research.  One particularly representative paper is
<a href="https://www.cs.rochester.edu/u/spear/ppopp09.pdf">A
Comprehensive Strategy for Contention Management in Software
Transactional Memory</a> by Michael F. Spear, Luke Dalessandro,
Virendra J. Marathe and Michael L. Scott.</p>

<h2>Conclusion</h2>
<p>Taking these ideas and applying them in the context of an
implementation of a complex high-level language like Python comes
with its own challanges.  In this context, using PyPy makes sense
as both an experimentation platform and as a platform that is
recently gaining attention for its performance.  The alternatives
are unattractive: doing it in CPython for example would mean
globally rewriting the interpreter.  In PyPy instead, we write it
as a transformation that is applied systematically at translation-time.
Also, PyPy is a general platform for generating fast interpreters
for dynamic languages; the STM implementation in PyPy would work
out of the box for other language implementations as well, instead
of just for Python.</p>
<br><p><b>Update:</b>
</p>
<ul>
<li>This is mostly me (Armin Rigo) ranting aloud and trying experiments;
this post should not be confused as meaning that the whole PyPy team
will now spend the next years working on it full-time.
As I said it is orthogonal to the actual Python interpreter, and it is in
any case a feature that can be turned on or off during translation; I know
that in many or most use cases, people are more interested in getting a
fast PyPy rather than one which is twice as slow but scales well.
</li>
<li>Nothing I said is really new.  For proof, see
<a href="https://sabi.net/nriley/pubs/dls6-riley.pdf">Riley and Zilles (2006)</a>
as well as <a href="https://www.cs.auckland.ac.nz/~fuad/parpycan.pdf">Tabba (2010)</a> who both experimented with <i>Hardware</i> Transactional Memory, turning CPython or PyPy interpreter's GIL into start/end transactions, as I describe here.
</li>
</ul>
</div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-2413167444689638946">
        <div class="comment-header">
          <a name="comment-2413167444689638946"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-23 13:40</span>:
        </div>
        <div class="comment-content">
          <p>How to handle composability ("with atomic") when something inside composed block turns out to make a system call? With explicit locking, this shouldn't be a problem.</p>
        </div>
      </div>
      <div class="comment comment-8338135678673808956">
        <div class="comment-header">
          <a name="comment-8338135678673808956"></a>
            <span class="author">ajuc</span> wrote on <span class="date">2011-08-23 14:43</span>:
        </div>
        <div class="comment-content">
          <p>Re sys calls in transactions:<br><br>In clojure it is solved by requiring that code in transaction is side effect free.<br><br>You can tag code as having side effects by macro "io!" :<br><br>(defn launch-missiles<br>“Launch attack on remote targets with everything we have.”<br>[]<br>(io!<br>(doseq [missile (all-silos)]<br>(fire missile))))<br><br>Then if you try to execut this code in transaction clojure will complain, because you can't really rollback launching nuclear missiles :)</p>
        </div>
      </div>
      <div class="comment comment-2528779304023100167">
        <div class="comment-header">
          <a name="comment-2528779304023100167"></a>
            <span class="author">ajuc</span> wrote on <span class="date">2011-08-23 14:49</span>:
        </div>
        <div class="comment-content">
          <p>Ehh, I should've thought more before posting.<br><br>Code in transactions need not be side effect free - in fact in clojure side effects are the whole point of transactions. But this code should only change STM controlled variables, not outside world.<br><br>And "io!" macro is for marking code that changes things outside of STM.<br><br>Sorry for confusion.</p>
        </div>
      </div>
      <div class="comment comment-219184611646737735">
        <div class="comment-header">
          <a name="comment-219184611646737735"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-08-23 14:56</span>:
        </div>
        <div class="comment-content">
          <p>Here are my current hacks in C, based on RSTM: https://bitbucket.org/arigo/arigo/raw/default/hack/stm/c , from the repo https://bitbucket.org/arigo/arigo .</p>
        </div>
      </div>
      <div class="comment comment-5883675089644735702">
        <div class="comment-header">
          <a name="comment-5883675089644735702"></a>
            <span class="author">Thomas Schilling</span> wrote on <span class="date">2011-08-23 14:56</span>:
        </div>
        <div class="comment-content">
          <p>Implementing STM at a core level is certainly a nice research topic, but I wonder whether it's the best way forward for Python.<br><br>STM works well in Haskell because it has the type system to enforce several constraints.  Also most data is immutable in Haskell, so threading is mostly safe by default.<br><br>Most Python objects are mutable (by default), so users have to be very careful when using multi-threading.  STM gives you a nice, composable primitive to protect your critical sections, but it does not tell <b>where</b> your critical sections are.<br><br>You dismiss multiprocessing because of serialization issues, but what about multiprocessing within the same process?  You have a VM already, so my guess would be that it wouldn't be that hard to implement software processes (a la Erlang).  Sure, using message passing may lead to a fair amount of copying, but I it seems to be much easier to implement and easier to use than shared-memory concurrency + STM.</p>
        </div>
      </div>
      <div class="comment comment-7844308630289145107">
        <div class="comment-header">
          <a name="comment-7844308630289145107"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-08-23 15:23</span>:
        </div>
        <div class="comment-content">
          <p>@Thomas Schilling: I don't see how having a "multiprocessing" that uses the same process, rather than different processes, makes a difference.  In both cases you need to write your threading code specially and care about explicitly transferring objects via shared memory --- either to another OS thread in the same process, or to a different process altogether.</p>
        </div>
      </div>
      <div class="comment comment-8697711460090759773">
        <div class="comment-header">
          <a name="comment-8697711460090759773"></a>
            <span class="author">René Dudfield</span> wrote on <span class="date">2011-08-23 16:04</span>:
        </div>
        <div class="comment-content">
          <p>closures</p>
        </div>
      </div>
      <div class="comment comment-4458303753132923233">
        <div class="comment-header">
          <a name="comment-4458303753132923233"></a>
            <span class="author">Sam Wilson</span> wrote on <span class="date">2011-08-23 16:32</span>:
        </div>
        <div class="comment-content">
          <p>I'm with illume... look at what Apple has done with blocks. This seems like a very efficient way forward.<br><br>Separately, you are missing something about the Java-side.<br><br>For many of the data structures in Java there are atomic and non-atomic versions. That is, when you are using a data structure on a single thread, you grab the non-atomic version. This way, you don't pay for the overhead of the locking. But, when you are sharing a data structure between threads, you use the atomic version. As a by-product of history, though it is a nice by-product, you usually get the atomic version by default. That is to say, you have to go looking for trouble by explicitly asking for the non-atomic version.<br><br>By baking this into the language, you are forcing a single policy on all programs, rather than letting the programmer choose what policy is going to be best in that scenario. Either that, or they will be forced to put code guards all over the place.<br><br>To me, it seems like the language/runtime should provide the most basic of atomic operations, and the run-time library on top should provide the policy. That's the Java approach, in a nutshell. It gives the programmer flexibility and keeps the core runtime simple and easier to optimize.<br><br>Granted, you want a high-level language where the programmer doesn't make a lot of these decisions. So... looking at your own arguments... you are expecting an initial 10x performance hit relative to the current GIL-python approach, with hopes of getting it down to 2x performance... If that's the case, why not just stick with the GIL and have Python programmers take advantage of multiprocessing by creating co-operative programs using a message passing API. In some ways, it's a little more TAUP to do it that way, isn't it?</p>
        </div>
      </div>
      <div class="comment comment-1944385877647011715">
        <div class="comment-header">
          <a name="comment-1944385877647011715"></a>
            <span class="author">nekto0n</span> wrote on <span class="date">2011-08-23 16:37</span>:
        </div>
        <div class="comment-content">
          <p>What about replaying syscalls? Is it possible that such situation will happen?</p>
        </div>
      </div>
      <div class="comment comment-4182713227454616753">
        <div class="comment-header">
          <a name="comment-4182713227454616753"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-08-23 16:45</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous: this case can be handled on a case-by-case basis (e.g. special-casing "prints" to buffer), but it also has a general solution: we turn the transaction into an "inevitable" transaction, i.e. one which cannot fail.<br><br>I already have support for this in my demo code, because it is needed to handle the cases where the nesting of the C program is such that setjmp/longjmp can no longer work.  The typical example is the RETURN_VALUE bytecode.  It starts a transaction, returns to the caller by popping off some C frames, then ends the transaction in the caller.  When we return from the C frame of the callee, in the middle of the transaction, we notice that we won't have the setjmp around any longer, so we are not allowed to abort and rollback any more.<br><br>Inevitable transactions have the property of being "a bit like" a GIL in the sense that you can only have one in total, and other transactions cannot commit before it does.  In case of the RETURN_VALUE, it's a very short transaction so it shouldn't really be a problem.  For the case of a user-specified "with atomic:" block, it can make all the other threads pause.  Not ideal, but at least better than nothing...</p>
        </div>
      </div>
      <div class="comment comment-4289702079798383834">
        <div class="comment-header">
          <a name="comment-4289702079798383834"></a>
            <span class="author">TomV</span> wrote on <span class="date">2011-08-23 16:49</span>:
        </div>
        <div class="comment-content">
          <p>Could you explain a bit more what PyPy currently does to prevent these kinds of problems?</p>
        </div>
      </div>
      <div class="comment comment-7974275952910874833">
        <div class="comment-header">
          <a name="comment-7974275952910874833"></a>
            <span class="author">nekto0n</span> wrote on <span class="date">2011-08-23 16:52</span>:
        </div>
        <div class="comment-content">
          <p>@TomV PyPy uses GIL</p>
        </div>
      </div>
      <div class="comment comment-7913053725739157011">
        <div class="comment-header">
          <a name="comment-7913053725739157011"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-08-23 16:54</span>:
        </div>
        <div class="comment-content">
          <p>@Sam Wilson: as you know, the PyPy approach is to sacrifice nothing to performance for the user, and get reasonably good (if not exactly Java-level) performance anyway :-)<br><br>I should also mention generally that for some programs that I have in mind, using a message-passing API would be a complete rewrite (if it is really possible at all), whereas "just" making them multithreaded can be done.  The "translate.py" of PyPy falls into this category.  It is a program that heavily use objects within objects within objects in a big non-nicely-separable "mess", and I would not dare to think about how to send parts of this object graph over a messaging API and get back localized updates.<br><br>Of course there are also other use cases where you can naturally get a model that plays nicely with message passing.</p>
        </div>
      </div>
      <div class="comment comment-104891792177726745">
        <div class="comment-header">
          <a name="comment-104891792177726745"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-08-23 17:03</span>:
        </div>
        <div class="comment-content">
          <p>@nekto0n: that's not really possible in general, because you need to have the return value of the syscall to decide what to do next, which normally means that you have to really do the syscall.</p>
        </div>
      </div>
      <div class="comment comment-2864503186023382210">
        <div class="comment-header">
          <a name="comment-2864503186023382210"></a>
            <span class="author">nekto0n</span> wrote on <span class="date">2011-08-23 17:12</span>:
        </div>
        <div class="comment-content">
          <p>@armin please describe what will happen if 2 threads call write() on single socket object? what exactly should/will happen when iterpreter begins to dispatch CALL bytecode?<br><br>I think, it's the most questionable part of STM approach.</p>
        </div>
      </div>
      <div class="comment comment-6034340347554293027">
        <div class="comment-header">
          <a name="comment-6034340347554293027"></a>
            <span class="author">Rodrigo Araújo</span> wrote on <span class="date">2011-08-23 17:33</span>:
        </div>
        <div class="comment-content">
          <p>some change in my code<br><br>https://paste.pocoo.org/show/463085/</p>
        </div>
      </div>
      <div class="comment comment-1522836423051094915">
        <div class="comment-header">
          <a name="comment-1522836423051094915"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-08-23 17:34</span>:
        </div>
        <div class="comment-content">
          <p>@nekto0n: nothing particular.  The two threads will run the calls in parallel, just like CPython, which calls the send() function without any GIL acquired.  What exactly occurs depends on the OS and not on the language.</p>
        </div>
      </div>
      <div class="comment comment-7048529467948214143">
        <div class="comment-header">
          <a name="comment-7048529467948214143"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-23 17:37</span>:
        </div>
        <div class="comment-content">
          <p>I dissagree to the fact that threads whose transactions would be invalidated, are stealing CPU timeshares from other processes / threads.<br><br>STM is an 'egoist' aproach</p>
        </div>
      </div>
      <div class="comment comment-1431667169635265661">
        <div class="comment-header">
          <a name="comment-1431667169635265661"></a>
            <span class="author">kost BebiX</span> wrote on <span class="date">2011-08-23 20:54</span>:
        </div>
        <div class="comment-content">
          <p>I know this might sound stupid, but is it possible to enable/disable STM on the fly? Like to enable it only for several threads involved.</p>
        </div>
      </div>
      <div class="comment comment-6504810880618934457">
        <div class="comment-header">
          <a name="comment-6504810880618934457"></a>
            <span class="author">kost BebiX</span> wrote on <span class="date">2011-08-23 20:55</span>:
        </div>
        <div class="comment-content">
          <p>Or just not open transaction when there's only 1 thread?</p>
        </div>
      </div>
      <div class="comment comment-3067222387706714551">
        <div class="comment-header">
          <a name="comment-3067222387706714551"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2011-08-23 22:43</span>:
        </div>
        <div class="comment-content">
          <p>Hi,<br><br>I thought a bit about what you said about Jython. Mostly, I was thinking about a way to do this automatically instead of making it explicitly.<br><br>I came up with this first draft: https://github.com/albertz/automatic_object_locking<br><br>This will obviously also be very slow but it should be possible to optimize this well (similarly to STM). And I think it is much easier than STM.<br><br>-Albert</p>
        </div>
      </div>
      <div class="comment comment-4022730260305278833">
        <div class="comment-header">
          <a name="comment-4022730260305278833"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-24 07:39</span>:
        </div>
        <div class="comment-content">
          <p>Funny to see how Python eats itself like an Ouroboros. Wrong design decisions that made concurrency almost impossible, dirty hacks ("dirty" compared to, for example, Erlang's approach to SMP — almost linear scalability with a number of cores with 10-20% static overhead thanks to locks) that PyPy team are trying to do to solve problems introduced by Guido's ignorance, and a lot of Python "programmers" that don't understand what SMP is. Python is a ghetto, for real.</p>
        </div>
      </div>
      <div class="comment comment-6664269580343846798">
        <div class="comment-header">
          <a name="comment-6664269580343846798"></a>
            <span class="author">Paul Harrison</span> wrote on <span class="date">2011-08-24 07:51</span>:
        </div>
        <div class="comment-content">
          <p>Seems like it should be possible to guarantee performance not much worse than with a GIL.<br><br>Am I right in thinking there is a locked section where changes are written to memory? The execution before this is effectively just some speculative computation to to speed up the locked section. If it turns out there's an inconsistency, just execute the locked section as you would normally. If the speculative computation is failing most of the time or is slow, switch to not doing it -- and we are back to GIL performance.</p>
        </div>
      </div>
      <div class="comment comment-475038430560503263">
        <div class="comment-header">
          <a name="comment-475038430560503263"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-08-24 10:29</span>:
        </div>
        <div class="comment-content">
          <p>@all: please come to the #pypy irc channel on irc.freenode.net if you want to discuss this further.</p>
        </div>
      </div>
      <div class="comment comment-2878747439923990242">
        <div class="comment-header">
          <a name="comment-2878747439923990242"></a>
            <span class="author">Thomas Schilling</span> wrote on <span class="date">2011-08-24 12:01</span>:
        </div>
        <div class="comment-content">
          <p>@Armin: Each in-memory process would use its own part of the heap so there would be no locking necessary except during message sending.  You also don't need to have a 1-to-1 mapping of OS threads to processes.  You could schedule N processes onto M OS threads (preferably chosen to match the number of CPU cores).<br><br>Of course, if you don't want a message-passing model (as you mentioned in another comment) then fine.<br><br>My argument is just that: STM is difficult to implement, difficult to make fast, and it still isn't that easy to use.  A message passing model is much easier to implement and easier to use for end users.  (You can still get deadlocks, but you could provide libraries for standard communication patterns which you only have to get right once, like Erlang's OTP.)</p>
        </div>
      </div>
      <div class="comment comment-8328783528921012438">
        <div class="comment-header">
          <a name="comment-8328783528921012438"></a>
            <span class="author">Jan Ziak (atomsymbol)</span> wrote on <span class="date">2011-08-24 13:39</span>:
        </div>
        <div class="comment-content">
          <p>I think that there is some confusion here about what the underlying problem that you are trying to solve is.<br><br>The underlying (fundamental) problem that transactional memory as a method to replace GIL in Python is trying to solve is: automatic parallelization. That *is* hard.<br><br>Mediocre implementations of transactional memory are trivial to implement. Almost anybody can do it. Of course, the performance will be horrible.<br><br>If we stick to the idea about the underlying problem (automatic parallelization) and keep it in our minds while thinking, it is clear and utterly obvious that *any* implementation of transactional memory which is slower than serial execution is simply missing the target. The target is, obviously, to run the program faster than serial execution. Otherwise, it would be totally pointless.<br><br>Based on this reasoning, it is an *obvious* conclusion that a transactional memory implementation simply cannot be allowed to result in lower performance than serial execution of the code. Allowing lower performance would be completely irrational.<br><br>We are humans, not animals. Rationality is our distinctive feature. We have to try to follow rationality.<br><br>In light of this, saying that "It is OK for transactional memory to result in 2x slowdown" is irrational. I will write it one more time: accepting 2x slowdown is irrational.<br><br>Now, it is crucial to note that there are various kinds of performance measurements. And it is OK to slow down one performance indicator while boosting another performance indicator. For example, in web server environment, it is OK to slow down the delivery of individual web pages by a factor 1.3 - while boosting the number of requests per second by 2.3. That is *rational* and perfectly OK. Also, 3x developer productivity boost would be OK.<br><br>Following this example, if transactional memory is allowed to slow down performance of the program (compared to serial execution) by 2x, a person who follows rationally would immediately be drawn to seek for the evidence of a greater-than-2x performance boost in another area of the program.<br><br>Omitting developer productivity, how are the PyPy developers going to deliver the *mandatory* greater-than-2x performance boost (in some area) without actually solving the underlying hard problems requiring hard-core code analysis?<br><br>If PyPy's transactional memory implementation would serialize calls to the Linux kernel (because it is hard to emulate them in user-space), then this alone would prevent some programs to achieve the more-than-2x performance boost. This is because it is impossible to boost program performance (in some areas, given a particular performance indicator) unless the modified program is allowed to call kernel functions out-of-order or in parallel.<br><br>-----<br><br>Note: I am *not* saying that PyPy should give up. I am just trying to note that you do not seem to know what you are doing. But I may be wrong.</p>
        </div>
      </div>
      <div class="comment comment-3607630193498235800">
        <div class="comment-header">
          <a name="comment-3607630193498235800"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-08-24 16:50</span>:
        </div>
        <div class="comment-content">
          <p>Of course the sentence "It is OK for transactional memory to result in 2x slowdown" was meant "on one thread".  As soon as your program uses more than 2 threads, on a more-than-2-CPUs machine, then you win.</p>
        </div>
      </div>
      <div class="comment comment-1231623510528316211">
        <div class="comment-header">
          <a name="comment-1231623510528316211"></a>
            <span class="author">Jan Ziak (atomsymbol)</span> wrote on <span class="date">2011-08-24 17:20</span>:
        </div>
        <div class="comment-content">
          <p>I read "Tabba (2010)" (Tabba: Adding Concurrency in Python Using a Commercial Processor’s Hardware Transactional Memory Support) just now.<br><br><br>The article:<br><br>- benchmark "iterate": This function is not making calls to other functions. The authors are independently running 16 instances of the "iterate" function on a 16-core CPU using 16 threads. The speedup in respect to unmodified CPython is 7x. The slowdown in respect to 16 CPython processes is 2.2x.<br><br>- benchmark "count": This is similar to "iterate". The speedup in respect to unmodified CPython is 4.5x. The slowdown in respect to 16 CPython processes is 3.5x.<br><br>- benchmark "pystone": This function is making calls to other functions. 16 instances of the "pystone" function on a 16-core CPU using 16 threads. The speedup in respect to unmodified CPython is 0.9x. The slowdown in respect to 16 CPython processes is 17x.<br><br><br>My analysis:<br><br>- iterate: The fact that N instances of this function can run in parallel without any interference can be determined easily. The algorithm to determine this is trivial. (Not to mention, the pointless loop in the function can be replaced by a NOP in a dead-code elimination pass).<br><br>- count: same as "iterate".<br><br>- pystone: It is not trivial to determine whether multiple instances can run in parallel. So, it should presumably run single-threaded.<br><br>- The article is *not* mentioning any real problem that was solved by TM in the case of "iterate", "count" or "pystone". That is logical, since the truth is that there is no real problem to solve here. The benchmark functions can be trivially run in 16 CPython Linux processes - anybody can do that (even your grandma).<br><br><br>My summary:<br><br>- In case of the two functions for which it *can* be trivially determined whether their instances can run in parallel, the TM approach results in a 2x-3x slowdown compared to the most basic auto-parallelization algorithm.<br><br>- In case of the function for which it *cannot* be trivially determined whether multiple instances can run in parallel, the TM approach running on 4-16 threads achieved 90% (loss of 10%) of the speed of single-threaded CPython without TM. On 1 thread, the TM approach is 2.1x slower.<br><br><br>Bottom line:<br><br>Call me crazy, but my conclusion from this article is that TM (at least the TM approach from the article) is not working at all.</p>
        </div>
      </div>
      <div class="comment comment-746133189326067689">
        <div class="comment-header">
          <a name="comment-746133189326067689"></a>
            <span class="author">Greg Wilson</span> wrote on <span class="date">2011-08-24 17:43</span>:
        </div>
        <div class="comment-content">
          <p>Cool to see this happening. What's also cool is the result reported in Rossbach et al's study (https://www.neverworkintheory.org/?p=122): novices using STM did <i>better</i> in simple programming problems than students using traditional mechanisms, even though they thought they had done <i>worse</i>.  "Baroque syntax" may be part of the problem; I'm sure the paper's authors would be happy to chat.</p>
        </div>
      </div>
      <div class="comment comment-5506709174254681028">
        <div class="comment-header">
          <a name="comment-5506709174254681028"></a>
            <span class="author">Timo</span> wrote on <span class="date">2011-08-27 13:52</span>:
        </div>
        <div class="comment-content">
          <p>⚛, you're missing a very important bit of the paper. In it, the authors say, that the Rock hardware only holds 256 bytes of write-buffer content, while Riley and Zilles¹ determined the average write-buffer size needed for transactions to not fail prematurely would be "less than 640 bytes", which is almost three times as much as Rock offers.<br><br>Thus, the big slowdown that the pystone benchmark experiences could be caused by the shortcomings of the TM built into Rock.<br><br>I do have to agree, though, that the "benchmarks" used in the paper are not very satisfactory. However, the magical "simple parallelization algorithm" you summon in your comment would break down quite easily shortly after the complexity of the situation increases by just a bit, would it not?<br><br>¹ I only briefly glanced over the paper, so if anyone read it more thoroughly, they can feel free to correct me.</p>
        </div>
      </div>
      <div class="comment comment-2404362792346927299">
        <div class="comment-header">
          <a name="comment-2404362792346927299"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2011-08-28 00:01</span>:
        </div>
        <div class="comment-content">
          <p>I thought Erlang successfully solved this problem years ago? And I don't think anything scales better than it. So why aren't we just copying them? Message passing, where each thread or process share absolutely nothing, is the sanest and safest way to do concurrent and multi-threaded programming. I mean, you don't even have to worry about locking! STM always seemed complicated to me.</p>
        </div>
      </div>
      <div class="comment comment-7195777204013130530">
        <div class="comment-header">
          <a name="comment-7195777204013130530"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-30 03:00</span>:
        </div>
        <div class="comment-content">
          <p>is there a branch we can check this out?</p>
        </div>
      </div>
      <div class="comment comment-7137530463798478310">
        <div class="comment-header">
          <a name="comment-7137530463798478310"></a>
            <span class="author">squeaky_pl</span> wrote on <span class="date">2011-09-01 10:31</span>:
        </div>
        <div class="comment-content">
          <p>Hardware transactional memory anyone? https://arstechnica.com/hardware/news/2011/08/ibms-new-transactional-memory-make-or-break-time-for-multithreaded-revolution.ars</p>
        </div>
      </div>
      <div class="comment comment-6853320227030958680">
        <div class="comment-header">
          <a name="comment-6853320227030958680"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-09-21 19:10</span>:
        </div>
        <div class="comment-content">
          <p>@squeaky_pl: thanks for the link.  In some way researching this is ultimately doomed: either transactional memory doesn't work, or it does and in 5 or 10 years all CPUs will have good hardware support and will be able to run existing software like CPython with minor changes.  :-)</p>
        </div>
      </div>
      <div class="comment comment-3941250212293457365">
        <div class="comment-header">
          <a name="comment-3941250212293457365"></a>
            <span class="author">staila</span> wrote on <span class="date">2011-11-03 05:31</span>:
        </div>
        <div class="comment-content">
          <p>We are actually working on implementing this directly into <a href="https://blog.staila.com/?p=81" rel="nofollow">stailaOS</a>.</p>
        </div>
      </div>
      <div class="comment comment-1824199823810479371">
        <div class="comment-header">
          <a name="comment-1824199823810479371"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2012-05-12 10:24</span>:
        </div>
        <div class="comment-content">
          <p>@Mystilleef agree 100%</p>
        </div>
      </div>
      <div class="comment comment-6697447537028188824">
        <div class="comment-header">
          <a name="comment-6697447537028188824"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2012-07-05 22:42</span>:
        </div>
        <div class="comment-content">
          <p>The high-level semantics that the Python VM provides through the GIL are perfect for most programs, and for most programmer's knowledge about concurrency.<br><br>What is the purpose of going after the GIL? <br><br>If it's just a performance boost on multiple cores, then an GIOL (global IO lock) implemented on the VM, as the GIL is, should be considered. The VM could run several OS threads blocking them on IO and releasing GIL.<br><br>If the purpose is to make concurrent programming easy and correct, it can be proven that it <b>is not possible</b>.<br><br>Yet, there are alternatives that don't alter the language or the semantics that can be explored. <br><br>Erlang-style message passing can be provided through object proxies implemented on top or beneath the VM, so the threads/processes can even run on different computers.<br><br>In short, an Actor model is much preferable to a shared-memory one.<br><br>https://en.wikipedia.org/wiki/Actor_model</p>
        </div>
      </div>
      <div class="comment comment-6835034220746341475">
        <div class="comment-header">
          <a name="comment-6835034220746341475"></a>
            <span class="author">Alex moner</span> wrote on <span class="date">2014-10-21 17:38</span>:
        </div>
        <div class="comment-content">
          <p>In general, it is completely impossible to map every operation that must be atomic to a single processor instruction.<a href="https://www.uni-collect.com/uniwebsite/Products/VQNAgency.aspx" rel="nofollow">Uni-source</a><br></p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/08/pypy-16-kickass-panda-559424594592497545.html" class="u-url">PyPy 1.6 - kickass panda</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/08/pypy-16-kickass-panda-559424594592497545.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-08-18T18:24:00Z" itemprop="datePublished" title="2011-08-18 18:24">2011-08-18 18:24</time></a>
            </p>
                <p class="commentline">20 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>We're pleased to announce the 1.6 release of PyPy. This release brings a lot
of bugfixes and performance improvements over 1.5, and improves support for
Windows 32bit and OS X 64bit. This version fully implements Python 2.7.1 and
has beta level support for loading CPython C extensions.  You can download it
here:</p>
<blockquote>
<a class="reference external" href="https://pypy.org/download.html">https://pypy.org/download.html</a>
</blockquote>
<div class="section" id="what-is-pypy">
<h3>What is PyPy?</h3>
<p>PyPy is a very compliant Python interpreter, almost a drop-in replacement for
CPython 2.7.1. It's fast (<a class="reference external" href="https://speed.pypy.org">pypy 1.6 and cpython 2.6.2</a> performance comparison)
due to its integrated tracing JIT compiler.</p>
<p>This release supports x86 machines running Linux 32/64 or Mac OS X.  Windows 32
is beta (it roughly works but a lot of small issues have not been fixed so
far).  Windows 64 is not yet supported.</p>
<p>The main topics of this release are speed and stability: on average on
our benchmark suite, PyPy 1.6 is between <strong>20% and 30%</strong> faster than PyPy 1.5,
which was already much faster than CPython on our set of benchmarks.</p>
<p>The speed improvements have been made possible by optimizing many of the
layers which compose PyPy.  In particular, we improved: the Garbage Collector,
the JIT warmup time, the optimizations performed by the JIT, the quality of
the generated machine code and the implementation of our Python interpreter.</p>
</div>
<div class="section" id="highlights">
<h3>Highlights</h3>
<ul class="simple">
<li>Numerous performance improvements, overall giving considerable speedups:<ul>
<li>better GC behavior when dealing with very large objects and arrays</li>
<li>
<strong>fast ctypes:</strong> now calls to ctypes functions are seen and optimized
by the JIT, and they are up to 60 times faster than PyPy 1.5 and 10 times
faster than CPython</li>
<li>improved generators(1): simple generators now are inlined into the caller
loop, making performance up to 3.5 times faster than PyPy 1.5.</li>
<li>improved generators(2): thanks to other optimizations, even generators
that are not inlined are between 10% and 20% faster than PyPy 1.5.</li>
<li>faster warmup time for the JIT</li>
<li>JIT support for single floats (e.g., for <tt class="docutils literal"><span class="pre">array('f')</span></tt>)</li>
<li>optimized dictionaries: the internal representation of dictionaries is now
dynamically selected depending on the type of stored objects, resulting in
faster code and smaller memory footprint.  For example, dictionaries whose
keys are all strings, or all integers. Other dictionaries are also smaller
due to bugfixes.</li>
</ul>
</li>
<li>JitViewer: this is the first official release which includes the JitViewer,
a web-based tool which helps you to see which parts of your Python code have
been compiled by the JIT, down until the assembler. The <a class="reference external" href="../posts/2011/08/visualization-of-jitted-code-6202490807361942120.html">jitviewer</a> 0.1 has
already been release and works well with PyPy 1.6.</li>
<li>The CPython extension module API has been improved and now supports many
more extensions. For information on which one are supported, please refer to
our <a class="reference external" href="https://bitbucket.org/pypy/compatibility/wiki/Home">compatibility wiki</a>.</li>
<li>Multibyte encoding support: this was of of the last areas in which we were
still behind CPython, but now we fully support them.</li>
<li>Preliminary support for NumPy: this release includes a preview of a very
fast NumPy module integrated with the PyPy JIT.  Unfortunately, this does
not mean that you can expect to take an existing NumPy program and run it on
PyPy, because the module is still unfinished and supports only some of the
numpy API. However, barring some details, what works should be
blazingly fast :-)</li>
<li>Bugfixes: since the 1.5 release we fixed 53 bugs in our <a class="reference external" href="https://bugs.pypy.org">bug tracker</a>, not
counting the numerous bugs that were found and reported through other
channels than the bug tracker.</li>
</ul>
<p>Cheers,</p>
<p>Hakan Ardo, Carl Friedrich Bolz, Laura Creighton, Antonio Cuni,
Maciej Fijalkowski, Amaury Forgeot d'Arc, Alex Gaynor,
Armin Rigo and the PyPy team</p>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-3659469181669900241">
        <div class="comment-header">
          <a name="comment-3659469181669900241"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-18 18:59</span>:
        </div>
        <div class="comment-content">
          <p>Finally :) I'm really looking forward to test this code out :)</p>
        </div>
      </div>
      <div class="comment comment-3848410435946137400">
        <div class="comment-header">
          <a name="comment-3848410435946137400"></a>
            <span class="author">René Dudfield</span> wrote on <span class="date">2011-08-18 19:01</span>:
        </div>
        <div class="comment-content">
          <p>Congrats team pypy!</p>
        </div>
      </div>
      <div class="comment comment-7462258398857428035">
        <div class="comment-header">
          <a name="comment-7462258398857428035"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-18 21:15</span>:
        </div>
        <div class="comment-content">
          <p>I look forward to support Python 3</p>
        </div>
      </div>
      <div class="comment comment-8144472339366115246">
        <div class="comment-header">
          <a name="comment-8144472339366115246"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-18 21:58</span>:
        </div>
        <div class="comment-content">
          <p>"and has beta level support for loading CPython C extensions"<br><br>does this mean that the regular Numpy and Scipy can be used with this.</p>
        </div>
      </div>
      <div class="comment comment-8006844875235003150">
        <div class="comment-header">
          <a name="comment-8006844875235003150"></a>
            <span class="author">almir karic</span> wrote on <span class="date">2011-08-18 22:54</span>:
        </div>
        <div class="comment-content">
          <p>no.<br><br>"Unfortunately, this does not mean that you can expect to take an existing NumPy program and run it on PyPy"<br><br>thanks for the release pypy team!</p>
        </div>
      </div>
      <div class="comment comment-7083460289628513660">
        <div class="comment-header">
          <a name="comment-7083460289628513660"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-19 03:37</span>:
        </div>
        <div class="comment-content">
          <p>Impressive as always. Thanks for releasing such great software.<br>Keep up the good work.<br><br>Anghel</p>
        </div>
      </div>
      <div class="comment comment-5528527683328039157">
        <div class="comment-header">
          <a name="comment-5528527683328039157"></a>
            <span class="author">profu</span> wrote on <span class="date">2011-08-19 05:13</span>:
        </div>
        <div class="comment-content">
          <p>Where is the windows version?</p>
        </div>
      </div>
      <div class="comment comment-1161461980373652647">
        <div class="comment-header">
          <a name="comment-1161461980373652647"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-19 07:36</span>:
        </div>
        <div class="comment-content">
          <p>I did some benchmark with some simple parameterized SELECT statements, and found that pg8000 on pypy 1.6 is more than one time slower than pg8000 on python 2.7.1, while the later is already more than one time slower than psycopg2 on python 2.7.1.</p>
        </div>
      </div>
      <div class="comment comment-2138600256746715092">
        <div class="comment-header">
          <a name="comment-2138600256746715092"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-19 07:55</span>:
        </div>
        <div class="comment-content">
          <p>Still can't build and run python-ldap extension... :(<br>That's a deal-breaker for me.</p>
        </div>
      </div>
      <div class="comment comment-35902219771385702">
        <div class="comment-header">
          <a name="comment-35902219771385702"></a>
            <span class="author">Maciej Szumocki</span> wrote on <span class="date">2011-08-19 08:34</span>:
        </div>
        <div class="comment-content">
          <p>What kind of problems prevent releasing a Windows 64bit version?</p>
        </div>
      </div>
      <div class="comment comment-6365185995299525195">
        <div class="comment-header">
          <a name="comment-6365185995299525195"></a>
            <span class="author">Lenz</span> wrote on <span class="date">2011-08-19 12:59</span>:
        </div>
        <div class="comment-content">
          <p>Congrats !!! Realy amazing job !! <br><br>By the way, where can I find more informations about the alredy implemented numpy functions ?<br><br>Thanks.</p>
        </div>
      </div>
      <div class="comment comment-5014667958154966380">
        <div class="comment-header">
          <a name="comment-5014667958154966380"></a>
            <span class="author">jensck</span> wrote on <span class="date">2011-08-19 18:28</span>:
        </div>
        <div class="comment-content">
          <p>Amazing - PyPy just keeps making leaps and bounds forward for compat. and processing performance.  I don't know how you guys keep up such pace, but I dig it!<br><br>How is typical memory usage these days?  It's been a while since anything was reported on its resource usage vs. CPython.  Maybe such a benchmark could be added to the speed site?</p>
        </div>
      </div>
      <div class="comment comment-8606031257541316014">
        <div class="comment-header">
          <a name="comment-8606031257541316014"></a>
            <span class="author">Jan Ziak (atomsymbol)</span> wrote on <span class="date">2011-08-19 21:24</span>:
        </div>
        <div class="comment-content">
          <p>PyPy 1.5: 68 seconds<br>PyPy 1.6: 65 seconds<br>Python 3.2 (Intel C compiler): 36 seconds<br><br>Extrapolation to the future:<br>PyPy 1.17: 35 seconds ?<br><br>Jokes aside, PyPy's compatibility with CPython is good.</p>
        </div>
      </div>
      <div class="comment comment-3188759319675119358">
        <div class="comment-header">
          <a name="comment-3188759319675119358"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-20 01:05</span>:
        </div>
        <div class="comment-content">
          <p>I'm still most looking forward to the day your jit makes some .pyj files. Initialization time for the jit is a bit high, especially if you use pypy integrated into other scripts where the init time might impact performance, numpy had no dtypes, and laked almost every function, but atleast it's a step in the right direction :)<br><br>Memory usage for one of my own test apps (building a one dimensional dict with int keys, and object (sometimes by reference from a second dict) resulted in 76MB to python2.7 and 108MB to pypy 1.6. So memory usage is still a bit behind tho (the pypy runtime was better with around 35% tho).</p>
        </div>
      </div>
      <div class="comment comment-251070607233962853">
        <div class="comment-header">
          <a name="comment-251070607233962853"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-21 14:54</span>:
        </div>
        <div class="comment-content">
          <p>Is there a 32-bit OSX version somewhere? 64-bit seems to eat up memory in my tests...<br>Impressive stuff, though :-)</p>
        </div>
      </div>
      <div class="comment comment-5923979407614933810">
        <div class="comment-header">
          <a name="comment-5923979407614933810"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-22 10:56</span>:
        </div>
        <div class="comment-content">
          <p>But man!  What's wrong with Windows?<br><br>Will the windows version will be dropped?<br><br>Version 1.5 does not fully work on Windows and now you release 1.6 you does not provide a windows version...<br><br>So, I really want to know if it will be future support for windows.<br><br>This will help to decide if pypy will be an option or one just have to find other options to speed up the programs.<br><br>Please, clarify this.<br><br>Bests,</p>
        </div>
      </div>
      <div class="comment comment-6091544356673559277">
        <div class="comment-header">
          <a name="comment-6091544356673559277"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-22 18:25</span>:
        </div>
        <div class="comment-content">
          <p>Pypy does support Windows 32bit, with a couple of bugs, the windows support have been improved from 1.5 to 1.6. Perhaps it will be fully working by 1.7.</p>
        </div>
      </div>
      <div class="comment comment-8672043407598427457">
        <div class="comment-header">
          <a name="comment-8672043407598427457"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-23 09:45</span>:
        </div>
        <div class="comment-content">
          <p>Ok, but where to download PYPY for Win32 ?</p>
        </div>
      </div>
      <div class="comment comment-2754392965589321223">
        <div class="comment-header">
          <a name="comment-2754392965589321223"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-23 12:48</span>:
        </div>
        <div class="comment-content">
          <p>I believe you got to compile it yourself.</p>
        </div>
      </div>
      <div class="comment comment-1902501114034996959">
        <div class="comment-header">
          <a name="comment-1902501114034996959"></a>
            <span class="author">vak</span> wrote on <span class="date">2011-08-24 11:16</span>:
        </div>
        <div class="comment-content">
          <p>just impressive. If you guys could resurrect the numpy operation like:<br><br>boolean_array = arr &gt; value<br><br>it would be just a dream. This important operation returns not an array, but a value now.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/08/visualization-of-jitted-code-6202490807361942120.html" class="u-url">Visualization of JITted code</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/08/visualization-of-jitted-code-6202490807361942120.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-08-12T17:39:00Z" itemprop="datePublished" title="2011-08-12 17:39">2011-08-12 17:39</time></a>
            </p>
                <p class="commentline">6 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hello.</p>
<p>We're proud to announce the first public release of the jitviewer. As of now,
jitviewer is a slightly internal tool that helps understanding how your Python
source code is compiled by the PyPy's JIT all the way down to machine code.</p>
<p>To install it, you need a very recent version of PyPy
(newer than 9th of August), for example one of the <a class="reference external" href="https://buildbot.pypy.org/nightly/trunk/">nightly builds</a>:</p>
<blockquote>
<ul class="simple">
<li>install <tt class="docutils literal">pip</tt> and <tt class="docutils literal">distribute</tt> either by creating a PyPy <a class="reference external" href="https://pypi.python.org/pypi/virtualenv">virtualenv</a>
or by following the <a class="reference external" href="https://doc.pypy.org/en/latest/getting-started.html#installing-pypy">installation instructions</a>.</li>
<li>make sure to have a <a class="reference external" href="https://bitbucket.org/pypy/pypy">source code checkout</a> of PyPy and put it in your
PYTHONPATH.</li>
<li>
<tt class="docutils literal">pip install jitviewer</tt>.  Note that you need to run the <tt class="docutils literal">pip</tt>
executable which belongs to PyPy, not the globally installed one.</li>
</ul>
</blockquote>
<p>Have a look at the <a class="reference external" href="https://bitbucket.org/pypy/jitviewer/src/24adc3403cd8/README">README</a> for how to start it, or try the <a class="reference external" href="https://wyvern.cs.uni-duesseldorf.de:5000/">online demo</a> if
you just want to play with it.</p>
<p>The jitviewer is a web application written with <tt class="docutils literal">flask</tt> and <tt class="docutils literal">jinja2</tt>.  If
you have experience with web development and you want to help PyPy, don't
hesitate to contact us, there are plenty of things to improve in it :-).</p>
<div class="section" id="what-does-the-jitviewer-really-do">
<h3>What does the jitviewer really do?</h3>
<p>At the top of the page, you will see the list of pieces of code which has been
compiled by the JIT.  You will see entries for both normal loops and for
"entry bridges".  This is not the right place to discuss the difference
between those, but you most probably want to look at loops, because usually
it's where most of the time is spent.</p>
<p>Note that for each loop, you will see the name of the function which contains
the <strong>first</strong> instruction of the loop.  However, thanks to the inlining done
by the JIT, it will contain also the code for other functions.</p>
<p>Once you select a loop, the jitviewer shows how the JIT has compiled the
Python source code into assembler in a hierarchical way. It displays four
levels:</p>
<ul>
<li>
<p class="first">Python source code: only the lines shown in azure have been compiled for
this particular loop, the ones in gray have not.</p>
</li>
<li>
<p class="first">Python bytecode, the one you would get by doing:</p>
<pre class="literal-block">
def f(a, b):
   return a + b

import dis
dis.dis(f)
</pre>
<p>The opcodes are e.g. <tt class="docutils literal">LOAD_FAST</tt>, <tt class="docutils literal">LOAD_GLOBAL</tt> etc.  The opcodes
which are not in bold have been completely optimized aways by the JIT.</p>
</li>
<li>
<p class="first">Intermediate representation of jit code (IR). This is a combination of
operations (like integer addition, reading fields out of structures) and
guards (which check that the assumptions we made are actually true). Guards
are in red.  These operations are "at the same level as C": so, for example,
<tt class="docutils literal">+</tt> takes two unboxed integers which can be stored into the register
of the CPU.</p>
</li>
<li>
<p class="first">Assembler: you can see it by clicking on "Show assembler" in the menu on the
right.</p>
</li>
</ul>
<p>Sometimes you'll find that a guard fails often enough that a new piece of
assembler is required to be compiled. This is an alternative path through the
code and it's called a bridge. You can see bridges in the jitviewer when
there is a link next to a guard. For more information about purpose look up
the <a class="reference external" href="https://doc.pypy.org/en/latest/jit/index.html">jit documentation</a>.</p>
</div>
<div class="section" id="i-m-still-confused">
<h3>I'm still confused</h3>
<p>Jitviewer is not perfect when it comes to explaining what's going on. Feel free
to pop up on IRC or send us a mail to the mailing list, we'll try to explain
and/or improve the situation. Consult the <a class="reference external" href="https://pypy.org/contact.html">contact</a> page for details.</p>
<p>Cheers,<br>
fijal &amp; antocuni</p>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-5979609594386414939">
        <div class="comment-header">
          <a name="comment-5979609594386414939"></a>
            <span class="author">Paul Smith</span> wrote on <span class="date">2011-08-13 20:47</span>:
        </div>
        <div class="comment-content">
          <p>I'm getting a TemplateNotFound jinja2 exception when I run the jitviewer.py as shown in the README.</p>
        </div>
      </div>
      <div class="comment comment-5955047807596511661">
        <div class="comment-header">
          <a name="comment-5955047807596511661"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-08-13 20:48</span>:
        </div>
        <div class="comment-content">
          <p>I think you have to python setup.py install it in a virtualenv. It might not work from the checkout any more.</p>
        </div>
      </div>
      <div class="comment comment-3251601327291865385">
        <div class="comment-header">
          <a name="comment-3251601327291865385"></a>
            <span class="author">Paul Smith</span> wrote on <span class="date">2011-08-13 21:31</span>:
        </div>
        <div class="comment-content">
          <p>That fixed it, thanks.</p>
        </div>
      </div>
      <div class="comment comment-4250441681166280774">
        <div class="comment-header">
          <a name="comment-4250441681166280774"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-14 10:25</span>:
        </div>
        <div class="comment-content">
          <p>Would it be possible to get some screenshots of jitviewer, as the online demo is currently down?</p>
        </div>
      </div>
      <div class="comment comment-5914361362857278156">
        <div class="comment-header">
          <a name="comment-5914361362857278156"></a>
            <span class="author">Garito</span> wrote on <span class="date">2011-08-16 19:23</span>:
        </div>
        <div class="comment-content">
          <p>The demo doesn't work<br><br>Please, could you put it back?<br><br>Thanks a lot!<br><br>I'm developing a programming languaje based on mindmaps and I would like to know if my code works with pypy...</p>
        </div>
      </div>
      <div class="comment comment-2809784003851823899">
        <div class="comment-header">
          <a name="comment-2809784003851823899"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2011-10-02 08:33</span>:
        </div>
        <div class="comment-content">
          <p>jitviewer repository - https://bitbucket.org/pypy/jitviewer</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2011/08/pypy-is-faster-than-c-again-string-6756589731691762127.html" class="u-url">PyPy is faster than C, again: string formatting</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2011/08/pypy-is-faster-than-c-again-string-6756589731691762127.html" rel="bookmark">
            <time class="published dt-published" datetime="2011-08-02T18:50:00Z" itemprop="datePublished" title="2011-08-02 18:50">2011-08-02 18:50</time></a>
            </p>
                <p class="commentline">52 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>String formatting is probably something you do just about every day in Python,
and never think about.  It's so easy, just <tt class="docutils literal">"%d %d" % (i, i)</tt> and you're
done.  No thinking about how to size your result buffer, whether your output
has an appropriate NULL byte at the end, or any other details.  A C
equivalent might be:</p>
<pre class="literal-block">
char x[44];
sprintf(x, "%d %d", i, i);
</pre>
<p>Note that we had to stop for a second and consider how big numbers might get
and overestimate the size (44 = length of the biggest number on 64bit (20) +
1 for the sign * 2 + 1 (for the space) + 1 (NUL byte)), it took the authors of
this post, fijal and alex, 3 tries to get the math right on this :-)</p>
<p>This is fine, except you can't even return <tt class="docutils literal">x</tt> from this function, a more
fair comparison might be:</p>
<pre class="literal-block">
char *x = malloc(44 * sizeof(char));
sprintf(x, "%d %d", i, i);
</pre>
<p><tt class="docutils literal">x</tt> is slightly overallocated in some situations, but that's fine.</p>
<p>But we're not here to just discuss the implementation of string
formatting, we're here to discuss how blazing fast PyPy is at it, with
the new <tt class="docutils literal"><span class="pre">unroll-if-alt</span></tt> branch.  Given the Python code:</p>
<pre class="literal-block">
def main():
    for i in xrange(10000000):
        "%d %d" % (i, i)

main()
</pre>
<p>and the C code:</p>
<pre class="literal-block">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;


int main() {
    int i = 0;
    char x[44];
    for (i = 0; i &lt; 10000000; i++) {
        sprintf(x, "%d %d", i, i);
    }
}
</pre>
<p>Run under PyPy, at the head of the <tt class="docutils literal"><span class="pre">unroll-if-alt</span></tt> branch, and
compiled with GCC 4.5.2 at -O4 (other optimization levels were tested,
this produced the best performance). It took <strong>0.85</strong> seconds to
execute under PyPy, and <strong>1.63</strong> seconds with the compiled binary. We
think this demonstrates the incredible potential of dynamic
compilation, GCC is unable to inline or unroll the <tt class="docutils literal">sprintf</tt> call,
because it sits inside of libc.</p>
<p>Benchmarking the C code:</p>
<pre class="literal-block">
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;


int main() {
    int i = 0;
    for (i = 0; i &lt; 10000000; i++) {
        char *x = malloc(44 * sizeof(char));
        sprintf(x, "%d %d", i, i);
        free(x);
    }
}
</pre>
<p>Which as discussed above, is more comperable to the Python, gives a
result of <strong>1.96</strong> seconds.</p>
<p>Summary of performance:</p>
<table border="1" class="docutils">
<colgroup>
<col width="20%">
<col width="19%">
<col width="19%">
<col width="12%">
<col width="30%">
</colgroup>
<tbody valign="top">
<tr>
<td>Platform</td>
<td>GCC (stack)</td>
<td>GCC (malloc)</td>
<td>CPython</td>
<td>PyPy (unroll-if-alt)</td>
</tr>
<tr>
<td>Time</td>
<td>1.63s</td>
<td>1.96s</td>
<td>10.2s</td>
<td>0.85s</td>
</tr>
<tr>
<td>relative to C</td>
<td>1x</td>
<td>0.83x</td>
<td>0.16x</td>
<td><strong>1.9x</strong></td>
</tr>
</tbody>
</table>
<p>Overall PyPy is almost <strong>2x</strong> faster. This is clearly win for dynamic
compilation over static - the <cite>sprintf</cite> function lives in libc and so
cannot be specializing over the constant string, which has to be parsed
every time it's executed. In the case of PyPy, we specialize
the assembler if we detect the left hand string of the modulo operator
to be constant.</p>
<p>Cheers,<br>
alex &amp; fijal</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-8549734260032489673">
        <div class="comment-header">
          <a name="comment-8549734260032489673"></a>
            <span class="author">salmon</span> wrote on <span class="date">2011-08-02 19:23</span>:
        </div>
        <div class="comment-content">
          <p>What about '{0}'.format('pypy') ?<br>Is this also faster?</p>
        </div>
      </div>
      <div class="comment comment-2346883473253657369">
        <div class="comment-header">
          <a name="comment-2346883473253657369"></a>
            <span class="author">JoeHillen</span> wrote on <span class="date">2011-08-02 19:59</span>:
        </div>
        <div class="comment-content">
          <p>Where can we see this "unroll-if-alt" branch?</p>
        </div>
      </div>
      <div class="comment comment-1848248575826608609">
        <div class="comment-header">
          <a name="comment-1848248575826608609"></a>
            <span class="author">Greg Haines</span> wrote on <span class="date">2011-08-02 20:13</span>:
        </div>
        <div class="comment-content">
          <p>Are you sure the compiler isn't optimizing away the actual execution since you're not doing anything with the result?</p>
        </div>
      </div>
      <div class="comment comment-4004366549817031533">
        <div class="comment-header">
          <a name="comment-4004366549817031533"></a>
            <span class="author">Thomas Schilling</span> wrote on <span class="date">2011-08-02 20:18</span>:
        </div>
        <div class="comment-content">
          <p>How are those two loops equivalent?  You're not printing anything in the Python loop.  I/O buffering etc. can eat quite a bit of runtime.  It would also be nice to see what the particular improvements in this "unroll-if-alt" branch are.</p>
        </div>
      </div>
      <div class="comment comment-1500656059432769204">
        <div class="comment-header">
          <a name="comment-1500656059432769204"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-02 20:19</span>:
        </div>
        <div class="comment-content">
          <p>How about doing something like that:<br>....<br>char p[5] = "%d %d"<br>//and then<br>sprintf(x, p, i,i);<br>....<br><br>?</p>
        </div>
      </div>
      <div class="comment comment-322236938083229206">
        <div class="comment-header">
          <a name="comment-322236938083229206"></a>
            <span class="author">Andrew Pendleton</span> wrote on <span class="date">2011-08-02 20:25</span>:
        </div>
        <div class="comment-content">
          <p>@Thomas the C one doesn't print anything, either; sprintf just returns a string.  printf is the one that prints.</p>
        </div>
      </div>
      <div class="comment comment-3077470909634093215">
        <div class="comment-header">
          <a name="comment-3077470909634093215"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-02 20:26</span>:
        </div>
        <div class="comment-content">
          <p>@Thomas the C one doesn't print anything either, so it sounds pretty equivalent to me.</p>
        </div>
      </div>
      <div class="comment comment-6210890615556329311">
        <div class="comment-header">
          <a name="comment-6210890615556329311"></a>
            <span class="author">Johan Tibell</span> wrote on <span class="date">2011-08-02 20:28</span>:
        </div>
        <div class="comment-content">
          <p>This doesn't really have anything to do with dynamic compilation, but cross module optimization. There are static compilers, such as the Glasgow Haskell Compiler, that do this. If the compilation strategy depended on runtime data (e.g. measure hot spots), it would be dynamic compilation.</p>
        </div>
      </div>
      <div class="comment comment-4459479332389604269">
        <div class="comment-header">
          <a name="comment-4459479332389604269"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-02 20:56</span>:
        </div>
        <div class="comment-content">
          <p>*yawn* If you want to see ridiculously fast string formatting, look at the Boost's Spirit library (specifically Karma). Small test case, but point well illustrated: https://www.boost.org/doc/libs/1_47_0/libs/spirit/doc/html/spirit/karma/performance_measurements/numeric_performance/int_performance.html Or look at Spirit's input parser for even integers: https://alexott.blogspot.com/2010/01/boostspirit2-vs-atoi.html</p>
        </div>
      </div>
      <div class="comment comment-207875433882257655">
        <div class="comment-header">
          <a name="comment-207875433882257655"></a>
            <span class="author">Antonio Cuni</span> wrote on <span class="date">2011-08-02 20:57</span>:
        </div>
        <div class="comment-content">
          <p>@JoeHillen: the unroll-if-alt branch is inside the main pypy repo on bitbucket (together with all the other branches).<br><br>@Greg: yes, we checked the generated code, it's not optimized away.<br><br>@anonymous: why it should be any faster? String literals in C are constants, it's not that you need to create a new one at each iteration<br><br>@Johan: note that the PyPy approach can generate code optimized for a formatting string loaded from a disk, or computed at runtime. No static compiler could do that.</p>
        </div>
      </div>
      <div class="comment comment-9006643932216839467">
        <div class="comment-header">
          <a name="comment-9006643932216839467"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-02 21:10</span>:
        </div>
        <div class="comment-content">
          <p>What machine are you on that an int is 64 bits?  Hardly anybody uses ILP64 or SILP64 data models ( https://en.wikipedia.org/wiki/64-bit#Specific_C-language_data_models ).  Maybe a fourth try is in order? :P</p>
        </div>
      </div>
      <div class="comment comment-5927487656373714703">
        <div class="comment-header">
          <a name="comment-5927487656373714703"></a>
            <span class="author">Johan Tibell</span> wrote on <span class="date">2011-08-02 21:14</span>:
        </div>
        <div class="comment-content">
          <p>Antonio, that is indeed neat.</p>
        </div>
      </div>
      <div class="comment comment-5934681704643865363">
        <div class="comment-header">
          <a name="comment-5934681704643865363"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2011-08-02 22:04</span>:
        </div>
        <div class="comment-content">
          <p>So when are you going to teach PyPy that the result of an unused string formatting can be deleted, and then delete the loop?  ;)<br><br>I'm not sure how you'd get there from a tracing JIT, though.  WIth Python, you still have to call all the formatting and stringification methods because they might have side effects.  You only get to know that the entire operation is a no-op after you've inlined everything, but by then it will be at a low enough representation that it's hard to tell.</p>
        </div>
      </div>
      <div class="comment comment-4906250535693137554">
        <div class="comment-header">
          <a name="comment-4906250535693137554"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-02 22:04</span>:
        </div>
        <div class="comment-content">
          <p>sizeof(char)==1.  By definition.  Argh.<br><br>PS: negative karma for lying headline</p>
        </div>
      </div>
      <div class="comment comment-81508328450910436">
        <div class="comment-header">
          <a name="comment-81508328450910436"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-02 22:15</span>:
        </div>
        <div class="comment-content">
          <p>Check that you're not spending all your time in malloc/free(). Also use the return value from a failed snprintf(), plus 1, to size your output buffer.</p>
        </div>
      </div>
      <div class="comment comment-8836770953439937355">
        <div class="comment-header">
          <a name="comment-8836770953439937355"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2011-08-02 22:21</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous 2: Even if all the time were spent in malloc/free, PyPy has to dynamically allocate the string data structure, as well as provide a buffer to fill with the characters from the integers, since it has no way of knowing how much space will be needed (could be a custom integer class).<br><br>However, you're right that malloc and free are slow and a good gc system would have a faster allocator.</p>
        </div>
      </div>
      <div class="comment comment-7916479943368840430">
        <div class="comment-header">
          <a name="comment-7916479943368840430"></a>
            <span class="author">vsergeev</span> wrote on <span class="date">2011-08-02 22:24</span>:
        </div>
        <div class="comment-content">
          <p>a quick tip to minimize the math in determining your sprintf buffer size for your experiment:<br>#include &lt; stdint.h &gt;<br>len = snprintf(NULL, 0, "%d %d", INT32_MIN, INT32_MIN);<br>will give you the string length required (not including null terminating byte) to fit the formatted string.<br><br>Similarly, %lld and INT64_MIN will do the trick (on the right platform) for 64-bit signed integers.<br><br>(not that I advocate fixed sized buffers for formatted strings based on min/max digit lengths for any real application)</p>
        </div>
      </div>
      <div class="comment comment-7428487986448993372">
        <div class="comment-header">
          <a name="comment-7428487986448993372"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-02 22:33</span>:
        </div>
        <div class="comment-content">
          <p>You wrote:<br><i>and compiled with GCC 4.5.2 at -O4</i><br><br>Please read the manual of GCC. There you will see that every optimization level above 3 is handled as it would be 3. '-O4' is nothing else than '-O3'.<br><br>It is also known that optimizing with -O3 may lead to several problems at runtime (e.g. memory delays for short programs or memory allocation failure in larger programs).<br>That's why the recommended optimization level is '2' (or 's' for embedded systems) and not '3'.<br><br>Did you test with a realtime kernel?<br>How about the scheduler?<br><br>Maybe you should double check your test environment.</p>
        </div>
      </div>
      <div class="comment comment-7606502834538440880">
        <div class="comment-header">
          <a name="comment-7606502834538440880"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-02 22:47</span>:
        </div>
        <div class="comment-content">
          <p>For all you complaining about test eviorment. Pypy would still have to do that internaly. If they should be truely comparable, then you need to also include snprintf inside the loop, making C even slower. Also, I doubt you will get 200% performance boost from scheduler change.<br><br>unroll-if-alt will be included in 1.6 right? Also when will 1.6 be released?</p>
        </div>
      </div>
      <div class="comment comment-1734383319348443493">
        <div class="comment-header">
          <a name="comment-1734383319348443493"></a>
            <span class="author">Thomas Schilling</span> wrote on <span class="date">2011-08-02 22:50</span>:
        </div>
        <div class="comment-content">
          <p>@Andrew, @hobbs: Oh, sorry I overlooked the "s" in "sprintf".  It would still be nice compare the generated machine code to explain the differences.<br><br>Whenever, someone claims language L1 implementation A is faster than language L2 implementation B there are obvious questions about (1) fairness of comparison, (2) what is being measured.  In this case PyPy is specializing on the format string interpreter (does that require library annotations?) which a C compiler could do in principle here (but probably doesn't.)  So, I'm always a bit suspicious when I see these kinds of comparisons.<br><br>@Johan: GHC's cross-module optimization often comes at the expense of binary compatibility.  A JIT has a big advantage here.</p>
        </div>
      </div>
      <div class="comment comment-7105823921529412975">
        <div class="comment-header">
          <a name="comment-7105823921529412975"></a>
            <span class="author">René Dudfield</span> wrote on <span class="date">2011-08-02 23:33</span>:
        </div>
        <div class="comment-content">
          <p>The python faster than C day has come!  Congrats.<br><br>ps. Did you try it with (Link Time Optimization)LTO?  that is with gcc the option: -flto ?  Also, are you using PGO with gcc?</p>
        </div>
      </div>
      <div class="comment comment-2792433294640140627">
        <div class="comment-header">
          <a name="comment-2792433294640140627"></a>
            <span class="author">nekto0n</span> wrote on <span class="date">2011-08-02 23:40</span>:
        </div>
        <div class="comment-content">
          <p>@salmon According to this <a href="https://bitbucket.org/pypy/pypy/changeset/f45b04b331a6#chg-pypy/objspace/std/newformat.py" rel="nofollow">commit</a> new style formatting is supported too.<br><br>Someone correct me if I'm wrong.</p>
        </div>
      </div>
      <div class="comment comment-1284618952149817016">
        <div class="comment-header">
          <a name="comment-1284618952149817016"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-02 23:49</span>:
        </div>
        <div class="comment-content">
          <p>I think that computation is not correct yet. IIRC, you only get 20 digits in an unsigned 64-bit quantity.<br><br>Worse, (again IIRC) sprintf is locale dependent. It may insert thousands separators.</p>
        </div>
      </div>
      <div class="comment comment-3559546173966141322">
        <div class="comment-header">
          <a name="comment-3559546173966141322"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-03 00:31</span>:
        </div>
        <div class="comment-content">
          <p>This is not a good performance test because all printf function have high constant complexity, without looking at format string, check it</p>
        </div>
      </div>
      <div class="comment comment-3674129131548011685">
        <div class="comment-header">
          <a name="comment-3674129131548011685"></a>
            <span class="author">Strohan</span> wrote on <span class="date">2011-08-03 01:54</span>:
        </div>
        <div class="comment-content">
          <p>wouldn't it be better to run your test with a more modern c++ library like cstring?</p>
        </div>
      </div>
      <div class="comment comment-9011623056493186601">
        <div class="comment-header">
          <a name="comment-9011623056493186601"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-03 03:07</span>:
        </div>
        <div class="comment-content">
          <p>If 1.9x is "almost 2x faster", then what is "1x faster"?</p>
        </div>
      </div>
      <div class="comment comment-8056695997075130866">
        <div class="comment-header">
          <a name="comment-8056695997075130866"></a>
            <span class="author">Poposhka</span> wrote on <span class="date">2011-08-03 05:09</span>:
        </div>
        <div class="comment-content">
          <p>post the Assembly code, map files and call graph or it didnt happen!!!!!!!!</p>
        </div>
      </div>
      <div class="comment comment-6225551896160358432">
        <div class="comment-header">
          <a name="comment-6225551896160358432"></a>
            <span class="author">Reinis I.</span> wrote on <span class="date">2011-08-03 07:13</span>:
        </div>
        <div class="comment-content">
          <p>"one time faster" is bad English.</p>
        </div>
      </div>
      <div class="comment comment-6625735662149629937">
        <div class="comment-header">
          <a name="comment-6625735662149629937"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-03 08:38</span>:
        </div>
        <div class="comment-content">
          <p>What performance impact does the malloc/free produce in the C code? AFAIK Python allocates memory in larger chunks from the operating system. Probably Python does not have to call malloc after initialization after it allocated the first chunk.<br><br>AFAIK each malloc/free crosses the boundaries between user-mode/kernel-mode.<br><br>So, IMHO you should compare the numbers of a C program which<br>does not allocate dynamic memory more than once and uses an internal memory management system.<br><br>These numbers would be interesting.<br><br>Have fun</p>
        </div>
      </div>
      <div class="comment comment-6708170690935286644">
        <div class="comment-header">
          <a name="comment-6708170690935286644"></a>
            <span class="author">Damian Cugley</span> wrote on <span class="date">2011-08-03 08:44</span>:
        </div>
        <div class="comment-content">
          <p>The point here is not that the Python implementation of formatting is better than the C standard library, but that dynamic optimisation can make a big difference.  The first time the formatting operator is called its format string is parsed and assembly code for assembling the output generated. The next 999999 times that assembly code is used without doing the parsing step. Even if sprintf were defined locally, a static compiler can’t optimise away the parsing step, so that work is done redundantly every time around the loop.<br><br>In a language like Haskell something similar happens. A string formatting function in the style of sprintf would take a format string as a parameter and return a new function that formats its arguments according to that string. The new function corresponds to the specialized assembly code generated by PyPy’s JIT. I think if you wanted to give the static compiler the opportunity to do optimizations that PyPy does at runtime you would need to use a custom type rather than a string as the formatting spec. (NB my knowledge of functional-language implementation is 20 years out of date so take the above with a pinch of salt.)</p>
        </div>
      </div>
      <div class="comment comment-415195237064488124">
        <div class="comment-header">
          <a name="comment-415195237064488124"></a>
            <span class="author">Dave Kirby</span> wrote on <span class="date">2011-08-03 12:50</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous:<br><br>The C code shown does not do any malloc/free.  The sprintf function formats the string into the char array x, which is allocated on the stack.  It is highly unlikely that the sprintf function itself mallocs any memory.</p>
        </div>
      </div>
      <div class="comment comment-1076746612002500813">
        <div class="comment-header">
          <a name="comment-1076746612002500813"></a>
            <span class="author">Paul Jaros</span> wrote on <span class="date">2011-08-03 15:45</span>:
        </div>
        <div class="comment-content">
          <p>I'm following the progress on pypy since many years and the potential is and has always been here. And boy, pypy has come a looong way.<br><br>You are my favorite open-source project and I am excited to see what will happen next. Go pypy-team, go!</p>
        </div>
      </div>
      <div class="comment comment-4060331138568834151">
        <div class="comment-header">
          <a name="comment-4060331138568834151"></a>
            <span class="author">Stepan Koltsov</span> wrote on <span class="date">2011-08-03 18:25</span>:
        </div>
        <div class="comment-content">
          <p>PyPy does nothing 1.9 times faster than C.</p>
        </div>
      </div>
      <div class="comment comment-442286616724560885">
        <div class="comment-header">
          <a name="comment-442286616724560885"></a>
            <span class="author">Jan Ziak (atomsymbol)</span> wrote on <span class="date">2011-08-03 19:51</span>:
        </div>
        <div class="comment-content">
          <p>You wrote: "We think this demonstrates the incredible potential of dynamic compilation, ..."<br><br>I disagree. You tested a microbenchmark. Claims about compiler or language X winning over Y should be made after observing patterns in real programs. That is: execute or analyse real C programs which are making use of 'sprintf', record their use of 'sprintf', create a statistics out of the recorded data and then finally use the statistical distributions to create Python programs with a similar distribution of calls to '%'.<br><br>Trivial microbenchmarks can be deceiving.</p>
        </div>
      </div>
      <div class="comment comment-3959527016695567914">
        <div class="comment-header">
          <a name="comment-3959527016695567914"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-04 01:11</span>:
        </div>
        <div class="comment-content">
          <p>@Dave Kirby:<br><br>There are two C programs there. One on the stack, one with a malloc / free in the loop.<br><br>Which one is used for the faster claim?</p>
        </div>
      </div>
      <div class="comment comment-4068929081443684480">
        <div class="comment-header">
          <a name="comment-4068929081443684480"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2011-08-04 08:47</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous: this branch, unroll-if-alt, will not be included in the release 1.6, which we're doing right now (it should be out any day now).  It will only be included in the next release, which we hope to do soonish.  It will also be in the nightly builds as soon as it is merged.</p>
        </div>
      </div>
      <div class="comment comment-7104690340776437777">
        <div class="comment-header">
          <a name="comment-7104690340776437777"></a>
            <span class="author">Connelly Barnes</span> wrote on <span class="date">2011-08-04 20:50</span>:
        </div>
        <div class="comment-content">
          <p>Is string/IO performance in general being worked on in Pypy? Last I looked Pypy showed it was faster than CPython in many cases on its benchmarks page, but for many string/IO intensive tasks I tried Pypy v1.5 on, it was slower.</p>
        </div>
      </div>
      <div class="comment comment-3710046042028182472">
        <div class="comment-header">
          <a name="comment-3710046042028182472"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2011-08-05 07:06</span>:
        </div>
        <div class="comment-content">
          <p>@Connelly yes, for some definition of working (being thought about). that's one reason why twisted_tcp is slower than other twisted benchmarks. We however welcome simple benchmarks as bugs on the issue tracker</p>
        </div>
      </div>
      <div class="comment comment-2471477793777742440">
        <div class="comment-header">
          <a name="comment-2471477793777742440"></a>
            <span class="author">tt</span> wrote on <span class="date">2011-08-05 10:05</span>:
        </div>
        <div class="comment-content">
          <p>This is a horribly flawed benchmark which illustrates absolutely nothing. First of all, an optimizing JIT should be (easily) able to detect that your inner loop has no side effects and optimize it away. Secondly, with code like that you should expect all kinds of weirds transformations by the compiler, hence - you can't be really sure what you are comparing here. As many here have pointed out, you should compare the output assembly.<br><br>Anyway, if you really want to do a benchmark like that, do it the right way. Make the loop grow a string by continuous appending and write the string to the file in the end (time the loop only). This way you will get accurate results which really compare the performance of two compilers performing the same task.</p>
        </div>
      </div>
      <div class="comment comment-9110238064234290648">
        <div class="comment-header">
          <a name="comment-9110238064234290648"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-05 11:28</span>:
        </div>
        <div class="comment-content">
          <p>try in nodejs:<br><br>var t = (new Date()).getTime();<br><br>function main() {<br>    var x;<br>    for (var i = 0; i &lt; 10000000; i++)<br>        x = i + " " + i;<br>    return x;<br>}<br>x = main();<br><br>t = (new Date()).getTime() - t;<br>console.log(x + ", " + t);</p>
        </div>
      </div>
      <div class="comment comment-2293194787886399905">
        <div class="comment-header">
          <a name="comment-2293194787886399905"></a>
            <span class="author">tt</span> wrote on <span class="date">2011-08-05 16:15</span>:
        </div>
        <div class="comment-content">
          <p>I have now put a small, slightly more realistic benchmark. I used following code.<br><br><b>Python </b><br><br>def main():<br> x = ""<br> for i in xrange(50000):<br>  x = "%s %d" % (x, i)<br> return x<br><br>x = main()<br><br>f = open("log.txt", "w")<br>f.write(x)<br>f.close()<br><br><b>C</b><br>#include <br>#include <br>#include <br><br><br>int main() {<br> int i;<br>    char *x = malloc(0);<br> FILE *file; <br><br>    *x = 0x00;<br><br>    for (i = 0; i &lt; 50000; i++) {<br>     char *nx = malloc(strlen(x) + 16); // +16 bytes to be on the safe side<br><br>        sprintf(nx, "%s %d", x, i);<br>        free(x);<br>        x = nx;<br>    }<br><br> file = fopen("log1.txt","w"); <br> fprintf(file, "%s", x); <br> fclose(file); <br>}<br><br><b>JavaScript (NodeJS)</b><br><br>var fs = require('fs');<br><br>String.prototype.format = function() {<br>    var formatted = this;<br>    for (var i = 0; i &lt; arguments.length; i++) {<br>        var regexp = new RegExp('\\{'+i+'\\}', 'gi');<br>        formatted = formatted.replace(regexp, arguments[i]);<br>    }<br>    return formatted;<br>};<br><br><br>function main() {<br>var x = "";<br>for (var i = 0; i &lt; 50000; i++)<br> x = "{0} {1}".format(x, i);<br>return(x)<br>}<br><br>x = main();<br>fs.writeFile('log.txt', x)<br><br><br>Note for JS example: I did not want to use the  stuff like i + " " + i because it bypasses the format function call. Obviously, using the + operator the nodejs example would be much faster (but pypy probably as well).<br><br>Also, I used PyPy 1.5 as I did not find any precompiled PyPy 1.6 for OS X. <br><br>Results:<br><br>PyPy: real 0m13.307s<br>NodeJS: real 0m44.350s<br>C: real 0m1.812s</p>
        </div>
      </div>
      <div class="comment comment-8196183060405841801">
        <div class="comment-header">
          <a name="comment-8196183060405841801"></a>
            <span class="author">Jan Ziak (atomsymbol)</span> wrote on <span class="date">2011-08-05 18:32</span>:
        </div>
        <div class="comment-content">
          <p>@tt: This is a very <b>inefficient</b> C/C++ implementation of the idea "make a loop grow a string by continuous appending and write the string to the file in the end". In addition, it appears to be an <b>uncommon</b> piece of C/C++ code.</p>
        </div>
      </div>
      <div class="comment comment-7034552848818282014">
        <div class="comment-header">
          <a name="comment-7034552848818282014"></a>
            <span class="author">tt</span> wrote on <span class="date">2011-08-05 20:03</span>:
        </div>
        <div class="comment-content">
          <p>Well, I never said anything about writing super efficient C code. Anyway, I don't see how you want to implement string formatting more efficiently  - if we talk about general usage scenario. You can't really reuse the old string buffer, you basically have to allocate new one each time the string grows. Or pre-allocate a larger string buffer and do some substring copies (which will result in a much more complicated code). Anyway, the malloc() on OS X is very fast.<br><br>My point is: even this C code, which you call inefficient is around 6 times faster then pypy 1.5</p>
        </div>
      </div>
      <div class="comment comment-8973221726765232994">
        <div class="comment-header">
          <a name="comment-8973221726765232994"></a>
            <span class="author">Antiplutocrat</span> wrote on <span class="date">2011-08-05 21:08</span>:
        </div>
        <div class="comment-content">
          <p>@tt except one of the main points of the post was that they had implemented a *new* feature (unroll-if-alt, I believe) that sped things up a bunch. <br><br>I'm not sure how much any comparison that *doesn't* use this new feature is worth ...<br><br>So many haters ! ;)</p>
        </div>
      </div>
      <div class="comment comment-8120059732906295773">
        <div class="comment-header">
          <a name="comment-8120059732906295773"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-06 00:59</span>:
        </div>
        <div class="comment-content">
          <p>The compare is good because both use standard langauge fetatures to do the same thing, using a third part lib is not the same, then I have to code the same implant in RPython and people would still complain do to RPython often being faster then C regardless.<br><br>Python could have detected that the loop is not doing anything, but give that one value had a __str__ call it could've broken some code. Anyway, C compiler could also see that you didn't do anything with the value and optimalize it the same way.</p>
        </div>
      </div>
      <div class="comment comment-82206416910885214">
        <div class="comment-header">
          <a name="comment-82206416910885214"></a>
            <span class="author">tt</span> wrote on <span class="date">2011-08-06 11:05</span>:
        </div>
        <div class="comment-content">
          <p>@Antiplutocrat:<br>Honestly, I expected a bit more objectivity from posters here. I am really disappointed that you compare me to "haters" (whoever that may be). <br><br>Your point about unroll-if-alt is absolutely valid and I myself have explicitly stated that I did not use that feature. At no point I have refuted that the original blog post was wrong - it is still very well possible that PyPy 1.6 is faster then C in this usage scenario. The main goal of my post was to make clear that the original benchmarks were flawed, as they grant the compiler too much  space for unpredictable optimizations. I believe that my benchmark code produces more realistic results and I suggest that the authors of this blog entry re-run the benchmark using my code (or something similar, which controls for unpredictable optimizations).</p>
        </div>
      </div>
      <div class="comment comment-3908814753897430340">
        <div class="comment-header">
          <a name="comment-3908814753897430340"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-06 16:35</span>:
        </div>
        <div class="comment-content">
          <p>@tt: Code is doing something else so it's not the same.</p>
        </div>
      </div>
      <div class="comment comment-2494310827119670528">
        <div class="comment-header">
          <a name="comment-2494310827119670528"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-12 17:07</span>:
        </div>
        <div class="comment-content">
          <p>Only a quick note OffTopic: in the python FAQ, one could update adding PyPy besides Psyco in the performance tips:<br><br>https://docs.python.org/faq/programming.html#my-program-is-too-slow-how-do-i-speed-it-up</p>
        </div>
      </div>
      <div class="comment comment-3529683872234984787">
        <div class="comment-header">
          <a name="comment-3529683872234984787"></a>
            <span class="author">Jan Ziak (atomsymbol)</span> wrote on <span class="date">2011-08-14 18:34</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous: I agree with your other paragraphs, but not with the one where you wrote that "... OLDER version (4.5.x) of GCC whilst a newer version (4.6.x) is available with major improvements to the optimizer in general".<br><br>I am not sure, what "major improvements" in GCC 4.6 do you mean? Do you have benchmark numbers to back up your claim?<br><br>As far as well-written C code is concerned, in my opinion, there haven't been any "major improvements" in GCC for more than 5+ years. There have been improvements of a few percent in a limited number of cases - but nothing major.<br><br>Even LTO (link-time optimization (and lets hope it will be safe/stable to use when GCC 4.7 is released)) isn't a major boost. I haven't seen LTO being able to optimize calls to functions living in dynamic libraries (the bsearch(3) function would be a nice candidate). And I also haven't seen GCC's LTO being able to optimize calls to/within the Qt GUI library when painting pixels or lines onto the screen.<br><br>The main point of the PyPy article was that run-time optimizations in PyPy have a chance of surpassing GCC in certain cases.<br><br>Personally, I probably wouldn't willingly choose to work on a project like PyPy - since, err, I believe that hard-core JIT optimizations on a dynamically typed language like Python are generally a bad idea - but I am (in a positive way) eager to see what the PyPy team will be able to do in this field in the years to come.</p>
        </div>
      </div>
      <div class="comment comment-29307641900913078">
        <div class="comment-header">
          <a name="comment-29307641900913078"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2011-08-14 20:17</span>:
        </div>
        <div class="comment-content">
          <p>@⚛: I indeed do not have benchmarks for these claims, but GCC 4.6 indeed added some newer optimization techniques to its assortment. Maybe these may not have had a significant influence in said case but they might have somewhere else. I'm merely saying: you can't really compare the latest hot inventions with something that is surpassed (e.g. compare Java 7 to a program output by Visual C++ back form the VS 2003 IDE).<br><br>All by all, I'm not saying that Python sucks and don't want to sound like a fanboy (on the contrary, Linux uses a great deal of Python and if this could mean a major speedup, then why the hell not ;).<br><br>I guess I was pissed off because the written article sounds very much fanboyish and pro-Python (just look at the title alone).</p>
        </div>
      </div>
      <div class="comment comment-4441342449976828444">
        <div class="comment-header">
          <a name="comment-4441342449976828444"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2012-11-01 17:25</span>:
        </div>
        <div class="comment-content">
          <p>So a loop that doesn't print in Python is compared to a loop in C that does and that was compiled on one of the slowest C compilers out there.<br><br><a href="https://linuxhaters.blogspot.ca/" rel="nofollow">YearOfTheLinuxDesktopIsAtHand(TM)</a></p>
        </div>
      </div>
      <div class="comment comment-6085220478226409773">
        <div class="comment-header">
          <a name="comment-6085220478226409773"></a>
            <span class="author">Cees Timmerman</span> wrote on <span class="date">2012-11-12 15:36</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous, "the C one doesn't print anything, either; sprintf just returns a string. printf is the one that prints." - Andrew Pendleton, this page, August 2, 2011 9:25 PM</p>
        </div>
      </div>
         </div>

</div>
</div>
<div class="sidebar">
<div>
  <h2>
    The PyPy blogposts
  </h2>
  <div>
    Create a guest post via a PR to the <a href="https://github.com/pypy/pypy.org">source repo</a>
  </div>
</div>
    <div id="global-recent-posts">
    <h2>
      Recent Posts
    </h2>
    <ul class="post-list">
      <li>
        <a href="/posts/2025/12/toy-load-store.html" class="listtitle">Load and store forwarding in the Toy Optimizer</a>
      </li>
      <li>
        <a href="/posts/2025/07/pypy-v7320-release.html" class="listtitle">PyPy v7.3.20 release</a>
      </li>
      <li>
        <a href="/posts/2025/06/rpython-gc-allocation-speed.html" class="listtitle">How fast can the RPython GC allocate?</a>
      </li>
      <li>
        <a href="/posts/2025/04/prospero-in-rpython.html" class="listtitle">Doing the Prospero-Challenge in RPython</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7319-release.html" class="listtitle">PyPy v7.3.19 release</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-gc-sampling.html" class="listtitle">Low Overhead Allocation Sampling with VMProf in PyPy's GC</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7318-release.html" class="listtitle">PyPy v7.3.18 release</a>
      </li>
      <li>
        <a href="/posts/2025/01/musings-tracing.html" class="listtitle">Musings on Tracing in PyPy</a>
      </li>
      <li>
        <a href="/posts/2025/01/towards-pypy311-an-update.html" class="listtitle">Towards PyPy3.11 - an update</a>
      </li>
      <li>
        <a href="/posts/2024/11/guest-post-final-encoding-in-rpython.html" class="listtitle">Guest Post: Final Encoding in RPython Interpreters</a>
      </li>
    </ul>
  </div>

          <div id="global-archive-list">
          <h2>
            Archives
          </h2>
          <ul class="archive-level archive-level-1">
            <li><a class="reference" href="/2007/">2007</a> (19)
            </li>
            <li><a class="reference" href="/2008/">2008</a> (62)
            </li>
            <li><a class="reference" href="/2009/">2009</a> (38)
            </li>
            <li><a class="reference" href="/2010/">2010</a> (44)
            </li>
            <li><a class="reference" href="/2011/">2011</a> (43)
            </li>
            <li><a class="reference" href="/2012/">2012</a> (44)
            </li>
            <li><a class="reference" href="/2013/">2013</a> (46)
            </li>
            <li><a class="reference" href="/2014/">2014</a> (22)
            </li>
            <li><a class="reference" href="/2015/">2015</a> (20)
            </li>
            <li><a class="reference" href="/2016/">2016</a> (20)
            </li>
            <li><a class="reference" href="/2017/">2017</a> (13)
            </li>
            <li><a class="reference" href="/2018/">2018</a> (12)
            </li>
            <li><a class="reference" href="/2019/">2019</a> (12)
            </li>
            <li><a class="reference" href="/2020/">2020</a> (9)
            </li>
            <li><a class="reference" href="/2021/">2021</a> (10)
            </li>
            <li><a class="reference" href="/2022/">2022</a> (13)
            </li>
            <li><a class="reference" href="/2023/">2023</a> (6)
            </li>
            <li><a class="reference" href="/2024/">2024</a> (13)
            </li>
            <li><a class="reference" href="/2025/">2025</a> (9)
            </li>
          </ul>
        </div>


          <div id="global-tag-list">
          <h2>
            Tags
          </h2>
          <ul>
            <li><a class="reference" href="/categories/arm.html">arm</a> (2)</li>
            <li><a class="reference" href="/categories/benchmarking.html">benchmarking</a> (1)</li>
            <li><a class="reference" href="/categories/casestudy.html">casestudy</a> (3)</li>
            <li><a class="reference" href="/categories/cli.html">cli</a> (1)</li>
            <li><a class="reference" href="/categories/compiler.html">compiler</a> (1)</li>
            <li><a class="reference" href="/categories/conda-forge.html">conda-forge</a> (1)</li>
            <li><a class="reference" href="/categories/cpyext.html">cpyext</a> (4)</li>
            <li><a class="reference" href="/categories/cpython.html">CPython</a> (3)</li>
            <li><a class="reference" href="/categories/ep2008.html">ep2008</a> (1)</li>
            <li><a class="reference" href="/categories/extension-modules.html">extension modules</a> (3)</li>
            <li><a class="reference" href="/categories/gc.html">gc</a> (3)</li>
            <li><a class="reference" href="/categories/guestpost.html">guestpost</a> (3)</li>
            <li><a class="reference" href="/categories/graalpython.html">GraalPython</a> (1)</li>
            <li><a class="reference" href="/categories/hpy.html">hpy</a> (1)</li>
            <li><a class="reference" href="/categories/heptapod.html">Heptapod</a> (1)</li>
            <li><a class="reference" href="/categories/jit.html">jit</a> (23)</li>
            <li><a class="reference" href="/categories/jython.html">jython</a> (1)</li>
            <li><a class="reference" href="/categories/kcachegrind.html">kcachegrind</a> (1)</li>
            <li><a class="reference" href="/categories/meta.html">meta</a> (1)</li>
            <li><a class="reference" href="/categories/numpy.html">numpy</a> (24)</li>
            <li><a class="reference" href="/categories/parser.html">parser</a> (1)</li>
            <li><a class="reference" href="/categories/performance.html">performance</a> (2)</li>
            <li><a class="reference" href="/categories/profiling.html">profiling</a> (7)</li>
            <li><a class="reference" href="/categories/pypy.html">pypy</a> (6)</li>
            <li><a class="reference" href="/categories/pypy3.html">pypy3</a> (16)</li>
            <li><a class="reference" href="/categories/pyqt4.html">PyQt4</a> (1)</li>
            <li><a class="reference" href="/categories/release.html">release</a> (66)</li>
            <li><a class="reference" href="/categories/releasecffi.html">releasecffi</a> (3)</li>
            <li><a class="reference" href="/categories/releaserevdb.html">releaserevdb</a> (1)</li>
            <li><a class="reference" href="/categories/releasestm.html">releasestm</a> (1)</li>
            <li><a class="reference" href="/categories/revdb.html">revdb</a> (1)</li>
            <li><a class="reference" href="/categories/roadmap.html">roadmap</a> (2)</li>
            <li><a class="reference" href="/categories/rpython.html">rpython</a> (1)</li>
            <li><a class="reference" href="/categories/rpyc.html">RPyC</a> (1)</li>
            <li><a class="reference" href="/categories/speed.html">speed</a> (6)</li>
            <li><a class="reference" href="/categories/sponsors.html">sponsors</a> (7)</li>
            <li><a class="reference" href="/categories/sprint.html">sprint</a> (3)</li>
            <li><a class="reference" href="/categories/sprints.html">sprints</a> (1)</li>
            <li><a class="reference" href="/categories/stm.html">stm</a> (14)</li>
            <li><a class="reference" href="/categories/sun.html">sun</a> (1)</li>
            <li><a class="reference" href="/categories/smalltalk.html">Smalltalk</a> (1)</li>
            <li><a class="reference" href="/categories/squeak.html">Squeak</a> (1)</li>
            <li><a class="reference" href="/categories/testing.html">testing</a> (1)</li>
            <li><a class="reference" href="/categories/toy-optimizer.html">toy-optimizer</a> (6)</li>
            <li><a class="reference" href="/categories/unicode.html">unicode</a> (1)</li>
            <li><a class="reference" href="/categories/valgrind.html">valgrind</a> (1)</li>
            <li><a class="reference" href="/categories/vmprof.html">vmprof</a> (3)</li>
            <li><a class="reference" href="/categories/z3.html">z3</a> (5)</li>
          </ul>
        </div></div>
</main>
</div>
<div style="clear: both; width: 75%; margin: 1em auto;">
        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-21.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-19.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
         
                 <footer id="footer"><p>
</p>
<div class="myfooter">
  <div class="logotext">
    © 2026 <a href="mailto:pypy-dev@pypy.org">The PyPy Team</a>
     
    Built with <a href="https://getnikola.com" rel="nofollow">Nikola</a>
     
    Last built 2026-01-17T00:22
  </div>
  <div style="margin-left: auto">
  <a href="../rss.xml">RSS feed</a>
</div>

            
        

    </div>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js" crossorigin="anonymous"></script><script src="../assets/js/styles.js"></script></footer>
</body>
</html>
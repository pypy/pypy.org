<!DOCTYPE html>
<html \ prefix="
        og: http://ogp.me/ns# article: http://ogp.me/ns/article#
    " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A Faster Python">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PyPy (old posts, page 8) | PyPy</title>
<link href="../assets/css/rst_base.css" rel="stylesheet" type="text/css">
<link href="../assets/css/nikola_rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../assets/css/styles.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://www.pypy.org/blog/index-8.html">
<link rel="icon" href="../favicon2.ico" sizes="16x16">
<link rel="icon" href="../favicon32x32.ico" sizes="32x32">
<link rel="prev" href="index-9.html" type="text/html">
<link rel="next" href="index-7.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="../assets/css/tipuesearch.css">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="container">
             <header id="header"><!-- Adapted from https://www.taniarascia.com/responsive-dropdown-navigation-bar --><section class="navigation"><div class="nav-container">
            <div class="brand">
                <a href="../index.html">
                    <image id="toplogo" src="../images/pypy-logo.svg" width="75px;" alt="PyPy/"></image></a>
            </div>
            <nav><ul class="nav-list">
<li> 
                <a href="#!">Features</a>
                <ul class="nav-dropdown">
<li> <a href="../features.html">What is PyPy?</a> </li>  
                    <li> <a href="../compat.html">Compatibility</a> </li>  
                    <li> <a href="../performance.html">Performance</a> </li>  
                </ul>
</li>
          <li> <a href="../download.html">Download</a> </li>  
          <li> <a href="http://doc.pypy.org">Dev Docs</a> </li>  
            <li> 
                <a href="#!">Blog</a>
                <ul class="nav-dropdown">
<li> <a href=".">Index</a> </li>  
                    <li> <a href="../categories/">Tags</a> </li>  
                    <li> <a href="../archive.html">Archive by year</a> </li>  
                    <li> <a href="../rss.xml">RSS feed</a> </li>  
                    <li> <a href="https://morepypy.blogspot.com/">Old site</a> </li>  
                </ul>
</li>
            <li> 
                <a href="#!">About</a>
                <ul class="nav-dropdown">
<li> <a href="https://bsky.app/profile/pypyproject.bsky.social">Bluesky</a> </li>  
                    <li> <a href="https://libera.irclog.whitequark.org/pypy">IRC logs</a> </li>  
                    <li> <a href="https://www.youtube.com/playlist?list=PLADqad94yVqDRQXuqxKrPS5QnVqbDLlRt">YouTube</a> </li>  
                    <li> <a href="https://www.twitch.tv/pypyproject">Twitch</a> </li>  
                    <li> <a href="../pypy-sponsors.html">Sponsors</a> </li>  
                    <li> <a href="../howtohelp.html">How To Help?</a> </li>  
                    <li> <a href="../contact.html">Contact</a> </li>  
                </ul>
</li>

                </ul></nav><div class="nav-mobile">
                <a id="nav-toggle" href="#!"> <span></span></a>
            </div>
        </div>
    </section><div class="searchform" role="search">
                
<form class="navbar-form navbar-left" action="../search.html" role="search">
    <div class="form-group">
        <input type="text" class="form-control" id="tipue_search_input" name="q" placeholder="Searchâ€¦" autocomplete="off">
</div>
    <input type="submit" value="Local Search" style="visibility: hidden;">
</form>

            </div>
    </header><main id="content"><div class="post">
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/12/porting-jit-to-cli-part-3-3519327524638923621.html" class="u-url">Porting the JIT to CLI (part 3)</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/antonio-cuni.html">Antonio Cuni</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/12/porting-jit-to-cli-part-3-3519327524638923621.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-12-07T13:39:00Z" itemprop="datePublished" title="2008-12-07 13:39">2008-12-07 13:39</time></a>
            </p>
                <p class="commentline">12 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>In my two <a class="reference" href="../posts/2008/11/porting-jit-to-cli-part-1-8712941279840156635.html">previous</a> <a class="reference" href="../posts/2008/11/porting-jit-to-cli-part-2-2456826431882963884.html">posts</a>, we talked about the PyPy JIT generator, seeing
that it can produce huge speedups and how its backend-independent frontend
works.</p>
<p>In this post, we will look closer at the internals of the CLI JIT backend; in
particular, we will see how we work around some serious limitations of the
platform, and why these workarounds didn't have any serious impact on the
performances of our <a class="reference" href="https://codespeak.net/svn/pypy/branch/oo-jit/pypy/jit/tl/tlc.py">toy virtual machine</a>.</p>
<div class="section">
<h2>Graphs, blocks, links</h2>

<a href="https://3.bp.blogspot.com/_4gR6Ggu8oHQ/STvSnEbJOxI/AAAAAAAAAEo/hu23h0n0mIc/s1600-h/flowgraph.png"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5277042957038992146" src="https://3.bp.blogspot.com/_4gR6Ggu8oHQ/STvSnEbJOxI/AAAAAAAAAEo/hu23h0n0mIc/s320/flowgraph.png" style="margin: 0pt 0pt 10px 10px; float: right; cursor: pointer; width: 239px; height: 301px;"></a>


<p>One of the core aspect of PyPy translator is the concept of <strong>flow graph</strong>: a
flow graph is a data structure that represents the code we are operating on.
It is composed by a set of <strong>basic blocks</strong>, each block containing a sequence
of operations; blocks are connected together by <strong>links</strong>, and each link can
carry a variable number of arguments whose value is passed to the target
block.  In case a block contains more than one outgoing links, the one to
follow is selected by looking at the value of a designated variable (the
<strong>exitswitch</strong>), thus making possible to implement conditional jumps.  To have
a more complete description of the flow graphs model, check the <a class="reference" href="https://codespeak.net/pypy/dist/pypy/doc/translation.html#the-flow-model">documentation</a>.</p>

<p>As we saw in the previous post, the generated JIT compiler makes heavy use of
<strong>flexswitches</strong> to generate efficient code, continuously intermixing
JIT-compile time and runtime.</p>
<p>In terms of graphs, we can think of a flexswitch as a special block whose
links change over time.  In particular, adding a new case to the flexswitch is
equivalent to create a link whose target is a new block where the just
generated code starts.  Thus, the graphs <strong>grows</strong> over the time, as showed by
the following images:</p>

<a href="https://4.bp.blogspot.com/_4gR6Ggu8oHQ/STvTL5PHePI/AAAAAAAAAEw/5-hp6dXBLSo/s1600-h/promotion-1.png"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5277043589690915058" src="https://4.bp.blogspot.com/_4gR6Ggu8oHQ/STvTL5PHePI/AAAAAAAAAEw/5-hp6dXBLSo/s320/promotion-1.png" style="cursor: pointer; width: 254px; height: 320px;"></a><a href="https://4.bp.blogspot.com/_4gR6Ggu8oHQ/STvTMOTC1WI/AAAAAAAAAE4/RzLhmkE3IbE/s1600-h/promotion-2.png"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5277043595344532834" src="https://4.bp.blogspot.com/_4gR6Ggu8oHQ/STvTMOTC1WI/AAAAAAAAAE4/RzLhmkE3IbE/s320/promotion-2.png" style="cursor: pointer; width: 305px; height: 320px;"></a>

<a href="https://3.bp.blogspot.com/_4gR6Ggu8oHQ/STvTMf84hxI/AAAAAAAAAFA/bXnegFVvIqI/s1600-h/promotion-3.png"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5277043600083420946" src="https://3.bp.blogspot.com/_4gR6Ggu8oHQ/STvTMf84hxI/AAAAAAAAAFA/bXnegFVvIqI/s320/promotion-3.png" style="cursor: pointer; width: 320px; height: 178px;"></a>


<p>In the images above, the block containing the flexswitch is colored in
cyan. In the first picture, there is only one block connected to the
flexswitch: this block contains the code to restart the JIT compilation.  The
second picture shows the graph after the first case has been added: you can
clearly see that a new block has been created and attached to the flexswitch.
Finally, the third picture shows the graph after a while, with a lot of new
blocks attached.</p>
</div>
<div class="section">
<h2>Translate graphs to CLI</h2>
<p>Conceptually, the goal of the CLI JIT backend is to express these graphs in
terms of CLI bytecode.</p>
<p>Translating the single block is easy, as it is just a list of sequential
operation, and it's straightforward to map each operation to the equivalent
CLI opcode or to a call to a helper method.  Moreover, we need a way to
express <strong>links</strong> between the various basic blocks: if the links are known in
advance, render them is as easy as emitting a (potentially conditional) jump to
the target block.  Thus, we won't discuss this part in detail, as it is quite
straightforward.</p>
<p>The hard part is how to implement <strong>flexswitches</strong>: at the time when we are
emitting the code, some of the blocks of this growable graph don't even exist:
how can we make a jump to a non existent block of code?  For backends that
emit assembly code, it is rather easy: when they need to add a new case to the
flexswitch, they can just <strong>patch</strong> the existing code to insert a jump to a
newly allocated area of the memory, where the new code is being generated in.</p>
<p>For CLI this approach is not feasible, as the VM will never allow us to modify
existing code. Thus, we need to think of a different approach.</p>
</div>
<div class="section">
<h2>Graphs and methods</h2>
<p>In .NET, the basic unit of compilation is the <strong>method</strong>: the only way to
execute some bytecode is to wrap it into a method.  Moreover, it is not
possible to execute a method until it has been completed, and after this point
it is no longer possible to <strong>add new code</strong>.</p>
<p>Because of all these constraints we cannot simply map each graph to its own
method, since we saw that our graphs can <strong>grow</strong> after they have already been
executed few times.</p>
<p>Hence, we need to distinguish between the two concepts:</p>
<blockquote>
<ul class="simple">
<li>a <strong>graph</strong> is the logical unit of code as seen by the JIT compiler:
concretely, the CLI JIT backend renders it as <em>one or more</em> methods;</li>
<li>a <strong>method</strong> is a collection of basic blocks; each method has the so
called <em>parent graph</em>, i.e. the graph its blocks logically belongs to.</li>
</ul>
</blockquote>
<p>The first method of a graph is called <strong>main method</strong> (which has
nothing to do with the <tt class="docutils literal"><span class="pre">Main</span></tt> static methods found in <tt class="docutils literal"><span class="pre">.exe</span></tt> files); other
methods are called <strong>children methods</strong>.</p>
<p>When we want to add a new case to the flexswitch, we create a method
containing all the new code; then we wrap the method inside a <a class="reference" href="https://msdn.microsoft.com/en-us/magazine/cc301810.aspx">delegate</a> (the
.NET equivalent of a function pointer) and pass it to the flexswitch, so that
it can later invoke it.</p>
</div>
<div class="section">
<h2>The hard bit: non-local links</h2>
<p>Using this approach, after a while the blocks of our original graph are
scattered over a lot of different methods; however, there are no constraints
about how these blocks can be linked together, so it happens to have links
between blocks which are not in the same method. In the following, we will
refer to them as <strong>non-local links</strong>.</p>
<p>If the non-local block we want to jump to happens to be at the beginning of
its containing method, it is enough to invoke the method; but, what if we want
to jump somewhere in the middle?  What we really want is to produce a method
which has <strong>multiple entry-points</strong>; again, doing it in assembly would be
trivial, but the virtual machine does not provide any support for it, so we
need a work around.</p>
<p>Each method in a graph is assigned an unique 16 bit <em>method id</em>; each block in
a method is assigned a progressive 16 bit <em>block number</em>.  From this two
numbers, we can compute the <em>block id</em> as an <tt class="docutils literal"><span class="pre">unsigned</span> <span class="pre">integer</span></tt>, by storing
the method id in the first 16 bits and the block number in the second 16 bits.
By construction, the block id is guaranteed to be unique in the graph.</p>
<p>The following picture shows a graph composed of three methods; the id of<a href="https://3.bp.blogspot.com/_4gR6Ggu8oHQ/STvT8_6SS1I/AAAAAAAAAFI/8JIdzVaTHPk/s1600-h/flexswitch-cli.png"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5277044433296182098" src="https://3.bp.blogspot.com/_4gR6Ggu8oHQ/STvT8_6SS1I/AAAAAAAAAFI/8JIdzVaTHPk/s320/flexswitch-cli.png" style="margin: 0pt 0pt 10px 10px; float: right; cursor: pointer; width: 266px; height: 320px;"></a> each
method is shown in red, while the block ids are shown in red (for the method
id part) and black (for the block number part).  The graph contains three
non-local links; in particular, note the link between blocks <tt class="docutils literal"><span class="pre">0x00020001</span></tt>
and <tt class="docutils literal"><span class="pre">0x00010001</span></tt> which connects two block that resides in different methods.</p>



<p>Every method contains a special <strong>dispatch block</strong>, (not shown in the picture above) whose goal is to jump to
the specified block number inside the method itself.  The first argument of a
child method is always a block id; when the method starts, it immediately
jumps to the dispatch block, and thus to the desired block.</p>
<a href="https://3.bp.blogspot.com/_4gR6Ggu8oHQ/STvT8_6SS1I/AAAAAAAAAFI/8JIdzVaTHPk/s1600-h/flexswitch-cli.png"></a><p>For example, suppose to have a method which contains 3 blocks numbered 0, 1,
2; here is how its dispatch blocks looks like; for simplicity it is shown as
C# code, but it is actually generated as IL bytecode:</p>
<pre class="literal-block">
// dispatch block
int methodid = (blockid &amp; 0xFFFF0000) &gt;&gt; 16); // take the first 16 bits
int blocknum = blockid &amp;&amp; 0x0000FFFF;         // take the second 16 bits

if (methodid != MY_METHOD_ID) {
// jump_to_unknown block
...
}

switch(blocknum) {
case 0:
goto block0;
case 1:
goto block1;
case 2:
goto block2;
default:
throw new Exception("Invalid block id");
}
</pre>
<p>Whenever we want to jump to a non-local block, it is enough to store the block
id in the appropriate variable and jump to the dispatch block.  If the block
resides in a different method, the <tt class="docutils literal"><span class="pre">jump_to_unknown</span></tt> block is entered; this
special block is implemented differently by the main method and the child
methods, as we will see soon.</p>
<p>Each time a new method is added to the graph, we build a delegate
for it, and store it in a special array
called <tt class="docutils literal"><span class="pre">method_map</span></tt>; since we assign the method id sequentially starting
from 0, we are sure that to fetch the method whose id is <tt class="docutils literal"><span class="pre">n</span></tt> we can simply
load the <tt class="docutils literal"><span class="pre">n</span></tt>-th element of the array.</p>
<p>The <tt class="docutils literal"><span class="pre">jump_to_unknown</span></tt> block of the main method uses this array to select the
right method, and calls it (<tt class="docutils literal"><span class="pre">FlexSwitchCase</span></tt> is the type of delegates for
all children methods):</p>
<pre class="literal-block">
// jump_to_unknown block of the main method
FlexSwitchCase meth = method_map[methodid];
blockid = meth(blockid, ...); // execute the method
goto dispatch_block;
</pre>
<p>Each child method returns a block id specifying the next block to jump to;
after its execution, we assign the return value to the <tt class="docutils literal"><span class="pre">blockid</span></tt> variable,
and jump again to the dispatch block, which will jump again to the appropriate
block.</p>
<p>Keeping this in mind, it is straightforward to implement the
<tt class="docutils literal"><span class="pre">jump_to_unknown</span></tt> block of children methods: it is enough to return the
target block id to the caller, and let its dispatch loop do the right thing.
If the caller is also a child method, it will return it again, until we reach
the dispatch loop of the main method, which will finally do the jump.  In
theory, we could implement things differently and jumping directly from a
child method to another one, but in that case the call stack could grows
indefinitely in case of a tight loop between two blocks residing in different
methods.</p>
<p>To implement the dispatch block we can exploit the <tt class="docutils literal"><span class="pre">switch</span></tt> opcode of the
CLI; if the .NET JIT is smart enough, it can render it using an indirect jump;
overall, jumping to a non-local block consists of an indirect function call
(by invoking the delegate) plus an indirect jump (by executing the <tt class="docutils literal"><span class="pre">switch</span></tt>
opcode); even if this is more costly than a simple direct jump, we will see in
the next section that this not the main source of overhead when following a
non-local link.</p>
<p>Obviously, the slow dispatching logic is needed only when we want to jump to a
non-local block; if the target block happens to reside in the same method as
the current one, we can directly jump to it, completely removing the overhead.</p>
<p>Moreover, the dispatch blocks are emitted only if needed, i.e. if the parent
graph contains at least one flexswitch; graphs without flexswitches are
rendered in the obvious way, by making one method per graph.</p>
</div>
<div class="section">
<h2>The slow bit: passing arguments</h2>
<p>Jumping to the correct block is not enough to follow a link: as we said
before, each link carries a <strong>set of arguments</strong> to be passed from the source to
the target block.  As usual, passing arguments across local links is easy, as
we can just use local variables to hold their values; on the other hand,
non-local links make things more complex.</p>
<p>The only way to jump to a block is to invoke its containing method, so the
first solution that comes to mind is to specify its input arguments as
parameter of the method; however, each block has potentially a different
number (and different types) of input arguments than every other block, so we
need to think of something else.</p>
<p>An alternative solution could be to compute the union of the sets of input
arguments of <strong>all the blocks in the method</strong>, and use this set as a signature
for the method; this way, there would be enough space to specify the input
arguments for every block we might want to jump to, each block ignoring the
exceeding unused parameters.</p>
<p>Unfortunately, all the children methods must have the <strong>very same signature</strong>,
as they are all called from the same calling site in the dispatch block of the
main method.  Since the union of the set of input arguments (and hence the
computed signature) varies from method to method, this solution cannot work.</p>
<p>We might think to determine the signature by computing the union of input
arguments of <strong>all blocks in the graph</strong>; this way, all the children methods
would have the same signature.  But as we said above, the graph grows new
blocks at runtime, so we cannot determine in advance which set of input
arguments we will need.</p>
<p>To solve the problem we need a way to pass a variable number of arguments
without knowing in advance neither their number nor their types.  Thus, we use
an instance of this class:</p>
<pre class="literal-block">
public class InputArgs {
public int[] ints;
public float[] floats;
public object[] objs;
...
}
</pre>
<p>Since the fields are arrays, they can grow as needed to contain any number of
arguments; arguments whose type is primitive are stored in the <tt class="docutils literal"><span class="pre">ints</span></tt> or
<tt class="docutils literal"><span class="pre">floats</span></tt> array, depending on their type; arguments whose type is a reference
type are stored in the <tt class="docutils literal"><span class="pre">objs</span></tt> array: it's up to each block to cast each
argument back to the needed type.</p>
<p>This solution impose a huge overhead on both writing and reading arguments:</p>
<blockquote>
<ul class="simple">
<li>when writing, we need to make sure that the arrays are big enough to
contains all the arguments we need; if not, we need to allocate a bigger
array.  Moreover, for each argument we store into the array the virtual
machine performs a bound-check, even if we know the index will never be
out of bounds (because we checked the size of the array in advance);</li>
<li>when reading, the same bound-check is performed for each argument read;
moreover, for each value read from the <tt class="docutils literal"><span class="pre">objs</span></tt> array we need to insert a
downcast.</li>
</ul>
</blockquote>
<p>To mitigate the performance drop, we avoid to allocate a new <tt class="docutils literal"><span class="pre">InputArgs</span></tt>
object each time we do a non-local jump; instead, we preallocate one at the
beginning of the main method, and reuse it all the time.</p>
<p>Our <a class="reference" href="https://codespeak.net/svn/user/antocuni/cli-bench/arguments.cs">benchmarks</a> show that passing arguments in arrays is about 10 times slower
than passing them as real parameter of a method.  Unfortunately, we couldn't
come up with anything better.</p>
</div>
<div class="section">
<h2>Implement flexswitches</h2>
<p>Now, we can exploit all this machinery to implement flexswitches, as this is
our ultimate goal.  As described above, the point is to be able to <strong>add new
cases</strong> at runtime, each case represented as a delegate.  Here is an excerpt
of the C# class that implements a flexswitch that switches over an integer
value:</p>
<pre class="literal-block">
public class IntLowLevelFlexSwitch:
{
public uint default_blockid = 0xFFFFFFFF;
public int numcases = 0;
public int[] values = new int[4];
public FlexSwitchCase[] cases = new FlexSwitchCase[4];

public void add_case(int value, FlexSwitchCase c)
{
...
}

public uint execute(int value, InputArgs args)
{
for(int i=0; i&lt;numcases; i++)
if (values[i] == value) {
 return cases[i](0, args);
}
return default_blockid;
}
}
</pre>
<p>For each case, we store both the triggering value and the corresponding
delegate; the <tt class="docutils literal"><span class="pre">add_case</span></tt> method takes care to append <tt class="docutils literal"><span class="pre">value</span></tt> and <tt class="docutils literal"><span class="pre">c</span></tt> to
the <tt class="docutils literal"><span class="pre">values</span></tt> and <tt class="docutils literal"><span class="pre">cases</span></tt> arrays, respectively (and resize them if
necessary).  The interesting bit is the <tt class="docutils literal"><span class="pre">execute</span></tt> method: it takes a value
and a set of input arguments to be passed across the link and jumps to the
right block by performing a linear search in the <tt class="docutils literal"><span class="pre">values</span></tt> array.</p>
<p>As shown by previous sections, the first argument of a <tt class="docutils literal"><span class="pre">FlexSwitchCase</span></tt> is
the block id to jump to; since when we go through a flexswitch we always want
to jump to the first block of the method, we pass the special value <tt class="docutils literal"><span class="pre">0</span></tt> as a
block id, which precisely means <tt class="docutils literal"><span class="pre">jump</span> <span class="pre">to</span> <span class="pre">the</span> <span class="pre">first</span> <span class="pre">block</span></tt>.  This little
optimization let us not to have to explicitly store the block id for the first
block of all the cases.</p>
<p>The value returned by <tt class="docutils literal"><span class="pre">execute</span></tt> is the next block id to jump to; if the
value is not found in the <tt class="docutils literal"><span class="pre">values</span></tt> array, we return the <tt class="docutils literal"><span class="pre">default_blockid</span></tt>,
whose value has been set before by the JIT compiler; <tt class="docutils literal"><span class="pre">default_blockid</span></tt>
usually points to a block containing code to restart the JIT compiler again;
when the JIT compiler restarts, it emits more code for the missing case, then
calls <tt class="docutils literal"><span class="pre">add_case</span></tt> on the flexswitch; from now on, the new blocks are wired
into the existing graph, and we finally managed to implement <strong>growable
graphs</strong>.</p>
</div>
<div class="section">
<h2>Performances</h2>
<p>As we saw, implementing growable graphs for CLI is a pain, as the virtual machine
offers very little support, so we need an incredible amount of workarounds.
Moreover, the code generated is much worse than what an assembly backend could
produce, and the cost of following a non-local link is very high compared to
local links.</p>
<p>However, our first <a class="reference" href="../posts/2008/11/porting-jit-to-cli-part-1-8712941279840156635.html">blog post</a> showed that we still get very good
performances; how is it possible?</p>
<p><a class="reference" href="https://en.wikipedia.org/wiki/Optimization_%28computer_science%29#Bottlenecks">As usual</a> in computer science, most of the time of a running program in
spent in a tiny fraction of the code; our benchmark is no exception, and the
vast majority of the time is spent in the inner loop that multiplies numbers;
the graph is built in such a way that all the blocks that are part of the
inner loop reside in the same method, so that all links inside are local (and
fast).</p>
<p>Flexswitches and non-local links play a key role to select the right
specialized implementation of the inner loop, but once it is selected they are
not executed anymore until we have finished the computation.</p>
<p>It is still unclear how things will look like when we will compile the full
Python language instead of a toy one; depending on the code, it could be
possible to have non-local links inside the inner loop, thus making
performance much worse.</p>
</div>
<div class="section">
<h2>Alternative implementations</h2>
<p>Before implementing the solution described here, we carefully studied a lot of
possible alternatives, but all of them either didn't work because of a
limitation of the virtual machine or they could work but with terrible
performances.</p>
<p>In particular, in theory it is possible to implement non-local links using
tail calls, by putting each block in its own method and doing a tail call
instead of a jump; this would also solve the problem of how to pass arguments,
as each method could have its own signature matching the input args of the
block.  I would like to explain this solution in a more detailed way as I
think it's really elegant and nice, but since this post is already too long,
I'll stop here :-).</p>
<p>In theory, if the .NET JIT were smart enough it could inline and optimize away
the tail calls (or at least many of those) and give us very efficient code.
However, one <a class="reference" href="https://codespeak.net/svn/user/antocuni/cli-bench/tailcall.il">benchmark</a> I wrote shows that tail calls are up to 10 times
slower (!!!) than normal calls, thus making impractical to use them for our
purposes.</p>
</div>
<div class="section">
<h2>Conclusion</h2>
<p>Despite the complexity of the implementation, our result are extremely good;
the speedup we got is impressive, and it proves that PyPy's approach to JIT
compiler can work well also on top of object oriented virtual machines like
.NET or the JVM.</p>
<p>Generating bytecode for those machine at runtime is not a new idea; Jython,
IronPython, JRuby and other languages have been doing this for years.
However, Jython and IronPython do only a simple "static" translation, which
doesn't take advantage of the informations gathered at runtime to generate
better, faster and specialized code.  Recently, JRuby grew a new strategy to
JIT-compile only hotspots, taking advantage of some informations gathered
while interpreting the code; this is still a "one-shot" compilation, where the
compiled code does not change over time.</p>
<p>To my knowledge, PyPy brings the first example of a
language which implements a truly JIT compiler on top of the underlying JIT
compiler of the virtual machine, emitting bytecode that changes and adapts
over the time.  If someone knows other languages doing that, I would really
like to know more.</p>
<p>Being so innovative, the problem of this approach is that the current virtual
machines are not designed to support it in a native way, and this forces us to
put a lot of workarounds that slow down the generated code.  The hope is that
in the future the virtual machines will grow features that help us to generate
such kind of code.  The experimental <a class="reference" href="https://openjdk.java.net/projects/mlvm/">Da Vinci VM</a> seems to go in the right
direction, so it is possible that in the future I will try to write a JIT
backend for it.</p>
<p>At the moment, the CLI JIT backend is almost complete, and all the hardest
problems seems to be solved; the next step is to fix all the remaining bugs
and implement some minor feature that it's still missing, then try to apply it
to the full Python language and see what is the outcome.</p>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-2194033314687750355">
        <div class="comment-header">
          <a name="comment-2194033314687750355"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2008-12-07 22:33</span>:
        </div>
        <div class="comment-content">
          <p>JikesRVM + LLVM<br><br>https://osdir.com/ml/java.jikes.rvm.devel/2003-09/msg00059.html<br><br>Don't know if it succeeded.</p>
        </div>
      </div>
      <div class="comment comment-748377385403308392">
        <div class="comment-header">
          <a name="comment-748377385403308392"></a>
            <span class="author">Yosef</span> wrote on <span class="date">2008-12-08 08:15</span>:
        </div>
        <div class="comment-content">
          <p>The comment about assembly-code patching is interesting. Do you mean assembly code backends can do runtime patching of previously generated code? I thought this is impossible, because operating systems mark executable pages as read-only. How is that dealt with?</p>
        </div>
      </div>
      <div class="comment comment-6576038423472035520">
        <div class="comment-header">
          <a name="comment-6576038423472035520"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2008-12-08 09:29</span>:
        </div>
        <div class="comment-content">
          <p>Most executable pages are read only, but there is nothing that stops you from creating ones that are rw. You just pass different flags to mmap.<br><br>Cheers,<br>fijal</p>
        </div>
      </div>
      <div class="comment comment-7246241916706251211">
        <div class="comment-header">
          <a name="comment-7246241916706251211"></a>
            <span class="author">Antonio Cuni</span> wrote on <span class="date">2008-12-08 19:11</span>:
        </div>
        <div class="comment-content">
          <p>@Yosef<br>about patching generated code, see fijal's comment. Btw, this is exactly the same approach used by psyco</p>
        </div>
      </div>
      <div class="comment comment-8190501007831824691">
        <div class="comment-header">
          <a name="comment-8190501007831824691"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-12-11 08:22</span>:
        </div>
        <div class="comment-content">
          <p>It's wrong to say IronPython only does static translation. The Call Site stuff happens and generates IL at run time, and generates different code depending on the types. In fact you may want to look at how they do it, becuase they regenerate the IL for a method multiple times, which may be another way of implementing Flex switches</p>
        </div>
      </div>
      <div class="comment comment-8820877857678846827">
        <div class="comment-header">
          <a name="comment-8820877857678846827"></a>
            <span class="author">Antonio Cuni</span> wrote on <span class="date">2008-12-12 09:02</span>:
        </div>
        <div class="comment-content">
          <p>@Ben Young<br>do you have a link that explains in more detail what you mean?<br>As far as I know, DLR's callsites are just a way to do polymorphic inline caches, but nothing more. In particular, they don't do any specialization of the called code.<br><br>You are right that we could do the same to implement flexswitches, though I think this is a minor optimization, as right now the real performance problem is how to pass arguments across non-local links.</p>
        </div>
      </div>
      <div class="comment comment-1844879182752100522">
        <div class="comment-header">
          <a name="comment-1844879182752100522"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-12-12 16:26</span>:
        </div>
        <div class="comment-content">
          <p>Hi Antonio<br><br>IronPython can create a method that looks like this<br><br>void object add(object a, object b)<br>{<br> throw new Exception("I don't know how to add")<br>}<br><br>into <br><br>void object add(object a, object b)<br>{<br>  if(a is int &amp;&amp; b is int)<br>    return (int)a + (int)b<br><br> throw new Exception("I don't know how to add")<br>}<br><br>and can further add new tests at runtime. The code do do the adding is written directly into the method body and there's no futher call needed. This is runtime code generation, not just caching<br><br>In your case, instead of having multiple methods implementing different blocks you could just rewrite the whole "master" method every time the flexswitch changes. That way there's no call overhead at all. That's what the DLR does. I think the main thing it's missing is promotion, so shared tests can't be moved up a level, and it doesn't do inlining.</p>
        </div>
      </div>
      <div class="comment comment-8958411298529361911">
        <div class="comment-header">
          <a name="comment-8958411298529361911"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-12-18 08:32</span>:
        </div>
        <div class="comment-content">
          <p>I'm a beginner programmer, so please excuse my beginner questions :-)<br><br>I just started learning Python as my first programming language.  Several of my programmer friends have said I should learn Java instead, one reason being the difference in performance - specifically for doing natural language processing / AI stuff which is the area I am interested in.<br><br>With PyPy, do you think it is likely that in the near future, Python's performance may be close to that of Java?  I do plan on learning multiple languages, but it would be nice if I could stick with Python for as long as possible :-)</p>
        </div>
      </div>
      <div class="comment comment-2383325692528445994">
        <div class="comment-header">
          <a name="comment-2383325692528445994"></a>
            <span class="author">Lucian</span> wrote on <span class="date">2008-12-20 12:56</span>:
        </div>
        <div class="comment-content">
          <p>@Anonymous<br><br>Probably. People have great hopes for PyPy, but you can never know how it will turn out, if at all.<br><br>Right now, you can use things like numpy, psycho, shedskin, cython/pyrex and a few others to speed up you code, only needing to know a few things about C or C++. Google them.</p>
        </div>
      </div>
      <div class="comment comment-652797454443116039">
        <div class="comment-header">
          <a name="comment-652797454443116039"></a>
            <span class="author">Luis</span> wrote on <span class="date">2008-12-20 14:38</span>:
        </div>
        <div class="comment-content">
          <p>@Sin<br><br>You don't need to know any c or c++ to use psyco or shedskin. Only python.</p>
        </div>
      </div>
      <div class="comment comment-4432448715501470958">
        <div class="comment-header">
          <a name="comment-4432448715501470958"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2009-03-05 03:07</span>:
        </div>
        <div class="comment-content">
          <p>WoW shares many <a href="https://www.game4power.com/" rel="nofollow">wow gold</a> of its features with previously launched games. Essentially, you battle with <a href="https://www.game4power.com/buy-gold/" rel="nofollow"> wow gold cheap </a> monsters and traverse the countryside, by yourself or as a <a href="https://www.game4power.com/buy-gold/" rel="nofollow">buy cheap wow gold</a> team, find challenging tasks, and go on to higher <a href="https://www.aocsale.com/" rel="nofollow">aoc gold</a> levels as you gain skill and experience. In the course of your journey, you will be gaining new powers that are increased as your skill rating goes up. All the same, in terms of its features and quality, that is a ture stroy for this.WoW is far ahead of all other games of the genre the <a href="https://www.gamelevelup.com/" rel="nofollow">wow power leveling</a> game undoubtedly is in a league of its own and <a href="https://www.wowgoldone.com/" rel="nofollow">cheapest wow gold </a> playing it is another experience altogether.<br><br>Even though WoW is a <a href="https://www.wowgoldone.com/" rel="nofollow"> Cheap Wow Gold </a> rather complicated game, the controls and interface are done in <a href="https://www.vipwarhammergold.com/" rel="nofollow"> warhammer gold</a> such a way that you don't feel the complexity. A good feature of the game is that it <a href="https://www.itemstores.com/" rel="nofollow">buy wow items</a> does not put off people with lengthy manuals. The instructions <a href="https://www.bygamer.org/" rel="nofollow">bygamer</a> cannot be simpler and the pop up tips can help you start playing the game <a href="https://www.itemchannel.com/" rel="nofollow">World Of Warcraft Gold </a> immediately. If on the other hand, you need a detailed manual, the instructions are there for you to access. Buy wow gold in this site,good for you, <a href="https://www.game4power.com/" rel="nofollow">BUY WOW GOLD</a>.</p>
        </div>
      </div>
      <div class="comment comment-3346727534881152825">
        <div class="comment-header">
          <a name="comment-3346727534881152825"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2009-04-11 04:00</span>:
        </div>
        <div class="comment-content">
          <p>My friends and I like to buy <a href="https://www.gamekoo.com/product/Anarchy_Online_credits.html" rel="nofollow">Anarchy credits</a>, because the <a href="https://www.gamekoo.com/product/Anarchy_Online_credits.html" rel="nofollow">Anarchy Online credits</a> is very useful to upgrade equipment. Only your equipment becomes better, then you can win this game. In <a href="https://www.gamekoo.com/product/Anarchy_Online_credits.html" rel="nofollow">Anarchy gold</a>, you can buy everything you want in this game. Tomorrow will be my birthday, so my friends promise to <a href="https://www.gamekoo.com/product/Anarchy_Online_credits.html" rel="nofollow">buy AO credits</a> as gifts. I am so happy. They understand me so well, <a href="https://www.gamekoo.com/product/Anarchy_Online_credits.html" rel="nofollow">Anarchy online gold</a> is my favorite. <br>I like <a href="https://www.gamekoo.com/product/Angels_Online_Gold.html" rel="nofollow">angels gold</a> very much because it is very useful. In fact at first sight I have fallen in love with <a href="https://www.gamekoo.com/product/Angels_Online_Gold.html" rel="nofollow">angels online gold</a>. So no matter how much I have spent to <a href="https://www.gamekoo.com/product/Angels_Online_Gold.html" rel="nofollow">buy angels gold</a>, I never regret. Because of <a href="https://www.gamekoo.com/product/Angels_Online_Gold.html" rel="nofollow">cheap angels online gold</a>, I meet a lot of friends.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/11/porting-jit-to-cli-part-2-2456826431882963884.html" class="u-url">Porting the JIT to CLI (part 2)</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/antonio-cuni.html">Antonio Cuni</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/11/porting-jit-to-cli-part-2-2456826431882963884.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-11-07T11:03:00Z" itemprop="datePublished" title="2008-11-07 11:03">2008-11-07 11:03</time></a>
            </p>
                <p class="commentline">6 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>In my <a class="reference" href="../posts/2008/11/porting-jit-to-cli-part-1-8712941279840156635.html">previous post</a>, we saw that PyPy JIT generator can produce huge
speedups when applied to the <a class="reference" href="https://www.blogger.com/.._%60sourcecode%60:https://codespeak.net/svn/pypy/branch/oo-jit/pypy/jit/tl/tlc.py">tlc</a> toy language.</p>
<p>In this post we will dive a bit into the internals of PyPy JIT, to see how it
manages to do so. Note that this is a <strong>very</strong> high level overview of how the
JIT works, and applies to all backends.  Then, in the third post of this
series, we will look closer at the CLI JIT backend, seeing how it works around
some .NET limitations and how the generated code looks like.</p>

<h2>PyPy JIT for dummies</h2>
<p>As you surely know, the key idea of PyPy is that we are too lazy to write a
JIT of our own: so, instead of passing nights writing a JIT, we pass years
coding a <strong>JIT generator</strong> that writes the JIT for us :-).</p>
<p>I'm not going to explain how the JIT generator does its job, (perhaps this
will be the subject of another blog post), but how the <strong>generated JIT</strong>
works.</p>
<p>There are values that, if known at compile-time (i.e., when the <em>JIT compiler</em>
runs), let the JIT to produce very efficient code.  In a dynamic language,
types are the primary example: for instance, suppose you are a compiler and
you have to compile to following Python function:</p>
<pre class="literal-block">
def mysum(a):
  return a + 1
</pre>
<p>At compile time, you don't have any knowledge about the type of the parameter:
it could be integer, float, an user defined object, etc.  In this situation,
the only safe choice is to emit code which does the usual, slow, full lookup
to know how to perform the operations.</p>
<p>On the other hand, suppose that you knew in advance that the parameter is an
integer: this time, you could emit code that exploits this <strong>extra
knowledge</strong>, by performing directly a fast integer addition.</p>
<p>The idea behind PyPy JIT is that if you don't have enough knowledge to
generate efficient code, you <strong>stop compiling</strong> and wait until you know
exactly what you need.  Concretely, you emit code that runs until the point
where you stopped the compilation, then it triggers a special procedure that
<strong>restarts the compiler</strong>.  This time the JIT compiler knows everything
you need, because you can inspect the state of the running program.</p>
<p>Let's see an example: the first time the JIT compiles <tt class="docutils literal"><span class="pre">mysum</span></tt>, it produces
something like this pseudo-code:</p>
<pre class="literal-block">
PyObject mysum_compiled(PyObject a)
{
  Type a_type = a.GetType();
  switch(a_type) {
      default: continue_compilation(a_type, &lt;position&gt;);
  }
}
</pre>
<p>If you call <tt class="docutils literal"><span class="pre">mysum(41)</span></tt>, the execution goes in the <em>default</em> branch of the
switch, thus calling <tt class="docutils literal"><span class="pre">continue_compilation</span></tt>: its job is to restart the JIT
compiler, which now can emit fast code because it knows the exact type of
<tt class="docutils literal"><span class="pre">a</span></tt>; then, it <strong>modifies</strong> the original <tt class="docutils literal"><span class="pre">mysum_compiled</span></tt> function, in
order to make it executing the newly generated code the next time it
encounters an integer at that point:</p>
<pre class="literal-block">
PyObject mysum_compiled(PyObject a)
{
  Type a_type = a.GetType();
  switch(a_type) {
      PyInteger: return new PyInteger(a.value+1); // fast path!
      default: continue_compilation(a_type, &lt;position&gt;);
  }
}
</pre>
<p>From now on, every time we call <tt class="docutils literal"><span class="pre">mysum</span></tt> with an integer argument, the JIT
compiler is not called anymore and the fast path is directly executed; if we
happen to call <tt class="docutils literal"><span class="pre">mysum</span></tt> with a float arguments, the switch goes again in the
<em>default</em> branch, and the JIT compiler is started once more to produce fast
code also for this case.  What happens in practice is that compile-time and
runtime are continuously intermixed, until the switches are stable enough and
the compiler is not needed anymore.</p>
<p>In PyPy jargon, this kind of "growable switch" is called <strong>flexswitch</strong>, and
it's one of the most important concept of our JIT generator.</p>

<h2>Promotion</h2>
<p>How can the JIT generator know which values are useful to know to generate
efficient code and which aren't?  Unfortunately it can't, or at least our JIT
generator is not smart enough at the moment.</p>
<p>To get the best from it, the developers of the VM need to instruct it by
<em>annotating</em> the variables on which we want the JIT to stop until it knows the
actual values; this is done by using particular <strong>hints</strong>, called <em>promote</em>
and <em>promote_class</em>; variables annotated with such hints are said to be
<strong>promoted</strong>. If something is promoted, a flexswitch is used to gain
information about it, as seen in the last section.</p>
<p>For an example, let's look at an excerpt from main dispatch loop of the <a class="reference" href="https://www.blogger.com/.._%60sourcecode%60:https://codespeak.net/svn/pypy/branch/oo-jit/pypy/jit/tl/tlc.py">tlc</a>
virtual machine:</p>
<pre class="literal-block">
elif opcode == ADD:
  a, b = stack.pop(), stack.pop()
  hint(a, promote_class=True)
  hint(b, promote_class=True)
  stack.append(b.add(a))
</pre>
<p>This the implementation of the <tt class="docutils literal"><span class="pre">ADD</span></tt> opcode: first, it pops two values from
the stack; then, it computes the result; finally, it push the result to the
stack again.  In between, both the classes of <tt class="docutils literal"><span class="pre">a</span></tt> and <tt class="docutils literal"><span class="pre">b</span></tt> have been
promoted: this means that when the JIT emits the code for <tt class="docutils literal"><span class="pre">b.add(a)</span></tt>, it
knows exactly what is happening: if it sees that both are instances of the
<tt class="docutils literal"><span class="pre">IntObj</span></tt> class, it inlines the method call and emits a fast integer addition
instead.</p>

<h2>Virtuals</h2>
<p>The other important concept of the JIT is the presence of <em>virtual
structures</em>, <em>virtual lists</em>, and <em>virtual dictionaries</em>.  Again, I'm not
going to explain in depth how they work, but only why they are so important for
generating highly efficient code.</p>
<p>The essence of virtuals is that you don't allocate objects until you really
need to do it, e.g. because they are being passed as an argument to some
external function.  Instead, we store all the informations we need as local
variables; e.g., in the case of a virtual structure, we create as many local
variables as the number of its fields: if the structure <em>escapes</em> the local
scope, we <em>force</em> it to a real object, by allocating memory on the heap and
initializing it after the current value of the local variables.</p>
<p>This technique allows the JIT to avoid the allocation of many temporary
objects that hold intermediate results; consider for example the following
Python loop:</p>
<pre class="literal-block">
result = 0
for i in range(N):
  result += i
return result
</pre>
<p>Without the JIT, at each iteration, a new <tt class="docutils literal"><span class="pre">int</span></tt> object is created and bound
to the <tt class="docutils literal"><span class="pre">result</span></tt> variable, while the previous one is discarded and not needed
anymore.  By combining virtuals and promotion, the JIT can emit code that does
the whole computation locally, and allocates a real object only at the end,
when it <em>escapes</em> from the local scope because it is returned from the
function.</p>

<h2>Putting it all together</h2>
<p>This is, essentially, how PyPy's generated JITs work.  To summarize, our JITs
emit multiple versions of each chunk of code: each version is <strong>specialized</strong>
and optimized for one particular case.</p>
<p>The cost of selecting the right specialization to use (through flexswitches)
is almost always negligible compared to how much time you save by running the
fast version instead of the more-general-but-slow one.  Moreover, each
specialized version knows the exact shape of the objects it's dealing with, so
they can be virtualized to make the generated code even more efficient.</p>
<p>At the end, the actual code generation is done by one of the <strong>JIT backends</strong>:
the backends exploit all the knowledge gathered by the previous steps to
produce highly efficient code, but this will be the subject of the next blog
post.</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-6902825876361907657">
        <div class="comment-header">
          <a name="comment-6902825876361907657"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-07 12:46</span>:
        </div>
        <div class="comment-content">
          <p>Wow... I love this approach. Keep up the great work and interesting posts!</p>
        </div>
      </div>
      <div class="comment comment-4821479895843489531">
        <div class="comment-header">
          <a name="comment-4821479895843489531"></a>
            <span class="author">Luis</span> wrote on <span class="date">2008-11-07 18:12</span>:
        </div>
        <div class="comment-content">
          <p>This is a very clear and didactic explanation. Thanks!</p>
        </div>
      </div>
      <div class="comment comment-950075562559359694">
        <div class="comment-header">
          <a name="comment-950075562559359694"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-07 20:14</span>:
        </div>
        <div class="comment-content">
          <p>can't wait for the next one</p>
        </div>
      </div>
      <div class="comment comment-4172511053662153109">
        <div class="comment-header">
          <a name="comment-4172511053662153109"></a>
            <span class="author">kbob</span> wrote on <span class="date">2008-11-08 01:35</span>:
        </div>
        <div class="comment-content">
          <p>What does the flexswitch compile into?  I'm guessing it would look like<br><br>    t = type(obj);<br>    if (t == int)<br>        ...<br>    else if (t == float)<br>        ...<br>    else<br>        ....<br><br>but maybe there's a better way (or maybe the answer is backend-dependent).</p>
        </div>
      </div>
      <div class="comment comment-2887523272693636820">
        <div class="comment-header">
          <a name="comment-2887523272693636820"></a>
            <span class="author">Antonio Cuni</span> wrote on <span class="date">2008-11-08 08:53</span>:
        </div>
        <div class="comment-content">
          <p>@bob<br>your guess is right, how to implement the flexswitch is backend-dependent.  This is the hardest part for .NET, as the flexswitch needs to grow dynamically (i.e., you have to add more case after the .NET method has already been compiled). It will be subject of the next blog post.</p>
        </div>
      </div>
      <div class="comment comment-463934217000221313">
        <div class="comment-header">
          <a name="comment-463934217000221313"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2009-01-05 22:46</span>:
        </div>
        <div class="comment-content">
          <p>It seems that an implication of the JIT way is that, by adopting a consistent habit of implementing type driven Generic Functions, the JIT could accomplish nearly all of the possible optimizations in a single pass. In other words, by definition, each type based variation of a Generic Function call can only be fired when data of that type is provided as a parameter.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/11/porting-jit-to-cli-part-1-8712941279840156635.html" class="u-url">Porting the JIT to CLI (part 1)</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/antonio-cuni.html">Antonio Cuni</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/11/porting-jit-to-cli-part-1-8712941279840156635.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-11-04T00:19:00Z" itemprop="datePublished" title="2008-11-04 00:19">2008-11-04 00:19</time></a>
            </p>
                <p class="commentline">7 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>As the readers of this blog <a class="reference" href="../posts/2008/05/progresses-on-cli-jit-backend-front-1021772190959551376.html">already know</a>, I have been working on the CLI
JIT backend for some months: last Friday, it reached an important milestone,
as it is now able to produce huge speedups for a little dynamic language.  To
know how huge the speedup is, read on :-).</p>
<p>The goal of PyPy JIT generator is to take an interpreter and, with the help of
few annotations, automatically generate a JIT compiler for it.  In this post,
we will talk about the <tt class="docutils literal"><span class="pre">tlc</span></tt> virtual machine: while <tt class="docutils literal"><span class="pre">tlc</span></tt> it is just a toy
language, it contains some features that make it an interesting target for our
JIT generator.</p>
<div class="section">
<h2>The <tt class="docutils literal"><span class="pre">tlc</span></tt> virtual machine</h2>
<p><tt class="docutils literal"><span class="pre">tlc</span></tt> is executed by a stack based, dynamically typed virtual machine (for
those who knows a bit about the Python VM: does it sound familiar? :-)).</p>
<p>There are three types of objects: <em>integers</em>, <em>nil</em>, and <em>cons cells</em> (i.e.
lisp-like pairs of objects).</p>
<p>As the VM is very simple, it provides only few opcodes:</p>
<blockquote>
<ul class="simple">
<li>opcodes to manipulate the stack, like <tt class="docutils literal"><span class="pre">PUSH</span></tt>, <tt class="docutils literal"><span class="pre">POP</span></tt>, etc.</li>
<li>integer operations, like <tt class="docutils literal"><span class="pre">ADD</span></tt>, <tt class="docutils literal"><span class="pre">MUL</span></tt>, all the comparisons, etc.:
these operations can only be applied to integers;</li>
<li>list operations, like <tt class="docutils literal"><span class="pre">CONS</span></tt>, <tt class="docutils literal"><span class="pre">CAR</span></tt>, <tt class="docutils literal"><span class="pre">CDR</span></tt>: these operations can
only be applied to lists;</li>
<li>other operations, including jumps and conditional jumps.</li>
</ul>
</blockquote>
<p>The VM is interesting for our purposes because it has a lot of similarities
with Python (though on a smaller scale, of course):</p>
<blockquote>
<ol class="arabic simple">
<li>it has to do type-checks at runtime before doing most of the operations;</li>
<li>every time you do an arithmetic operation, it has to unbox the operand,
do the computation, and the box the result again.</li>
</ol>
</blockquote>
<p>This means that even if you have a program which only uses integers, you are
paying a lot of overhead.</p>
<p>To know more about this toy VM, look at its <a class="reference" href="https://codespeak.net/svn/pypy/branch/oo-jit/pypy/jit/tl/tlc.py">source code</a>: the interesting
bits are the classes used to represent objects, and the <tt class="docutils literal"><span class="pre">interp_eval</span></tt>
function, which contains the main loop of the virtual machine.  As you can
see, the implementation is quite straightforward; all the <tt class="docutils literal"><span class="pre">hint</span></tt> calls you
see are the special annotations needed by the JIT generator to produce better
code.</p>
</div>
<div class="section">
<h2>Let's JIT it!</h2>
<p>So, the whole point is to generate a JIT compiler from it, isn't it?</p>
<p>First, checkout a fresh copy of the <tt class="docutils literal"><span class="pre">oo-jit</span></tt> branch:</p>
<pre class="literal-block">
$ svn co https://codespeak.net/svn/pypy/branch/oo-jit
</pre>
<p>Then, go to the <tt class="docutils literal"><span class="pre">oo-jit/pypy/jit/tl</span> <span class="pre">directory</span></tt>, and compile the <tt class="docutils literal"><span class="pre">tlc</span></tt> VM
with the CLI backend and JIT enabled:</p>
<pre class="literal-block">
$ cd oo-jit/pypy/jit/tl/
$ ../../translator/goal/translate.py -b cli --jit --batch targettlc
...
lot of texts
...
</pre>
<p>If everything went OK, you now have a <tt class="docutils literal"><span class="pre">targettlc-cli</span></tt> executable, which
accepts two arguments: the name of the file containing the <tt class="docutils literal"><span class="pre">tlc</span></tt> program we
want to run, and an integer to be passed to it.</p>
<p>Luckily, in the same directory we have a <tt class="docutils literal"><span class="pre">factorial.tlc</span></tt> file that contains
the bytecode for a function that -- guess? -- computes the factorial of a
given integer; let's try it:</p>
<pre class="literal-block">
$ ./targettlc-cli factorial.tlc 5
Non jitted:    120 (0.009371 seconds)
Warmup jitted: 120 (0.208954 seconds)
Warmed jitted: 120 (0.000323999999999991 seconds)
</pre>
<p>Cool, it seems that the result was computed correcly :-). As you can see from
the output, we ran the program three times:</p>
<blockquote>
<ol class="arabic simple">
<li>by plain interpretation, without any jitting;</li>
<li>with the jit enabled: this run includes the time spent by doing the
compilation itself, plus the time spent by running the produced code;</li>
<li>again with the jit enabled, but this time the compilation has already
been done, so we are actually measuring how good is the code we produced.</li>
</ol>
</blockquote>
<p>So, it's time to run a benchmark: let's try to compute the factorial of a very
big number; the result will be 0, because obviously after a while we overflow,
but after all we are interested in the time spent, not in the result:</p>
<pre class="literal-block">
$ ./targettlc-cli factorial.tlc 5000000
Non jitted:    0 (19.93247 seconds)
Warmup jitted: 0 (0.293229999999998 seconds)
Warmed jitted: 0 (0.0494239999999984 seconds)

$ python -c 'print 19.93247/0.0494239999999984'
403.295362577
</pre>
<p>And no, I didn't make any mistake in copying&amp;pasting: the jitted version is
really 400 times faster that the non jitted one!</p>
<p><strong>Warning</strong>: my laptop seems to be not very well suited for benchmarks, as the
results vary a lot from run to run; I've run the benchmarks a lot of times,
and I got speedup factors up to 500 times, so your results may be different.</p>
</div>
<div class="section">
<h2>More benchmarks</h2>
<p>It's also interesting to compare the result with a manual written <a class="reference" href="https://codespeak.net/svn/user/antocuni/cli-bench/factorial.cs">C#
version</a> of the factorial, to see how good is code we produced; to get
reasonable results, we need to compute a larger factorial, to let to code to
run a bit more:</p>
<pre class="literal-block">
$ ./targettlc-cli --onlyjit factorial.tlc 100000000
Warmup jitted: 0 (0.980856 seconds)
Warmed jitted: 0 (0.769716 seconds)

$ mono factorial.exe 100000000
C#:            0 (0.153777 seconds)

$ python -c 'print 0.769716/0.153777'
5.00540392907
</pre>
<p>We know that the generated code is far from being optimal, but probably the
factor of five is at least partially due to the fact that Mono's own JIT is optimized for
C#-like code, and our code has a completely different shape.</p>
<p>All the benchmarks above were run under Linux, with Mono 1.9.1.  Here are the
results for the same benchmarks, but run with Microsoft CLR (on a different
machine, so the absolute values are not comparable):</p>
<pre class="literal-block">
$ ./targettlc-cli factorial.tlc 5000000
Non jitted:    0 (15,640625 seconds)
Warmup jitted: 0 (0,4375 seconds)
Warmed jitted: 0 (0,03125 seconds)

$ python -c 'print 15.640625/0.03125'
500.5

$ ./targettlc-cli --onlyjit factorial.tlc 100000000
Warmup jitted: 0 (0,90625 seconds)
Warmed jitted: 0 (0,515625 seconds)

$ ./factorial.exe 100000000
C#:            0 (0,34375 seconds)

$ python -c 'print 0.515625/0.34375'
1.5
</pre>
<p>The results are even better than before; this is probably thanks to CLR's JIT,
that does a better job than Mono when faced to something which is different
than the usual C#-like code.</p>
</div>
<div class="section">
<h2>Conclusions (for now)</h2>
<p>This is a very important result, because it proves that PyPy's approach to JIT
compilers can be applied effectively also to OO virtual machines; the result
is even better than what I expected, because when generating code for .NET we
have much less freedom than when generating assembly code, and I had to play
some tricks to work around some .NET limitations.</p>
<p>Moreover, it worked at the first try :-). I tried to compile the <tt class="docutils literal"><span class="pre">tlc</span></tt>
virtual machine as soon as all the related JIT tests were passing, and
surprisingly everything worked just fine, even if it was the very first time I
was trying to apply some features of the JIT to something bigger than a test:
I think this is yet another prove that Test Driven Development just works!</p>
<p>Even if this is a major milestone, the CLI JIT backend is not yet completed:
as a consequence it can't still be used for the full PyPy, but all the
hardest problems should have been solved now.</p>
<p>Since a lot of readers asked for more technical details, especially about the
JIT, I will try to soon write a second blog post explaining how the CLI backend works
internally, with a brief look to the generated code to see how it looks like.</p>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-9060136630321512750">
        <div class="comment-header">
          <a name="comment-9060136630321512750"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-04 01:50</span>:
        </div>
        <div class="comment-content">
          <p>If you are benchmarking on Linux then watch out for CPU speed scaling.  For example on Ubuntu by default the ondemand governor is used which runs the CPU at lowest possible speed and until there is any CPU demand at which point it runs at fastest.  The time to switch varies (eg sometimes it can be instantaneous, other times a second or two, and other times not at all).<br><br>Make sure to use:  cpufreq-set -g performance<br><br>That will run at maximum CPU speed the whole time.</p>
        </div>
      </div>
      <div class="comment comment-4483810101473439195">
        <div class="comment-header">
          <a name="comment-4483810101473439195"></a>
            <span class="author">Lucian</span> wrote on <span class="date">2008-11-04 02:27</span>:
        </div>
        <div class="comment-content">
          <p>Woohoo! Can't wait for more (and the jvm counterpart).</p>
        </div>
      </div>
      <div class="comment comment-516463502224307256">
        <div class="comment-header">
          <a name="comment-516463502224307256"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-04 08:46</span>:
        </div>
        <div class="comment-content">
          <p>I agree that this result is very important. For me and many others too i guess, a very good JIT is the most important missing part in python and other dynamic languages. Speed *is* important,<br><br>For integers, psycho also showed huge performance gains. But i think the real proof of pypy's approach would be to show similar results for floating point operations also...</p>
        </div>
      </div>
      <div class="comment comment-4776979169227410801">
        <div class="comment-header">
          <a name="comment-4776979169227410801"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-04 11:09</span>:
        </div>
        <div class="comment-content">
          <p>awesome!</p>
        </div>
      </div>
      <div class="comment comment-111541136724327650">
        <div class="comment-header">
          <a name="comment-111541136724327650"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-04 11:47</span>:
        </div>
        <div class="comment-content">
          <p>Keep it on!</p>
        </div>
      </div>
      <div class="comment comment-1709143628204354758">
        <div class="comment-header">
          <a name="comment-1709143628204354758"></a>
            <span class="author">Antonio Cuni</span> wrote on <span class="date">2008-11-04 16:50</span>:
        </div>
        <div class="comment-content">
          <p>hi, thanks for your valuable comments.<br><br>Some notes:<br><br>  - I know about cpufreq-set, but even setting the governor to performance doesn't help, the timings vary a lot between different runs. If someone knows a way to run reliable benchmarks, it would be very appreciated!<br><br>  - I have plans to experiment also the JIT on the JVM: since HotSpot usually does a better job than CLR's JIT, it's possible/likely that the JVM is a better platform for our purposes. Also, the experimental Da Vinci Machine contains features that could be very useful for us.  Unfortunately the PyPy non-JIT JVM backend is not as advanced as the CLI one, and it lacks some features that are really needed for writing a JIT backend.<br><br>  - Float operations are already (mostly) supported by our JIT backends; I bet that if you add a FloatObj to the tlc interpreter, you will see huge speedups as well.  However, the real point of PyPy's approach is that once finished it will optimize much more than ints and floats, including features that are currently not implemented by psyco (e.g. generators).</p>
        </div>
      </div>
      <div class="comment comment-4397979347740737376">
        <div class="comment-header">
          <a name="comment-4397979347740737376"></a>
            <span class="author">ÎŸÏÎ­ÏƒÏ„Î·Ï‚</span> wrote on <span class="date">2008-11-04 21:21</span>:
        </div>
        <div class="comment-content">
          <p>Brilliant post! Keep us updated!</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/11/one-year-pypy-blog-3267056180369310162.html" class="u-url">One year PyPy Blog</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/11/one-year-pypy-blog-3267056180369310162.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-11-02T17:03:00Z" itemprop="datePublished" title="2008-11-02 17:03">2008-11-02 17:03</time></a>
            </p>
                <p class="commentline">19 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Last Friday the PyPy Status Blog had its first anniversary. Yay! After not
really buying into any of this new-fangled "blog" stuff for a long time we just
bit the bullet and got started. Totally surprisingly it even worked. We posted
76 post in the last year, more than one per week. By now we have more than 800
subscribers (according to feedburner), which is quite cool for a rather niche
blog.</p>
<p>To make our blog even more interesting, I would like to ask for some feedback
via the comments:</p>
<blockquote>
<ul class="simple">
<li>Which posts did you like in particular?</li>
<li>What sort of posts would you be interested in getting more of?</li>
<li>Any other improvements we could make?</li>
</ul>
</blockquote>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-5390012227892681987">
        <div class="comment-header">
          <a name="comment-5390012227892681987"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-02 18:28</span>:
        </div>
        <div class="comment-content">
          <p>For me the most interesting posts is about status of PyPy project. It will be great if you could post more frequently.</p>
        </div>
      </div>
      <div class="comment comment-381931695913090770">
        <div class="comment-header">
          <a name="comment-381931695913090770"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2008-11-02 20:40</span>:
        </div>
        <div class="comment-content">
          <p>+1</p>
        </div>
      </div>
      <div class="comment comment-111743648207369834">
        <div class="comment-header">
          <a name="comment-111743648207369834"></a>
            <span class="author">Michael Foord</span> wrote on <span class="date">2008-11-02 21:09</span>:
        </div>
        <div class="comment-content">
          <p>It's been great to read about PyPy progress, congratulations for surviving a year and many thanks.<br><br>I also like to hear the status updates - and wouldn't mind a bit more technical detail.<br><br>In fact some deep dives into individual aspects of PyPy would be *great*, even if they're more effort to write...</p>
        </div>
      </div>
      <div class="comment comment-6676742899101097269">
        <div class="comment-header">
          <a name="comment-6676742899101097269"></a>
            <span class="author">Eduardo O. Padoan</span> wrote on <span class="date">2008-11-02 21:45</span>:
        </div>
        <div class="comment-content">
          <p>Greetings!<br>What about quick Weekly Status Updates, with a summary of svn activity and stuff?</p>
        </div>
      </div>
      <div class="comment comment-5188191239847816923">
        <div class="comment-header">
          <a name="comment-5188191239847816923"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-03 02:41</span>:
        </div>
        <div class="comment-content">
          <p>It's not just a first for you, it's a first for me. This is the first blog I have ever subscribed to. You can attribute that to the fact that this subject is geniunly interesting.<br><br>The blog format has many benefits. For one, it amortizes the effort required to understand the project. This allows me to take my time, wiki whatever I need to, and savor the details. It takes time for me to learn the concepts but in due time, I can see myself eventually contributing to the project. The other benefit is I can see all the hot topics revolved around the various pypy projects. The whole partial evaulation, for example, was something new I learned about.<br><br>I would agree that increasing the rate of posts would be nice. While I can't say for others, in my personal experience, it seems that logged projects tend to finish faster than unlogged projects.</p>
        </div>
      </div>
      <div class="comment comment-7480418186043398582">
        <div class="comment-header">
          <a name="comment-7480418186043398582"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-03 02:42</span>:
        </div>
        <div class="comment-content">
          <p>It's not just a first for you, it's a first for me. This is the first blog I have ever subscribed to. You can attribute that to the fact that this subject is geniunly interesting.<br><br>The blog format has many benefits. For one, it amortizes the effort required to understand the project. This allows me to take my time, wiki whatever I need to, and savor the details. It takes time for me to learn the concepts but in due time, I can see myself eventually contributing to the project. The other benefit is I can see all the hot topics revolved around the various pypy projects. The whole partial evaulation, for example, was something new I learned about.<br><br>I would agree that increasing the rate of posts would be nice. While I can't say for others, in my personal experience, it seems that logged projects tend to finish faster than unlogged projects.</p>
        </div>
      </div>
      <div class="comment comment-4242124613372805563">
        <div class="comment-header">
          <a name="comment-4242124613372805563"></a>
            <span class="author">Bill Mill</span> wrote on <span class="date">2008-11-03 03:09</span>:
        </div>
        <div class="comment-content">
          <p>&gt;  Which posts did you like in particular?<br><br>I just scanned a bunch of entries, and "List comprehension implementation details" jumped out at me as a really nice one. I like that it points out some of the deep details of python that are easy for me to not think about because I'm not implementing it.<br><br>&gt; What sort of posts would you be interested in getting more of?<br><br>More technical details posts, I really like the one about the JIT and Prolog too.<br><br>I post your articles to reddit too, and I think "we can now run big software X" and efficency milestones are successfuly at attracting a lot of attention (if that's what you want!)</p>
        </div>
      </div>
      <div class="comment comment-8824465693245360440">
        <div class="comment-header">
          <a name="comment-8824465693245360440"></a>
            <span class="author">Benjamin Peterson</span> wrote on <span class="date">2008-11-03 03:11</span>:
        </div>
        <div class="comment-content">
          <p>Thanks so much for doing this! It makes me very jealous over here in CPython land.<br><br>I like to hear about specific new projects and ideas you guys are working on.</p>
        </div>
      </div>
      <div class="comment comment-8854443732363758183">
        <div class="comment-header">
          <a name="comment-8854443732363758183"></a>
            <span class="author">nshepperd</span> wrote on <span class="date">2008-11-03 05:39</span>:
        </div>
        <div class="comment-content">
          <p>For me the most interesting things were the technical details posts, like Bill Mill said. But I get excited any time there is a new blog post. :)</p>
        </div>
      </div>
      <div class="comment comment-2219459878006487858">
        <div class="comment-header">
          <a name="comment-2219459878006487858"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2008-11-03 06:35</span>:
        </div>
        <div class="comment-content">
          <p>Being in the scientific computation area at the moment, I'm very eager to hear about progress in the JIT framework, esp. for 64 bit Linux.<br><br>Yet most other posts are also interesting.</p>
        </div>
      </div>
      <div class="comment comment-5398587621952103739">
        <div class="comment-header">
          <a name="comment-5398587621952103739"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-03 11:01</span>:
        </div>
        <div class="comment-content">
          <p>&gt; Which posts did you like in particular?<br><br>Anything about the JIT and its progress.<br><br>Good luck!</p>
        </div>
      </div>
      <div class="comment comment-3427065518351169326">
        <div class="comment-header">
          <a name="comment-3427065518351169326"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2008-11-03 12:57</span>:
        </div>
        <div class="comment-content">
          <p>Hi,<br><br>I think the blog is pretty good. Weekly summaries would make it rock, though.<br><br>And I am also especially interested in hearing about progress on JIT work. And about any use of LLVM.<br><br>Best<br>Anders</p>
        </div>
      </div>
      <div class="comment comment-2834932968685213018">
        <div class="comment-header">
          <a name="comment-2834932968685213018"></a>
            <span class="author">Carl Friedrich Bolz-Tereick</span> wrote on <span class="date">2008-11-03 14:46</span>:
        </div>
        <div class="comment-content">
          <p>Thanks for all the friendly comments!<br><br>So more technical posts it will be :-). Those are mostly even fun to write, it's  just usually quite a bit of work. I actually have a vague plan to give a basic introduction of the ideas behind the JIT (but will still take some time, I am busy with the lecture at the moment).<br><br>About more summaries of what happens: it requires a lot of discipline (see the Python-dev summaries) and I am not sure we have that :-). It would need somebody dedicated to care for it, and that one won't be me at the moment.</p>
        </div>
      </div>
      <div class="comment comment-8358032256896787194">
        <div class="comment-header">
          <a name="comment-8358032256896787194"></a>
            <span class="author">Luis</span> wrote on <span class="date">2008-11-03 15:03</span>:
        </div>
        <div class="comment-content">
          <p>Personaly, I get very anxious when you say "it will be ready when it's ready". Aaarghhh! Please, at least lie a little bit :-). <br>For example: "Pypy is now 1.8x slower than cpython, but after [feature here] it will be 10x faster".<br>Well, just kidding. Congratulations for all the great work and keep it up!</p>
        </div>
      </div>
      <div class="comment comment-4492214284750269721">
        <div class="comment-header">
          <a name="comment-4492214284750269721"></a>
            <span class="author">Damian Cugley</span> wrote on <span class="date">2008-11-03 16:03</span>:
        </div>
        <div class="comment-content">
          <p>I am not especially in favour of weekly summaries, unless there is some interesting progress to report. Otherwise you end up with someone filling in progress reports because they feel obliged to, rather than to celebrate new features, and it becomes a chore.<br><br>That said, PyPy has many subprojects; maybe having a round-robin system where we get a progress report from a different project every week would be interesting.</p>
        </div>
      </div>
      <div class="comment comment-1416926804337491600">
        <div class="comment-header">
          <a name="comment-1416926804337491600"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-11-03 23:05</span>:
        </div>
        <div class="comment-content">
          <p>I'm a regular Python user that wishes often for something a little faster with the same flexibility.  So generally, I read this because I think you guys are on a good track for JIT optimisation and other fun things.<br><br>I guess I'm looking forward to the eventual series of posts that talks about how you can start using this on your system to replace your system Python, followed by you talking the regular Python core developers into working directly in PyPy instead. =)</p>
        </div>
      </div>
      <div class="comment comment-3155101080501044247">
        <div class="comment-header">
          <a name="comment-3155101080501044247"></a>
            <span class="author">Paul D. Eden</span> wrote on <span class="date">2008-11-03 23:36</span>:
        </div>
        <div class="comment-content">
          <p>For me the best parts are the tutorials and howtos relating to rpython, translating to c, etc.</p>
        </div>
      </div>
      <div class="comment comment-549827835447234733">
        <div class="comment-header">
          <a name="comment-549827835447234733"></a>
            <span class="author">Konrad</span> wrote on <span class="date">2008-11-07 11:54</span>:
        </div>
        <div class="comment-content">
          <p>I'm interested in status updates and longer descriptions on how elements of PyPy work. Sprint summaries are fine as long as they carry one of the above (they usually do, though :&gt;)</p>
        </div>
      </div>
      <div class="comment comment-4434762755254228245">
        <div class="comment-header">
          <a name="comment-4434762755254228245"></a>
            <span class="author">John Mudd</span> wrote on <span class="date">2008-11-10 13:27</span>:
        </div>
        <div class="comment-content">
          <p>I'm interested in anything to do with multi-thread support, GIL elimination, general status, progress and future plans.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/10/sprint-discussions-jit-generator-3301578822967655604.html" class="u-url">Sprint Discussions: JIT Generator Planning</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/10/sprint-discussions-jit-generator-3301578822967655604.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-10-14T12:28:00Z" itemprop="datePublished" title="2008-10-14 12:28">2008-10-14 12:28</time></a>
            </p>
                <p class="commentline">6 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <h2>Background</h2>
<p>Finally, the JIT post :-). First some background: Despite our plans at the end
of the EU period, PyPy's Python interpreter didn't get a good and widely
applicable JIT in the last year. The reason for that was that we discovered that
although the basic idea to generate JIT compilers is good, the concrete
prototype made during the EU period is basically flawed. It could have
been pushed a bit farther, but would have run into deep troubles eventually. One
of the problems would have been performance instability: change a seemingly
unrelated bit in your source program, and the performance changes in unexpected
ways, which is clearly not desirable. Another problem with that old approach is
that too much assembler code is generated, leading to memory problems, and also
that the generated assembler is bad in various ways, e.g. it is hard in that
approach to do proper register allocation.</p>
<p>Therefore we decided that it would be worthless to pursue this direction much
further. Instead we tried to research approaches to fixing the inherent
problems. This research was <a class="reference" href="../posts/2008/06/hi-all-some-news-from-jit-front-7534695765973581706.html">largely done in Prolog</a> and I eventually wrote my
<a class="reference" href="../posts/2008/10/prolog-jit-masters-thesis-finished-5462132148241449867.html">Master's thesis</a> about it. From the Prolog work we got some good insights into
what needs to be done and what sorts of techniques are needed. Also, it inspired
Armin to do some more exploration on a small <a class="reference" href="https://codespeak.net/svn/user/cfbolz/jitpl/branch/pyjitpl/">Python prototype</a> which used the
lessons learned from Prolog and also some additional ideas from tracing JITs. So
far, however, the prototype is neither in RPython, nor much integrated with
PyPy.</p>
<p>This research is not the only thing happening in the JIT-area. During the last
year, Antonio Cuni was working on <a class="reference" href="../posts/2008/05/progresses-on-cli-jit-backend-front-1021772190959551376.html">bringing the JIT to pypy-cli</a>. This
consisted mostly of writing a .NET backend for the old JIT-generator. Some
further work is being done since August by John Witulski, who is writing an
AMD64 backend for the JIT-generator for his Bachelor's thesis.</p>

<h2>Where to go from there</h2>
<a href="https://3.bp.blogspot.com/_zICyAWqZbLA/SPSRF3IFyqI/AAAAAAAAAFs/5rg6Vg9wqMI/s1600-h/jit.JPG"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5256986194931468962" src="https://3.bp.blogspot.com/_zICyAWqZbLA/SPSRF3IFyqI/AAAAAAAAAFs/5rg6Vg9wqMI/s320/jit.JPG" style="margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;"></a><p>During the sprint we discussed in which directions we should continue now. We
plan to work quite a bit on the JIT in the coming months. Both Armin and Anto
are in DÃ¼sseldorf for four months, and them and me plan to mostly work on the
JIT (as well as giving a lecture on "Dynamic Programming Languages", trying to
ensnare some more students).</p>
<p>The first step will be to experiment a bit more with Armin's prototype. So far
it looks rather promising, but there are some unsolved issues that we need to
look into first. The first issue is to think a bit about how to efficiently do
profiling to compile only important code paths. The other large issue are
so-called "virtualizables". Roughly speaking, they are the frame objects of the
interpreter from which the JIT is generated. They need special treatment,
because on the one hand it is important that they get optimized away to make the
code fast, since the frames are accessed all the time for the local variables;
on the other hand they should still be usable for introspection if code is
around that is trying to look into them.</p>
<p>When this is done, the prototype needs to be ported to RPython, which is a
non-trivial task, since it is rather dynamic so far (it is rather important that
the unresolved issues are done before the porting, because once the prototype is
in RPython, experimentation will be harder). The porting has the potential to be
tedious, but in a sense it is "just work", as opposed to unclear research.</p>
<p>At this point it will become important to think about the backend interface. The
interface that the old frontend used to produce assembler code won't be usable
for the new approach, so things need to be rearranged slightly. Afterwards the
backends will have more information and be invoked at a slightly higher level,
which should allow them to produce better code.</p>
<p>When all this is done, the JIT generator will be in a rather good state and it
should become possible (modulo a lot of details, of course), to use it on the
Python interpreter.</p>
<h2>Conclusion</h2>
<p>I am intentionally not attaching any time estimates to this blog post. So far
our time estimates have not been very accurate when it comes to the JIT, which
only lead to disappointment when the JIT failed to materialize. We hope that we
will progress in interesting ways in the next four months, but who knows. Note
that we are really quite disappointed ourselves that it took so much longer than
we planned and hoped. The reason for this is mostly that this work really is
research and sometimes it is just hard to predict what sort of problems turn up.
Partial evaluation (the basis for our JIT generator) is a 30 years old technique
that was always just promising and never really successful, so the fact that we
think we can solve its problems in a few years is very much hubris anyway :-).
On the positive side, we think that we now know these problems much better than
ever before and that we have a plan that has a chance to succeed.</p>
<p>Also we are still convinced that our approach has huge potential, despite the
difficulties. If we manage to pull it off, it should be significantly simpler to
support new language features in the JIT and also to get speedups on some rather
interesting bits of the language. Some ideas we are having include generating a
JIT for the regex engine or speed up ctypes-bindings to be nearly as fast as an
extension module (or faster?). Also the JIT will be such that by construction
the JIT-generated code behaves identical to the original code, which isn't
always true for Psyco, for example.</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-6648210290105958623">
        <div class="comment-header">
          <a name="comment-6648210290105958623"></a>
            <span class="author">Luis</span> wrote on <span class="date">2008-10-14 19:20</span>:
        </div>
        <div class="comment-content">
          <p>Thank you very much for this post Carl! I have a couple of questions:<br>You said you were experimenting with some ideas from tracing JITs. I wonder how much the new javascript VMs are influencing your work in pypy. Were these techniques  considered from the beginning or this is just because of the latest success of tracemonkey? And if so, are there ideas from Chrome's v8 or Squirellfish than could be applied too?<br>Do they change in any way your expectations regarding the potential of pypy concerning speed?</p>
        </div>
      </div>
      <div class="comment comment-4054740391911355440">
        <div class="comment-header">
          <a name="comment-4054740391911355440"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-10-15 05:42</span>:
        </div>
        <div class="comment-content">
          <p>Hi,<br><br>I quess you will now the work done about hybrid frames in VisualWorks works, but since you mentioned the problem, anyone else could benefit from the following link:<br><br>https://pages.cs.wisc.edu/~cymen/misc/interests/oopsla99-contexts.pdf</p>
        </div>
      </div>
      <div class="comment comment-1330741637107259779">
        <div class="comment-header">
          <a name="comment-1330741637107259779"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2008-10-15 13:21</span>:
        </div>
        <div class="comment-content">
          <p>@luis: we definitely read same papers (by Michael Franz and others) as tracemonkey authors. Work on tracing jit for pypy is older than release of tracemonkey (it's even older than first work on tracemonkey). Regarding chrome's v8 it seems the main optimization is implementation of hidden classes, which we kind of get for free combining jit and our existing optimization called shared dict.</p>
        </div>
      </div>
      <div class="comment comment-1841066453630053674">
        <div class="comment-header">
          <a name="comment-1841066453630053674"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-10-27 05:59</span>:
        </div>
        <div class="comment-content">
          <p>From the sound of it, it sounds like it would be useful to have a 64-bit version of Psyco, otherwise there is no stopgap in the meantime...</p>
        </div>
      </div>
      <div class="comment comment-7067055714122436884">
        <div class="comment-header">
          <a name="comment-7067055714122436884"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-12-01 13:50</span>:
        </div>
        <div class="comment-content">
          <p>When will PyPy match Psyco's speed?</p>
        </div>
      </div>
      <div class="comment comment-5059102671266175555">
        <div class="comment-header">
          <a name="comment-5059102671266175555"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2009-01-18 20:40</span>:
        </div>
        <div class="comment-content">
          <p>Yes, especially on Linux, everything is moving to 64-bit.  If there's no 64-bit Psyco, you can't get the benefits of 64-bit Python (memory) and use Psyco at the same time.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/10/sprint-discussions-c-library-bindings-249141169883996521.html" class="u-url">Sprint Discussions: C++ Library Bindings</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/10/sprint-discussions-c-library-bindings-249141169883996521.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-10-14T12:03:00Z" itemprop="datePublished" title="2008-10-14 12:03">2008-10-14 12:03</time></a>
            </p>
                <p class="commentline">11 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>At the beginning of this year, PyPy grew <a class="reference" href="../posts/2008/02/running-pyglet-on-pypy-3191536711417589549.html">ctypes support</a>, thanks to generous
<a class="reference" href="../posts/2008/06/pypy-improvements-5272963843122158791.html">support by Google</a>. This made it possible to interface with C libraries from
our Python interpreter, something that was possible but <a class="reference" href="https://codespeak.net/pypy/dist/pypy/doc/rffi.html">rather tedious</a> before.
What we are lacking so far is a way to interface to large C++ libraries (like
GUI libraries). During the sprint we had a brainstorming session about possible
approaches for fixing this shortcoming.</p>
<a href="https://3.bp.blogspot.com/_zICyAWqZbLA/SPSLYmHqcDI/AAAAAAAAAFk/1qE8zrafv7c/s1600-h/cpp_bindings.jpg"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5256979919714021426" src="https://3.bp.blogspot.com/_zICyAWqZbLA/SPSLYmHqcDI/AAAAAAAAAFk/1qE8zrafv7c/s320/cpp_bindings.jpg" style="margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;"></a><p>For CPython there are a number of approaches in common use:</p>
<blockquote>
<ul class="simple">
<li>
<a class="reference" href="https://www.riverbankcomputing.com/software/sip/intro">SIP</a>, mainly used for <a class="reference" href="https://www.riverbankcomputing.com/software/pyqt/intro">PyQT</a>
</li>
<li><a class="reference" href="https://www.swig.org/">SWIG</a></li>
<li><a class="reference" href="https://www.boost.org/doc/libs/1_36_0/libs/python/doc/index.html">Boost.Python</a></li>
</ul>
</blockquote>
<p>Those all have the property that they produce some code that is then compiled
with a compiler to produce a CPython extension. The produced code also uses
functions from CPython's C-API. This model is not simple to use for PyPy in its
current state. Since PyPy generates C code automatically, a fixed C-level API
does not exist (it is not unlikely that at one point in the future we might have
to provide one, but not yet). At the moment, PyPy very much has a "Don't call
us, we call you"-approach.</p>
<p>A very different approach is followed by the <a class="reference" href="https://seal-reflex.web.cern.ch/seal-reflex/index.html">Reflex package</a>, which is
developed at CERN (which has an incredible amount of C++ libraries). It is not
mainly intended for writing Python bindings for C++ libraries but instead
provides reflection capabilities for C++. The idea is that for every C++ shared
library, an additional shared library is produced, which allows together with
Reflex to introspect properties of C++ classes, methods, etc. at runtime. These
facilities are then used for writing a small generic CPython extension module,
that allows CPython to use any C++ library for which this reflection information
was generated.</p>
<p>This approach is a bit similar to the <tt class="docutils literal"><span class="pre">ctypes</span></tt> module, apart from the fact
that <tt class="docutils literal"><span class="pre">ctypes</span></tt> does not use any reflection information, but the user has to
specify the data structures that occur in the C code herself. This makes it
sometimes rather burdensome to write cross-platform library bindings.</p>
<p>For PyPy the approach seems rather fitting: We would need to implement only the
generic extension module and could then use any number of C++ libraries. Of
course some more evaluation is needed (e.g. to find out whether there are any
restrictions for the C++ code that the library can use and how bothersome it is
to get this reflection information for a large library) but so far it seems
promising.</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-7344974875948180344">
        <div class="comment-header">
          <a name="comment-7344974875948180344"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-10-14 17:39</span>:
        </div>
        <div class="comment-content">
          <p>I've done a fair amount of complicated Boost.Python wrapping, and also implemented a small replacement for it with most of the complexity removed.  There are two main reasons why Boost.Python is so complicated:<br><br>1. It supports arbitrarily complex memory and sharing semantics on the C++ classes (and is runtime polymorphic on how the memory of wrapped objects is managed).<br><br>2. It supports arbitrary overloading of C++ functions.<br><br>If you remove those two generality requirements (by requiring that wrapped C++ objects are also PyObjects and banning overloading), it's possible to write very lightweight C++ bindings.  Therefore, I think it's critical to factor the C/C++ API design so that as much of it as possible is writable in application level python on top of a small core that does the final C++ dispatch.<br><br>For example, if you wrap a C++ vector class with a bunch of overloads of operator+ in Boost.Python, each call to __add__ has to do a runtime search through all the overloads asking whether each one matches the arguments passed.  Each such check does a runtime search through a table of converters.  It would a terrible shame if that overhead isn't stripped by the JIT, which means it has to be in python.<br><br>I think a good test library for thinking about these issues is numpy, since it has some memory management complexity as well as internal overloading.<br><br>I could go on, but it'd probably be better to do that via email. :)<br><br>Geoffrey</p>
        </div>
      </div>
      <div class="comment comment-8420773313276512091">
        <div class="comment-header">
          <a name="comment-8420773313276512091"></a>
            <span class="author">RenÃ© Dudfield</span> wrote on <span class="date">2008-10-14 21:05</span>:
        </div>
        <div class="comment-content">
          <p>Please add a C API :)<br><br>Once that is done, it's lots easier to interface with the outside world.<br><br>For a lot of C++ apis I find it easy enough to write a C api on top of it.<br><br>In fact many C++ apis provide a C API.  Since that makes it easier to work with different C++ compilers.  As you probably know different C++ compilers mangle things differently.<br><br>It is possible to look at C++ code at runtime.  You just need to be able to interpret the C++ symbols.  I know someone did a prototype of this for vc6 on windows.  He parsed the symbols, and then created the functions at run time with ctypes.  However the approach is not portible between platforms, compilers, or even different versions of compilers.  Of course this didn't allow you to use many of the C++ features, but only some.<br><br>If you look at how swig works, you will see it kind of generates a C API for many C++ things.<br><br><br>For libraries, it is custom to provide a C API.  It just makes things easier.</p>
        </div>
      </div>
      <div class="comment comment-1727823290717078602">
        <div class="comment-header">
          <a name="comment-1727823290717078602"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2008-10-15 01:11</span>:
        </div>
        <div class="comment-content">
          <p>you might want to look at PyRoot [1,2,3] which is using the Reflex library to automatically wrap (and pythonize) the C++ libraries/types for which a Reflex dictionary has been (beforehand) generated.<br><br>theoretically any piece of C++ can be wrapped as Reflex is using gccxml[4] to extract informations from a library and to generat the dictionary library.<br><br>Using it in one of CERN's LHC experiment which makes heavy (ab)use of templates (Boost) I can say that we almost had basically no problem.<br>Usually the only problems we got were either at the gccxml level (resolution of typedef, default template arguments,...) or at the gccxml-to-reflex level (mainly naming conventions problems interfering with the autoloading of types at runtime)<br><br>Being a client of gccxml is a rather annoying as the development is... opaque.<br><br>I know the Reflex guys were investigating at some point to migrate to an LLVM version (with GCC as a frontend) to replace gccxml.<br><br>[1] https://wlav.web.cern.ch/wlav/pyroot/<br>[2] https://root.cern.ch/viewcvs/trunk/bindings/pyroot/<br>[3] https://www.scipy.org/PyRoot<br>[4] https://www.gccxml.org/</p>
        </div>
      </div>
      <div class="comment comment-2048198123273238014">
        <div class="comment-header">
          <a name="comment-2048198123273238014"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2008-10-15 11:16</span>:
        </div>
        <div class="comment-content">
          <p>There's been some (small) discussion in the SWIG project of making an alternative output method which creates a simple C API for a C++ project, and wraps that with ctypes (generating the python side of the ctypes bindings, too).  So far, this is purely theoretical, but all the pieces needed to do it are present in the SWIG source code.  If reflex doesn't work out, this might be a reasonable alternative approach.</p>
        </div>
      </div>
      <div class="comment comment-7425454300577642785">
        <div class="comment-header">
          <a name="comment-7425454300577642785"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2008-10-15 13:41</span>:
        </div>
        <div class="comment-content">
          <p>Wow. A lot of very informative posts. We'll definitely look to evaluate more what you all posted. Also, in case you want to discuss more, <a href="https://codespeak.net/mailman/listinfo/pypy-dev" rel="nofollow">mailing list</a> is usually better place for discussions. Feel free to send new ideas or more detailed info there.<br><br>Cheers,<br>fijal</p>
        </div>
      </div>
      <div class="comment comment-7626459277764409237">
        <div class="comment-header">
          <a name="comment-7626459277764409237"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-10-16 00:44</span>:
        </div>
        <div class="comment-content">
          <p>fyi check out Elsa ( https://www.cubewano.org/oink ). It is much better than Reflex.</p>
        </div>
      </div>
      <div class="comment comment-8684336506651217510">
        <div class="comment-header">
          <a name="comment-8684336506651217510"></a>
            <span class="author">Carl Friedrich Bolz-Tereick</span> wrote on <span class="date">2008-10-16 10:57</span>:
        </div>
        <div class="comment-content">
          <p>illume: Adding a C-API is rather hard, and probably not on our todo list, unless somebody pays for it :-).<br><br>anonymous: From a quick glance I am not sure Elsa would really help. Yes, you can get use it to parse c++ headers and get information about it. But as far as I see it, you cannot use it to create shared libraries that can be used to dynamically construct classes and dynamically call methods on them. Besides, the idea is to have a solution that works on both CPython and PyPy. Reflex already has a way to bind C++ libraries to CPython, so we only need to do the PyPy part.<br><br>Anyway, if anybody is interested in more detailed discussions, we should all move to pypy-dev.</p>
        </div>
      </div>
      <div class="comment comment-8578464745954318705">
        <div class="comment-header">
          <a name="comment-8578464745954318705"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2008-10-19 09:40</span>:
        </div>
        <div class="comment-content">
          <p>I also have done a fair amount of Boost.Python wrapping. I even created a code generator for it - Py++( www.language-binding.net). <br><br>The problem you want to solve is very complex. Exposing C++ code, as is, is not enough. You will have to create "bridge" between different concepts. <br><br>For example C++ and Python iterators. In C++, in order to get the iterator value, you have to dereference it. In Python, you just have it( value ).<br><br>This is just a single example, I have others. <br><br>If you continue with the project - I would like to be involved.</p>
        </div>
      </div>
      <div class="comment comment-3723090757282786568">
        <div class="comment-header">
          <a name="comment-3723090757282786568"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2008-10-21 02:48</span>:
        </div>
        <div class="comment-content">
          <p>Roman,<br><br>I haven't looked at the code of Boost.Python since a long time, but the way "we" do the pythonization of the iteration over STL sequences is rather simple.<br><br>when one writes:<br> foos = std.vector('FooKlass')()<br> # fill foos<br> # iterate<br> for foo in foos:<br>   print foo.value()<br><br>what the PyROOT/Reflex layer is doing is looking at the dictionary for the <i>std::vector(FooKlass)</i>, discovering that there is a pair of functions 'begin' and 'end' and it figures out one can create a python iterator from that pair.<br><br>anyways, as Maciej pointed it out, we could try to move this discussion here[1] or there[2]...<br><br>cheers,<br>sebastien.<br><br>[1] https://codespeak.net/pipermail/pypy-dev/2008q4/004847.html<br><br>[2] https://codespeak.net/pipermail/pypy-dev/2008q4/004843.html</p>
        </div>
      </div>
      <div class="comment comment-5410569276647263980">
        <div class="comment-header">
          <a name="comment-5410569276647263980"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2008-10-30 12:27</span>:
        </div>
        <div class="comment-content">
          <p>Just to let you know, there is an upcoming paper in PythonPapers.org review on the topic of C++ wrapping in Python. Just watch out!</p>
        </div>
      </div>
      <div class="comment comment-4771363685087214171">
        <div class="comment-header">
          <a name="comment-4771363685087214171"></a>
            <span class="author">ilkosta</span> wrote on <span class="date">2008-12-09 14:08</span>:
        </div>
        <div class="comment-content">
          <p>maybe it's worth also evaluate the library <a href="https://www.ischo.com/xrtti/" rel="nofollow">xrtti</a>, a Reflex comparable library but without CERN and ROOT dependencies.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/10/sprint-discussions-release-planning-7097053444808236145.html" class="u-url">Sprint Discussions: Release Planning</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/10/sprint-discussions-release-planning-7097053444808236145.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-10-12T16:10:00Z" itemprop="datePublished" title="2008-10-12 16:10">2008-10-12 16:10</time></a>
            </p>
                <p class="commentline">1 comment</p>

        </div>
    </header><div class="p-summary entry-summary">
    <a href="https://4.bp.blogspot.com/_zICyAWqZbLA/SPIh1kj1g6I/AAAAAAAAAFU/xxGowgK863w/s1600-h/release1.JPG"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5256300919325557666" src="https://4.bp.blogspot.com/_zICyAWqZbLA/SPIh1kj1g6I/AAAAAAAAAFU/xxGowgK863w/s320/release1.JPG" style="margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;"></a><a href="https://1.bp.blogspot.com/_zICyAWqZbLA/SPIh_BAgkpI/AAAAAAAAAFc/MCSODJX-XPo/s1600-h/release2.JPG"><img alt="" border="0" id="BLOGGER_PHOTO_ID_5256301081580835474" src="https://1.bp.blogspot.com/_zICyAWqZbLA/SPIh_BAgkpI/AAAAAAAAAFc/MCSODJX-XPo/s320/release2.JPG" style="margin: 0px auto 10px; display: block; text-align: center; cursor: pointer; width: 275px; height: 108px;"></a>


<p>One of the discussions that happened during the sprint was about how to approach
the next PyPy release. There hasn't been a release since the end of the EU
period, which is not an optimal situation. Therefore we plan to make a 1.1
release at the beginning of next year, ideally before Pycon US. We'd also like
to move towards time-based releases. This will be greatly helped by the
new <a class="reference" href="https://codespeak.net:8099/summary?branch=&lt;trunk&gt;">buildbot infrastructure</a>, which allows us to decide when the
state of the codebase is stable enough to release.</p>
<p>Another goal of the release is to involve more people from the wider PyPy
community by having bugdays and generally asking for more support. This will be
particularly useful for bugs on platforms that no one of the core developers
group is using.</p>
<p>Feature-wise the release will mostly contain CPython 2.5 language support,
including some new extension modules (like <tt class="docutils literal"><span class="pre">ctypes</span></tt>, <tt class="docutils literal"><span class="pre">expat</span></tt>, <tt class="docutils literal"><span class="pre">sqlite</span></tt>).
In addition we plan to make it easier to actually install and use the PyPy
Python interpreter, which means some sort of proper installation procedure and
supporting <tt class="docutils literal"><span class="pre">distutils</span></tt> on top of PyPy. Another part of the release will be
support for <a class="reference" href="https://codespeak.net/pypy/dist/pypy/doc/sandbox.html">fully sand-boxing</a> an interpreter.</p>
<p>Additionally there were also a large number of improvements on several levels
since the last release, like <a class="reference" href="../posts/2008/05/general-performance-improvements-838741900863354293.html">optimizations</a>, faster <a class="reference" href="../posts/2007/12/faster-implementation-of-classic-1021557618590043616.html">oldstyle-classes</a>, better
<a class="reference" href="../posts/2007/12/good-news-from-garbage-collection-front-2678138026363485439.html">GCs</a>, correct <a class="reference" href="../posts/2008/02/python-finalizers-semantics-part-2-2748812428675325525.html">finalization behaviour</a>, lots and lots of bugfixes, <a class="reference" href="../posts/2008/05/threads-and-gcs-1126087726480790112.html">better
threading support</a> (still with the GIL), some work on improving memory
behaviour,  ...</p>
<p>In contrast to our last release, we will focus mainly on PyPy's Python
Intepreter and more particularly its C-version.   There are also various
experimental interpreters that PyPy contains, like for Prolog, Smalltalk,
JavaScript and Scheme. We also don't intend to put the LLVM and Javascipt
backends in the release, since they are essentially unmaintained and at least
partially broken.  If anybody is particularly interested in one of these
components, please feel free to step up and take responsibility for them.
Another thing that the release won't contain is a JIT.  I plan to make another
blog-post about this soon, stay tuned.</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-7759731486658591799">
        <div class="comment-header">
          <a name="comment-7759731486658591799"></a>
            <span class="author">Michael Foord</span> wrote on <span class="date">2008-10-12 17:58</span>:
        </div>
        <div class="comment-content">
          <p>Looking forward to news on the JIT.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/10/dsseldorf-sprint-report-days-1-3-5256639868851086032.html" class="u-url">DÃ¼sseldorf Sprint Report Days 1-3</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/10/dsseldorf-sprint-report-days-1-3-5256639868851086032.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-10-11T08:11:00Z" itemprop="datePublished" title="2008-10-11 08:11">2008-10-11 08:11</time></a>
            </p>
                <p class="commentline">2 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>The DÃ¼sseldorf sprint is currently in full progress and this post will try to
summarize what progress has been made in the last days. We are (again) sprinting
at the <a class="reference" href="https://www.stups.uni-duesseldorf.de">STUPS group</a> of the DÃ¼sseldorf University. You can find <a class="reference" href="https://codespeak.net/pypy/extradoc/sprintinfo/october-2008/announcement.html">the sprint
announcement</a> and the <a class="reference" href="https://codespeak.net/pypy/extradoc/sprintinfo/october-2008/planning.txt">daily planning file</a>.</p>
<p>Holger and Samuele put quite some effort over several days into setting up and
improving PyPy's testing infrastructure. PyPy has a variety of tests. On the one
hand, there are of course our own tests. But then we also have the CPython tests
that should be run on top of <tt class="docutils literal"><span class="pre">pypy-c</span></tt>. Up to now we used a custom-made pile of
hacks, held together by lots of duct-tape. It consisted of a variety of
different machines running different things with different reporting solutions.
Some of the old test-results can still be <a class="reference" href="https://wyvern.cs.uni-duesseldorf.de/pypytest/summary-old.html">found on wyvern</a>. Now we are moving
to a buildbot based solution together with a custom reporter to have a view
similar to the old one. Some details are not quite finished yet, but most of the
things are already <a class="reference" href="https://codespeak.net:8099/">working rather well</a> (currently all the results displayed
are from the 2.5-merge branch).</p>
<p>Another large (and ongoing) topic of work is the <a class="reference" href="https://codespeak.net/viewvc/pypy/branch/2.5-merge/">2.5 branch</a>. It contains the
work done by our Summer-of-Code student, <a class="reference" href="https://blog.brunogola.com.br/">Bruno Gola</a>, of adding CPython 2.5
features to PyPy's Python interpreter. While Bruno implemented most language
features and imported the 2.5 stdlib into PyPy, a lot of details were still
missing. In the last days nearly everybody worked on fixing small issues and
failing stdlib tests. While doing that we tried to categorize some CPython tests
as <em>implementation dependant</em> so that we can skip them when running on PyPy.</p>

<h2>Memory Improvements</h2>
<p>One goal of the sprint is to measure and to reduce the memory behaviour of our
Python interpreter. The idea is to make <tt class="docutils literal"><span class="pre">pypy-c</span></tt> a realistic option for use on
embedded devices. By memory behaviour we mean both the
dynamic memory usage (how much bytes does a dict or an instance take) as well as
the size of the executable and details of the GC strategy.</p>
<p>Alexander, Carl Friedrich and Antonio did some work on analyzing the static data
that a <tt class="docutils literal"><span class="pre">pypy-c</span></tt> executable contains. Our executables have the tendency to be
rather large, both due to a lot of code and due to a large amount of static
data. The analysis didn't give any really surprising results, the problem is
mostly that we have a lot of static data originating from a bit everywhere in
our program. Two big offenders are the <tt class="docutils literal"><span class="pre">unicodedata</span></tt>-module with about 750 KB
of static data and the multimethod-tables with about 150 KB of data.</p>
<p>Armin, Iko, Anto and Maciek worked on a new approach to malloc-removal. This is
(for PyPy) a crucial optimization of the translation toolchain that performs
escape analysis to find out which objects don't outlive the frame they were
allocated in. Since RPython is garbage-collected we usually have a lot of
allocations, so it is important to statically get rid of many of them. To
successfully do that, some inlining is needed to give the analysis more context.
This leads to the fact that we have rather aggressive inlining-settings to allow
as much malloc-removal as possible. The new approach tries to inline functions
only if this actually leads to the successful removal of a malloc operation. The
code is not finished quite yet, so it remains to be seen how successful it will
be.</p>
<p>Before the sprint Maciek had started to work on a <a class="reference" href="https://en.wikipedia.org/wiki/Mark-compact_algorithm">mark-compact GC</a> for PyPy. The
idea is that it is better for memory-constrained-environments because it does
not double the memory-requirements during collections. During the sprint Armin
and Maciek worked on cleaning up the code a bit and then merging the branch.
An interesting property of the mark-compact GC is that after a collection all
the memory that is not currently used by the program is returned to the
operating system. Right now the GC is not as fast as our more mature ones, but
it probably will be the basis for future tweaking.</p>
<p>A small thing that was done by Alexander and Carl Friedrich to make objects smaller is
to enable <a class="reference" href="https://codespeak.net/pypy/dist/pypy/doc/interpreter-optimizations.html#sharing-dicts">shared instance dictionaries</a> also for instances of old-style
classes. Before it worked only for instances of new-style classes. Shared
instance dictionaries are a way to reduce the memory-usage of instances. In the
optimal case, it gives the same memory-savings that <tt class="docutils literal"><span class="pre">__slots__</span></tt> are giving,
but without any behavioural changes. Conceptually it is very similar e.g. to
the notion of "map" in the <a class="reference" href="https://en.wikipedia.org/wiki/Self_programming_language">Self project</a>, or the <a class="reference" href="https://code.google.com/apis/v8/design.html#prop_access">hidden classes</a> that Google Chrome's V8
is using (click on the link, there are nice graphics). The
difference is that for them it is mostly a way to get faster attribute access,
and PyPy is so far only using it form memory savings (but that might change in
the future).</p>
<p>In parallel to all the other work, John Witulski worked tirelessly on advancing
the AMD64-JIT-backend. John has the implementation of this backend as the topic
of his Bachelor's thesis. He is progressing quite well (especially also
considering that this is his first sizeable Python project ever), just sometimes
being impaired by such annoyances as errors in the official Intel documentation.
By now the backend is supporting many integer operations and control flow.</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-1812930812718902447">
        <div class="comment-header">
          <a name="comment-1812930812718902447"></a>
            <span class="author">RenÃ© Dudfield</span> wrote on <span class="date">2008-10-12 07:55</span>:
        </div>
        <div class="comment-content">
          <p>Hello,<br><br>sounds like some fun sprinting :)<br><br>Have you considered mmap for some of those big memory users?<br><br>Especially for unicode stuff, which mostly won't be used for many applications, it should be a good win -- both for load time, and memory use.<br><br>Double plus extra combo win!!! for if you use multiple processes.<br><br>cu,</p>
        </div>
      </div>
      <div class="comment comment-7404092177482104060">
        <div class="comment-header">
          <a name="comment-7404092177482104060"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2008-10-21 18:05</span>:
        </div>
        <div class="comment-content">
          <p>I'm not sure of what you mean.<br>But modern operating systems (at least Linux) use on-demand loading of executables and libraries, so they never copy anything from an executable file to memory unless it is used.<br><br>In fact, process startup is implemented internally by mmap()ing the executable and libraries into the address space of the new process.<br><br>If you use multiple processes, it still works well - also data pages are shared, until some process writes to them.<br><br>For MMU-less devices, the above does not apply (and ucLinux allows Linux to run on them).<br>But in that case, I guess that no demand loading is available, and that mmap() copies the mapped data in memory - you need to explicitly swap in and out code segments (i.e. to use good old overlays), and no modern programming environment has direct support for them any more I guess.<br><br>You can still emulate overlays with advanced usage of linker scripts however - you put some code in a section, create variables containing the begin and end offset of that section in the linker script, and copy data in memory from that section; but I think that making relocations to that code work flawlessly is impossible, you need to always refer to the buffer containing loaded data.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/10/prolog-jit-masters-thesis-finished-5462132148241449867.html" class="u-url">Prolog-JIT Master's-Thesis Finished</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/10/prolog-jit-masters-thesis-finished-5462132148241449867.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-10-10T17:05:00Z" itemprop="datePublished" title="2008-10-10 17:05">2008-10-10 17:05</time></a>
            </p>
            
        </div>
    </header><div class="p-summary entry-summary">
    <p>As we <a class="reference" href="../posts/2008/06/hi-all-some-news-from-jit-front-7534695765973581706.html">already blogged</a>, in the last half-year or so, Michael Leuschel, Armin
and me did a lot of JIT generator work on a <a class="reference" href="https://codespeak.net/viewvc/user/cfbolz/jitpl/dist/">Prolog prototype</a>. The idea was to
experiment more quickly with some techniques than what would have been possible
with RPython. These experiments were quite successful in themselves. With very
little code we managed to get a JIT that is not doing too badly when compared to
existing projects for Prolog.</p>
<p>This Prolog work was also the subject of my <a class="reference" href="https://codespeak.net/svn/user/cfbolz/jitpl/thesis/final-master.pdf">Master's thesis</a>. I finished the
thesis about two weeks ago (and since then have been mostly sleeping and then
sprinting). The thesis should be self-contained when it comes to explaining the
JIT concepts but needs knowledge of Prolog to be understandable.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2008/09/pypypython-at-maemo-summit-6115106472056714072.html" class="u-url">PyPy/Python at the Maemo summit</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/holger-krekel.html">holger krekel</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2008/09/pypypython-at-maemo-summit-6115106472056714072.html" rel="bookmark">
            <time class="published dt-published" datetime="2008-09-20T13:58:00Z" itemprop="datePublished" title="2008-09-20 13:58">2008-09-20 13:58</time></a>
            </p>
            
        </div>
    </header><div class="p-summary entry-summary">
    <p>Maciej and me visited the <a class="reference" href="https://wiki.maemo.org/Maemo_Summit_2008">Maemo Summit</a> in Berlin -
a community meetup around Nokia's Linux based
mobile platform.  We spontaneously did a lightning
talk about a first running pypy-c on Maemo
and got nice feedback.  

</p>
<p>We also had a nice lunch with guys from the <a href="https://www.indt.org.br/institutional/index.php">INDT</a> in Brazil, including Marcio Marcedo and <a href="https://www.marceloeduardo.com/blog/">Marcelo Eduardo</a>.  It turns out that Python is used a lot on Maemo, for example the nice <a>Canola</a> UI is done with it.  Will be interesting to see how this shapes up in relation to the iPhone and Android.

</p>
<p>A lot of Nokia engineers were around and they announced that from October on they are going for weekly new releases of their SDK for the new Fremantle (Maemo-5) debian-based platform until the SDK becomes final - if we got this right.  

</p>
<p>Funnily enough, we met <a href="https://mg.pov.lt/blog">Marius Gedminas</a> from the Programmers of Vilnius - he gave a lightning talk on his impressions as a community member.  We think python programmers really should go much more to non-Python centric conferences.

</p>
<p>The whole event took place at the <a href="https://www.c-base.org">C-Base</a> - was a bit
crammed in some of the <a href="https://wiki.maemo.org/Maemo_Summit_2008">sessions</a> with something like 200 people attending.
<br>
cheers, Maciej and Holger</p>
    </div>
    </article>
</div>
</div>
<div class="sidebar">
<div>
  <h2>
    The PyPy blogposts
  </h2>
  <div>
    Create a guest post via a PR to the <a href="https://github.com/pypy/pypy.org">source repo</a>
  </div>
</div>
    <div id="global-recent-posts">
    <h2>
      Recent Posts
    </h2>
    <ul class="post-list">
      <li>
        <a href="/posts/2025/12/toy-load-store.html" class="listtitle">Load and store forwarding in the Toy Optimizer</a>
      </li>
      <li>
        <a href="/posts/2025/07/pypy-v7320-release.html" class="listtitle">PyPy v7.3.20 release</a>
      </li>
      <li>
        <a href="/posts/2025/06/rpython-gc-allocation-speed.html" class="listtitle">How fast can the RPython GC allocate?</a>
      </li>
      <li>
        <a href="/posts/2025/04/prospero-in-rpython.html" class="listtitle">Doing the Prospero-Challenge in RPython</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7319-release.html" class="listtitle">PyPy v7.3.19 release</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-gc-sampling.html" class="listtitle">Low Overhead Allocation Sampling with VMProf in PyPy's GC</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7318-release.html" class="listtitle">PyPy v7.3.18 release</a>
      </li>
      <li>
        <a href="/posts/2025/01/musings-tracing.html" class="listtitle">Musings on Tracing in PyPy</a>
      </li>
      <li>
        <a href="/posts/2025/01/towards-pypy311-an-update.html" class="listtitle">Towards PyPy3.11 - an update</a>
      </li>
      <li>
        <a href="/posts/2024/11/guest-post-final-encoding-in-rpython.html" class="listtitle">Guest Post: Final Encoding in RPython Interpreters</a>
      </li>
    </ul>
  </div>

          <div id="global-archive-list">
          <h2>
            Archives
          </h2>
          <ul class="archive-level archive-level-1">
            <li><a class="reference" href="/2007/">2007</a> (19)
            </li>
            <li><a class="reference" href="/2008/">2008</a> (62)
            </li>
            <li><a class="reference" href="/2009/">2009</a> (38)
            </li>
            <li><a class="reference" href="/2010/">2010</a> (44)
            </li>
            <li><a class="reference" href="/2011/">2011</a> (43)
            </li>
            <li><a class="reference" href="/2012/">2012</a> (44)
            </li>
            <li><a class="reference" href="/2013/">2013</a> (46)
            </li>
            <li><a class="reference" href="/2014/">2014</a> (22)
            </li>
            <li><a class="reference" href="/2015/">2015</a> (20)
            </li>
            <li><a class="reference" href="/2016/">2016</a> (20)
            </li>
            <li><a class="reference" href="/2017/">2017</a> (13)
            </li>
            <li><a class="reference" href="/2018/">2018</a> (12)
            </li>
            <li><a class="reference" href="/2019/">2019</a> (12)
            </li>
            <li><a class="reference" href="/2020/">2020</a> (9)
            </li>
            <li><a class="reference" href="/2021/">2021</a> (10)
            </li>
            <li><a class="reference" href="/2022/">2022</a> (13)
            </li>
            <li><a class="reference" href="/2023/">2023</a> (6)
            </li>
            <li><a class="reference" href="/2024/">2024</a> (13)
            </li>
            <li><a class="reference" href="/2025/">2025</a> (9)
            </li>
          </ul>
        </div>


          <div id="global-tag-list">
          <h2>
            Tags
          </h2>
          <ul>
            <li><a class="reference" href="/categories/arm.html">arm</a> (2)</li>
            <li><a class="reference" href="/categories/benchmarking.html">benchmarking</a> (1)</li>
            <li><a class="reference" href="/categories/casestudy.html">casestudy</a> (3)</li>
            <li><a class="reference" href="/categories/cli.html">cli</a> (1)</li>
            <li><a class="reference" href="/categories/compiler.html">compiler</a> (1)</li>
            <li><a class="reference" href="/categories/conda-forge.html">conda-forge</a> (1)</li>
            <li><a class="reference" href="/categories/cpyext.html">cpyext</a> (4)</li>
            <li><a class="reference" href="/categories/cpython.html">CPython</a> (3)</li>
            <li><a class="reference" href="/categories/ep2008.html">ep2008</a> (1)</li>
            <li><a class="reference" href="/categories/extension-modules.html">extension modules</a> (3)</li>
            <li><a class="reference" href="/categories/gc.html">gc</a> (3)</li>
            <li><a class="reference" href="/categories/guestpost.html">guestpost</a> (3)</li>
            <li><a class="reference" href="/categories/graalpython.html">GraalPython</a> (1)</li>
            <li><a class="reference" href="/categories/hpy.html">hpy</a> (1)</li>
            <li><a class="reference" href="/categories/heptapod.html">Heptapod</a> (1)</li>
            <li><a class="reference" href="/categories/jit.html">jit</a> (23)</li>
            <li><a class="reference" href="/categories/jython.html">jython</a> (1)</li>
            <li><a class="reference" href="/categories/kcachegrind.html">kcachegrind</a> (1)</li>
            <li><a class="reference" href="/categories/meta.html">meta</a> (1)</li>
            <li><a class="reference" href="/categories/numpy.html">numpy</a> (24)</li>
            <li><a class="reference" href="/categories/parser.html">parser</a> (1)</li>
            <li><a class="reference" href="/categories/performance.html">performance</a> (2)</li>
            <li><a class="reference" href="/categories/profiling.html">profiling</a> (7)</li>
            <li><a class="reference" href="/categories/pypy.html">pypy</a> (6)</li>
            <li><a class="reference" href="/categories/pypy3.html">pypy3</a> (16)</li>
            <li><a class="reference" href="/categories/pyqt4.html">PyQt4</a> (1)</li>
            <li><a class="reference" href="/categories/release.html">release</a> (66)</li>
            <li><a class="reference" href="/categories/releasecffi.html">releasecffi</a> (3)</li>
            <li><a class="reference" href="/categories/releaserevdb.html">releaserevdb</a> (1)</li>
            <li><a class="reference" href="/categories/releasestm.html">releasestm</a> (1)</li>
            <li><a class="reference" href="/categories/revdb.html">revdb</a> (1)</li>
            <li><a class="reference" href="/categories/roadmap.html">roadmap</a> (2)</li>
            <li><a class="reference" href="/categories/rpython.html">rpython</a> (1)</li>
            <li><a class="reference" href="/categories/rpyc.html">RPyC</a> (1)</li>
            <li><a class="reference" href="/categories/speed.html">speed</a> (6)</li>
            <li><a class="reference" href="/categories/sponsors.html">sponsors</a> (7)</li>
            <li><a class="reference" href="/categories/sprint.html">sprint</a> (3)</li>
            <li><a class="reference" href="/categories/sprints.html">sprints</a> (1)</li>
            <li><a class="reference" href="/categories/stm.html">stm</a> (14)</li>
            <li><a class="reference" href="/categories/sun.html">sun</a> (1)</li>
            <li><a class="reference" href="/categories/smalltalk.html">Smalltalk</a> (1)</li>
            <li><a class="reference" href="/categories/squeak.html">Squeak</a> (1)</li>
            <li><a class="reference" href="/categories/testing.html">testing</a> (1)</li>
            <li><a class="reference" href="/categories/toy-optimizer.html">toy-optimizer</a> (6)</li>
            <li><a class="reference" href="/categories/unicode.html">unicode</a> (1)</li>
            <li><a class="reference" href="/categories/valgrind.html">valgrind</a> (1)</li>
            <li><a class="reference" href="/categories/vmprof.html">vmprof</a> (3)</li>
            <li><a class="reference" href="/categories/z3.html">z3</a> (5)</li>
          </ul>
        </div></div>
</main>
</div>
<div style="clear: both; width: 75%; margin: 1em auto;">
        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-9.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-7.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
         
                 <footer id="footer"><p>
</p>
<div class="myfooter">
  <div class="logotext">
    Â© 2026 <a href="mailto:pypy-dev@pypy.org">The PyPy Team</a>
    Â 
    Built with <a href="https://getnikola.com" rel="nofollow">Nikola</a>
    Â 
    Last built 2026-01-17T00:22
  </div>
  <div style="margin-left: auto">
  <a href="../rss.xml">RSS feed</a>
</div>

            
        

    </div>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js" crossorigin="anonymous"></script><script src="../assets/js/styles.js"></script></footer>
</body>
</html>
<!DOCTYPE html>
<html \ prefix="
        og: http://ogp.me/ns# article: http://ogp.me/ns/article#
    " vocab="http://ogp.me/ns" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="A Faster Python">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>PyPy (old posts, page 33) | PyPy</title>
<link href="../assets/css/rst_base.css" rel="stylesheet" type="text/css">
<link href="../assets/css/nikola_rst.css" rel="stylesheet" type="text/css">
<link href="../assets/css/code.css" rel="stylesheet" type="text/css">
<link href="../assets/css/theme.css" rel="stylesheet" type="text/css">
<link href="../assets/css/styles.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://www.pypy.org/blog/index-33.html">
<link rel="icon" href="../favicon2.ico" sizes="16x16">
<link rel="icon" href="../favicon32x32.ico" sizes="32x32">
<link rel="prev" href="index-34.html" type="text/html">
<link rel="next" href="index-32.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5shiv-printshiv.min.js"></script><![endif]--><link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="../assets/css/tipuesearch.css">
</head>
<body>
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>
    <div id="container">
             <header id="header"><!-- Adapted from https://www.taniarascia.com/responsive-dropdown-navigation-bar --><section class="navigation"><div class="nav-container">
            <div class="brand">
                <a href="../index.html">
                    <image id="toplogo" src="../images/pypy-logo.svg" width="75px;" alt="PyPy/"></image></a>
            </div>
            <nav><ul class="nav-list">
<li> 
                <a href="#!">Features</a>
                <ul class="nav-dropdown">
<li> <a href="../features.html">What is PyPy?</a> </li>  
                    <li> <a href="../compat.html">Compatibility</a> </li>  
                    <li> <a href="../performance.html">Performance</a> </li>  
                </ul>
</li>
          <li> <a href="../download.html">Download</a> </li>  
          <li> <a href="http://doc.pypy.org">Dev Docs</a> </li>  
            <li> 
                <a href="#!">Blog</a>
                <ul class="nav-dropdown">
<li> <a href=".">Index</a> </li>  
                    <li> <a href="../categories/">Tags</a> </li>  
                    <li> <a href="../archive.html">Archive by year</a> </li>  
                    <li> <a href="../rss.xml">RSS feed</a> </li>  
                    <li> <a href="https://morepypy.blogspot.com/">Old site</a> </li>  
                </ul>
</li>
            <li> 
                <a href="#!">About</a>
                <ul class="nav-dropdown">
<li> <a href="https://bsky.app/profile/pypyproject.bsky.social">Bluesky</a> </li>  
                    <li> <a href="https://libera.irclog.whitequark.org/pypy">IRC logs</a> </li>  
                    <li> <a href="https://www.youtube.com/playlist?list=PLADqad94yVqDRQXuqxKrPS5QnVqbDLlRt">YouTube</a> </li>  
                    <li> <a href="https://www.twitch.tv/pypyproject">Twitch</a> </li>  
                    <li> <a href="../pypy-sponsors.html">Sponsors</a> </li>  
                    <li> <a href="../howtohelp.html">How To Help?</a> </li>  
                    <li> <a href="../contact.html">Contact</a> </li>  
                </ul>
</li>

                </ul></nav><div class="nav-mobile">
                <a id="nav-toggle" href="#!"> <span></span></a>
            </div>
        </div>
    </section><div class="searchform" role="search">
                
<form class="navbar-form navbar-left" action="../search.html" role="search">
    <div class="form-group">
        <input type="text" class="form-control" id="tipue_search_input" name="q" placeholder="Search…" autocomplete="off">
</div>
    <input type="submit" value="Local Search" style="visibility: hidden;">
</form>

            </div>
    </header><main id="content"><div class="post">
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/06/pypy-260-release-8983050552628070433.html" class="u-url">PyPy 2.6.0 release</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/mattip.html">mattip</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/06/pypy-260-release-8983050552628070433.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-06-01T16:20:00Z" itemprop="datePublished" title="2015-06-01 16:20">2015-06-01 16:20</time></a>
            </p>
                <p class="commentline">4 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">
<div class="document">
<div class="section" id="pypy-2-6-0-cameo-charm">
<h2 style="text-align: left;">
PyPy 2.6.0 - Cameo Charm</h2>
<div style="text-align: left;">
We’re pleased to announce PyPy 2.6.0, only two months after PyPy 2.5.1. We are particulary happy to update <a class="reference external" href="https://cffi.readthedocs.org/">cffi</a> to version 1.1, which makes the popular ctypes-alternative even easier to use, and to support the new <a class="reference external" href="https://vmprof.readthedocs.org/">vmprof</a> statistical profiler.</div>
<div style="text-align: left;">
</div>
<div style="text-align: left;">
You can download the PyPy 2.6.0 release here:</div>
<div style="text-align: left;">
</div>
<blockquote style="text-align: left;">
<div>
<a class="reference external" href="https://pypy.org/download.html">https://pypy.org/download.html</a>
</div>
</blockquote>
<div style="text-align: left;">
</div>
<div style="text-align: left;">
We would like to thank our donors for the continued support of the PyPy project, and for those who donate to our three sub-projects, as well as our volunteers and contributors.</div>
<div style="text-align: left;">
</div>
<div style="text-align: left;">
Thanks also to Yury V. Zaytsev and David Wilson who recently started running nightly builds on Windows and MacOSX buildbots.</div>
<div style="text-align: left;">
</div>
<div style="text-align: left;">
We’ve shown quite a bit of progress, but we’re slowly running out of funds. Please consider donating more, or even better convince your employer to donate, so we can finish those projects! The three sub-projects are:</div>
<div style="text-align: left;">
</div>
<ul class="simple" style="text-align: left;">
<li>
<a class="reference external" href="https://pypy.org/py3donate.html">Py3k</a> (supporting Python 3.x): We have released a Python 3.2.5 compatible version we call PyPy3 2.4.0, and are working toward a Python 3.3 compatible version</li>
<li>
<a class="reference external" href="https://pypy.org/tmdonate2.html">STM</a> (software transactional memory): We have released a first working version, and continue to try out new promising paths of achieving a fast multithreaded Python</li>
<li>
<a class="reference external" href="https://pypy.org/numpydonate.html">NumPy</a> which requires installation of our fork of upstream numpy, available <a class="reference external" href="https://www.bitbucket.org/pypy/numpy">on bitbucket</a>
</li>
</ul>
<div style="text-align: left;">
</div>
<div style="text-align: left;">
We would also like to encourage new people to join the project. PyPy has many layers and we need help with all of them: <a class="reference external" href="https://doc.pypy.org/">PyPy</a> and <a class="reference external" href="https://rpython.readthedocs.org/">RPython</a> documentation improvements, tweaking popular <a class="reference external" href="https://doc.pypy.org/en/latest/project-ideas.html#make-more-python-modules-pypy-friendly">modules</a> to run on pypy, or general <a class="reference external" href="https://doc.pypy.org/en/latest/project-ideas.html">help</a> with making RPython’s JIT even better. Nine new people contributed since the last release, you too could be one of them.</div>
<div class="section" id="what-is-pypy">
<h2 style="text-align: left;">
What is PyPy?</h2>
<div style="text-align: left;">
PyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It’s fast (<a class="reference external" href="https://speed.pypy.org/">pypy and cpython 2.7.x</a> performance comparison) due to its integrated tracing JIT compiler.</div>
<div style="text-align: left;">
</div>
<div style="text-align: left;">
This release supports <b>x86</b> machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows, <a class="reference external" href="https://cvsweb.openbsd.org/cgi-bin/cvsweb/ports/lang/pypy">OpenBSD</a>, <a class="reference external" href="https://svnweb.freebsd.org/ports/head/lang/pypy/">freebsd</a>), as well as newer <b>ARM</b> hardware (ARMv6 or ARMv7, with VFPv3) running Linux.</div>
<div style="text-align: left;">
</div>
<div style="text-align: left;">
While we support 32 bit python on Windows, work on the native Windows 64 bit python is still stalling, we would welcome a volunteer to <a class="reference external" href="https://doc.pypy.org/en/latest/windows.html#what-is-missing-for-a-full-64-bit-translation">handle that</a>. We also welcome developers with other operating systems or <a class="reference external" href="https://pypyjs.org/">dynamic languages</a> to see what RPython can do for them.</div>
<div style="text-align: left;">
</div>
</div>
<div class="section" id="highlights" style="text-align: left;">
<h2 style="text-align: left;">
Highlights</h2>
<ul class="simple" style="text-align: left;">
<li>Python compatibility:<br><ul>
<li>Improve support for TLS 1.1 and 1.2</li>
<li>Windows downloads now package a pypyw.exe in addition to pypy.exe</li>
<li>Support for the PYTHONOPTIMIZE environment variable (impacting builtin’s __debug__ property)</li>
<li>Issues reported with our previous release were <a class="reference external" href="https://doc.pypy.org/en/latest/whatsnew-2.6.0.html">resolved</a> after reports from users on our issue tracker at <a class="reference external" href="https://foss.heptapod.net/pypy/pypy/-/issues">https://foss.heptapod.net/pypy/pypy/-/issues</a> or on IRC at #pypy.</li>
</ul>
</li>
<li>New features:<br><ul>
<li>Add preliminary support for a new lightweight statistical profiler <a class="reference external" href="https://vmprof.readthedocs.org/">vmprof</a>, which has been designed to accomodate profiling JITted code</li>
</ul>
</li>
<li>Numpy:<br><ul>
<li>Support for <code class="docutils literal"><span class="pre">object</span></code> dtype via a garbage collector hook</li>
<li>Support for .can_cast and .min_scalar_type as well as beginning a refactoring of the internal casting rules</li>
<li>Better support for subtypes, via the __array_interface__, __array_priority__, and __array_wrap__ methods (still a work-in-progress)</li>
<li>Better support for ndarray.flags</li>
</ul>
</li>
<li>Performance improvements:<br><ul>
<li>Slight improvement in frame sizes, improving some benchmarks</li>
<li>Internal refactoring and cleanups leading to improved JIT performance</li>
</ul>
<ul>
<li>Improved IO performance of <code class="docutils literal"><span class="pre">zlib</span></code> and <code class="docutils literal"><span class="pre">bz2</span></code> modules</li>
<li>We continue to improve the JIT’s optimizations. Our benchmark suite is now over 7 times faster than cpython</li>
</ul>
</li>
</ul>
</div>
<div class="document" style="text-align: left;">
<div class="section" id="pypy-2-6-0-cameo-charm">
<div class="section" id="highlights">
Please try it out and let us know what you think. We welcome success stories, <a class="reference external" href="../posts/2015/02/experiments-in-pyrlang-with-rpython-8103387814587972227.html">experiments</a>,  or <a class="reference external" href="https://mithrandi.net/blog/2015/03/axiom-benchmark-results-on-pypy-2-5-0">benchmarks</a>, we know you are using PyPy, please tell us about it!<br>
Cheers<br>
The PyPy Team</div>
</div>
</div>
<br><br><footer><div class="rst-footer-buttons">
</div>
</footer>
</div>
</div>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-3279787617543610127">
        <div class="comment-header">
          <a name="comment-3279787617543610127"></a>
            <span class="author">mattip</span> wrote on <span class="date">2015-06-01 16:32</span>:
        </div>
        <div class="comment-content">
          <p>PyPy 2.6.0 - Cameo Charm since PyPy looks best in profile (well, vmprof anyway)</p>
        </div>
      </div>
      <div class="comment comment-387265460779297375">
        <div class="comment-header">
          <a name="comment-387265460779297375"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-06-01 17:57</span>:
        </div>
        <div class="comment-content">
          <p>How is matplotlib state in numpypy ?</p>
        </div>
      </div>
      <div class="comment comment-2832709449242530718">
        <div class="comment-header">
          <a name="comment-2832709449242530718"></a>
            <span class="author">mattip</span> wrote on <span class="date">2015-06-02 10:51</span>:
        </div>
        <div class="comment-content">
          <p>No GUI backend, but this fork should work (version 1.4) for non-interactive plotting<br>https://github.com/mattip/matplotlib<br>You will need to install our fork of numpy as a prerequisite<br>https://bitbucket.org/pypy/numpy<br><br>Help with the cffi port of WxPython could get us a GUI backend (or a updated matplotlib)<br>https://doc.pypy.org/en/latest/project-ideas.html#make-more-python-modules-pypy-friendly</p>
        </div>
      </div>
      <div class="comment comment-4313008881484914505">
        <div class="comment-header">
          <a name="comment-4313008881484914505"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-06-02 12:07</span>:
        </div>
        <div class="comment-content">
          <p>Thanks for the information</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/05/cffi-101-released-756545636419794802.html" class="u-url">CFFI 1.0.1 released</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/05/cffi-101-released-756545636419794802.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-05-21T10:55:00Z" itemprop="datePublished" title="2015-05-21 10:55">2015-05-21 10:55</time></a>
            </p>
                <p class="commentline">3 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>CFFI 1.0.1 final has now been released for CPython!  CFFI is a (CPython and PyPy) module to interact with C code from Python.</p>
<p>The main news from CFFI 0.9 is the new way to build extension modules:
the "out-of-line" mode, where you have a separate build script.  When
this script is executed, it produces the extension module.  This comes
with associated Setuptools support that fixes the headache of
distributing your own CFFI-using packages.  It also massively cuts
down the import times.</p>
<p>Although this is a major new version, it should be fully
backward-compatible: existing projects should continue to work, in
what is now called the "in-line mode".</p>
<p>The <a class="reference external" href="https://cffi.readthedocs.org/en/latest/">documentation</a> has been reorganized and split into a few pages.
For more information about this new "out-of-line" mode, as well as
more general information about what CFFI is and how to use it, read the <a href="https://cffi.readthedocs.org/en/latest/#goals">Goals</a> and proceed to
the <a class="reference external" href="https://cffi.readthedocs.org/en/latest/overview.html">Overview</a>.</p>
<p>Unlike the <a class="reference external" href="../posts/2015/05/cffi-10-beta-1-4375652711495636911.html">1.0 beta 1 version</a> (ffi.dlopen(), instead of only
<tt class="docutils literal">ffi.verify()</tt>.</p>
<p><em>PyPy support:</em> PyPy needs integrated support for efficient JITting,
so you cannot install a different version of CFFI on top of an
existing PyPy.  You need to wait for the upcoming PyPy 2.6 to use
CFFI 1.0---or get a <a class="reference external" href="https://buildbot.pypy.org/nightly/trunk/">nightly build</a>.</p>
<p>My thanks again to the PSF (Python Software Foundation) for their
financial support!</p>

<strong>UPDATE:</strong><p>Bug with the first example "ABI out-of-line": variadic functions (like printf, ending in a "..." argument) crash.  Fixed in CFFI 1.0.2.</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-7923615339031201303">
        <div class="comment-header">
          <a name="comment-7923615339031201303"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2015-05-22 17:21</span>:
        </div>
        <div class="comment-content">
          <p>it's really great!</p>
        </div>
      </div>
      <div class="comment comment-9221486608454160352">
        <div class="comment-header">
          <a name="comment-9221486608454160352"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2015-05-22 23:32</span>:
        </div>
        <div class="comment-content">
          <p>Awesome! Thanks for this. I think is the best way to make extension modules for cpython and pypy.</p>
        </div>
      </div>
      <div class="comment comment-2945161822739351278">
        <div class="comment-header">
          <a name="comment-2945161822739351278"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2015-05-22 23:33</span>:
        </div>
        <div class="comment-content">
          <p>Awesome! Thanks for this. I think is the best way to make extension modules for cpython and pypy.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/05/cffi-10-beta-1-4375652711495636911.html" class="u-url">CFFI 1.0 beta 1</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/05/cffi-10-beta-1-4375652711495636911.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-05-05T18:20:00Z" itemprop="datePublished" title="2015-05-05 18:20">2015-05-05 18:20</time></a>
            </p>
                <p class="commentline">3 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Finally!  CFFI 1.0 is almost ready.  CFFI gives Python developers a convenient way to call external C libraries.  Here "Python" == "CPython or PyPy", but this post is mostly about the CPython side of CFFI, as the PyPy version is not ready yet.</p>
<p>On CPython, you can download the version
"1.0.0b1" either by looking for the <code>cffi-1.0</code> branch in
<a href="https://foss.heptapod.net/cffi/cffi">the repository</a>, or by
saying</p>

<tt class="docutils literal">pip install <span class="pre">"cffi&gt;=1.0.dev0"</span></tt>

<p>(Until 1.0 final is ready,
<tt class="docutils literal">pip install cffi</tt> will still give you version 0.9.2.)</p>
<p>The main news: you can now explicitly generate and compile a CPython C
extension module from a "build" script.  Then in the rest of your
program or library, you no longer need to import cffi at all.
Instead, you simply say:</p>
<pre class="literal-block">
from _my_custom_module import ffi, lib
</pre>
<p>Then you use <tt class="docutils literal">ffi</tt> and <tt class="docutils literal">lib</tt> just like you did in your
<tt class="docutils literal">verify()</tt>-based project in CFFI 0.9.2.  (The <tt class="docutils literal">lib</tt> is what used to
be the result of <tt class="docutils literal">verify()</tt>.)  The details of <em>how</em> you use them
should not have changed at all, so that the rest of your program should
not need any update.</p>
<div class="section" id="benefits">
<h2>Benefits</h2>
<p>This is a big step towards standard practices for making and
distributing Python packages with C extension modules:</p>
<ul class="simple">
<li>on the one hand, you need an explicit compilation step, triggered
here by running the "build" script;</li>
<li>on the other hand, what you gain in return is better control over
when and why the C compilation occurs, and more standard ways to write
distutils- or setuptools-based <tt class="docutils literal">setup.py</tt> files (see below).</li>
</ul>
<p>Additionally, this completely removes one of the main drawbacks of using
CFFI to interface with large C APIs: the start-up time.  In some cases
it could be extreme on slow machines (cases of 10-20 seconds on ARM
boards occur commonly).  Now, the <tt class="docutils literal">import</tt> above is instantaneous.</p>
<p>In fact, none of the pure Python <tt class="docutils literal">cffi</tt> package is needed any more at
runtime (it needs only an internal extension module from CFFI, which
can be installed by doing "<tt class="docutils literal">pip install <span class="pre">cffi-runtime</span></tt>" [*] if you only need that).
The <tt class="docutils literal">ffi</tt> object you get by the import above is of a
completely different class written entirely in C.  The two
implementations might get merged in the future; for now they are
independent, but give two compatible APIs.  The differences are that
some methods like <tt class="docutils literal">cdef()</tt> and <tt class="docutils literal">verify()</tt> and <tt class="docutils literal">set_source()</tt> are
omitted from the C version, because it is supposed to be a complete FFI
already; and other methods like <tt class="docutils literal">new()</tt>, which take as parameter a
string describing a C type, are faster now because that string is parsed
using a custom small-subset-of-C parser, written in C too.</p>
</div>
<div class="section" id="in-practice">
<h2>In practice</h2>
<p>CFFI 1.0 beta 1 was tested on CPython 2.7 and 3.3/3.4, on Linux and to
some extent on Windows and OS/X.  Its PyPy version is not ready yet,
and the only docs available so far are those below.</p>
<p>This is <em>beta</em> software, so there might be bugs and details may change.  We are interested in hearing any feedback (irc.freenode.net #pypy) or <a href="https://foss.heptapod.net/cffi/cffi/issues?status=new&amp;status=open">bug reports.</a></p>
<p>To use the new features, create a source file that is <em>not</em> imported by the rest of
your project, in which you place (or move) the code to build the FFI
object:</p>
<pre class="literal-block">
# foo_build.py
import cffi
ffi = cffi.FFI()

ffi.cdef("""
    int printf(const char *format, ...);
""")

ffi.set_source("_foo", """
    #include &lt;stdio.h&gt;
""")   # and other arguments like libraries=[...]

if __name__ == '__main__':
    ffi.compile()
</pre>
<p>The <tt class="docutils literal">ffi.set_source()</tt> replaces the <tt class="docutils literal">ffi.verify()</tt> of CFFI 0.9.2.
Calling it attaches the given source code to the ffi object, but this call doesn't
compile or return anything by itself.  It may be placed above the <tt class="docutils literal">ffi.cdef()</tt>
if you prefer.  Its first argument is the name of the C extension module
that will be produced.</p>
<p>Actual compilation (including generating the complete C sources) occurs
later, in one of two places: either in <tt class="docutils literal">ffi.compile()</tt>, shown above,
or indirectly from the <tt class="docutils literal">setup.py</tt>, shown next.</p>
<p>If you directly execute the file <tt class="docutils literal">foo_build.py</tt> above, it will
generate a local file <tt class="docutils literal">_foo.c</tt> and compile it to <tt class="docutils literal">_foo.so</tt> (or the
appropriate extension, like <tt class="docutils literal">_foo.pyd</tt> on Windows).  This is the
extension module that can be used in the rest of your program by saying
"<tt class="docutils literal">from _foo import ffi, lib</tt>".</p>
</div>
<div class="section" id="distutils">
<h2>Distutils</h2>
<p>If you want to distribute your program, you write a <tt class="docutils literal">setup.py</tt> using
either distutils or setuptools.  Using setuptools is generally
recommended nowdays, but using distutils is possible too.  We show it
first:</p>
<pre class="literal-block">
# setup.py
from distutils.core import setup
import foo_build

setup(
    name="example",
    version="0.1",
    py_modules=["example"],
    ext_modules=[foo_build.ffi.distutils_extension()],
)
</pre>
<p>This is similar to the CFFI 0.9.2 way.  It only works if cffi was
installed previously, because otherwise <tt class="docutils literal">foo_build</tt> cannot be
imported.  The difference is that you use <tt class="docutils literal">ffi.distutils_extension()</tt>
instead of <tt class="docutils literal">ffi.verifier.get_extension()</tt>, because there is no longer
any <tt class="docutils literal">verifier</tt> object if you use <tt class="docutils literal">set_source()</tt>.</p>
</div>
<div class="section" id="setuptools">
<h2>Setuptools</h2>
<p>The modern way is to write <tt class="docutils literal">setup.py</tt> files based on setuptools, which
can (among lots of other things) handle dependencies.  It is what you
normally get with <tt class="docutils literal">pip install</tt>, too.  Here is how you'd write it:</p>
<pre class="literal-block">
# setup.py
from setuptools import setup

setup(
    name="example",
    version="0.1",
    py_modules=["example"],
    setup_requires=["cffi&gt;=1.0.dev0"],
    cffi_modules=["foo_build:ffi"],
    install_requires=["cffi-runtime"],    # see [*] below
)
</pre>
<p>Note that "cffi" is mentioned on three lines here:</p>
<ul class="simple">
<li>the first time is in <tt class="docutils literal">setup_requires</tt>, which means that cffi will
be locally downloaded and used for the setup.</li>
<li>the second mention is a custom <tt class="docutils literal">cffi_modules</tt> argument.  This
argument is handled by cffi as soon as it is locally downloaded.  It
should be a list of <tt class="docutils literal">"module:ffi"</tt> strings, where the <tt class="docutils literal">ffi</tt> part
is the name of the global variable in that module.</li>
<li>the third mention is in <tt class="docutils literal">install_requires</tt>.  It means that in
order to install this example package, "cffi-runtime" must also be
installed.  This is (or will be) a PyPI entry that only contains a
trimmed down version of CFFI, one that does not include the pure
Python "cffi" package and its dependencies.  None of it is needed at
runtime.</li>
</ul>
<p><strong>[*] NOTE: The "cffi-runtime" PyPI entry is not ready yet.  For now, use "cffi&gt;=1.0.dev0" instead.</strong>  Considering PyPy, which has got a built-in "_cffi_backend" module, the "cffi-runtime" package could never be upgraded there; but it would still be nice if we were able to upgrade the "cffi" pure Python package on PyPy.  This might require some extra care in writing the interaction code.  We need to sort it out now...</p>
</div>
<div class="section" id="thanks">
<h2>Thanks</h2>
<p>Special thanks go to the PSF (Python Software Foundation) for their
financial support, without which this work---er... it might likely have occurred anyway, but at an unknown future date :-)</p>
<p>(For reference, the amount I asked for (and got) is equal to one
month of what a Google Summer of Code student gets, for work that will
take a bit longer than one month. At least I personally am running mostly
on such money, and so I want to thank the PSF again for their
contribution to CFFI---and while I'm at it, thanks to all other
contributors to PyPy---for making this job more than an unpaid hobby on
the side :-)</p>
<br><p><i>Armin Rigo</i></p>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-6451876041916411609">
        <div class="comment-header">
          <a name="comment-6451876041916411609"></a>
            <span class="author">Mahmoud</span> wrote on <span class="date">2015-05-05 20:59</span>:
        </div>
        <div class="comment-content">
          <p>This is great news! We're loving using CFFI via <a href="https://github.com/pyca/cryptography" rel="nofollow">cryptography</a> and <a href="https://github.com/pyca/pyopenssl" rel="nofollow">PyOpenSSL</a>.</p>
        </div>
      </div>
      <div class="comment comment-4202254424879643599">
        <div class="comment-header">
          <a name="comment-4202254424879643599"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2015-05-05 21:37</span>:
        </div>
        <div class="comment-content">
          <p>An easier way to install cffi 1.0 beta releases is with<br><br>pip install --pre cffi<br><br>The --pre flag indicates pre-releases are acceptable for installation.</p>
        </div>
      </div>
      <div class="comment comment-2598073686607984945">
        <div class="comment-header">
          <a name="comment-2598073686607984945"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2015-05-06 08:54</span>:
        </div>
        <div class="comment-content">
          <p>That's great news! Hard to read though if you're not familiar with CFFI behaviour from before.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/03/pypy-stm-251-released-1342113838236225773.html" class="u-url">PyPy-STM 2.5.1 released</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/armin-rigo.html">Armin Rigo</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/03/pypy-stm-251-released-1342113838236225773.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-03-30T16:10:00Z" itemprop="datePublished" title="2015-03-30 16:10">2015-03-30 16:10</time></a>
            </p>
                <p class="commentline">6 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <h2 class="title">PyPy-STM 2.5.1 - Mawhrin-Skel</h2>

<p>We're pleased to announce PyPy-STM 2.5.1, codenamed Mawhrin-Skel.
This is the second official release of PyPy-STM.  You can download
this release here (64-bit Linux only):</p>
<blockquote>
<a class="reference external" href="https://pypy.org/download.html">https://pypy.org/download.html</a>
</blockquote>
<p>Documentation:</p>
<blockquote>
<a class="reference external" href="https://pypy.readthedocs.org/en/latest/stm.html">https://pypy.readthedocs.org/en/latest/stm.html</a>
</blockquote>
<p>PyPy is an implementation of the Python programming language which focuses
on performance. So far we've been relentlessly optimizing for the single
core/process scenario. PyPy STM brings to the table a version of PyPy
that does not have the infamous Global Interpreter Lock, hence can run
multiple threads on multiple cores. Additionally it comes with a set
of primitives that make writing multithreaded applications a lot easier,
as explained below (see TransactionQueue) and in the documentation.</p>
<p>Internally, PyPy-STM is based on the Software Transactional Memory
plug-in called stmgc-c7.  This version comes with a relatively
reasonable single-core overhead but scales only up to around 4 cores
on some examples; the next version of the plug-in, stmgc-c8, is in
development and should address that limitation (as well as reduce the
overhead).  These versions only support 64-bit Linux; we'd welcome
someone to port the upcoming stmgc-c8 to other (64-bit) platforms.</p>
<p>This release passes all regular PyPy tests, except for a few
special cases.  In other words, you should be able to drop in
PyPy-STM instead of the regular PyPy and your program should still
work.  See <a class="reference external" href="https://pypy.readthedocs.org/en/latest/stm.html#current-status-stmgc-c7">current status</a> for more information.</p>
<p>This work was done by Remi Meier and Armin Rigo.  Thanks to all donors
for <a class="reference external" href="https://pypy.org/tmdonate2.html">crowd-funding the STM work</a> so far!  As usual, it took longer
than we would have thought.  I really want to thank the people that
kept making donations anyway.  Your trust is greatly appreciated!</p>
<br><div class="section" id="what-s-new">
<h2>What's new?</h2>
<p>Compared to the <a class="reference external" href="../posts/2014/07/pypy-stm-first-interesting-release-8684276541915333814.html">July 2014 release</a>, the main addition is a way to
get reports about STM conflicts.  This is an essential new feature.</p>
<p>To understand why this is so important, consider that if you already
played around with the previous release, chances are that you didn't
get very far.  It probably felt like a toy: on very small examples it
would nicely scale, but on any larger example it would not scale at
all.  You didn't get any feedback about why, but the underlying reason
is that, in a typical large example, there are some STM conflicts that
occur all the time and that won't be immediately found just by
thinking.  This prevents any parallelization.</p>
<p>Now PyPy-STM is no longer a black box: you have a way to learn about
these conflicts, fix them, and try again.  The tl;dr version is to run:</p>
<pre class="literal-block">
    PYPYSTM=stmlog ./pypy-stm example.py
    ./print_stm_log.py stmlog
</pre>
<p>More details in <a class="reference external" href="https://pypy.readthedocs.org/en/latest/stm.html#user-guide">the STM user guide</a>.</p>
</div>
<br><div class="section" id="performance">
<h2>Performance</h2>
<p>The performance is now more stable than it used to be.  More
precisely, the best case is still "25%-40% single-core slow-down with
very good scaling up to 4 threads", but the average performance seems
not too far from that.  There are still dark spots --- notably, the
JIT is still slower to warm up, though it was improved a lot.  These
are documented in the <a class="reference external" href="https://pypy.readthedocs.org/en/latest/stm.html#current-status-stmgc-c7">current status</a> section.  Apart from
that, we should not get more than 2x single-core slow-down in the
worst case.  Please report such cases as bugs!</p>
</div>
<br><div class="section" id="transactionqueue">
<h2>TransactionQueue</h2>
<p>As explained before, PyPy-STM is more than "just" a Python without
GIL.  It is a Python in which you can do minor tweaks to your
existing, <em>non-multithreaded</em> programs and get them to use multiple
cores.  You identify medium- or large-sized, likely-independent parts
of the code and to ask PyPy-STM to run these parts in parallel.  An
example would be every iteration of some outermost loop over all items
of a dictionary.  This is done with a new API:
<tt class="docutils literal">transaction.TransactionQueue()</tt>.  See <tt class="docutils literal">help(TransactionQueue)</tt> or
read more about it in <a class="reference external" href="https://pypy.readthedocs.org/en/latest/stm.html#user-guide">the STM user guide</a>.</p>
<p>This is not a 100% mechanical change: very likely, you need to hunt
for and fix "STM conflicts" that prevent parallel execution (see
<a class="reference external" href="https://pypy.readthedocs.org/en/latest/stm.html#transaction-transactionqueue">docs</a>).  However, at all points your program runs correctly, and you
can stop the hunt when you get acceptable performance.  You don't get
deadlocks or corrupted state.</p>
</div>
<i></i><p>Thanks for reading!<br>
Armin, Remi, Fijal</p>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-1675406256288212006">
        <div class="comment-header">
          <a name="comment-1675406256288212006"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2015-03-31 09:45</span>:
        </div>
        <div class="comment-content">
          <p>From your explanation in this post, STM sounds similar to OpenMP. Can you explain the differences?<br><br>→ https://openmp.org/wp/openmp-specifications/</p>
        </div>
      </div>
      <div class="comment comment-7832060604560329376">
        <div class="comment-header">
          <a name="comment-7832060604560329376"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2015-03-31 10:20</span>:
        </div>
        <div class="comment-content">
          <p>This is explained in https://pypy.readthedocs.org/en/latest/stm.html#how-to-write-multithreaded-programs-the-10-000-feet-view</p>
        </div>
      </div>
      <div class="comment comment-4675537695460074362">
        <div class="comment-header">
          <a name="comment-4675537695460074362"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2015-03-31 15:14</span>:
        </div>
        <div class="comment-content">
          <p>Nice - thanks!<br><br>»TransactionQueue is in part similar: your program needs to have “some chances” of parallelization before you can apply it. But I believe that the scope of applicability is much larger with TransactionQueue than with other approaches. It usually works without forcing a complete reorganization of your existing code, and it works on any Python program which has got latent and imperfect parallelism. Ideally, it only requires that the end programmer identifies where this parallelism is likely to be found«<br><br>If I understand that correctly, for STM the parallelism only needs to be <b>likely</b> and can be <b>imperfect</b>, because it can recover from errors.<br><br>This would fix a whole class of problems I experienced in OpenMP Fortran code: Turning a crash or (worse) undefined behavior into a mere performance loss - and that’s really cool!<br><br>Thank you for working on that!</p>
        </div>
      </div>
      <div class="comment comment-9015646929928189615">
        <div class="comment-header">
          <a name="comment-9015646929928189615"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-04-23 13:07</span>:
        </div>
        <div class="comment-content">
          <p>Why do you always ask for money if nothing actually works?</p>
        </div>
      </div>
      <div class="comment comment-7637389952394259059">
        <div class="comment-header">
          <a name="comment-7637389952394259059"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2015-04-23 15:17</span>:
        </div>
        <div class="comment-content">
          <p>the alternative is to ask for money for stuff that already works, and that's a terrible strategy. suggest better alternatives</p>
        </div>
      </div>
      <div class="comment comment-8813475604335825911">
        <div class="comment-header">
          <a name="comment-8813475604335825911"></a>
            <span class="author">Armin Rigo</span> wrote on <span class="date">2015-04-24 00:53</span>:
        </div>
        <div class="comment-content">
          <p>Your comment suggests PyPy-STM doesn't actually work for you.  If you have found a bug, please contribute a bug report, even if only if you have an example of program that should parallelize and doesn't; such bug reports are very useful.  Alternatively, you're complaining that PyPy-STM is useless for you.  Maybe I've been bad at explaining what you should expect and not expect from it in the first place, so I've given you wrong expectations.  In that case, sorry.  (The 3rd alternative would be that you're just trolling, but let's discard it for now.)</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/03/pypy-251-released-5657064769385723517.html" class="u-url">PyPy 2.5.1 Released</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/mattip.html">mattip</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/03/pypy-251-released-5657064769385723517.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-03-26T04:38:00Z" itemprop="datePublished" title="2015-03-26 04:38">2015-03-26 04:38</time></a>
            </p>
                <p class="commentline">3 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">
<div class="document">
            
  <div class="section" id="pypy-2-5-1-pineapple-bromeliad">
<h2 style="text-align: left;">
PyPy 2.5.1 - Pineapple Bromeliad</h2>
We’re pleased to announce PyPy 2.5.1, Pineapple <a class="reference external" href="https://xkcd.com/1498">Bromeliad</a> following on the heels of 2.5.0. You can download the PyPy 2.5.1 release here:<br><blockquote>
<div>
<a class="reference external" href="https://pypy.org/download.html">https://pypy.org/download.html</a>
</div>
</blockquote>
We would like to thank our donors for the continued support of the PyPy
project, and for those who donate to our three sub-projects, as well as our
volunteers and contributors.
We’ve shown quite a bit of progress, but we’re slowly running out of funds.
Please consider donating more, or even better convince your employer to donate,
so we can finish those projects! The three sub-projects are:<br><ul>
<li>
<dl class="first docutils">
<dt>
<a class="reference external" href="https://pypy.org/py3donate.html">Py3k</a> (supporting Python 3.x): We have released a Python 3.2.5 compatible version we call PyPy3 2.4.0, and are working toward a Python 3.3 compatible version</dt>
<dt> </dt>
</dl>
</li>
<li>
<div class="first">
<a class="reference external" href="https://pypy.org/tmdonate2.html">STM</a> (software transactional memory): We have released a first working version,
and continue to try out new promising paths of achieving a fast multithreaded Python</div>
<div class="first">
<br>
</div>
</li>
<li>
<div class="first">
<a class="reference external" href="https://pypy.org/numpydonate.html">NumPy</a> which requires installation of our fork of upstream numpy,
available <a class="reference external" href="https://www.bitbucket.org/pypy/numpy">on bitbucket</a>
</div>
</li>
</ul>
We would also like to encourage new people to join the project. PyPy has many
layers and we need help with all of them: <a class="reference external" href="https://doc.pypy.org/">PyPy</a> and <a class="reference external" href="https://rpython.readthedocs.org/">Rpython</a> documentation
improvements, tweaking popular <a class="reference external" href="https://doc.pypy.org/en/latest/project-ideas.html#make-more-python-modules-pypy-friendly">modules</a> to run on pypy, or general <a class="reference external" href="https://doc.pypy.org/en/latest/project-ideas.html">help</a> with making
Rpython’s JIT even better.<br><br><div class="section" id="what-is-pypy">
<h3 style="text-align: left;">
What is PyPy?</h3>
PyPy is a very compliant Python interpreter, almost a drop-in replacement for
CPython 2.7. It’s fast (<a class="reference external" href="https://speed.pypy.org/">pypy and cpython 2.7.x</a> performance comparison)
due to its integrated tracing JIT compiler.<br><br>

This release supports <strong>x86</strong> machines on most common operating systems
(Linux 32/64, Mac OS X 64, Windows, and OpenBSD),
as well as newer <strong>ARM</strong> hardware (ARMv6 or ARMv7, with VFPv3) running Linux.<br><br>
While we support 32 bit python on Windows, work on the native Windows 64
bit python is still stalling, we would welcome a volunteer
to <a class="reference external" href="https://doc.pypy.org/en/latest/windows.html#what-is-missing-for-a-full-64-bit-translation">handle that</a>.<br><br>
</div>
<div class="section" id="highlights">
<h3 style="text-align: left;">
Highlights</h3>
<ul class="simple">
<li>The past months have seen pypy mature and grow, as rpython becomes the goto
solution for writing fast dynamic language interpreters. Our separation of
Rpython from the python interpreter PyPy is now much clearer in the
<a class="reference external" href="https://doc.pypy.org/">PyPy documentation</a>  and we now have seperate <a class="reference external" href="https://rpython.readthedocs.org/">RPython documentation</a>.
Tell us what still isn’t clear, or even better help us improve the documentation. </li>
</ul>
</div>
<div class="section" id="highlights">
<ul class="simple" style="text-align: left;">
<li>We merged version 2.7.9 of python’s stdlib. From the python release notice:<ul>
<li>The entirety of Python 3.4’s <a class="reference external" href="https://docs.python.org/3/library/ssl.html">ssl module</a> has been backported.
See <a class="reference external" href="https://www.python.org/dev/peps/pep-0466">PEP 466</a> for justification.</li>
<li>HTTPS certificate validation using the system’s certificate store is now
enabled by default. See <a class="reference external" href="https://www.python.org/dev/peps/pep-0476">PEP 476</a> for details.</li>
<li>SSLv3 has been disabled by default in httplib and its reverse dependencies
due to the <a class="reference external" href="https://www.imperialviolet.org/2014/10/14/poodle.html">POODLE attack</a>.</li>
<li>The <a class="reference external" href="https://docs.python.org/2/library/ensurepip.html">ensurepip module</a> has been backported, which provides the pip
package manager in every Python 2.7 installation. See <a class="reference external" href="https://www.python.org/dev/peps/pep-0477">PEP 477</a>.</li>
</ul>
<br>
</li>
<li>The garbage collector now ignores parts of the stack which did not change
since the last collection, another performance boost </li>
</ul>
<ul class="simple" style="text-align: left;">
<li>errno and LastError are saved around cffi calls so things like pdb will not
overwrite it </li>
</ul>
<ul class="simple" style="text-align: left;">
<li>We continue to asymptotically approach a score of 7 times faster than cpython
on our benchmark suite, we now rank 6.98 on latest runs </li>
</ul>
<ul class="simple" style="text-align: left;">
<li>Issues reported with our previous release were <a class="reference external" href="https://doc.pypy.org/en/latest/whatsnew-2.5.1.html">resolved</a> after reports from users on
our issue tracker at <a class="reference external" href="https://foss.heptapod.net/pypy/pypy/-/issues">https://foss.heptapod.net/pypy/pypy/-/issues</a> or on IRC at
#pypy.</li>
</ul>
Please try it out and let us know what you think. We welcome
success stories, <a class="reference external" href="../posts/2015/02/experiments-in-pyrlang-with-rpython-8103387814587972227.html">experiments</a>,  or <a class="reference external" href="https://mithrandi.net/blog/2015/03/axiom-benchmark-results-on-pypy-2-5-0">benchmarks</a>, we know you are using PyPy, please tell us about it!<br>

Cheers<br>

The PyPy Team<br>
</div>
</div>
</div>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-3550915737515261359">
        <div class="comment-header">
          <a name="comment-3550915737515261359"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-03-26 11:10</span>:
        </div>
        <div class="comment-content">
          <p>You mentioned about speed of PyPy over CPython. I'm interesting in memory footprint too in addition to speed up. Please, add to speed.pypy.org memory footprint metric. It's harder to find cheap and huge amount of memory for VPS than slow old cpu. Nice to know minimal memory requirements for django sites on pypy.</p>
        </div>
      </div>
      <div class="comment comment-1330603758825069000">
        <div class="comment-header">
          <a name="comment-1330603758825069000"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-03-26 11:15</span>:
        </div>
        <div class="comment-content">
          <p>Is scores from speed.pypy.org applied to PyPy3 too? Later it was written PyPy3 was not fast as PyPy2.</p>
        </div>
      </div>
      <div class="comment comment-6594177931272237550">
        <div class="comment-header">
          <a name="comment-6594177931272237550"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2015-03-26 11:56</span>:
        </div>
        <div class="comment-content">
          <p>Memory footprint is tricky to measure. PyPy usually starts at 60M (as opposed to say 6 for cpython), but then data structures are smaller. We'll try to get some measurements going on some point. Benchmarking is hard :-)<br><br>No, PyPy3 is not as fast as PyPy2. We should really look into it at some point.</p>

        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html" class="u-url">Pydgin: Using RPython to Generate Fast Instruction-Set Simulators</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/maciej-fijalkowski.html">Maciej Fijalkowski</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-03-13T09:31:00Z" itemprop="datePublished" title="2015-03-13 09:31">2015-03-13 09:31</time></a>
            </p>
                <p class="commentline">1 comment</p>

        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">

<p><strong>Note:</strong> This is a guest blog post by Derek Lockhart and Berkin Ilbeyi from
Computer Systems Laboratory of Cornell University.</p>
<p>In this blog post I'd like to describe some recent work on using the RPython
translation toolchain to generate fast instruction set simulators.
Our open-source framework, Pydgin <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#a" id="id1">[a]</a>, provides a domain-specific
language (DSL) embedded in Python for concisely describing instruction set
architectures <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#b" id="id2">[b]</a> and then uses these descriptions to generate fast,
JIT-enabled simulators.
Pydgin will be presented at the <em>IEEE International Symposium on Performance
Analysis of Systems and Software (ISPASS)</em> and in this post we provide a
preview of that work.
In addition, we discuss some additional progress updates that occurred after
the publishing deadline and will not appear in the final paper <a class="footnote-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id16" id="id3">[1]</a>.</p>
<p>Our area of research expertise is computer architecture, which is perhaps an
unfamiliar topic for some readers of the PyPy blog.
Below we provide some brief background on hardware simulation in the field of
computer architecture, as well as some context as to why instruction set
simulators in particular are such an important tool.</p>
<div class="section" id="simulators-designing-hardware-with-software">
<h3>Simulators: Designing Hardware with Software</h3>
<p>For computer architects in both academia and industry, a key step in designing
new computational hardware (e.g., CPUs, GPUs, and mobile system-on-chips) is
simulation <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#c" id="id4">[c]</a> of the target system.
While numerous models for simulation exist, three classes are particularly
important in hardware design.</p>
<p><strong>Functional Level</strong> models simulate the <em>behavior</em> of the target system.
These models are useful for creating a "golden" reference which can serve as an
executable specification or alternatively as an emulation platform for software
development.</p>
<p><strong>Cycle Level</strong> models aim to simulate both the <em>behavior</em> and the approximate
<em>timing</em> of a hardware component.
These models help computer architects explore design tradeoffs and quickly
determine things like how big caches should be, how many functional units are
needed to meet throughput targets, and how the addition of a custom accelerator
block may impact total system performance.</p>
<p><strong>Register-Transfer Level</strong> (RTL) models specify the <em>behavior</em>, <em>timing</em>, and
<em>resources</em> (e.g., registers, wires, logic gates) of a hardware component.
RTL models are bit-accurate hardware specifications typically written in a
hardware description language (HDL) such as Verilog or VHDL.
Once verified through extensive simulation, HDL specifications can be passed
into synthesis and place-and-route tools to estimate area/energy/timing or to
create FPGA or ASIC prototypes.</p>
<p>An <em>instruction set simulator</em> (ISS) is a special kind of
<em>functional-level</em> model that simulates the behavior of a processor or
system-on-chip (SOC).  ISSs serve an important role in hardware design
because they model the instruction set architecture (ISA) interface: the
contractual boundary between hardware designers and software developers.
ISSs allow hardware designers to quickly experiment with adding new processor
instructions while also allowing software developers to build new compilers,
libraries, and applications long before physical silicon is available.</p>
</div>
<div class="section" id="instruction-set-simulators-must-be-fast-and-productive">
<h3>Instruction-Set Simulators Must be Fast and Productive</h3>
<p>Instruction-set simulators are more important than ever because the ISA
boundary has become increasingly fluid.
While <a class="reference external" href="https://en.wikipedia.org/wiki/Moore%27s_law">Moore's law</a> has continued to deliver larger numbers of transistors
which computer architects can use to build increasingly complex chips, limits
in <a class="reference external" href="https://en.wikipedia.org/wiki/Dennard_scaling#Recent_breakdown_of_Dennard_scaling">Dennard scaling</a> have restricted how these transistors can be used <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#d" id="id5">[d]</a>.
In more simple terms, thermal constraints (and energy constraints in mobile
devices) have resulted in a growing interest in pervasive <em>specialization</em>:
using custom accelerators to more efficiently perform compute intensive tasks.
This is already a reality for designers of mobile SOCs who continually add new
accelerator blocks and custom processor instructions in order to achieve higher
performance with less energy consumption.
ISSs are indispensable tools in this SOC design process for both hardware
architects building the silicon and software engineers developing the software
stack on top of it.</p>
<p>An instruction set simulator has two primary responsibilities: 1) accurately
emulating the external execution behavior of the target, and 2) providing
observability by accurately reproducing the target's internal state (e.g.,
register values, program counter, status flags) at each time step.
However, other qualities critical to an effective ISS are <strong>simulation
performance</strong> and <strong>designer productivity</strong>.
Simulation performance is important because shorter simulation times allow
developers to more quickly execute and verify large software applications.
Designer productivity is important because it allows hardware architects to
easily experiment with adding new instructions and estimate their impact on
application performance.</p>
<p>To improve simulation performance, high-performance ISSs use dynamic binary
translation (DBT) as a mechanism to translate frequently visited blocks of
target instructions into optimized sequences of host instructions.
To improve designer productivity, many design toolchains automatically generate
ISSs from an architectural description language (ADL): a special
domain-specific language for succinctly specifying instruction encodings and
instruction semantics of an ISA.
Very few existing systems have managed to encapsulate the design complexity of
DBT engines such that high-performance, DBT-accelerated ISSs could be
automatically generated from ADLs <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#e" id="id6">[e]</a>.
Unfortunately, tools which have done so are either proprietary software or
leave much to be desired in terms of performance or productivity.</p>
</div>
<div class="section" id="why-rpython">
<h3>Why RPython?</h3>
<p>Our research group learned of the RPython translation toolchain through our
experiences with PyPy, which we had used in conjunction with our Python
hardware modeling framework to achieve significant improvements in simulation
performance <a class="footnote-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id17" id="id7">[2]</a>.
We realized that the RPython translation toolchain could potentially be adapted
to create fast instruction set simulators since the process of interpreting
executables comprised of binary instructions shared many similarities with the
process of interpreting bytecodes in a dynamic-language VM.
In addition, we were inspired by PyPy's meta-tracing approach to JIT-optimizing
VM design which effectively separates the process of specifying a language
interpreter from the optimization machinery needed to achieve good performance.</p>
<p>Existing ADL-driven ISS generators have tended to use domain-specific
languages that require custom parsers or verbose C-based syntax that
distracts from the instruction specification.
Creating an embedded-ADL within Python provides several benefits over these
existing approaches including a gentler learning curve for new users, access to
better debugging tools, and easier maintenance and extension by avoiding a
custom parser.
Additionally, we have found that the ability to directly execute Pydgin
ISA descriptions in a standard Python interpreter such as CPython or PyPy
significantly helps debugging and testing during initial ISA exploration.
Python's concise, pseudocode-like syntax also manages to map quite closely to
the pseudocode specifications provided by many ISA manuals <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#f" id="id8">[f]</a>.</p>
</div>
<div class="section" id="the-pydgin-embedded-adl">
<h3>The Pydgin embedded-ADL</h3>
<p>Defining a new ISA in the Pydgin embedded-ADL requires four primary pieces of
information: the architectural state (e.g. register file, program counter,
control registers), the bit encodings of each instruction, the instruction
fields, and the semantic definitions for each instruction. Pydgin aims to make
this process as painless as possible by providing helper classes and functions
where possible.</p>
<p>For example, below we provide a truncated example of the ARMv5 instruction
encoding table. Pydgin maintains encodings of all instructions in a centralized
<tt class="docutils literal">encodings</tt> data structure for easy maintenance and quick lookup. The
user-provided instruction names and bit encodings are used to automatically
generate decoders for the simulator. Unlike many ADLs, Pydgin does not require
that the user explicitly specify instruction types or mask bits for field
matching because the Pydgin decoder generator can automatically infer decoder
fields from the encoding table.</p>
<pre class="code python literal-block">
<span class="name">encodings</span> <span class="operator">=</span> <span class="punctuation">[</span>
  <span class="punctuation">[</span><span class="literal string">'adc'</span><span class="punctuation">,</span>      <span class="literal string">'xxxx00x0101xxxxxxxxxxxxxxxxxxxxx'</span><span class="punctuation">],</span>
  <span class="punctuation">[</span><span class="literal string">'add'</span><span class="punctuation">,</span>      <span class="literal string">'xxxx00x0100xxxxxxxxxxxxxxxxxxxxx'</span><span class="punctuation">],</span>
  <span class="punctuation">[</span><span class="literal string">'and'</span><span class="punctuation">,</span>      <span class="literal string">'xxxx00x0000xxxxxxxxxxxxxxxxxxxxx'</span><span class="punctuation">],</span>
  <span class="punctuation">[</span><span class="literal string">'b'</span><span class="punctuation">,</span>        <span class="literal string">'xxxx1010xxxxxxxxxxxxxxxxxxxxxxxx'</span><span class="punctuation">],</span>
  <span class="punctuation">[</span><span class="literal string">'bl'</span><span class="punctuation">,</span>       <span class="literal string">'xxxx1011xxxxxxxxxxxxxxxxxxxxxxxx'</span><span class="punctuation">],</span>
  <span class="punctuation">[</span><span class="literal string">'bic'</span><span class="punctuation">,</span>      <span class="literal string">'xxxx00x1110xxxxxxxxxxxxxxxxxxxxx'</span><span class="punctuation">],</span>
  <span class="punctuation">[</span><span class="literal string">'bkpt'</span><span class="punctuation">,</span>     <span class="literal string">'111000010010xxxxxxxxxxxx0111xxxx'</span><span class="punctuation">],</span>
  <span class="punctuation">[</span><span class="literal string">'blx1'</span><span class="punctuation">,</span>     <span class="literal string">'1111101xxxxxxxxxxxxxxxxxxxxxxxxx'</span><span class="punctuation">],</span>
  <span class="punctuation">[</span><span class="literal string">'blx2'</span><span class="punctuation">,</span>     <span class="literal string">'xxxx00010010xxxxxxxxxxxx0011xxxx'</span><span class="punctuation">],</span>
  <span class="comment"># ...</span>
  <span class="punctuation">[</span><span class="literal string">'teq'</span><span class="punctuation">,</span>      <span class="literal string">'xxxx00x10011xxxxxxxxxxxxxxxxxxxx'</span><span class="punctuation">],</span>
  <span class="punctuation">[</span><span class="literal string">'tst'</span><span class="punctuation">,</span>      <span class="literal string">'xxxx00x10001xxxxxxxxxxxxxxxxxxxx'</span><span class="punctuation">],</span>
<span class="punctuation">]</span>
</pre>
<p>A major goal of Pydgin was ensuring instruction semantic definitions map to ISA
manual specifications as much as possible. The code below shows one such
definition for the ARMv5 <tt class="docutils literal">add</tt> instruction.
A user-defined <tt class="docutils literal">Instruction</tt> class (not shown) specifies field names that can
be used to conveniently access bit positions within an instruction (e.g.
<tt class="docutils literal">rd</tt>, <tt class="docutils literal">rn</tt>, <tt class="docutils literal">S</tt>).
Additionally, users can choose to define their own helper functions, such as
the <tt class="docutils literal">condition_passed</tt> function, to create more concise syntax that better
matches the ISA manual.</p>
<pre class="code python literal-block">
<span class="keyword">def</span> <span class="name function">execute_add</span><span class="punctuation">(</span> <span class="name">s</span><span class="punctuation">,</span> <span class="name">inst</span> <span class="punctuation">):</span>
  <span class="keyword">if</span> <span class="name">condition_passed</span><span class="punctuation">(</span> <span class="name">s</span><span class="punctuation">,</span> <span class="name">inst</span><span class="operator">.</span><span class="name">cond</span><span class="punctuation">()</span> <span class="punctuation">):</span>
    <span class="name">a</span><span class="punctuation">,</span>   <span class="operator">=</span> <span class="name">s</span><span class="operator">.</span><span class="name">rf</span><span class="punctuation">[</span> <span class="name">inst</span><span class="operator">.</span><span class="name">rn</span><span class="punctuation">()</span> <span class="punctuation">]</span>
    <span class="name">b</span><span class="punctuation">,</span> <span class="name">_</span> <span class="operator">=</span> <span class="name">shifter_operand</span><span class="punctuation">(</span> <span class="name">s</span><span class="punctuation">,</span> <span class="name">inst</span> <span class="punctuation">)</span>
    <span class="name">result</span> <span class="operator">=</span> <span class="name">a</span> <span class="operator">+</span> <span class="name">b</span>
    <span class="name">s</span><span class="operator">.</span><span class="name">rf</span><span class="punctuation">[</span> <span class="name">inst</span><span class="operator">.</span><span class="name">rd</span><span class="punctuation">()</span> <span class="punctuation">]</span> <span class="operator">=</span> <span class="name">trim_32</span><span class="punctuation">(</span> <span class="name">result</span> <span class="punctuation">)</span>

    <span class="keyword">if</span> <span class="name">inst</span><span class="operator">.</span><span class="name">S</span><span class="punctuation">():</span>
      <span class="keyword">if</span> <span class="name">inst</span><span class="operator">.</span><span class="name">rd</span><span class="punctuation">()</span> <span class="operator">==</span> <span class="literal number integer">15</span><span class="punctuation">:</span>
        <span class="keyword">raise</span> <span class="name">FatalError</span><span class="punctuation">(</span><span class="literal string">'Writing SPSR not implemented!'</span><span class="punctuation">)</span>
      <span class="name">s</span><span class="operator">.</span><span class="name">N</span> <span class="operator">=</span> <span class="punctuation">(</span><span class="name">result</span> <span class="operator">&gt;&gt;</span> <span class="literal number integer">31</span><span class="punctuation">)</span><span class="operator">&amp;</span><span class="literal number integer">1</span>
      <span class="name">s</span><span class="operator">.</span><span class="name">Z</span> <span class="operator">=</span> <span class="name">trim_32</span><span class="punctuation">(</span> <span class="name">result</span> <span class="punctuation">)</span> <span class="operator">==</span> <span class="literal number integer">0</span>
      <span class="name">s</span><span class="operator">.</span><span class="name">C</span> <span class="operator">=</span> <span class="name">carry_from</span><span class="punctuation">(</span> <span class="name">result</span> <span class="punctuation">)</span>
      <span class="name">s</span><span class="operator">.</span><span class="name">V</span> <span class="operator">=</span> <span class="name">overflow_from_add</span><span class="punctuation">(</span> <span class="name">a</span><span class="punctuation">,</span> <span class="name">b</span><span class="punctuation">,</span> <span class="name">result</span> <span class="punctuation">)</span>

    <span class="keyword">if</span> <span class="name">inst</span><span class="operator">.</span><span class="name">rd</span><span class="punctuation">()</span> <span class="operator">==</span> <span class="literal number integer">15</span><span class="punctuation">:</span>
      <span class="keyword">return</span>

  <span class="name">s</span><span class="operator">.</span><span class="name">rf</span><span class="punctuation">[</span><span class="name">PC</span><span class="punctuation">]</span> <span class="operator">=</span> <span class="name">s</span><span class="operator">.</span><span class="name">fetch_pc</span><span class="punctuation">()</span> <span class="operator">+</span> <span class="literal number integer">4</span>
</pre>
<p>Compared to the ARM ISA Reference manual shown below, the Pydgin instruction
definition is a fairly close match. Pydgin's definitions could certainly be
made more concise by using a custom DSL, however, this would lose many of the
debugging benefits afforded to a well-supported language such as Python and
additionally require using a custom parser that would likely need modification
for each new ISA.</p>
<pre class="code literal-block">
if ConditionPassed(cond) then
   Rd = Rn + shifter_operand
   if S == 1 and Rd == R15 then
     if CurrentModeHasSPSR() then CPSR = SPSR
   else UNPREDICTABLE else if S == 1 then
     N Flag = Rd[31]
     Z Flag = if Rd == 0 then 1 else 0
     C Flag = CarryFrom(Rn + shifter_operand)
     V Flag = OverflowFrom(Rn + shifter_operand)
</pre>
<p>Creating an ISS that can run real applications is a rather complex task, even
for a bare metal simulator with no operating system such as Pydgin.
Each system call in the C library must be properly implemented, and
bootstrapping code must be provided to set up the program stack and
architectural state.
This is a very tedious and error prone process which Pydgin tries to
encapsulate so that it remains as transparent to the end user as possible.
In future versions of Pydgin we hope to make bootstrapping more painless and
support a wider variety of C libraries.</p>
<!-- Architectural state... leave out for now. -->
<!-- ::

class State( object ):
  _virtualizable_ = ['pc', 'ncycles']
  def __init__( self, memory, debug, reset_addr=0x400 ):
    self.pc       = reset_addr
    self.rf       = ArmRegisterFile( self, num_regs=16 )
    self.mem      = memory

    self.rf[ 15 ]  = reset_addr

    # current program status register (CPSR)
    self.N    = 0b0      # Negative condition
    self.Z    = 0b0      # Zero condition
    self.C    = 0b0      # Carry condition
    self.V    = 0b0      # Overflow condition

    # other registers
    self.status        = 0
    self.ncycles       = 0

  def fetch_pc( self ):
    return self.pc -->
</div>
<div class="section" id="pydgin-performance">
<h3>Pydgin Performance</h3>
<p>In order to achieve good simulation performance from Pydgin ISSs, significant
work went into adding appropriate JIT annotations to the Pydgin library
components.
These optimization hints, which allow the JIT generated by the RPython
translation toolchain to produce more efficient code, have been specifically
selected for the unique properties of ISSs.
For the sake of brevity, we do not talk about the exact optimizations here but
a detailed discussion can be found in the ISPASS paper <a class="footnote-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id16" id="id9">[1]</a>.
In the paper we evaluate two ISSs, one for a simplified MIPS ISA and another
for the ARMv5 ISA, whereas below we only discuss results for the ARMv5 ISS.</p>
<p>The performance of Pydgin-generated ARMv5 ISSs were compared against
several reference ISSs: the <a class="reference external" href="https://www.gem5.org/">gem5</a> ARM atomic simulator (<em>gem5</em>),
interpretive and JIT-enabled versions of <a class="reference external" href="https://simit-arm.sourceforge.net/">SimIt-ARM</a> (<em>simit-nojit</em> and
<em>simit-jit</em>), and <a class="reference external" href="https://wiki.qemu.org/">QEMU</a>.
Atomic models from the gem5 simulator were chosen for comparison due their wide
usage amongst computer architects <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#g" id="id10">[g]</a>.
SimIt-ARM was selected because it is currently the highest performance
ADL-generated DBT-ISS publicly available.
QEMU has long been held as the gold-standard for DBT simulators due to its
extremely high performance, however, QEMU is generally intended for usage as an
emulator rather than a simulator <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#c" id="id11">[c]</a> and therefore achieves its excellent
performance at the cost of observability.
Unlike QEMU, all other simulators in our study faithfully track architectural
state at an instruction level rather than block level.
Pydgin ISSs were generated with and without JITs using the RPython translation
toolchain in order to help quantify the performance benefit of the meta-tracing
JIT.</p>
<p>The figure below shows the performance of each ISS executing applications from
the SPEC CINT2006 benchmark suite <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#h" id="id12">[h]</a>.
Benchmarks were run to completion on the high-performance DBT-ISSs
(<em>simit-jit</em>, <em>pydgin-jit</em>, and QEMU), but were terminated after only
10 billion simulated instructions for the non-JITed interpretive ISSs
(these would require many hours, in some cases days, to run to completion).
Simulation performance is measured in MIPS <a class="citation-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#i" id="id13">[i]</a> and plotted on a <strong>log
scale</strong> due to the wide variance in performance.
The <em>WHMEAN</em> group summarizes each ISS's performance across all benchmarks
using the weighted harmonic mean.</p>

<div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-fsfrUJOQKZg/VQKqZzgcQsI/AAAAAAAACAA/20NoWKRzmvU/s1600/arm-bar-plot.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://4.bp.blogspot.com/-fsfrUJOQKZg/VQKqZzgcQsI/AAAAAAAACAA/20NoWKRzmvU/s640/arm-bar-plot.png"></a></div>

<p>A few points to take away from these results:</p>
<ul class="simple">
<li>ISSs without JITs (<em>gem5</em>, <em>simit-nojit</em>, and <em>pydgin-nojit</em>) demonstrate
relatively consistent performance across applications, whereas ISSs with JITs
(<em>simit-jit</em>, <em>pydgin-jit</em>, and QEMU) demonstrate much greater
performance variability from application-to-application.</li>
<li>The <em>gem5</em> atomic model demonstrates particularly miserable performance, only
2-3 MIPS!</li>
<li>QEMU lives up to its reputation as a gold-standard for simulator performance,
leading the pack on nearly every benchmark and reaching speeds of 240-1120
MIPS.</li>
<li>
<em>pydgin-jit</em> is able to outperform <em>simit-jit</em> on four of the
applications, including considerable performance improvements of 1.44–1.52×
for the applications <em>456.hmmer</em>, <em>462.libquantum</em>, and <em>471.omnetpp</em>
(managing to even outperform QEMU on <em>471.omnetpp</em>).</li>
<li>
<em>simit-jit</em> is able to obtain much more consistent performance (230-459
MIPS across all applications) than <em>pydgin-jit</em> (9.6-659 MIPS).  This is
due to <em>simit-jit</em>'s page-based approach to JIT optimization compared to
<em>pydgin-jit</em>'s tracing-based approach.</li>
<li>
<em>464.h264ref</em> displays particularly bad pathological behavior in Pydgin’s
tracing JIT and is the only application to perform worse on <em>pydgin-jit</em>
than <em>pydgin-nojit</em> (9.6 MIPS vs. 21 MIPS).</li>
</ul>
<p>The pathological behavior demonstrated by <em>464.h264ref</em> was of particular
concern because it caused <em>pydgin-jit</em> to perform even worse than having no
JIT at all. RPython JIT logs indicated that the reason for this performance
degradation was a large number of tracing aborts due to JIT traces growing too
long. However, time limitations before the publication deadline prevented us
from investigating this issue thoroughly.</p>
<p>Since the deadline we've applied some minor bug fixes and made some small
improvements in the memory representation.
More importantly, we've addressed the performance degradation in <em>464.h264ref</em>
by increasing trace lengths for the JIT.
Below we show how the performance of <em>464.h264ref</em> changes as the
<strong>trace_limit</strong> parameter exposed by the RPython JIT is varied from the default
size of 6000 operations.</p>


<div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/-rOklyrr1tzY/VQKqg3GJu9I/AAAAAAAACAI/jfoHvpJbMF8/s1600/trace-length-plot.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://2.bp.blogspot.com/-rOklyrr1tzY/VQKqg3GJu9I/AAAAAAAACAI/jfoHvpJbMF8/s640/trace-length-plot.png"></a></div>

<p>By quadrupling the trace limit we achieve an 11x performance improvement in
<em>464.h264ref</em>.
The larger trace limit allows the JIT to optimize long code paths that were
previously triggering trace aborts, greatly helping amortize the costs of
tracing.
Note that arbitrarily increasing this limit can potentially hurt performance if
longer traces are not able to detect optimizable code sequences.</p>
<p>After performing similar experiments across the applications in the SPEC
CINT2006 benchmark suite, we settled on a trace limit of 400,000 operations.
In the figure below we show how the updated Pydgin ISS (<em>pydgin-400K</em>) improves
performance across all benchmarks and fixes the performance degradation
previously seen in <em>464.h264ref</em>. Note that the non-JITted simulators have been
removed for clarity, and simulation performance is now plotted on a
<strong>linear scale</strong> to more clearly distinguish the performance gap between
each ISS.</p>

<div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-DSAtuNZ7fnQ/VQKqm0HPBfI/AAAAAAAACAQ/8hYCDeZujq8/s1600/new-bar-plot.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://1.bp.blogspot.com/-DSAtuNZ7fnQ/VQKqm0HPBfI/AAAAAAAACAQ/8hYCDeZujq8/s640/new-bar-plot.png"></a></div>

<p>With these improvements, we are now able to beat <em>simit-jit</em> on all but two
benchmarks. In future work we hope to further close the gap with QEMU as well.</p>
</div>
<div class="section" id="conclusions-and-future-work">
<h3>Conclusions and Future Work</h3>
<p>Pydgin demonstrates that the impressive work put into the RPython translation
toolchain, designed to simplify the process of building fast dynamic-language
VMs, can also be leveraged to build fast instruction set simulators.
Our prototype ARMv5 ISS shows that Pydgin can generate ISSs with performance
competitive to SimIt-ARM while also providing a more productive development
experience: RPython allowed us to develop Pydgin with only four person-months
of work.
Another significant benefit of the Pydgin approach is that any performance
improvements applied to the RPython translation toolchain immediately benefit
Pydgin ISSs after a simple software download and retranslation.
This allows Pydgin to track the continual advances in JIT technology introduced
by the PyPy development team.</p>
<p>Pydgin is very much a work in progress. There are many features we would like
to add, including:</p>
<ul class="simple">
<li>more concise syntax for accessing arbitrary instruction bits</li>
<li>support for other C libraries such as glibc, uClibc, and musl
(we currently only support binaries compiled with newlib)</li>
<li>support for self-modifying code</li>
<li>features for more productive debugging of target applications</li>
<li>ISS descriptions for other ISAs such as RISC-V, ARMv8, and x86</li>
<li>automatic generation of compilers and toolchains from Pydgin descriptions</li>
</ul>
<p>In addition, we think there are opportunities for even greater performance
improvements with more advanced techniques such as:</p>
<ul class="simple">
<li>automatic generation of optimized instruction decoders</li>
<li>optimizations for floating-point intensive applications</li>
<li>multiple tracing-JITs for parallel simulation of multicore SOCs</li>
<li>a parallel JIT compilation engine as proposed by Böhm et al. <a class="footnote-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id18" id="id14">[3]</a>
</li>
</ul>
<p>We hope that Pydgin can be of use to others, so if you try it out please let us
know what you think. Feel free to contact us if you find any of the above
development projects interesting, or simply fork the project on GitHub and hack
away!</p>
<p>-- Derek Lockhart and Berkin Ilbeyi</p>
</div>
<div class="section" id="acknowledgements">
<h3>Acknowledgements</h3>
<p> We would like to sincerely thank Carl Friedrich Bolz and Maciej Fijalkowski for their feedback on the Pydgin publication and their guidance on improving the JIT performance of our simulators. We would also like to thank for the whole PyPy team for their incredible work on the PyPy and the RPython translation toolchain. Finally, thank you to our research advisor, Prof. Christopher Batten, and the sponsors of this work which include the National Science Foundation, the Defense Advanced Research Projects Agency, and Intel Corporation.</p>
</div>
<div class="section" id="footnotes">
<h3>Footnotes</h3>
<table class="docutils citation" frame="void" id="a" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id1">[a]</a></td>
<td>Pydgin loosely stands for [Py]thon [D]SL for [G]enerating
[In]struction set simulators and is pronounced the same as “pigeon”. The
name is inspired by the word “pidgin” which is a grammatically simplified
form of language and captures the intent of the Pydgin embedded-ADL.
<a class="reference external" href="https://github.com/cornell-brg/pydgin">https://github.com/cornell-brg/pydgin</a>
</td>
</tr></tbody>
</table>
<table class="docutils citation" frame="void" id="b" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id2">[b]</a></td>
<td>Popular instruction set architectures (ISAs) include MIPs, ARM,
x86, and more recently RISC-V</td>
</tr></tbody>
</table>
<table class="docutils citation" frame="void" id="c" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[c]</td>
<td>
<em>(<a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id4">1</a>, <a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id11">2</a>)</em> For a good discussion of simulators vs. emulators, please see the
following post on StackOverflow:
<a class="reference external" href="https://stackoverflow.com/questions/1584617/simulator-or-emulator-what-is-the-difference">https://stackoverflow.com/questions/1584617/simulator-or-emulator-what-is-the-difference</a>
</td>
</tr></tbody>
</table>
<table class="docutils citation" frame="void" id="d" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id5">[d]</a></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Dark_silicon">https://en.wikipedia.org/wiki/Dark_silicon</a></td>
</tr></tbody>
</table>
<table class="docutils citation" frame="void" id="e" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id6">[e]</a></td>
<td>Please see the Pydgin paper for a more detailed discussion of prior work.</td>
</tr></tbody>
</table>
<table class="docutils citation" frame="void" id="f" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id8">[f]</a></td>
<td>
<p class="first">For more examples of Pydgin ISA specifications, please see the ISPASS
paper <a class="footnote-reference" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id16" id="id15">[1]</a> or the Pydgin source code on GitHub.</p>
<p>Pydgin instruction definitions for a simple MIPS-inspired ISA can be
found here:</p>
<ul class="simple">
<li><a class="reference external" href="https://github.com/cornell-brg/pydgin/blob/master/parc/isa.py">https://github.com/cornell-brg/pydgin/blob/master/parc/isa.py</a></li>
</ul>
<p>Pydgin instruction definitions for a simplified ARMv5 ISA can be found
here:</p>
<ul class="last simple">
<li><a class="reference external" href="https://github.com/cornell-brg/pydgin/blob/master/arm/isa.py">https://github.com/cornell-brg/pydgin/blob/master/arm/isa.py</a></li>
</ul>
</td>
</tr></tbody>
</table>
<table class="docutils citation" frame="void" id="g" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id10">[g]</a></td>
<td>
<p class="first">gem5 is a cycle-level simulation framework that contains both
functional-level (atomic) and cycle-level processor models. Although
primarily used for detailed, cycle-approximate processor simulation,
gem5's atomic model is a popular tool for many ISS tasks.</p>
<ul class="last simple">
<li><a class="reference external" href="https://www.m5sim.org/SimpleCPU">https://www.m5sim.org/SimpleCPU</a></li>
</ul>
</td>
</tr></tbody>
</table>
<table class="docutils citation" frame="void" id="h" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id12">[h]</a></td>
<td>All performance measurements were taken on an unloaded server-class
machine.</td>
</tr></tbody>
</table>
<table class="docutils citation" frame="void" id="i" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id13">[i]</a></td>
<td>Millions of instructions per second.</td>
</tr></tbody>
</table>
</div>
<div class="section" id="references">
<h3>References</h3>
<table class="docutils footnote" frame="void" id="id16" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[1]</td>
<td>
<em>(<a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id3">1</a>, <a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id9">2</a>, <a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id15">3</a>)</em> <p>Derek Lockhart, Berkin Ilbeyi, and Christopher Batten. "Pydgin:
Generating Fast Instruction Set Simulators from Simple Architecture
Descriptions with Meta-Tracing JIT Compilers." IEEE Int'l Symp. on
Performance Analysis of Systems and Software (ISPASS), Mar. 2015.</p>
<ul class="last simple">
<li><a class="reference external" href="https://csl.cornell.edu/~cbatten/pdfs/lockhart-pydgin-ispass2015.pdf">https://csl.cornell.edu/~cbatten/pdfs/lockhart-pydgin-ispass2015.pdf</a></li>
<li><a class="reference external" href="https://github.com/cornell-brg/pydgin">https://github.com/cornell-brg/pydgin</a></li>
</ul>
</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="id17" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id7">[2]</a></td>
<td>
<p class="first">Derek Lockhart, Gary Zibrat, and Christopher Batten. "PyMTL: A Unified
Framework for Vertically Integrated Computer Architecture Research." 47th
ACM/IEEE Int'l Symp. on Microarchitecture (MICRO-47), Dec. 2014.</p>
<ul class="last simple">
<li><a class="reference external" href="https://csl.cornell.edu/~cbatten/pdfs/lockhart-pymtl-micro2014.pdf">https://csl.cornell.edu/~cbatten/pdfs/lockhart-pymtl-micro2014.pdf</a></li>
<li><a class="reference external" href="https://github.com/cornell-brg/pymtl">https://github.com/cornell-brg/pymtl</a></li>
</ul>
</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="id18" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="../posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html#id14">[3]</a></td>
<td>I. Böhm, B. Franke, and N. Topham. Generalized Just-In-Time Trace
Compilation Using a Parallel Task Farm in a Dynamic Binary Translator.
ACM SIGPLAN Conference on Programming Language Design and Implementation
(PLDI), Jun 2011.</td>
</tr></tbody>
</table>
</div>

<br>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-3867318553284676076">
        <div class="comment-header">
          <a name="comment-3867318553284676076"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-03-30 12:14</span>:
        </div>
        <div class="comment-content">
          <p>For reference 3: <a href="https://groups.inf.ed.ac.uk/pasta/pub_PLDI_2011.html" rel="nofollow">https://groups.inf.ed.ac.uk/pasta/pub_PLDI_2011.html</a>.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/02/experiments-in-pyrlang-with-rpython-8103387814587972227.html" class="u-url">Experiments in Pyrlang with RPython</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/carl-friedrich-bolz-tereick.html">Carl Friedrich Bolz-Tereick</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/02/experiments-in-pyrlang-with-rpython-8103387814587972227.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-02-25T10:13:00Z" itemprop="datePublished" title="2015-02-25 10:13">2015-02-25 10:13</time></a>
            </p>
                <p class="commentline">1 comment</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p><a href="https://bitbucket.org/hrc706/pyrlang/overview">Pyrlang</a> is an Erlang BEAM bytecode interpreter written in RPython.</p>
<p>It implements approximately 25% of BEAM instructions. It can support
integer calculations (but not bigint), closures, exception handling,
some operators to atom, list and tuple, user modules, and multi-process
in single core. Pyrlang is still in development.</p>
<p>There are some differences between BEAM and the VM of PyPy:</p>
<ul class="simple">
<li>BEAM is a register-based VM, whereas the VM in PyPy is stack-based.</li>
<li>There is no traditional call-stack in BEAM. The Y register in BEAM is
similar to a call-stack, but the Y register can sometimes store some
variables.</li>
<li>There are no typical language-level threads and OS-level threads in
BEAM; only language-level processes, whose behavior is very similar
to the actor model.</li>
</ul>
<p>Regarding bytecode dispatch loop, Pyrlang uses a while loop to fetch
instructions and operands, call the function corresponding to every
instruction, and jump back to the head of the while loop. Due to the
differences between the RPython call-stack and BEAM’s Y register, we
decided to implement and manage the Y register by hand. On the other
hand, PyPy uses RPython’s call stack to implement Python’s call stack.
As a result, the function for the dispatch loop in PyPy calls itself
recursively. This does not happen in Pyrlang.</p>
<p>The Erlang compiler (erlc) usually compiles the bytecode instructions
for function invocation into CALL (for normal invocation) and CALL_ONLY
(for tail recursive invocation). You can use a trampoline semantic to
implement it:</p>
<ul class="simple">
<li>CALL instruction: The VM pushes the current instruction pointer (or
called-program counter in PyPy) to the Y register, and jumps to the
destination label. When encountering a RETURN instruction, the VM
pops the instruction pointer from the Y register and returns to the
location of the instruction pointer to continue executing the outer
function.</li>
<li>CALL_ONLY instruction: The VM simply jumps to the destination label,
without any modification of the Y register. As a result, the tail
recursive invocation never increases the Y register.</li>
</ul>
<p>The current implementation only inserts the JIT hint of can_enter_jit
following the CALL_ONLY instruction. This means that the JIT only
traces the tail-recursive invocation in Erlang code, which has a very
similar semantic to the loop in imperative programming languages like
Python.</p>
<p>We have also written a single scheduler to implement the language level
process in a single core. There is a runable queue in the scheduler. On
each iteration, the scheduler pops one element (which is a process
object with dispatch loop) from the queue, and executes the dispatch
loop of the process object. In the dispatch loop, however, there is a
counter-call “reduction” inside the dispatch loop. The reduction
decrements during the execution of the loop, and when the reduction
becomes 0, the dispatch loop terminates. Then the scheduler pushes that
element into the runable queue again, and pops the next element for the
queue, and so on.</p>
<p>We are planning to implement a multi-process scheduler for multi-core
CPUs, which will require multiple schedulers and even multiple runable
queues for each core, but that will be another story. :-)</p>
<div class="section" id="methods">
<h2>Methods</h2>
<p>We wrote two benchmark programs of Erlang:</p>
<ul class="simple">
<li>FACT: A benchmark to calculate the factorial in a tail-recursive
style, but because we haven’t implemented big int, we do a remainder
calculation to the argument for the next iteration, so the number
never overflows.</li>
<li>REVERSE: The benchmark creates a reversed list of numbers, such as
[20000, 19999, 19998, …], and applies a bubble sort to it.</li>
</ul>
</div>
<div class="section" id="results">
<h2>Results</h2>
<div class="section" id="the-value-of-reduction">
<h3>The Value of Reduction</h3>
<p>We used REVERSE to evaluate the JIT with different values of
reduction:</p>
<a href="https://3.bp.blogspot.com/-iAQtrDEcs7s/VOxUT54I7GI/AAAAAAAAF2Q/UdJt9stUKAY/s1600/reverse.png"><img border="0" src="https://3.bp.blogspot.com/-iAQtrDEcs7s/VOxUT54I7GI/AAAAAAAAF2Q/UdJt9stUKAY/s600/reverse.png"></a>
<a href="https://3.bp.blogspot.com/-GAOD7G7gdkU/VOxUTi19QeI/AAAAAAAAF2M/xXS3VXQwEwI/s1600/fact.png"><img border="0" src="https://3.bp.blogspot.com/-GAOD7G7gdkU/VOxUTi19QeI/AAAAAAAAF2M/xXS3VXQwEwI/s600/fact.png"></a>
<p>The X axis is the value of reduction, and the Y axis is the execution
time (by second).</p>
<p>It seems that when the value of reduction is small, the reduction
influences the performance significantly, but when reduction becomes
larger, it only increases the speed very slightly. In fact, we use 2000
as the default reduction value (as well as the reduction value in the
official Erlang interpreter).</p>
<p>Surprisingly, the trace is always generated even when the reduction is
very small, such as 0, which means the dispatch loop can only run for a
very limited number of iterations, and the language level process
executes fewer instructions than an entire loop in one switch of the
scheduler). The generated trace is almost the same, regardless of
different reduction values.</p>
<p>Actually, the RPython JIT only cares what code it meets, but does not
care who executes it, thus the JIT always generates the results above.
The trace even can be shared among different threads if they execute the
same code.</p>
<p>The overhead at low reduction value may be due to the scheduler, which
switches from different processes too frequently, or from the
too-frequent switching between bytecode interpreter and native code, but
not from JIT itself.</p>
<p>Here is more explanation from Armin Rigo:</p>
<blockquote>
“The JIT works well because you’re using a scheme where some counter
is decremented (and the soft-thread interrupted when it reaches
zero) only once in each app-level loop. The soft-thread switch is
done by returning to some scheduler, which will resume a different
soft-thread by calling it. It means the JIT can still compile each
of the loops as usual, with the generated machine code containing
the decrease-and-check-for-zero operation which, when true, exits
the assembler."</blockquote>
</div>
<div class="section" id="fair-process-switching-vs-unfair-process-switching">
<h3>Fair Process Switching vs. Unfair Process Switching</h3>
<p>We are also concerned about the timing for decreasing reduction value.
In our initial version of Pyrlang, we decrease reduction value at every
local function invocation, module function invocation, and BIF (built-in
function) invocation, since this is what the official Erlang interpreter
does. However, since the JIT in RPython basically traces the target
language loop (which is the tail recursive invocation in Pyrlang) it is
typically better to keep the loop whole during a switch of the language
level process. We modified Pyrlang, and made the reduction decrement
only occur after CALL_ONLY, which is actually the loop boundary of the
target language.</p>
<p>Of course, this strategy may cause an “unfair” execution among language
level processes. For example, if one process has only a single
long-sequence code, it executes until the end of the code. On the other
hand, if a process has a very short loop, it may be executed by very
limited steps then be switched out by the scheduler. However, in the
real world, this “unfairness” is usually considered acceptable, and is
used in many VM implementations including PyPy for improving the overall
performance.</p>
<p>We compared these two versions of Pyrlang in the FACT benchmark. The
reduction decrement is quite different because there are some BIF
invocations inside the loop. In the old version the process can be
suspended at loop boundaries or other function invocation, but in the
new version, it can be suspended only at loop boundaries.</p>
<p>We show that the strategy is effective, removing around 7% of the
overhead. We have also compared it in REVERSE, but since there are no
extra invocations inside the trace, it cannot provide any performance
improvement. In the real world, we believe there is usually more than
one extra invocation inside a single loop, so this strategy is effective
for most cases.</p>
</div>
<div class="section" id="comparison-with-default-erlang-and-hipe">
<h3>Comparison with Default Erlang and HiPE</h3>
<p>We compared the performance of Pyrlang with the default Erlang
interpreter and the HiPE (High Performance Erlang) complier. HiPE is an
official Erlang compiler that can compile Erlang source code to native
code. The speed of Erlang programs obviously improves but loses its
generality instead.</p>
<p>Please note that Pyrlang is still in development, so in some situations
it does less work than the default Erlang interpreter, such as not
checking integer overflow when dealing with big integer, and not
checking and adding locks when accessing message queues in the
language-level process, so is therefore faster. The final version of
Pyrlang may be slower.</p>
<p>We used the two benchmark programs above, and made sure both of them are
executed for more than five seconds to cover the JIT warm-up time for
RPython. The experiment environment is a OS X 10.10 machine with 3.5GHZ
6-core Intel Xeon E5 CPU and 14GB 1866 MHz DDR3 ECC memory.</p>
<p>Let’s look at the result of FACT. The graph shows that Pyrlang runs
177.41% faster on average than Erlang, and runs at almost the same speed
as HiPE. However, since we haven’t implemented big integer in Pyrlang,
the arithmetical operators do not do any extra overflow checking. It is
reasonable that the final version for Pyrlang will be slower than the
current version and HiPE.</p>
<a href="https://4.bp.blogspot.com/-uWegj1T-Uqo/VOxU0q3i0vI/AAAAAAAAF2k/0fI1h0eUtuY/s1600/comparison.png"><img border="0" src="https://4.bp.blogspot.com/-uWegj1T-Uqo/VOxU0q3i0vI/AAAAAAAAF2k/0fI1h0eUtuY/s600/comparison.png"></a>
<p>As for REVERSE, the graph shows that Pyrlang runs 45.09% faster than
Erlang, but 63.45% slower than HiPE on average. We think this is
reasonable because there are only few arithmetical operators in this
benchmark so the speeds of these three implementations are closer.
However, we observed that at the scale of 40,000, the speed of Pyrlang
slowed down significantly (111.35% slower than HiPE) compared with the
other two scales (56.38% and 22.63% slower than HiPE).</p>
<p>Until now we can only hypothesize why Pyrlang slows down at that scale.
We guess that the overhead might be from GC. This is because the BEAM
bytecode provides some GC hints to help the default Erlang compiler to
perform some GC operations immediately. For example, using GC_BIF
instead of a BIF instruction tells the VM that there may be a GC
opportunity, and tells the VM how many live variables should be around
one instruction. In Pyrlang we do not use these kinds of hints but rely
on RPython’s GC totally. When there are a huge number of objects during
runtime, (as for REVERSE, it should be the Erlang list object) the speed
therefore slows down.</p>
<a href="https://2.bp.blogspot.com/-DsEMfJl3g50/VOxU_iKhsVI/AAAAAAAAF2s/2omW5vUSZD8/s1600/reverse_comparison.png"><img border="0" src="https://2.bp.blogspot.com/-DsEMfJl3g50/VOxU_iKhsVI/AAAAAAAAF2s/2omW5vUSZD8/s600/reverse_comparison.png"></a>
<p>Ruochen Huang</p>
</div>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-6625331584140411790">
        <div class="comment-header">
          <a name="comment-6625331584140411790"></a>
            <span class="author">peterfirefly</span> wrote on <span class="date">2015-02-26 12:14</span>:
        </div>
        <div class="comment-content">
          <p>'there is a counter-call “reduction”' should probably be:<br><br>'there is a counter called “reduction”'.</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/02/linalg-support-in-pypynumpy-1131217944329711855.html" class="u-url">linalg support in pypy/numpy</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/mattip.html">mattip</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/02/linalg-support-in-pypynumpy-1131217944329711855.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-02-23T20:36:00Z" itemprop="datePublished" title="2015-02-23 20:36">2015-02-23 20:36</time></a>
            </p>
                <p class="commentline">23 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">
<h2 style="text-align: left;">
</h2>
<h1>
Introduction</h1>
PyPy's numpy support has matured enough that it can now support the  lapack/blas libraries through the numpy.linalg module. To install the  version of numpy this blog post refers to, install PyPy version 2.5.0 or  newer, and run this:<br><br><pre>pypy -m pip install git+https://bitbucket.org/pypy/numpy.git
</pre>
<br>
This update is a major step forward for PyPy's numpy support. Many of  the basic matrix operations depend on linalg, even matplotlib requires  it to display legends (a pypy-friendly version of matplotlib 1.3 is  available  at <a href="https://github.com/mattip/matplotlib" rel="noreferrer">https://github.com/mattip/matplotlib</a>).<br><br>
A number of improvements and adaptations, some of which are in the newly-released PyPy 2.5.0, made this possible:<br><ul class="task-list">
<li>Support for an extended <tt>frompyfunc()</tt>, which in the PyPy  version supports much of the ufunc API (signatures, multiple dtypes)  allowing creation of pure-python, jit-friendly ufuncs. An additional  keyword allows choosing between <tt>out = func(in)</tt> or <tt>func(in, out)</tt> ufunc signatures. More explanation follows.</li>
<li>Support for GenericUfuncs via PyPy's (slow) capi-compatibility  layer. The underlying mechanism actually calls the internal  implementation of <tt>frompyfunc()</tt>.</li>
<li>A cffi version of <tt>_umath_linalg</tt>. Since cffi uses <tt>dlopen()</tt>  to call into shared objects, we added support in the numpy build system  to create non-python shared libraries from source code in the numpy  tree. We also rewrote parts of the c-based <tt>_umath_linalg.c.src</tt> in python, renamed numpy's <tt>umath_linalg</tt> capi module to <tt>umath_linag_capi</tt>, and use it as a shared object through cffi.</li>
</ul>
<div>
<h1>
<a class="anchor" href="https://gist.github.com/mattip/25680e68fe7e2856fe0c#status" name="user-content-status" rel="noreferrer"></a>Status</h1>
We have not completely implemented all the linalg features. dtype  resolution via casting is missing, especially for complex ndarrays,  which leads to slight numerical errors where numpy uses a more precise  type for intermediate calculations. Other missing features in PyPy's  numpy support may have implications for complete linalg support.<br><br>
Some OSX users have noticed they need to update pip to version 6.0.8 to overcome a regression in pip, and it is not clear if we support all combinations of blas/lapack implementations on all platforms.<br><br>
Over  the next few weeks we will be ironing out these issues.</div>
<div>
<h1>
<a class="anchor" href="https://gist.github.com/mattip/25680e68fe7e2856fe0c#performance" name="user-content-performance" rel="noreferrer"></a>Performance</h1>
A simple benchmark is shown below, but let's state the obvious:  PyPy's JIT and the iterators built into PyPy's ndarray implementation  will in most cases be no faster than CPython's numpy. The JIT can help  where there is a mixture of python and numpy-array code. We do have  plans to implement lazy evaluation and to further optimize PyPy's  support for numeric python, but numpy is quite good at what it does.</div>
<div>
<h1>
<a class="anchor" href="https://gist.github.com/mattip/25680e68fe7e2856fe0c#howto-for-pypys-extended-frompyfunc" name="user-content-howto-for-pypys-extended-frompyfunc" rel="noreferrer"></a>HowTo for PyPy's extended <tt>frompyfunc</tt> </h1>
The magic enabling blas support is a rewrite of the <tt>_umath_linalg</tt> c-based module as a cffi-python module that creates ufuncs via <tt>frompyfunc</tt>. We extended the numpy <tt>frompyfunc</tt> to allow it to function as a replacement for the generic ufunc available in numpy only through the c-api.<br><br>
We start with the basic <tt>frompyfunc</tt>, which wraps a python function into a ufunc:<br><pre> </pre>
<pre>def times2(in0):
    return in0 * 2
ufunc = frompyfunc(times2, 1, 1)
</pre>
<br>
In cpython's numpy the dtype of the result is always object, which is  not implemented (yet) in PyPy, so this example will fail. While the  utility of object dtypes can be debated, in the meantime we add a  non-numpy-compatible keyword argument <tt>dtypes</tt> to <tt>frompyfunc</tt>. If <tt>dtype=['match']</tt> the output dtype will match the dtype of the first input ndarray:<br><br><pre>ufunc = frompyfunc(times2, 1, 1, dtype=['match'])
ai = arange(24).reshape(3, 4, 2)
ao = ufunc(ai)
assert  (ao == ai * 2).all()
</pre>
<br>
I hear you ask "why is the dtypes keyword argument a list?" This is so we can support the <a href="https://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html" rel="noreferrer">Generalized Universal Function API</a>, which allows specifying a number of specialized functions and the input-output dtypes each specialized function accepts.<br>
Note that the function feeds the values of <tt>ai</tt> one at a time,  the function operates on scalar values. To support more complicated  ufunc calls, the generalized ufunc API allows defining a signature,  which specifies the layout of the ndarray inputs and outputs. So we extended <tt>frompyfunc </tt>with a signature keyword as well.<br>
We add one further extension to <tt>frompyfunc</tt>: we allow a Boolean keyword <tt>stack_inputs</tt> to specify the argument layout of the function itself. If the function is of the form:<br><pre> </pre>
<pre>out0, out1, ... = func(in0, in1,...)
</pre>
<br>
then stack_inputs is False. If it is True the function is of the form:<br><pre> </pre>
<pre>func(in0, in1, ... out0, out1, ...)
</pre>
<br>
Here is a complete example of using <tt>frompyfunc</tt> to create a ufunc, based on <a href="https://docs.scipy.org/doc/numpy/user/c-info.ufunc-tutorial.html" rel="noreferrer">this link</a>:<br><pre> </pre>
<pre>def times2(in_array, out_array):
    in_flat = in_array.flat
    out_flat = out_array.flat
    for i in range(in_array.size):
        out_flat[i] = in_flat[i] * 2
ufunc = frompyfunc([times2, times2], 1, 1,
                signature='(i)-&gt;(i)',
                dtypes=[dtype(int), dtype(int),
                        dtype(float), dtype(float),
                       ],
                stack_inputs=True,
                )
ai = arange(10, dtype=int)
ai2 = ufunc(ai)
assert all(ai2 == ai * 2)
</pre>
<br>
Using this extended syntax, we rewrote the lapack calls into the blas  functions in pure python, no c needed. Benchmarking this approach  actually was <b>much slower</b> than using the upstream <tt>umath_linalg</tt>  module via cpyext, as can be seen in the following benchmarks. This is  due to the need to copy c-aligned data into Fortran-aligned format. Our <tt>__getitem__</tt> and <tt>__setitem__</tt> iterators are not as fast as pointer arithmetic in C. So we next tried a hybrid approach: compile and use numpy's <tt>umath_linalg</tt> python module as a shared object, and call the optimized specific wrapper function from it.</div>
<div>
<h1>
<a class="anchor" href="https://gist.github.com/mattip/25680e68fe7e2856fe0c#benchmarks" name="user-content-benchmarks" rel="noreferrer"></a>Benchmarks</h1>
Here are some benchmarks, running a tight loop of the different versions of <tt>linalg.inv(a)</tt>, where <tt>a</tt> is a 10x10 double ndarray. The benchmark ran on an i7 processor running ubuntu 14.04 64 bit:<br><table border="all">
<thead valign="bottom"><tr>
<th>Impl.</th> <th>Time after warmup</th> </tr></thead>
<tbody valign="top">
<tr>
<td>CPython 2.7 + numpy 1.10.dev + lapack</td> <td>8.9 msec/1000 loops</td> </tr>
<tr>
<td>PyPy 2.5.0  + numpy + lapack via cpyext</td> <td>8.6 msec/1000 loops</td> </tr>
<tr>
<td>PyPy 2.5.0  + numpy + lapack via pure python + cffi</td> <td>19.9 msec/1000 loops</td> </tr>
<tr>
<td>PyPy 2.5.0  + numpy + lapack via python + c + cffi</td> <td>9.5 msec/1000 loops</td>
</tr>
</tbody>
</table>
<br><table border="all"><tbody valign="top"><tr></tr></tbody></table>
</div>
<div>
<br>
While no general conclusions may be drawn from a single micro-benchmark, it does indicate that there is some merit in the approach taken. <br><h1>
Conclusion</h1>
PyPy's numpy now includes a working linalg module. There are still  some rough corners, but hopefully we have implemented the parts you  need. While the speed of the isolated linalg function is no faster than  CPython and upstream numpy, it should not be significantly slower  either. Your use case may see an improvement if you use a mix of python  and lapack, which is the usual case.<br><br>
Please let us know how it goes. We love to hear success stories too.<br><br>
We still have challenges at all levels of programming,and are always  looking for people willing to contribute, so stop by on IRC at #pypy.<br><br>
mattip and the PyPy Team</div>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-6964427589034752822">
        <div class="comment-header">
          <a name="comment-6964427589034752822"></a>
            <span class="author">Olivier Grisel</span> wrote on <span class="date">2015-02-24 10:20</span>:
        </div>
        <div class="comment-content">
          <p>Interesting work although benchmarking linear algebra routines on 10x10 arrays feels wrong: typical linear algebra applications use hundreds or thousands of dimensions. Would you mind re-rerunning those benchmarks on 1000x1000 arrays instead? The use of the CPU cache and multiple threads can be very impacting for such workloads.<br><br>Also some numpy / scipy developers are working on supporting OpenBLAS as the default BLAS/LAPACK by default for the Windows wheel packages and maybe later for the OSX packages as well.<br><br>Under Linux (Debian / Ubuntu) it's pretty easy to have libblas.so / liblapack.so be symlinks to either ATLAS or OpenBLAS using the update-alternative syste,</p>
        </div>
      </div>
      <div class="comment comment-3338463064440291817">
        <div class="comment-header">
          <a name="comment-3338463064440291817"></a>
            <span class="author">Maciej Fijalkowski</span> wrote on <span class="date">2015-02-24 10:23</span>:
        </div>
        <div class="comment-content">
          <p>What blog post somehow fails to mention is that we do not reimplement those but reuse whatever underlaying library is there. The measurements of the actual speed is then not that interesting, because we're only interested in the overhead of call.</p>

        </div>
      </div>
      <div class="comment comment-2207571289333943194">
        <div class="comment-header">
          <a name="comment-2207571289333943194"></a>
            <span class="author">Olivier Grisel</span> wrote on <span class="date">2015-02-24 10:26</span>:
        </div>
        <div class="comment-content">
          <p>It might still be interesting to run that kind of benchmarks on more realistic workloads (maybe in addition to some micro-workloads) to see the importance of the remaining overhead in a typical usage scenario.</p>
        </div>
      </div>
      <div class="comment comment-7974828938642324982">
        <div class="comment-header">
          <a name="comment-7974828938642324982"></a>
            <span class="author">mattip</span> wrote on <span class="date">2015-02-24 16:39</span>:
        </div>
        <div class="comment-content">
          <p>The most interesting benchmark is probably the one only you can run, i.e. how does pypy perform for you on your workload.<br><br>As far as lapack vs openblas, we will try to imitate what numpy does. If cpython/numpy supports a variation of lapack and pypy/numpy doesn't, that should be considered a bug. <br><br>Please let us know how it works for you.</p>
        </div>
      </div>
      <div class="comment comment-4982838066074429452">
        <div class="comment-header">
          <a name="comment-4982838066074429452"></a>
            <span class="author">Olivier Grisel</span> wrote on <span class="date">2015-02-24 17:50</span>:
        </div>
        <div class="comment-content">
          <p>&gt; The most interesting benchmark is probably the one only you can run, i.e. how does pypy perform for you on your workload.<br><br>I agree, but inverting a 10x10 matrix is probably not representative of anybody's workload.<br><br>While it's important not to introduce too much overhead in the bindings, I think it's also good to keep in mind that an overhead of the order of the micro-second is completely negligible compared to the execution time of a typical linear algebra operation running on realistically sized data. Hence my original remark.<br><br>&gt; As far as lapack vs openblas, we will try to imitate what numpy does. If cpython/numpy supports a variation of lapack and pypy/numpy doesn't, that should be considered a bug.<br><br>Just to clarify OpenBLAS is an implementation of the standard BLAS API that also includes the official LAPACK implementation from netlib linked against its own optimized BLAS routines. The 2 main open source optimized implementations of BLAS/LAPACK supported by numpy &amp; scipy are ATLAS and OpenBLAS.</p>
        </div>
      </div>
      <div class="comment comment-4956345905201871571">
        <div class="comment-header">
          <a name="comment-4956345905201871571"></a>
            <span class="author">Romain Guillebert</span> wrote on <span class="date">2015-02-24 19:12</span>:
        </div>
        <div class="comment-content">
          <p>&gt; While it's important not to introduce too much overhead in the bindings, I think it's also good to keep in mind that an overhead of the order of the micro-second is completely negligible compared to the execution time of a typical linear algebra operation running on realistically sized data. Hence my original remark.<br><br>But then you're just benchmarking the underlying library, which is the exact same library as numpy.</p>
        </div>
      </div>
      <div class="comment comment-7295018257636297863">
        <div class="comment-header">
          <a name="comment-7295018257636297863"></a>
            <span class="author">Olivier Grisel</span> wrote on <span class="date">2015-02-24 20:21</span>:
        </div>
        <div class="comment-content">
          <p>&gt; But then you're just benchmarking the underlying library, which is the exact same library as numpy.<br><br>Yes I agree. I just want to highlight that for most common real life use cases, a small performance overhead in those those LAPACK bindings are almost never a problem. <br><br>Otherwise your readers might be mislead into thinking that the "PyPy 2.5.0 + numpy + lapack via pure python + cffi" version is significantly suboptimal (2x slowdown!) while in practice a couple of additional microseconds might be completely undetectable compared to the actual execution time of the "inv" function that typically lasts more than a millisecond on anything that is non-toy data.</p>
        </div>
      </div>
      <div class="comment comment-7230419183409976005">
        <div class="comment-header">
          <a name="comment-7230419183409976005"></a>
            <span class="author">Romain Guillebert</span> wrote on <span class="date">2015-02-24 20:57</span>:
        </div>
        <div class="comment-content">
          <p>ok, makes sense :)</p>
        </div>
      </div>
      <div class="comment comment-7106410907157682052">
        <div class="comment-header">
          <a name="comment-7106410907157682052"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-02-24 21:12</span>:
        </div>
        <div class="comment-content">
          <p>Additional data point: repeatedly inverting a ~10x10 matrix is exactly what I need performance on - for running an extended Kalman Filter. : )</p>
        </div>
      </div>
      <div class="comment comment-1313014789827047698">
        <div class="comment-header">
          <a name="comment-1313014789827047698"></a>
            <span class="author">Olivier Grisel</span> wrote on <span class="date">2015-02-24 21:18</span>:
        </div>
        <div class="comment-content">
          <p>&gt; Additional data point: repeatedly inverting a ~10x10 matrix is exactly what I need performance on - for running an extended Kalman Filter. : ) <br><br>Fair enough: so there actually exists a use case for that benchmark. Optimizing the bindings overhead might thus be worthy in the end.</p>
        </div>
      </div>
      <div class="comment comment-5085174953190852789">
        <div class="comment-header">
          <a name="comment-5085174953190852789"></a>
            <span class="author">Yaacov</span> wrote on <span class="date">2015-03-01 22:07</span>:
        </div>
        <div class="comment-content">
          <p>I love hearing about the progress and wish I could test on my benchmarks. Any chance of windows support?</p>
        </div>
      </div>
      <div class="comment comment-5764930300690008052">
        <div class="comment-header">
          <a name="comment-5764930300690008052"></a>
            <span class="author">mattip</span> wrote on <span class="date">2015-03-02 11:01</span>:
        </div>
        <div class="comment-content">
          <p>Yaacov what is missing for you to try it?<br>Here is the way I verify that the code works on windows 7 64 bit and windows 8.1:<br><br>download and install compiler<br>https://www.microsoft.com/en-us/download/details.aspx?id=44266<br><br>download pypy and open the zip<br>https://bitbucket.org/pypy/pypy/downloads/pypy-2.5.0-win32.zip<br><br>install pip into pypy<br>https://bootstrap.pypa.io/get-pip.py<br><br>install numpy into pypy<br>pip install git+https://bitbucket.org/pypy/numpy.git</p>
        </div>
      </div>
      <div class="comment comment-931046829961589278">
        <div class="comment-header">
          <a name="comment-931046829961589278"></a>
            <span class="author">Yaacov</span> wrote on <span class="date">2015-03-06 04:14</span>:
        </div>
        <div class="comment-content">
          <p>I get a tracback ending in<br><br>\appdata\local\temp\pip-wdyqtr-build\numpy\distutils\mi<br>sc_util.py", line 872, in _get_configuration_from_setup_py<br><br>        config = setup_module.configuration(*args)<br><br>      File "numpy\linalg\setup.py", line 85, in configuration<br><br>        library_dirs = [sys.real_prefix + '/include',<br><br>    AttributeError: 'module' object has no attribute 'real_prefix'<br><br>in a warning "using unoptimized lapack"</p>
        </div>
      </div>
      <div class="comment comment-2258526201919302343">
        <div class="comment-header">
          <a name="comment-2258526201919302343"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2015-03-06 19:24</span>:
        </div>
        <div class="comment-content">
          <p>I still get non-existing conjugate method error when using, e.g., linalg.pinv. Any plan on getting this working?</p>
        </div>
      </div>
      <div class="comment comment-6622089329894245029">
        <div class="comment-header">
          <a name="comment-6622089329894245029"></a>
            <span class="author">mattip</span> wrote on <span class="date">2015-03-07 22:01</span>:
        </div>
        <div class="comment-content">
          <p>fixed, try a nightly (from tomorrow)</p>
        </div>
      </div>
      <div class="comment comment-7290341467128346272">
        <div class="comment-header">
          <a name="comment-7290341467128346272"></a>
            <span class="author">Derek Z</span> wrote on <span class="date">2015-03-13 14:56</span>:
        </div>
        <div class="comment-content">
          <p>I got an error message:<br><br>OSError: Cannot load library /usr/local/Cellar/pypy/2.5.0/libexec/site-packages/numpy/linalg/libumath_linalg_cffi.so: dlopen(/usr/local/Cellar/pypy/2.5.0/libexec/site-packages/numpy/linalg/libumath_linalg_cffi.so, 2): image not found<br><br>Is there anything I am not doing right for the installation?  I have pypy 2.5, and Mac OS 10.10.</p>
        </div>
      </div>
      <div class="comment comment-4873465271754498998">
        <div class="comment-header">
          <a name="comment-4873465271754498998"></a>
            <span class="author">mattip</span> wrote on <span class="date">2015-03-13 15:18</span>:
        </div>
        <div class="comment-content">
          <p>Are you installing via pip, if so we have had reports of older versions of pip failing. You should have pip 6.0.8 or later. See https://bitbucket.org/pypy/numpy/issue/21</p>
        </div>
      </div>
      <div class="comment comment-8102406893667165323">
        <div class="comment-header">
          <a name="comment-8102406893667165323"></a>
            <span class="author">melbic</span> wrote on <span class="date">2015-03-19 11:13</span>:
        </div>
        <div class="comment-content">
          <p>Same problem here. (OSX 10.10) I've got the newest pip (6.0.8) and setuptools (14.0.3) version installed.</p>
        </div>
      </div>
      <div class="comment comment-4606142989761201752">
        <div class="comment-header">
          <a name="comment-4606142989761201752"></a>
            <span class="author">melbic</span> wrote on <span class="date">2015-03-19 11:13</span>:
        </div>
        <div class="comment-content">
          <p>Same problem here. (OSX 10.10) I've got the newest pip (6.0.8) and setuptools (14.0.3) version installed.</p>
        </div>
      </div>
      <div class="comment comment-7593605573280519542">
        <div class="comment-header">
          <a name="comment-7593605573280519542"></a>
            <span class="author">mattip</span> wrote on <span class="date">2015-03-19 17:01</span>:
        </div>
        <div class="comment-content">
          <p>I can't reproduce this as I do not have a MacOS machine. The place to follow this up is on our issue tracker, https://bitbucket.org/pypy/numpy/issue/21<br><br>It would be most helpful to attach a full log from "pip install" and 'pypy -c "import numpy"' to that issue</p>
        </div>
      </div>
      <div class="comment comment-7944098347287035155">
        <div class="comment-header">
          <a name="comment-7944098347287035155"></a>
            <span class="author">Nimrod</span> wrote on <span class="date">2015-03-29 15:34</span>:
        </div>
        <div class="comment-content">
          <p>One way pypy might be able to outperform numpy is by eliminating temporaries. <br><br>Just converting the BLAS functions to chain operations efficiently and sometimes update in-place rather than allocating and de-allocating arrays should help a lot.</p>
        </div>
      </div>
      <div class="comment comment-8801437651437937766">
        <div class="comment-header">
          <a name="comment-8801437651437937766"></a>
            <span class="author">Unknown</span> wrote on <span class="date">2015-04-05 23:07</span>:
        </div>
        <div class="comment-content">
          <p>This is great! But I can't use this for almost any of my code before np.einsum is supported :/ IMO, it is a super useful function for almost anything. Any plans for supporting it?</p>
        </div>
      </div>
      <div class="comment comment-4450709297250405468">
        <div class="comment-header">
          <a name="comment-4450709297250405468"></a>
            <span class="author">mattip</span> wrote on <span class="date">2015-04-07 09:01</span>:
        </div>
        <div class="comment-content">
          <p>Koos Zevenhoven - we have plans to implement all of numpy. With that, it looks like einsum will take quite a bit of work</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/02/numpypy-status-january-2015-5092986229783279944.html" class="u-url">NumPyPy status - January 2015</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/romain-guillebert.html">Romain Guillebert</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/02/numpypy-status-january-2015-5092986229783279944.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-02-11T14:46:00Z" itemprop="datePublished" title="2015-02-11 14:46">2015-02-11 14:46</time></a>
            </p>
                <p class="commentline">4 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <p>Hi Everyone<br><br>
Here is what has been done in January thanks to the <a href="https://pypy.org/numpydonate.html" target="_blank">funding of NumPyPy</a>, I would like to thank all the donors and tell you that you can still donate :<br></p>
<ul>
<li>I have focused on implementing the object dtype this month, it is now possible to store objects inside ndarrays using the object dtype</li>
<li>It is also possible to add an object ndarray to any other ndarray (implementing other operators is trivial)</li>
</ul>
<div>
The next things I plan on working on next are :</div>
<div>
<ul>
<li>Implementing the missing operations for object arrays</li>
<li>Implementing garbage collection support for object arrays (currently, storing an object inside an ndarray doesn't keep the object alive)</li>
<li>Packaging NumPyPy on PyPI</li>
</ul>
<div>
Cheers</div>
</div>
<div>
Romain</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-7046468104383062411">
        <div class="comment-header">
          <a name="comment-7046468104383062411"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-02-12 02:15</span>:
        </div>
        <div class="comment-content">
          <p>Thanks for the post! This sounds pretty cool. <br><br>The previous post suggested that there would be an update in regards to linalg. Does this mean linalg is working? Is having a working linalg what stands in the way of a working matplotlib? Thanks for answering what might be a naive question!</p>
        </div>
      </div>
      <div class="comment comment-1386753939188982881">
        <div class="comment-header">
          <a name="comment-1386753939188982881"></a>
            <span class="author">mattip</span> wrote on <span class="date">2015-02-12 22:27</span>:
        </div>
        <div class="comment-content">
          <p>Linalg is basically usable with the usual caveats: use PyPy 2.5.0 or later, use pypy/numpy from the bitbucket repo, you can even use matplotlib from my fork at https://github.com/mattip/matplotlib but there is no gui backend available yet, so you can only save the plots to files. Watch this space for the promised blog post, hopefully next week.</p>
        </div>
      </div>
      <div class="comment comment-7867406920203090063">
        <div class="comment-header">
          <a name="comment-7867406920203090063"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-02-13 10:34</span>:
        </div>
        <div class="comment-content">
          <p>Great to hear there is some progress on numpy!<br><br>About matplotlib @mattip. Maybe a GSoC project for the GUI?</p>
        </div>
      </div>
      <div class="comment comment-1029169893055553131">
        <div class="comment-header">
          <a name="comment-1029169893055553131"></a>
            <span class="author">Jami</span> wrote on <span class="date">2015-03-06 20:01</span>:
        </div>
        <div class="comment-content">
          <p>Regarding matplotlib, I whipped up a quick hack that can do at least very simple matplotlib stuff. Based on running a "slave" CPython using RpyC, as I recall was already done in 2011 or so demos.<br><br>Simple stuff can run unmodified, although can be of course slow if there's a lot or frequent data passing from PyPy to CPython.<br><br>Could be probably quite easily done in other direction to, ie running PyPy from CPython.<br><br>https://github.com/jampekka/cpyproxy</p>
        </div>
      </div>
         </div>

    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="../posts/2015/02/pypy-250-released-247160062953533060.html" class="u-url">PyPy 2.5.0 released</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                <a href="../authors/mattip.html">mattip</a>
            </span></p>
            <p class="dateline">
            <a href="../posts/2015/02/pypy-250-released-247160062953533060.html" rel="bookmark">
            <time class="published dt-published" datetime="2015-02-03T21:25:00Z" itemprop="datePublished" title="2015-02-03 21:25">2015-02-03 21:25</time></a>
            </p>
                <p class="commentline">3 comments</p>

        </div>
    </header><div class="p-summary entry-summary">
    <div dir="ltr" style="text-align: left;">
<div class="document">
<div class="section" id="pypy-2-5-0-pincushion-protea">
<h2 style="text-align: left;">
<span style="font-family: inherit;">PyPy 2.5.0 - Pincushion Protea</span>
</h2>
We’re pleased to announce PyPy 2.5, which contains significant performance
enhancements and bug fixes.<br>
You can download the PyPy 2.5.0 release here:<br><blockquote>
<div>
<a class="reference external" href="https://pypy.org/download.html">https://pypy.org/download.html</a>
</div>
</blockquote>
We would like to thank our donors for the continued support of the PyPy
project, and for those who donate to our three sub-projects, as well as our
volunteers and contributors (10 new commiters joined PyPy since the last
release).
We’ve shown quite a bit of progress, but we’re slowly running out of funds.
Please consider donating more, or even better convince your employer to donate,
so we can finish those projects! The three sub-projects are:<br><ul>
<li>
<dl class="first docutils">
<dt>
<a class="reference external" href="https://pypy.org/py3donate.html">Py3k</a> (supporting Python 3.x): We have released a Python 3.2.5 compatible version</dt>
<dd>
<div class="first last">
we call PyPy3 2.4.0, and are working toward a Python 3.3 compatible version</div>
</dd>
</dl>
</li>
<li>
<div class="first">
<a class="reference external" href="https://pypy.org/tmdonate2.html">STM</a> (software transactional memory): We have released a first working version,
and continue to try out new promising paths of achieving a fast multithreaded Python</div>
</li>
<li>
<div class="first">
<a class="reference external" href="https://pypy.org/numpydonate.html">NumPy</a> which requires installation of our fork of upstream numpy,
available <a class="reference external" href="https://www.bitbucket.org/pypy/numpy">on bitbucket</a>
</div>
</li>
</ul>
<div class="section" id="what-is-pypy">
<h2>
What is PyPy?</h2>
PyPy is a very compliant Python interpreter, almost a drop-in replacement for
CPython 2.7. It’s fast (<a class="reference external" href="https://speed.pypy.org/">pypy and cpython 2.7.x</a> performance comparison)
due to its integrated tracing JIT compiler.<br>
This release supports <b>x86</b> machines on most common operating systems
(Linux 32/64, Mac OS X 64, Windows, and OpenBSD),
as well as newer <b>ARM</b> hardware (ARMv6 or ARMv7, with VFPv3) running Linux.<br>
While we support 32 bit python on Windows, work on the native Windows 64
bit python is still stalling, we would welcome a volunteer
to <a class="reference external" href="https://doc.pypy.org/en/latest/windows.html#what-is-missing-for-a-full-64-bit-translation">handle that</a>.</div>
<div class="section" id="highlights">
<h2>
Highlights</h2>
<ul class="simple">
<li>The past months have seen pypy mature and grow, as rpython becomes the goto
solution for writing fast dynamic language interpreters. Our separation of
rpython and the python interpreter PyPy is now much clearer in the
<a class="reference external" href="https://doc.pypy.org/">PyPy documentation</a>  and we now have separate <a class="reference external" href="https://rpython.readthedocs.org/">RPython documentation</a>.</li>
<li>We have improved warmup time as well as jitted code performance: more than 10%
compared to pypy-2.4.0.
We no longer zero-out memory allocated in the gc nursery by default, work that
was started during a GSoC.</li>
<li>Passing objects between C and PyPy has been improved. We are now able to pass
raw pointers to C (without copying) using <b>pinning</b>. This improves I/O;
benchmarks that use networking intensively improved by about 50%. File()
operations still need some refactoring but are already showing a 20%
improvement on our benchmarks. Let us know if you see similar improvements.</li>
<li>Our integrated numpy support gained much of the GenericUfunc api in order to
support the lapack/blas linalg module of numpy. This dovetails with work in the
pypy/numpy repository to support linalg both through the (slower) cpyext capi
interface and also via (the faster) pure python cffi interface, using an
extended frompyfunc() api. We will soon post a seperate blog post specifically
about linalg and PyPy.</li>
<li>Dictionaries are now ordered by default, see the <a class="reference external" href="../posts/2015/01/faster-more-memory-efficient-and-more-4096950404745375390.html">blog post</a>
</li>
<li>Our nightly translations use –shared by default, including on OS/X and linux</li>
<li>We now more carefully handle errno (and GetLastError, WSAGetLastError) tying
the handlers as close as possible to the external function call, in non-jitted
as well as jitted code.</li>
<li>Issues reported with our previous release were <a class="reference external" href="https://doc.pypy.org/en/latest/whatsnew-2.5.0.html">resolved</a> after reports from users on
our issue tracker at <a class="reference external" href="https://foss.heptapod.net/pypy/pypy/-/issues">https://foss.heptapod.net/pypy/pypy/-/issues</a> or on IRC at
#pypy.</li>
</ul>
We have further improvements on the way: rpython file handling,
finishing numpy linalg compatibility, numpy object dtypes, a better profiler,
as well as support for Python stdlib 2.7.9.<br>
Please try it out and let us know what you think. We especially welcome
success stories, we know you are using PyPy, please tell us about it!<br>
Cheers<br>
The PyPy Team</div>
</div>
</div>
</div>
    </div>
    </article><div class="comment-level comment-level-1">
      <div class="comment comment-7417196832509096711">
        <div class="comment-header">
          <a name="comment-7417196832509096711"></a>
            <span class="author">Anonymous</span> wrote on <span class="date">2015-02-04 07:48</span>:
        </div>
        <div class="comment-content">
          <p>Many-many thanks for your work!</p>
        </div>
      </div>
      <div class="comment comment-8641233999214483467">
        <div class="comment-header">
          <a name="comment-8641233999214483467"></a>
            <span class="author">rndblnch</span> wrote on <span class="date">2015-02-04 15:59</span>:
        </div>
        <div class="comment-content">
          <p>any release schedule for pypy3-2.5?<br>how can we help with pypy3?</p>
        </div>
      </div>
      <div class="comment comment-1633753359600116542">
        <div class="comment-header">
          <a name="comment-1633753359600116542"></a>
            <span class="author">Jami</span> wrote on <span class="date">2015-02-09 11:48</span>:
        </div>
        <div class="comment-content">
          <p>Sorry to nag, but when are the news about Scipy/Matplotlib compatibility plans coming? I've been checking daily since the November 28th teaser!</p>
        </div>
      </div>
         </div>

</div>
</div>
<div class="sidebar">
<div>
  <h2>
    The PyPy blogposts
  </h2>
  <div>
    Create a guest post via a PR to the <a href="https://github.com/pypy/pypy.org">source repo</a>
  </div>
</div>
    <div id="global-recent-posts">
    <h2>
      Recent Posts
    </h2>
    <ul class="post-list">
      <li>
        <a href="/posts/2025/07/pypy-v7320-release.html" class="listtitle">PyPy v7.3.20 release</a>
      </li>
      <li>
        <a href="/posts/2025/06/rpython-gc-allocation-speed.html" class="listtitle">How fast can the RPython GC allocate?</a>
      </li>
      <li>
        <a href="/posts/2025/04/prospero-in-rpython.html" class="listtitle">Doing the Prospero-Challenge in RPython</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7319-release.html" class="listtitle">PyPy v7.3.19 release</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-gc-sampling.html" class="listtitle">Low Overhead Allocation Sampling with VMProf in PyPy's GC</a>
      </li>
      <li>
        <a href="/posts/2025/02/pypy-v7318-release.html" class="listtitle">PyPy v7.3.18 release</a>
      </li>
      <li>
        <a href="/posts/2025/01/musings-tracing.html" class="listtitle">Musings on Tracing in PyPy</a>
      </li>
      <li>
        <a href="/posts/2025/01/towards-pypy311-an-update.html" class="listtitle">Towards PyPy3.11 - an update</a>
      </li>
      <li>
        <a href="/posts/2024/11/guest-post-final-encoding-in-rpython.html" class="listtitle">Guest Post: Final Encoding in RPython Interpreters</a>
      </li>
      <li>
        <a href="/posts/2024/10/jit-peephole-dsl.html" class="listtitle">A DSL for Peephole Transformation Rules of Integer Operations in the PyPy JIT</a>
      </li>
    </ul>
  </div>

          <div id="global-archive-list">
          <h2>
            Archives
          </h2>
          <ul class="archive-level archive-level-1">
            <li><a class="reference" href="/2007/">2007</a> (19)
            </li>
            <li><a class="reference" href="/2008/">2008</a> (62)
            </li>
            <li><a class="reference" href="/2009/">2009</a> (38)
            </li>
            <li><a class="reference" href="/2010/">2010</a> (44)
            </li>
            <li><a class="reference" href="/2011/">2011</a> (43)
            </li>
            <li><a class="reference" href="/2012/">2012</a> (44)
            </li>
            <li><a class="reference" href="/2013/">2013</a> (46)
            </li>
            <li><a class="reference" href="/2014/">2014</a> (22)
            </li>
            <li><a class="reference" href="/2015/">2015</a> (20)
            </li>
            <li><a class="reference" href="/2016/">2016</a> (20)
            </li>
            <li><a class="reference" href="/2017/">2017</a> (13)
            </li>
            <li><a class="reference" href="/2018/">2018</a> (12)
            </li>
            <li><a class="reference" href="/2019/">2019</a> (12)
            </li>
            <li><a class="reference" href="/2020/">2020</a> (9)
            </li>
            <li><a class="reference" href="/2021/">2021</a> (10)
            </li>
            <li><a class="reference" href="/2022/">2022</a> (13)
            </li>
            <li><a class="reference" href="/2023/">2023</a> (6)
            </li>
            <li><a class="reference" href="/2024/">2024</a> (13)
            </li>
            <li><a class="reference" href="/2025/">2025</a> (8)
            </li>
          </ul>
        </div>


          <div id="global-tag-list">
          <h2>
            Tags
          </h2>
          <ul>
            <li><a class="reference" href="/categories/arm.html">arm</a> (2)</li>
            <li><a class="reference" href="/categories/benchmarking.html">benchmarking</a> (1)</li>
            <li><a class="reference" href="/categories/casestudy.html">casestudy</a> (3)</li>
            <li><a class="reference" href="/categories/cli.html">cli</a> (1)</li>
            <li><a class="reference" href="/categories/compiler.html">compiler</a> (1)</li>
            <li><a class="reference" href="/categories/conda-forge.html">conda-forge</a> (1)</li>
            <li><a class="reference" href="/categories/cpyext.html">cpyext</a> (4)</li>
            <li><a class="reference" href="/categories/cpython.html">CPython</a> (3)</li>
            <li><a class="reference" href="/categories/ep2008.html">ep2008</a> (1)</li>
            <li><a class="reference" href="/categories/extension-modules.html">extension modules</a> (3)</li>
            <li><a class="reference" href="/categories/gc.html">gc</a> (3)</li>
            <li><a class="reference" href="/categories/guestpost.html">guestpost</a> (3)</li>
            <li><a class="reference" href="/categories/graalpython.html">GraalPython</a> (1)</li>
            <li><a class="reference" href="/categories/hpy.html">hpy</a> (1)</li>
            <li><a class="reference" href="/categories/heptapod.html">Heptapod</a> (1)</li>
            <li><a class="reference" href="/categories/jit.html">jit</a> (23)</li>
            <li><a class="reference" href="/categories/jython.html">jython</a> (1)</li>
            <li><a class="reference" href="/categories/kcachegrind.html">kcachegrind</a> (1)</li>
            <li><a class="reference" href="/categories/meta.html">meta</a> (1)</li>
            <li><a class="reference" href="/categories/numpy.html">numpy</a> (24)</li>
            <li><a class="reference" href="/categories/parser.html">parser</a> (1)</li>
            <li><a class="reference" href="/categories/performance.html">performance</a> (2)</li>
            <li><a class="reference" href="/categories/profiling.html">profiling</a> (7)</li>
            <li><a class="reference" href="/categories/pypy.html">pypy</a> (6)</li>
            <li><a class="reference" href="/categories/pypy3.html">pypy3</a> (16)</li>
            <li><a class="reference" href="/categories/pyqt4.html">PyQt4</a> (1)</li>
            <li><a class="reference" href="/categories/release.html">release</a> (66)</li>
            <li><a class="reference" href="/categories/releasecffi.html">releasecffi</a> (3)</li>
            <li><a class="reference" href="/categories/releaserevdb.html">releaserevdb</a> (1)</li>
            <li><a class="reference" href="/categories/releasestm.html">releasestm</a> (1)</li>
            <li><a class="reference" href="/categories/revdb.html">revdb</a> (1)</li>
            <li><a class="reference" href="/categories/roadmap.html">roadmap</a> (2)</li>
            <li><a class="reference" href="/categories/rpython.html">rpython</a> (1)</li>
            <li><a class="reference" href="/categories/rpyc.html">RPyC</a> (1)</li>
            <li><a class="reference" href="/categories/speed.html">speed</a> (6)</li>
            <li><a class="reference" href="/categories/sponsors.html">sponsors</a> (7)</li>
            <li><a class="reference" href="/categories/sprint.html">sprint</a> (3)</li>
            <li><a class="reference" href="/categories/sprints.html">sprints</a> (1)</li>
            <li><a class="reference" href="/categories/stm.html">stm</a> (14)</li>
            <li><a class="reference" href="/categories/sun.html">sun</a> (1)</li>
            <li><a class="reference" href="/categories/smalltalk.html">Smalltalk</a> (1)</li>
            <li><a class="reference" href="/categories/squeak.html">Squeak</a> (1)</li>
            <li><a class="reference" href="/categories/testing.html">testing</a> (1)</li>
            <li><a class="reference" href="/categories/toy-optimizer.html">toy-optimizer</a> (5)</li>
            <li><a class="reference" href="/categories/unicode.html">unicode</a> (1)</li>
            <li><a class="reference" href="/categories/valgrind.html">valgrind</a> (1)</li>
            <li><a class="reference" href="/categories/vmprof.html">vmprof</a> (3)</li>
            <li><a class="reference" href="/categories/z3.html">z3</a> (5)</li>
          </ul>
        </div></div>
</main>
</div>
<div style="clear: both; width: 75%; margin: 1em auto;">
        <nav class="postindexpager"><ul class="pager">
<li class="previous">
                <a href="index-34.html" rel="prev">Newer posts</a>
            </li>
            <li class="next">
                <a href="index-32.html" rel="next">Older posts</a>
            </li>
        </ul></nav>
</div>
         
                 <footer id="footer"><p>
</p>
<div class="myfooter">
  <div class="logotext">
    © 2025 <a href="mailto:pypy-dev@pypy.org">The PyPy Team</a>
     
    Built with <a href="https://getnikola.com" rel="nofollow">Nikola</a>
     
    Last built 2025-07-07T11:01
  </div>
  <div style="margin-left: auto">
  <a href="../rss.xml">RSS feed</a>
</div>

            
        

    </div>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js" crossorigin="anonymous"></script><script src="../assets/js/styles.js"></script></footer>
</body>
</html>
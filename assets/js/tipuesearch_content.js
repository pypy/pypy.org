var tipuesearch = {
  "pages": [
    {
      "title": "How to help?",
      "text": "How to help PyPy?\nHere are some ideas to help PyPy move forward:\n\nuse pypy for your projects and provide detailed feedback\ntalk to us about how to support Python 3.x\nwrite blog posts or tweets about your experiences\nhelp porting to new platforms\ncontact us and get involved\ndonate some money to enable others to help\ntake on our consultants and make PyPy work better for your",
      "tags": "",
      "url": "https://www.pypy.org/howtohelp.html"
    },
    {
      "title": "Load and store forwarding in the Toy Optimizer",
      "text": "This is a cross-post from Max Bernstein from his blog where he writes\nabout programming languages, compilers, optimizations, virtual machines.\n\nA long, long time ago (two years!) CF Bolz-Tereick and I made a video\nabout load/store forwarding and an accompanying GitHub Gist\nabout load/store forwarding (also called load elimination) in the Toy Optimizer. I\nsaid I would write a blog post about it, but never found the time\u2014it got lost\namid a sea of large life changes.\nIt's a neat idea: do an abstract interpretation over the trace, modeling the\nheap at compile-time, eliminating redundant loads and stores. That means it's\npossible to optimize traces like this:\nv0 = ...\nv1 = load(v0, 5)\nv2 = store(v0, 6, 123)\nv3 = load(v0, 6)\nv4 = load(v0, 5)\nv5 = do_something(v1, v3, v4)\n\n\ninto traces like this:\nv0 = ...\nv1 = load(v0, 5)\nv2 = store(v0, 6, 123)\nv5 = do_something(v1, 123, v1)\n\n\n(where load(v0, 5) is equivalent to *(v0+5) in C syntax and store(v0, 6,\n123) is equvialent to *(v0+6)=123 in C syntax)\nThis indicates that we were able to eliminate two redundant loads by keeping\naround information about previous loads and stores. Let's get to work making\nthis possible.\nThe usual infrastructure\nWe'll start off with the usual infrastructure from the Toy\nOptimizer series: a very stringly-typed representation of a\ntrace-based SSA IR and a union-find rewrite mechanism.\nThis means we can start writing some new optimization pass and our first test:\ndef optimize_load_store(bb: Block):\n    opt_bb = Block()\n    # TODO: copy an optimized version of bb into opt_bb\n    return opt_bb\n\ndef test_two_loads():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.load(var0, 0)\n    var2 = bb.load(var0, 0)\n    bb.escape(var1)\n    bb.escape(var2)\n    opt_bb = optimize_load_store(bb)\n    assert bb_to_str(opt_bb) == \"\"\"\\\nvar0 = getarg(0)\nvar1 = load(var0, 0)\nvar2 = escape(var1)\nvar3 = escape(var1)\"\"\"\n\n\nThis test is asserting that we can remove duplicate loads. Why load twice if we\ncan cache the result? Let's make that happen.\nCaching loads\nTo do this, we'll model the the heap at compile-time. When I say \"model\", I\nmean that we will have an imprecise but correct abstract representation of the\nheap: we don't (and can't) have knowledge of every value, but we can know for\nsure that some addresses have certain values.\nFor example, if we have observed a load from object O at offset 8 v0 =\nload(O, 8), we know that the SSA value v0 is at heap[(O, 8)]. That sounds\ntautological, but it's not. Future loads can make use of this information.\ndef get_num(op: Operation, index: int=1):\n    assert isinstance(op.arg(index), Constant)\n    return op.arg(index).value\n\ndef optimize_load_store(bb: Block):\n    opt_bb = Block()\n    # Stores things we know about the heap at... compile-time.\n    # Key: an object and an offset pair acting as a heap address\n    # Value: a previous SSA value we know exists at that address\n    compile_time_heap: Dict[Tuple[Value, int], Value] = {}\n    for op in bb:\n        if op.name == \"load\":\n            obj = op.arg(0)\n            offset = get_num(op, 1)\n            load_info = (obj, offset)\n            previous = compile_time_heap.get(load_info)\n            if previous is not None:\n                op.make_equal_to(previous)\n                continue\n            compile_time_heap[load_info] = op\n        opt_bb.append(op)\n    return opt_bb\n\n\nThis pass records information about loads and uses the result of a previous\ncached load operation if available. We treat the pair of (SSA value, offset) as\nan address into our abstract heap.\nThat's great! If you run our simple test, it should now pass. But what happens\nif we store into that address before the second load? Oops...\ndef test_store_to_same_object_offset_invalidates_load():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.load(var0, 0)\n    var2 = bb.store(var0, 0, 5)\n    var3 = bb.load(var0, 0)\n    bb.escape(var1)\n    bb.escape(var3)\n    opt_bb = optimize_load_store(bb)\n    assert bb_to_str(opt_bb) == \"\"\"\\\nvar0 = getarg(0)\nvar1 = load(var0, 0)\nvar2 = store(var0, 0, 5)\nvar3 = load(var0, 0)\nvar4 = escape(var1)\nvar5 = escape(var3)\"\"\"\n\n\nThis test fails because we are incorrectly keeping around var1 in our\nabstract heap. We need to get rid of it and not replace var3 with var1.\nInvalidating cached loads\nSo it turns out we have to also model stores in order to cache loads correctly.\nOne valid, albeit aggressive, way to do that is to throw away all the\ninformation we know at each store operation:\ndef optimize_load_store(bb: Block):\n    opt_bb = Block()\n    compile_time_heap: Dict[Tuple[Value, int], Value] = {}\n    for op in bb:\n        if op.name == \"store\":\n            compile_time_heap.clear()\n        elif op.name == \"load\":\n            # ...\n        opt_bb.append(op)\n    return opt_bb\n\n\nThat makes our test pass\u2014yay!\u2014but at great cost. It means any store\noperation mucks up redundant loads. In our world where we frequently read from\nand write to objects, this is what we call a huge bummer.\nFor example, a store to offset 4 on some object should never interfere with a\nload from a different offset on the same object1. We should be able to\nkeep our load from offset 0 cached here:\ndef test_store_to_same_object_different_offset_does_not_invalidate_load():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.load(var0, 0)\n    var2 = bb.store(var0, 4, 5)\n    var3 = bb.load(var0, 0)\n    bb.escape(var1)\n    bb.escape(var3)\n    opt_bb = optimize_load_store(bb)\n    assert bb_to_str(opt_bb) == \"\"\"\\\nvar0 = getarg(0)\nvar1 = load(var0, 0)\nvar2 = store(var0, 4, 5)\nvar3 = escape(var1)\nvar4 = escape(var1)\"\"\"\n\n\nWe could try instead checking if our specific (object, offset) pair is in the\nheap and only removing cached information about that offset and that object.\nThat would definitely help!\ndef optimize_load_store(bb: Block):\n    opt_bb = Block()\n    compile_time_heap: Dict[Tuple[Value, int], Value] = {}\n    for op in bb:\n        if op.name == \"store\":\n            load_info = (op.arg(0), get_num(op, 1))\n            if load_info in compile_time_heap:\n                del compile_time_heap[load_info]\n        elif op.name == \"load\":\n            # ...\n        opt_bb.append(op)\n    return opt_bb\n\n\nIt makes our test pass, too, which is great news.\nUnfortunately, this runs into problems due to aliasing: it's entirely possible\nthat our compile-time heap could contain a pair (v0, 0) and a pair (v1, 0) where v0\nand v1 are the same object (but not known to the optimizer). Then we might\nrun into a situation where we incorrectly cache loads because the optimizer\ndoesn't know our abstract addresses (v0, 0) and (v1, 0) are actually the\nsame pointer at run-time.\nThis means that we are breaking abstract interpretation rules: our abstract\ninterpreter has to correctly model all possible outcomes at run-time. This\nmeans to me that we should instead pick some tactic in-between clearing all\ninformation (correct but over-eager) and clearing only exact matches of\nobject+offset (incorrect).\nThe term that will help us here is called an alias class. It is a name for a\nway to efficiently partition objects in your abstract heap into completely\ndisjoint sets. Writes to any object in one class never affect objects in\nanother class.\nOur very scrappy alias classes will be just based on the offset: each offset is\na different alias class. If we write to any object at offset K, we have to\ninvalidate all of our compile-time offset K knowledge\u2014even if it's for\nanother object. This is a nice middle ground, and it's possible because our\n(made up) object system guarantees that distinct objects do not overlap, and\nalso that we are not writing out-of-bounds.2\nSo let's remove all of the entries from compile_time_heap where the offset\nmatches the offset in the current store:\ndef optimize_load_store(bb: Block):\n    opt_bb = Block()\n    compile_time_heap: Dict[Tuple[Value, int], Value] = {}\n    for op in bb:\n        if op.name == \"store\":\n            offset = get_num(op, 1)\n            compile_time_heap = {\n                load_info: value\n                for load_info, value in compile_time_heap.items()\n                if load_info[1] != offset\n            }\n        elif op.name == \"load\":\n            # ...\n        opt_bb.append(op)\n    return opt_bb\n\n\nGreat! Now our test passes.\nThis concludes the load optimization section of the post. We have modeled\nenough of loads and stores that we can eliminate redundant loads. Very cool.\nBut we can go further.\nCaching stores\nStores don't just invalidate information. They also give us new information!\nAny time we see an operation of the form v1 = store(v0, 8, 5) we also learn\nthat load(v0, 8) == 5! Until it gets invalidated, anyway.\nFor example, in this test, we can eliminate the load from var0 at offset 0:\ndef test_load_after_store_removed():\n    bb = Block()\n    var0 = bb.getarg(0)\n    bb.store(var0, 0, 5)\n    var1 = bb.load(var0, 0)\n    var2 = bb.load(var0, 1)\n    bb.escape(var1)\n    bb.escape(var2)\n    opt_bb = optimize_load_store(bb)\n    assert bb_to_str(opt_bb) == \"\"\"\\\nvar0 = getarg(0)\nvar1 = store(var0, 0, 5)\nvar2 = load(var0, 1)\nvar3 = escape(5)\nvar4 = escape(var2)\"\"\"\n\n\nMaking that work is thankfully not very hard; we need only add that new\ninformation to the compile-time heap after removing all the\npotentially-aliased info:\ndef optimize_load_store(bb: Block):\n    opt_bb = Block()\n    compile_time_heap: Dict[Tuple[Value, int], Value] = {}\n    for op in bb:\n        if op.name == \"store\":\n            offset = get_num(op, 1)\n            compile_time_heap = # ... as before ...\n            obj = op.arg(0)\n            new_value = op.arg(2)\n            compile_time_heap[(obj, offset)] = new_value  # NEW!\n        elif op.name == \"load\":\n            # ...\n        opt_bb.append(op)\n    return opt_bb\n\n\nThis makes the test pass. It makes another test fail, but only\nbecause\u2014oops\u2014we now know more. You can delete the old test because the new\ntest supersedes it.\nNow, note that we are not removing the store. This is because we have nothing\nin our optimizer that keeps track of what might have observed the side-effects\nof the store. What if the object got escaped? Or someone did a load later on?\nWe would only be able to remove the store (continue) if we could guarantee it\nwas not observable.\nIn our current framework, this only happens in one case: someone is doing a\nstore of the exact same value that already exists in our compile-time heap.\nThat is, either the same constant, or the same SSA value. If we see this, then\nwe can completely skip the second store instruction.\nHere's a test case for that, where we have gained information from the load\ninstruction that we can then use to get rid of the store instruction:\ndef test_load_then_store():\n    bb = Block()\n    arg1 = bb.getarg(0)\n    var1 = bb.load(arg1, 0)\n    bb.store(arg1, 0, var1)\n    bb.escape(var1)\n    opt_bb = optimize_load_store(bb)\n    assert bb_to_str(opt_bb) == \"\"\"\\\nvar0 = getarg(0)\nvar1 = load(var0, 0)\nvar2 = escape(var1)\"\"\"\n\n\nLet's make it pass. To do that, first we'll make an equality function that\nworks for both constants and operations. Constants are equal if their values\nare equal, and operations are equal if they are the identical (by\naddress/pointer) operation.\ndef eq_value(left: Value|None, right: Value) -> bool:\n    if isinstance(left, Constant) and isinstance(right, Constant):\n        return left.value == right.value\n    return left is right\n\n\nThis is a partial equality: if two operations are not equal under eq_value,\nit doesn't mean that they are different, only that we don't know that they are\nthe same.\nThen, after that, we need only check if the current value in the compile-time\nheap is the same as the value being stored in. If it is, wonderful. No need to\nstore. continue and don't append the operation to opt_bb:\ndef optimize_load_store(bb: Block):\n    opt_bb = Block()\n    compile_time_heap: Dict[Tuple[Value, int], Value] = {}\n    for op in bb:\n        if op.name == \"store\":\n            obj = op.arg(0)\n            offset = get_num(op, 1)\n            store_info = (obj, offset)\n            current_value = compile_time_heap.get(store_info)\n            new_value = op.arg(2)\n            if eq_value(current_value, new_value):  # NEW!\n                continue\n            compile_time_heap = # ... as before ...\n            # ...\n        elif op.name == \"load\":\n            load_info = (op.arg(0), get_num(op, 1))\n            if load_info in compile_time_heap:\n                op.make_equal_to(compile_time_heap[load_info])\n                continue\n            compile_time_heap[load_info] = op\n        opt_bb.append(op)\n    return opt_bb\n\n\nThis makes our load-then-store pass and it also makes other tests pass too,\nlike eliminating a store after another store!\ndef test_store_after_store():\n    bb = Block()\n    arg1 = bb.getarg(0)\n    bb.store(arg1, 0, 5)\n    bb.store(arg1, 0, 5)\n    opt_bb = optimize_load_store(bb)\n    assert bb_to_str(opt_bb) == \"\"\"\\\nvar0 = getarg(0)\nvar1 = store(var0, 0, 5)\"\"\"\n\n\nUnfortunately, this only works if the values\u2014constants or SSA values\u2014are\nknown to be the same. If we store different values, we can't optimize. In the\nlive stream, we left this an exercise for the viewer:\n@pytest.mark.xfail\ndef test_exercise_for_the_reader():\n    bb = Block()\n    arg0 = bb.getarg(0)\n    var0 = bb.store(arg0, 0, 5)\n    var1 = bb.store(arg0, 0, 7)\n    var2 = bb.load(arg0, 0)\n    bb.escape(var2)\n    opt_bb = optimize_load_store(bb)\n    assert bb_to_str(opt_bb) == \"\"\"\\\nvar0 = getarg(0)\nvar1 = store(var0, 0, 7)\nvar2 = escape(7)\"\"\"\n\n\nWe would only be able to optimize this away if we had some notion of a store\nbeing dead. In this case, that is a store in which the value is never read\nbefore being overwritten.\nRemoving dead stores\nTODO, I suppose. I have not gotten this far yet. If I get around to it, I will\ncome back and update the post.\nIn the real world\nThis small optimization pass may seem silly or fiddly\u2014when would we ever see\nsomething like this in a real IR?\u2014but it's pretty useful. Here's the Ruby\ncode that got me thinking about it again some years later for ZJIT:\nclass C\n  def initialize\n    @a = 1\n    @b = 2\n    @c = 3\n  end\nend\n\n\nCRuby has a shape system and ZJIT makes use of it, so we end up optimizing this\ncode (if it's monomorphic) into a series of shape checks and stores. The HIR\nmight end up looking something like the mess below, where I've annotated the\nshape guards (can be thought of as loads) and stores with asterisks:\nfn initialize@tmp/init.rb:3:\n# ...\nbb2(v6:BasicObject):\n  v10:Fixnum[1] = Const Value(1)\n  v31:HeapBasicObject = GuardType v6, HeapBasicObject\n* v32:HeapBasicObject = GuardShape v31, 0x400000\n* StoreField v32, :@a@0x10, v10\n  WriteBarrier v32, v10\n  v35:CShape[0x40008e] = Const CShape(0x40008e)\n* StoreField v32, :_shape_id@0x4, v35\n  v16:Fixnum[2] = Const Value(2)\n  v37:HeapBasicObject = GuardType v6, HeapBasicObject\n* v38:HeapBasicObject = GuardShape v37, 0x40008e\n* StoreField v38, :@b@0x18, v16\n  WriteBarrier v38, v16\n  v41:CShape[0x40008f] = Const CShape(0x40008f)\n* StoreField v38, :_shape_id@0x4, v41\n  v22:Fixnum[3] = Const Value(3)\n  v43:HeapBasicObject = GuardType v6, HeapBasicObject\n* v44:HeapBasicObject = GuardShape v43, 0x40008f\n* StoreField v44, :@c@0x20, v22\n  WriteBarrier v44, v22\n  v47:CShape[0x400090] = Const CShape(0x400090)\n* StoreField v44, :_shape_id@0x4, v47\n  CheckInterrupts\n  Return v22\n\n\nIf we had store-load forwarding in ZJIT, we could get rid of the intermediate\nshape guards; they would know the shape from the previous StoreField\ninstruction. If we had dead store elimination, we could get rid of the\nintermediate shape writes; they are never read. (And the repeated type guards\nto check if it's a heap object still are just silly and need to get removed\neventually.)\nThis is on the roadmap and will make object initialization even faster than it\nis right now.\nWrapping up\nThanks for reading the text version of the video that CF and I made a while\nback. Now you know how to do load/store elimination on traces.\nI think this does not need too much extra work to get it going on full CFGs; a\nblock is pretty much the same as a trace, so you can do a block-local version\nwithout much fuss. If you want to go global, you need dominator information and\ngen-kill sets.\nMaybe I will touch on this in a future post...\nThank you\nThank you to CF, who walked me through this live on a stream two years ago!\nThis blog post wouldn't be possible without you.\n\n\n\n\nIn this toy optimizer example, we are assuming that all reads and writes\nare the same size and different offsets don't overlap at all. This is often\nthe case for managed runtimes, where object fields are pointer-sized and\nall reads/writes are pointed aligned.\u00a0\u21a9\n\n\nWe could do better. If we had type information, we could also use that\nto make alias classes. Writes to a List will never overlap with writes to a\nMap, for example. This requires your compiler to have strict aliasing\u2014if\nyou can freely cast between types, as in C, then this tactic goes out the\nwindow.\nThis is called Type-based alias analysis (PDF).\u00a0\u21a9",
      "tags": "toy-optimizer",
      "url": "https://www.pypy.org/posts/2025/12/toy-load-store.html"
    },
    {
      "title": "PyPy v7.3.20 release",
      "text": "PyPy v7.3.20: release of python 2.7, 3.11\nThe PyPy team is proud to release version 7.3.20 of PyPy after the previous\nrelease on Feb 26, 2025. The release fixes some subtle bugs in ctypes and\nOrderedDict and makes PyPy3.11 compatible with an upcoming release of\nCython.\nThe release includes two different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.11, which is an interpreter supporting the syntax and the features of\nPython 3.11, including the stdlib for CPython 3.11.13.\n\nThe interpreters are based on much the same codebase, thus the double\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases.\nWe recommend updating. You can find links to download the releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear\nabout it and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with\nmaking RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy. In any case, cibuildwheel supports building wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython\nIt's fast (PyPy and CPython performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64) and macos (macos_arm64).\n\nPyPy supports Windows 32-bit, Linux PPC64 big- and little-endian, Linux ARM\n32 bit, RISC-V RV64IMAFD Linux, and s390x Linux but does not release binaries.\nPlease reach out to us if you wish to sponsor binary releases for those\nplatforms. Downstream packagers provide binary builds for debian, Fedora,\nconda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.20 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2025/07/pypy-v7320-release.html"
    },
    {
      "title": "How fast can the RPython GC allocate?",
      "text": "While working on a paper about allocation profiling in\nVMProf I got curious\nabout how quickly the RPython GC can allocate an object. I wrote a small\nRPython benchmark program to get an idea of the order of magnitude.\nThe basic idea is to just allocate an instance in a tight loop:\nclass A(object):\n    pass\n\ndef run(loops):\n    # preliminary idea, see below\n    for i in range(loops):\n        a = A()\n        a.i = i\n\n\nThe RPython type inference will find out that instances of A have a single\ni field, which is an integer. In addition to that field, every RPython object\nneeds one word of GC meta-information. Therefore one instance of A needs 16\nbytes on a 64-bit architecture.\nHowever, measuring like this is not good enough, because the RPython static\noptimizer would remove the allocation since the object isn't used. But we can\nconfuse the escape analysis sufficiently by always keeping two instances alive\nat the same time:\nclass A(object):\n    pass\n\ndef run(loops):\n    a = prev = None\n    for i in range(loops):\n        prev = a\n        a = A()\n        a.i = i\n    print(prev, a) # print the instances at the end\n\n\n(I confirmed that the allocation isn't being removed by looking at the C code\nthat the RPython compiler generates from this.)\nThis is doing a little bit more work than needed, because of the a.i = i\ninstance attribute write. We can also (optionally) leave the field\nuninitialized.\ndef run(initialize_field, loops):\n    t1 = time.time()\n    if initialize_field:\n        a = prev = None\n        for i in range(loops):\n            prev = a\n            a = A()\n            a.i = i\n        print(prev, a) # make sure always two objects are alive\n    else:\n        a = prev = None\n        for i in range(loops):\n            prev = a\n            a = A()\n        print(prev, a)\n    t2 = time.time()\n    print(t2 - t1, 's')\n    object_size_in_words = 2 # GC header, one integer field\n    mem = loops * 8 * object_size_in_words / 1024.0 / 1024.0 / 1024.0\n    print(mem, 'GB')\n    print(mem / (t2 - t1), 'GB/s')\n\n\nThen we need to add some RPython scaffolding:\ndef main(argv):\n    loops = int(argv[1])\n    with_init = bool(int(argv[2]))\n    if with_init:\n        print(\"with initialization\")\n    else:\n        print(\"without initialization\")\n    run(with_init, loops)\n    return 0\n\ndef target(*args):\n    return main\n\n\nTo build a binary:\npypy rpython/bin/rpython targetallocatealot.py\n\n\nWhich will turn the RPython code into C code and use a C compiler to turn that\ninto a binary, containing both our code above as well as the RPython garbage\ncollector.\nThen we can run it (all results again from my AMD Ryzen 7 PRO 7840U, running\nUbuntu Linux 24.04.2):\n$ ./targetallocatealot-c 1000000000 0\nwithout initialization\n<A object at 0x7c71ad84cf60> <A object at 0x7c71ad84cf70>\n0.433825 s\n14.901161 GB\n34.348322 GB/s\n$ ./targetallocatealot-c 1000000000 1\nwith initialization\n<A object at 0x71b41c82cf60> <A object at 0x71b41c82cf70>\n0.501856 s\n14.901161 GB\n29.692100 GB/s\n\n\nLet's compare it with the Boehm GC:\n$ pypy rpython/bin/rpython --gc=boehm --output=targetallocatealot-c-boehm targetallocatealot.py \n...\n$ ./targetallocatealot-c-boehm 1000000000 0\nwithout initialization\n<A object at 0xffff8bd058a6e3af> <A object at 0xffff8bd058a6e3bf>\n9.722585 s\n14.901161 GB\n1.532634 GB/s\n$ ./targetallocatealot-c-boehm 1000000000 1\nwith initialization\n<A object at 0xffff88e1132983af> <A object at 0xffff88e1132983bf>\n9.684149 s\n14.901161 GB\n1.538717 GB/s\n\n\nThis is not a fair comparison, because the Boehm GC uses conservative stack\nscanning, therefore it cannot move objects, which requires much more\ncomplicated allocation.\nLet's look at perf stats\nWe can use perf to get some statistics about the executions:\n$ perf stat -e cache-references,cache-misses,cycles,instructions,branches,faults,migrations ./targetallocatealot-c 10000000000 0\nwithout initialization\n<A object at 0x7aa260e35980> <A object at 0x7aa260e35990>\n4.301442 s\n149.011612 GB\n34.642245 GB/s\n\n Performance counter stats for './targetallocatealot-c 10000000000 0':\n\n     7,244,117,828      cache-references                                                      \n        23,446,661      cache-misses                     #    0.32% of all cache refs         \n    21,074,240,395      cycles                                                                \n   110,116,790,943      instructions                     #    5.23  insn per cycle            \n    20,024,347,488      branches                                                              \n             1,287      faults                                                                \n                24      migrations                                                            \n\n       4.303071693 seconds time elapsed\n\n       4.297557000 seconds user\n       0.003998000 seconds sys\n\n$ perf stat -e cache-references,cache-misses,cycles,instructions,branches,faults,migrations ./targetallocatealot-c 10000000000 1\nwith initialization\n<A object at 0x77ceb0235980> <A object at 0x77ceb0235990>\n5.016772 s\n149.011612 GB\n29.702688 GB/s\n\n Performance counter stats for './targetallocatealot-c 10000000000 1':\n\n     7,571,461,470      cache-references                                                      \n       241,915,266      cache-misses                     #    3.20% of all cache refs         \n    24,503,497,532      cycles                                                                \n   130,126,387,460      instructions                     #    5.31  insn per cycle            \n    20,026,280,693      branches                                                              \n             1,285      faults                                                                \n                21      migrations                                                            \n\n       5.019444749 seconds time elapsed\n\n       5.012924000 seconds user\n       0.005999000 seconds sys\n\n\nThis is pretty cool, we can run this loop with >5 instructions per cycle. Every\nallocation takes 110116790943 / 10000000000 \u2248 11 instructions and\n21074240395 / 10000000000 \u2248 2.1 cycles, including the loop around it.\nHow often does the GC run?\nThe RPython GC queries the L2 cache size to determine the size of the nursery.\nWe can find out what it is by turning on PYPYLOG, selecting the proper logging\ncategories, and printing to stdout via :-:\n$ PYPYLOG=gc-set-nursery-size,gc-hardware:- ./targetallocatealot-c 1 1\n[f3e6970465723] {gc-set-nursery-size\nnursery size: 270336\n[f3e69704758f3] gc-set-nursery-size}\n[f3e697047b9a1] {gc-hardware\nL2cache = 1048576\n[f3e69705ced19] gc-hardware}\n[f3e69705d11b5] {gc-hardware\nmemtotal = 32274210816.000000\n[f3e69705f4948] gc-hardware}\n[f3e6970615f78] {gc-set-nursery-size\nnursery size: 4194304\n[f3e697061ecc0] gc-set-nursery-size}\nwith initialization\nNULL <A object at 0x7fa7b1434020>\n0.000008 s\n0.000000 GB\n0.001894 GB/s\n\n\nSo the nursery is 4 MiB. This means that when we allocate 14.9 GiB the GC needs to perform 10000000000 * 16 / 4194304 \u2248 38146 minor collections. Let's confirm that:\n$ PYPYLOG=gc-minor:out ./targetallocatealot-c 10000000000 1\nwith initialization\nw<A object at 0x7991e3835980> <A object at 0x7991e3835990>\n5.315511 s\n149.011612 GB\n28.033356 GB/s\n$ head out\n[f3ee482f4cd97] {gc-minor\n[f3ee482f53874] {gc-minor-walkroots\n[f3ee482f54117] gc-minor-walkroots}\nminor collect, total memory used: 0\nnumber of pinned objects: 0\ntotal size of surviving objects: 0\ntime taken: 0.000029\n[f3ee482f67b7e] gc-minor}\n[f3ee4838097c5] {gc-minor\n[f3ee48380c945] {gc-minor-walkroots\n$ grep \"{gc-minor-walkroots\" out | wc -l\n38147\n\n\nEach minor collection is very quick, because a minor collection is\nO(surviving objects), and in this program only one object survive each time\n(the other instance is in the process of being allocated).\nAlso, the GC root shadow stack is only one entry, so walking that is super\nquick as well. The time the minor collections take is logged to the out file:\n$ grep \"time taken\" out | tail\ntime taken: 0.000002\ntime taken: 0.000002\ntime taken: 0.000002\ntime taken: 0.000002\ntime taken: 0.000002\ntime taken: 0.000002\ntime taken: 0.000002\ntime taken: 0.000003\ntime taken: 0.000002\ntime taken: 0.000002\n$ grep \"time taken\" out | grep -o \"0.*\" | numsum\n0.0988160000000011\n\n\n(This number is super approximate due to float formatting rounding.)\nthat means that 0.0988160000000011 / 5.315511 \u2248 2% of the time is spent in the GC.\nWhat does the generated machine code look like?\nThe allocation fast path of the RPython GC is a simple bump pointer, in Python\npseudo-code it would look roughly like this:\nresult = gc.nursery_free\n# Move nursery_free pointer forward by totalsize\ngc.nursery_free = result + totalsize\n# Check if this allocation would exceed the nursery\nif gc.nursery_free > gc.nursery_top:\n    # If it does => collect the nursery and al\n    result = collect_and_reserve(totalsize)\nresult.hdr = <GC flags and type id of A>\n\n\nSo we can disassemble the compiled binary targetallocatealot-c and try to\nfind the equivalent logic in machine code. I'm super bad at reading machine\ncode, but I tried to annotate what I think is the core loop (the version\nwithout initializing the i field) below:\n    ...\n    cb68:   mov    %rbx,%rdi \n    cb6b:   mov    %rdx,%rbx\n\n    # initialize object header of object allocated in previous iteration\n    cb6e:   movq   $0x4c8,(%rbx)\n\n    # loop termination check\n    cb75:   cmp    %rbp,%r12\n    cb78:   je     ccb8\n\n    # load nursery_free\n    cb7e:   mov    0x33c13(%rip),%rdx\n\n    # increment loop counter\n    cb85:   add    $0x1,%rbp\n\n    # add 16 (size of object) to nursery_free\n    cb89:   lea    0x10(%rdx),%rax\n\n    # compare nursery_top with new nursery_free\n    cb8d:   cmp    %rax,0x33c24(%rip)\n\n    # store new nursery_free\n    cb94:   mov    %rax,0x33bfd(%rip)\n\n    # if new nursery_free exceeds nursery_top, fall through to slow path, if not, start at top\n    cb9b:   jae    cb68\n\n    # slow path from here on:\n    # save live object from last iteration to GC shadow stack\n    cb9d:   mov    %rbx,-0x8(%rcx)\n    cba1:   mov    %r13,%rdi\n    cba4:   mov    $0x10,%esi\n    # do minor collection\n    cba9:   call   20800 <pypy_g_IncrementalMiniMarkGC_collect_and_reserve>\n    ...\n\n\nRunning the benchmark as regular Python code\nSo far we ran this code as RPython, i.e. type inference is performed and the\nprogram is translated to a C binary. We can also run it on top of PyPy, as a\nregular Python3 program. However, an instance of a user-defined class in regular\nPython when run on PyPy is actually a much larger object, due to dynamic\ntyping.\nIt's at least 7 words, which is 56 bytes.\nHowever, we can simply use int objects instead. Integers are allocated on the\nheap and consist of two words, one for the GC and one with the\nmachine-word-sized integer value, if the integer fits into a signed 64-bit\nrepresentation (otherwise a less compact different representation is used,\nwhich can represent arbitrarily large integers).\nTherefore, we can simply use this kind of code:\nimport sys, time\n\n\ndef run(loops):\n    t1 = time.time()\n    a = prev = None\n    for i in range(loops):\n        prev = a\n        a = i\n    print(prev, a) # make sure always two objects are alive\n    t2 = time.time()\n    object_size_in_words = 2 # GC header, one integer field\n    mem = loops * 28 / 1024.0 / 1024.0 / 1024.0\n    print(mem, 'GB')\n    print(mem / (t2 - t1), 'GB/s')\n\ndef main(argv):\n    loops = int(argv[1])\n    run(loops)\n    return 0\n\nif __name__ == '__main__':\n    sys.exit(main(sys.argv))\n\n\nIn this case we can't really leave the value uninitialized though.\nWe can run this both with and without the JIT:\n$ pypy3 allocatealot.py 1000000000\n999999998 999999999\n14.901161193847656 GB\n17.857494904899553 GB/s\n$ pypy3 --jit off allocatealot.py 1000000000\n999999998 999999999\n14.901161193847656 GB\n0.8275382375297171 GB/s\n\n\nThis is obviously much less efficient than the C code, the PyPy JIT generates\nmuch less efficient machine code than GCC. Still, \"only\" twice as slow is kind\nof cool anyway.\n(Running it with CPython doesn't really make sense for this measurements, since\nCPython ints are bigger \u2013 sys.getsizeof(5) reports 28 bytes.)\nThe machine code that the JIT generates\nUnfortunately it's a bit of a journey to show the machine code that PyPy's JIT generates for this. First we need to run with all jit logging categories:\n$ PYPYLOG=jit:out pypy3 allocatealot.py 1000000000\n\n\nThen we can read the log file to find the trace IR for the loop under the logging category jit-log-opt:\n+532: label(p0, p1, p6, p9, p11, i34, p13, p19, p21, p23, p25, p29, p31, i44, i35, descr=TargetToken(137358545605472))\ndebug_merge_point(0, 0, 'run;/home/cfbolz/projects/gitpypy/allocatealot.py:6-9~#24 FOR_ITER')\n\n# are we at the end of the loop\n+552: i45 = int_lt(i44, i35)\n+555: guard_true(i45, descr=<Guard0x7ced4756a160>) [p0, p6, p9, p11, p13, p19, p21, p23, p25, p29, p31, p1, i44, i35, i34]\n+561: i47 = int_add(i44, 1)\ndebug_merge_point(0, 0, 'run;/home/cfbolz/projects/gitpypy/allocatealot.py:6-9~#26 STORE_FAST')\ndebug_merge_point(0, 0, 'run;/home/cfbolz/projects/gitpypy/allocatealot.py:6-10~#28 LOAD_FAST')\ndebug_merge_point(0, 0, 'run;/home/cfbolz/projects/gitpypy/allocatealot.py:6-10~#30 STORE_FAST')\ndebug_merge_point(0, 0, 'run;/home/cfbolz/projects/gitpypy/allocatealot.py:6-11~#32 LOAD_FAST')\ndebug_merge_point(0, 0, 'run;/home/cfbolz/projects/gitpypy/allocatealot.py:6-11~#34 STORE_FAST')\ndebug_merge_point(0, 0, 'run;/home/cfbolz/projects/gitpypy/allocatealot.py:6-11~#36 JUMP_ABSOLUTE')\n\n# update iterator object\n+565: setfield_gc(p25, i47, descr=<FieldS pypy.module.__builtin__.functional.W_IntRangeIterator.inst_current 8>)\n+569: guard_not_invalidated(descr=<Guard0x7ced4756a1b0>) [p0, p6, p9, p11, p19, p21, p23, p25, p29, p31, p1, i44, i34]\n\n# check for signals\n+569: i49 = getfield_raw_i(137358624889824, descr=<FieldS pypysig_long_struct_inner.c_value 0>)\n+582: i51 = int_lt(i49, 0)\n+586: guard_false(i51, descr=<Guard0x7ced4754db78>) [p0, p6, p9, p11, p19, p21, p23, p25, p29, p31, p1, i44, i34]\ndebug_merge_point(0, 0, 'run;/home/cfbolz/projects/gitpypy/allocatealot.py:6-9~#24 FOR_ITER')\n\n# allocate the integer (allocation sunk to the end of the trace)\n+592: p52 = new_with_vtable(descr=<SizeDescr 16>)\n+630: setfield_gc(p52, i34, descr=<FieldS pypy.objspace.std.intobject.W_IntObject.inst_intval 8 pure>)\n+634: jump(p0, p1, p6, p9, p11, i44, p52, p19, p21, p23, p25, p29, p31, i47, i35, descr=TargetToken(137358545605472))\n\n\nTo find the machine code address of the trace, we need to search for this line:\nLoop 1 (run;/home/cfbolz/projects/gitpypy/allocatealot.py:6-9~#24 FOR_ITER) \\\n    has address 0x7ced473ffa0b to 0x7ced473ffbb0 (bootstrap 0x7ced473ff980)\n\n\nThen we can use a script in the PyPy repo to disassemble the generated machine code:\n$ pypy rpython/jit/backend/tool/viewcode.py out\n\n\nThis will dump all the machine code to stdout, and open a pygame-based\ngraphviz cfg. In there\nwe can search for the address and see this:\n\nHere's an annotated version with what I think this code does:\n# increment the profile counter\n7ced473ffb40:   48 ff 04 25 20 9e 33    incq   0x38339e20\n7ced473ffb47:   38 \n\n# check whether the loop is done\n7ced473ffb48:   4c 39 fe                cmp    %r15,%rsi\n7ced473ffb4b:   0f 8d 76 01 00 00       jge    0x7ced473ffcc7\n\n# increment iteration variable\n7ced473ffb51:   4c 8d 66 01             lea    0x1(%rsi),%r12\n\n# update iterator object\n7ced473ffb55:   4d 89 61 08             mov    %r12,0x8(%r9)\n\n# check for ctrl-c/thread switch\n7ced473ffb59:   49 bb e0 1b 0b 4c ed    movabs $0x7ced4c0b1be0,%r11\n7ced473ffb60:   7c 00 00 \n7ced473ffb63:   49 8b 0b                mov    (%r11),%rcx\n7ced473ffb66:   48 83 f9 00             cmp    $0x0,%rcx\n7ced473ffb6a:   0f 8c 8f 01 00 00       jl     0x7ced473ffcff\n\n# load nursery_free pointer\n7ced473ffb70:   49 8b 8b d8 30 f6 fe    mov    -0x109cf28(%r11),%rcx\n\n# add size (16)\n7ced473ffb77:   48 8d 51 10             lea    0x10(%rcx),%rdx\n\n# compare against nursery top\n7ced473ffb7b:   49 3b 93 f8 30 f6 fe    cmp    -0x109cf08(%r11),%rdx\n\n# jump to slow path if nursery is full\n7ced473ffb82:   0f 87 41 00 00 00       ja     0x7ced473ffbc9\n\n# store new value of nursery free\n7ced473ffb88:   49 89 93 d8 30 f6 fe    mov    %rdx,-0x109cf28(%r11)\n\n# initialize GC header\n7ced473ffb8f:   48 c7 01 30 11 00 00    movq   $0x1130,(%rcx)\n\n# initialize integer field\n7ced473ffb96:   48 89 41 08             mov    %rax,0x8(%rcx)\n7ced473ffb9a:   48 89 f0                mov    %rsi,%rax\n7ced473ffb9d:   48 89 8d 60 01 00 00    mov    %rcx,0x160(%rbp)\n7ced473ffba4:   4c 89 e6                mov    %r12,%rsi\n7ced473ffba7:   e9 94 ff ff ff          jmp    0x7ced473ffb40\n7ced473ffbac:   0f 1f 40 00             nopl   0x0(%rax)\n\n\nConclusion\nThe careful design of the RPython GC's allocation fast path gives pretty good\nallocation rates. This technique isn't really new, it's a pretty typical way to\ndesign a GC. Apart from that, my main conclusion would be that computers are\nfast or something? Indeed, when we ran the same code on my colleague's\ntwo-year-old AMD, we got quite a bit worse results, so a lot of the speed seems\nto be due to the hard work of CPU architects.",
      "tags": "benchmarking,gc,rpython",
      "url": "https://www.pypy.org/posts/2025/06/rpython-gc-allocation-speed.html"
    },
    {
      "title": "Doing the Prospero-Challenge in RPython",
      "text": "Recently I had a lot of fun playing with the Prospero\nChallenge by Matt\nKeeter. The challenge is to render a 1024x1024 image of\na quote from The Tempest by Shakespeare. The input is a mathematical formula\nwith 7866 operations, which is evaluated once per pixel.\nWhat made the challenge particularly enticing for me personally was the fact\nthat the formula is basically a trace in\nSSA-form \u2013 a\nlinear sequence of operations, where every variable is assigned exactly once.\nThe challenge is to evaluate the formula as fast as possible. I tried a number\nof ideas how to speed up execution and will talk about them in this somewhat\nmeandering post. Most of it follows Matt's implementation\nFidget very closely. There are two points\nof difference:\n\nI tried to add more peephole optimizations, but they didn't end up helping\n  much.\nI implemented a \"demanded information\" optimization that removes a lot of\n  operations by only keeping the sign of the result. This optimization ended up\n  being useful.\n\nMost of the prototyping in this post was done in RPython (a statically typable\nsubset of Python2, that can be compiled to C), but I later rewrote the program\nin C to get better performance. All the code can be found on\nGithub.\nInput program\nThe input program is a sequence of operations, like this:\n_0 const 2.95\n_1 var-x\n_2 const 8.13008\n_3 mul _1 _2\n_4 add _0 _3\n_5 const 3.675\n_6 add _5 _3\n_7 neg _6\n_8 max _4 _7\n...\n\n\nThe first column is the name of the result variable, the second column is the\noperation, and the rest are the arguments to the operation. var-x is a\nspecial operation that returns the x-coordinate of the pixel being rendered,\nand equivalently for var-y the y-coordinate. The sign of the result gives the\ncolor of the pixel, the absolute value is not important.\nA baseline interpreter\nTo run the program, I first parse them and replace the register names with\nindexes, to avoid any dictionary lookups at runtime.\nThen I implemented a simple interpreter for the SSA-form\ninput program. The interpreter is a simple register machine, where every\noperation is executed in order. The result of the operation is stored into a\nlist of results, and the next operation is executed. This was the slow baseline\nimplementation of the interpreter but it's very useful to compare against the optimized\nversions.\nThis is roughly what the code looks like\nclass DirectFrame(object):\n    def __init__(self, program):\n        self.program = program\n        self.next = None\n\n    def run_floats(self, x, y, z):\n        self.setxyz(x, y, z)\n        return self.run()\n\n    def setxyz(self, x, y, z):\n        self.x = x\n        self.y = y\n        self.z = z\n\n    def run(self):\n        program = self.program\n        num_ops = program.num_operations()\n        floatvalues = [0.0] * num_ops\n        for op in range(num_ops):\n            func, arg0, arg1 = program.get_func_and_args(op)\n            if func == OPS.const:\n                floatvalues[op] = program.consts[arg0]\n                continue\n            farg0 = floatvalues[arg0]\n            farg1 = floatvalues[arg1]\n            if func == OPS.var_x:\n                res = self.x\n            elif func == OPS.var_y:\n                res = self.y\n            elif func == OPS.var_z:\n                res = self.z\n            elif func == OPS.add:\n                res = self.add(farg0, farg1)\n            elif func == OPS.sub:\n                res = self.sub(farg0, farg1)\n            elif func == OPS.mul:\n                res = self.mul(farg0, farg1)\n            elif func == OPS.max:\n                res = self.max(farg0, farg1)\n            elif func == OPS.min:\n                res = self.min(farg0, farg1)\n            elif func == OPS.square:\n                res = self.square(farg0)\n            elif func == OPS.sqrt:\n                res = self.sqrt(farg0)\n            elif func == OPS.exp:\n                res = self.exp(farg0)\n            elif func == OPS.neg:\n                res = self.neg(farg0)\n            elif func == OPS.abs:\n                res = self.abs(farg0)\n            else:\n                assert 0\n            floatvalues[op] = res\n        return self.floatvalues[num_ops - 1]\n\n    def add(self, arg0, arg1):\n        return arg0 + arg1\n\n    def sub(self, arg0, arg1):\n        return arg0 - arg1\n\n    def mul(self, arg0, arg1):\n        return arg0 * arg1\n\n    def max(self, arg0, arg1):\n        return max(arg0, arg1)\n\n    def min(self, arg0, arg1):\n        return min(arg0, arg1)\n\n    def square(self, arg0):\n        val = arg0\n        return val*val\n\n    def sqrt(self, arg0):\n        return math.sqrt(arg0)\n\n    def exp(self, arg0):\n        return math.exp(arg0)\n\n    def neg(self, arg0):\n        return -arg0\n\n    def abs(self, arg0):\n        return abs(arg0)\n\n\nRunning the naive interpreter on the prospero image file is super slow, since\nit performs 7866 * 1024 * 1024 float operations, plus the interpretation overhead.\nUsing Quadtrees to render the picture\nThe approach that Matt describes in his really excellent\ntalk is to use\nquadtrees: recursively subdivide the\nimage into quadrants, and evaluate the formula in each quadrant. For every\nquadrant you can simplify the formula by doing a range analysis. After a few\nrecursion steps, the formula becomes significantly smaller, often only a few\nhundred or a few dozen operations.\nAt the bottom of the recursion you either reach a square where the range\nanalysis reveals that the sign for all pixels is determined, then you can fill\nin all the pixels of the quadrant. Or you can evaluate the (now much simpler)\nformula in the quadrant by executing it for every pixel.\nThis is an interesting use case of JIT compiler/optimization techniques,\nrequiring the optimizer itself to execute really quickly since it is an essential\npart of the performance of the algorithm. The optimizer runs literally hundreds\nof times to render a single image. If the algorithm is used for 3D models\nit becomes even more crucial.\nWriting a simple optimizer\nImplementing the quadtree recursion is straightforward. Since the program has\nno control flow the optimizer is very simple to write. I've written a couple of\nblog posts on how to easily write optimizers for linear sequences of\noperations, and I'm using the approach described in these Toy\nOptimizer posts. The interval\nanalysis is basically an abstract\ninterpretation of the\noperations. The optimizer does a sequential forward pass over the input\nprogram. For every operation, the output interval is computed. The optimizer\nalso performs optimizations based on the computed intervals, which helps in\nreducing the number of operations executed (I'll talk about this further down).\nHere's a sketch of the Python code that does the optimization:\nclass Optimizer(object):\n    def __init__(self, program):\n        self.program = program\n        num_operations = program.num_operations()\n        self.resultops = ProgramBuilder(num_operations)\n        self.intervalframe = IntervalFrame(self.program)\n        # old index -> new index\n        self.opreplacements = [0] * num_operations\n        self.index = 0\n\n    def get_replacement(self, op):\n        return self.opreplacements[op]\n\n    def newop(self, func, arg0=0, arg1=0):\n        return self.resultops.add_op(func, arg0, arg1)\n\n    def newconst(self, value):\n        const = self.resultops.add_const(value)\n        self.intervalframe.minvalues[const] = value\n        self.intervalframe.maxvalues[const] = value\n        #self.seen_consts[value] = const\n        return const\n\n    def optimize(self, a, b, c, d, e, f):\n        program = self.program\n        self.intervalframe.setxyz(a, b, c, d, e, f)\n        numops = program.num_operations()\n        for index in range(numops):\n            newop = self._optimize_op(index)\n            self.opreplacements[index] = newop\n        return self.opreplacements[numops - 1]\n\n    def _optimize_op(self, op):\n        program = self.program\n        intervalframe = self.intervalframe\n        func, arg0, arg1 = program.get_func_and_args(op)\n        assert arg0 >= 0\n        assert arg1 >= 0\n        if func == OPS.var_x:\n            minimum = intervalframe.minx\n            maximum = intervalframe.maxx\n            return self.opt_default(OPS.var_x, minimum, maximum)\n        if func == OPS.var_y:\n            minimum = intervalframe.miny\n            maximum = intervalframe.maxy\n            return self.opt_default(OPS.var_y, minimum, maximum)\n        if func == OPS.var_z:\n            minimum = intervalframe.minz\n            maximum = intervalframe.maxz\n            return self.opt_default(OPS.var_z, minimum, maximum)\n        if func == OPS.const:\n            const = program.consts[arg0]\n            return self.newconst(const)\n        arg0 = self.get_replacement(arg0)\n        arg1 = self.get_replacement(arg1)\n        assert arg0 >= 0\n        assert arg1 >= 0\n        arg0minimum = intervalframe.minvalues[arg0]\n        arg0maximum = intervalframe.maxvalues[arg0]\n        arg1minimum = intervalframe.minvalues[arg1]\n        arg1maximum = intervalframe.maxvalues[arg1]\n        if func == OPS.neg:\n            return self.opt_neg(arg0, arg0minimum, arg0maximum)\n        if func == OPS.min:\n            return self.opt_min(arg0, arg1, arg0minimum, arg0maximum, arg1minimum, arg1maximum)\n        ...\n\n    def opt_default(self, func, minimum, maximum, arg0=0, arg1=0):\n        self.intervalframe._set(newop, minimum, maximum)\n        return newop\n\n    def opt_neg(self, arg0, arg0minimum, arg0maximum):\n        # peephole rules go here, see below\n        minimum, maximum = self.intervalframe._neg(arg0minimum, arg0maximum)\n        return self.opt_default(OPS.neg, minimum, maximum, arg0)\n\n    @symmetric\n    def opt_min(self, arg0, arg1, arg0minimum, arg0maximum, arg1minimum, arg1maximum):\n        # peephole rules go here, see below\n        minimum, maximum = self.intervalframe._max(arg0minimum, arg0maximum, arg1minimum, arg1maximum)\n        return self.opt_default(OPS.max, minimum, maximum, arg0, arg1)\n\n    ...\n\n\nThe resulting optimized traces are then simply interpreted at the bottom of the\nquadtree recursion. Matt talks about also generating machine code from them,\nbut when I tried to use PyPy's JIT for that it was way too slow at\nproducing machine code.\nTesting soundness of the interval abstract domain\nTo make sure that my interval computation in the optimizer is correct, I\nimplemented a hypothesis-based property based test. It checks the abstract\ntransfer functions of the interval domain for soundness. It does so by\ngenerating random concrete input values for an operation and random intervals that\nsurround the random concrete values, then performs the concrete operation to\nget the concrete output, and finally checks that the abstract transfer function applied\nto the input intervals gives an interval that contains the concrete output.\nFor example, the random test for the square operation would look like this:\nfrom hypothesis import given, strategies, assume\nfrom pyfidget.vm import IntervalFrame, DirectFrame\nimport math\n\nregular_floats = strategies.floats(allow_nan=False, allow_infinity=False)\n\ndef make_range_and_contained_float(a, b, c):\n    a, b, c, = sorted([a, b, c])\n    return a, b, c\n\nframe = DirectFrame(None)\nintervalframe = IntervalFrame(None)\n\nrange_and_contained_float = strategies.builds(make_range_and_contained_float, regular_floats, regular_floats, regular_floats)\n\ndef contains(res, rmin, rmax):\n    if math.isnan(rmin) or math.isnan(rmax):\n        return True\n    return rmin <= res <= rmax\n\n\n@given(range_and_contained_float)\ndef test_square(val):\n    a, b, c = val\n    rmin, rmax = intervalframe._square(a, c)\n    res = frame.square(b)\n    assert contains(res, rmin, rmax)\n\n\nThis test generates a random float b, and two other floats a and c such\nthat the interval [a, c] contains b. The test then checks that the result\nof the square operation on b is contained in the interval [rmin, rmax]\nreturned by the abstract transfer function for the square operation.\nPeephole rewrites\nThe only optimization that Matt does in his implementation is a peephole\noptimization rule that removes min and max operations where the intervals\nof the arguments don't overlap. In that case, the optimizer statically can know\nwhich of the arguments will be the result of the operation. I implemented this\npeephole optimization in my implementation as well, but I also added a few more\npeephole optimizations that I thought would be useful.\nclass Optimizer(object):\n\n    def opt_neg(self, arg0, arg0minimum, arg0maximum):\n        # new: add peephole rule --x => x\n        func, arg0arg0, _ = self.resultops.get_func_and_args(arg0)\n        if func == OPS.neg:\n            return arg0arg0\n        minimum, maximum = self.intervalframe._neg(arg0minimum, arg0maximum)\n        return self.opt_default(OPS.neg, minimum, maximum, arg0)\n\n    @symmetric\n    def opt_min(self, arg0, arg1, arg0minimum, arg0maximum, arg1minimum, arg1maximum):\n        # Matt's peephole rule\n        if arg0maximum < arg1minimum:\n            return arg0 # we can use the intervals to decide which argument will be returned\n        # new one by me: min(x, x) => x \n        if arg0 == arg1:\n            return arg0\n        func, arg0arg0, arg0arg1 = self.resultops.get_func_and_args(arg0)\n        minimum, maximum = self.intervalframe._max(arg0minimum, arg0maximum, arg1minimum, arg1maximum)\n        return self.opt_default(OPS.max, minimum, maximum, arg0, arg1)\n\n    ...\n\n\nHowever, it turns out that all my attempts at adding other peephole\noptimization rules were not very useful. Most rules never fired, and the ones\nthat did only had a small effect on the performance of the program. The only\npeephole optimization that I found to be useful was the one that Matt describes\nin his talk. Matt's min/max optimization were 96% of all rewrites that my\npeephole optimizer applied for the prospero.vm input. The remaining 4% of\nrewrites were (the percentages are of that 4%):\n--x => x                          4.65%\n(-x)**2 => x ** 2                 0.99%\nmin(x, x) => x                   20.86%\nmin(x, min(x, y)) =>  min(x, y)  52.87%\nmax(x, x) => x                   16.40%\nmax(x, max(x, y)) => max(x, y)    4.23%\n\n\nIn the end it turned out that having these extra optimization rules made the\ntotal runtime of the system go up. Checking for the rewrites isn't free, and\nsince they apply so rarely they don't pay for their own cost in terms of\nimproved performance.\nThere are some further rules that I tried that never fired at all:\na * 0 => 0\na * 1 => a\na * a => a ** 2\na * -1 => -a\na + 0 => a\na - 0 => a\nx - x => 0\nabs(known positive number x) => x\nabs(known negative number x) => -x\nabs(-x) => abs(x)\n(-x) ** 2 => x ** 2\n\n\nThis investigation is clearly way too focused on a single program and should be\nre-done with a larger set of example inputs, if this were an actually serious\nimplementation.\nDemanded Information Optimization\nLLVM has an static analysis pass called 'demanded bits'. It is a backwards analysis that\nallows you to determine which bits of a value are actually used in the final\nresult. This information can then be used in peephole optimizations. For\nexample, if you have an expression that computes a value, but only the last\nbyte of that value is used in the final result, you can optimize the expression\nto only compute the last byte.\nHere's an example. Let's say we first byte-swap a 64-bit int, and then mask off the last byte:\nuint64_t byteswap_then_mask(uint64_t a) {\n    return byteswap(a) & 0xff;\n}\n\n\nIn this case, the \"demanded bits\" of the byteswap(a) expression are\n0b0...011111111, which inversely means that we don't care about the upper 56\nbits. Therefore the whole expression can be optimized to a >> 56.\nFor the Prospero challenge, we can observe that for the resulting pixel values, the value of\nthe result is not used at all, only its sign. Essentially, every program ends\nimplicitly with a sign operation that returns 0.0 for negative values and\n1.0 for positive values. For clarity, I will show this sign operation in\nthe rest of the section, even if it's not actually in the real code.\nThis makes it possible to simplify certain min/max\noperations further. Here is an example of a program, together with the\nintervals of the variables:\nx var-x     # [0.1, 1]\ny var-y     # [-1, 1]\nm min x y # [-1, 1]\nout sign m\n\n\nThis program can be optimized to:\ny var-y\nout sign m\n\n\nBecause that expression has the same result as the original expression: if x >\n0.1, for the result of min(x, y) to be negative then y needs to be negative.\nAnother, more complex, example is this:\nx var-x        # [1, 100]\ny var-y        # [-10, 10]\nz var-z        # [-100, 100]\nm1 min x y     # [-10, 10]\nm2 max z out   # [-10, 100]\nout sign m2\n\n\nWhich can be optimized to this:\ny var-y\nz var-z\nm2 max z y\nout sign m2\n\n\nThis is because the sign of min(x, y) is the same as the sign of y if x >\n0, and the sign of max(z, min(x, y)) is thus the same as the sign of max(z,\ny).\nTo implement this optimization, I do a backwards pass over the program after\nthe peephole optimization forward pass. For every min call I encounter, where\none of the arguments is positive, I can optimize the min call away and\nreplace it with the other argument. For max calls I simplify their arguments\nrecursively.\nThe code looks roughly like this:\ndef work_backwards(resultops, result, minvalues, maxvalues):\n    def demand_sign_simplify(op):\n        func, arg0, arg1 = resultops.get_func_and_args(op)\n        if func == OPS.max:\n            narg0 = demand_sign_simplify(arg0)\n            if narg0 != arg0:\n                resultops.setarg(op, 0, narg0)\n            narg1 = demand_sign_simplify(arg1)\n            if narg1 != arg1:\n                resultops.setarg(op, 1, narg1)\n        if func == OPS.min:\n            if minvalues[arg0] > 0.0:\n                return demand_sign_simplify(arg1)\n            if minvalues[arg1] > 0.0:\n                return demand_sign_simplify(arg0)\n            narg0 = demand_sign_simplify(arg0)\n            if narg0 != arg0:\n                resultops.setarg(op, 1, narg0)\n            narg1 = demand_sign_simplify(arg1)\n            if narg1 != arg1:\n                resultops.setarg(op, 1, narg1)\n        return op\n    return demand_sign_simplify(result)\n\n\nIn my experiment, this optimization lets me remove 25% of all operations in\nprospero, at the various levels of my octree. I'll briefly look at performance\nresults further down.\nFurther ideas about the demanded sign simplification\nThere is another idea how to short-circuit the evaluation of expressions that I\ntried briefly but didn't pursue to the end. Let's go back to the first example\nof the previous subsection, but with different intervals:\nx var-x     # [-1, 1]\ny var-y     # [-1, 1]\nm min x y   # [-1, 1]\nout sign m\n\n\nNow we can't use the \"demanded sign\" trick in the optimizer, because neither\nx nor y are known positive. However, during execution of the program, if\nx turns out to be negative we can end the execution of this trace\nimmediately, since we know that the result must be negative.\nSo I experimented with adding return_early_if_neg flags to all operations\nwith this property. The interpreter then checks whether the flag is set on an\noperation and if the result is negative, it stops the execution of the program\nearly:\nx var-x[return_early_if_neg]\ny var-y[return_early_if_neg]\nm min x y\nout sign m\n\n\nThis looked pretty promising, but it's also a trade-off because the cost of\nchecking the flag and the value isn't zero. Here's a sketch to the change in the interpreter:\nclass DirectFrame(object):\n    ...\n    def run(self):\n        program = self.program\n        num_ops = program.num_operations()\n        floatvalues = [0.0] * num_ops\n        for op in range(num_ops):\n            ...\n            if func == OPS.var_x:\n                res = self.x\n            ...\n            else:\n                assert 0\n            if program.get_flags(op) & OPS.should_return_if_neg and res < 0.0:\n                return res\n            floatvalues[op] = res\n        return self.floatvalues[num_ops - 1]\n\n\nI implemented this in the RPython\nversion, but didn't end up porting it to C, because it interferes with SIMD.\nDead code elimination\nMatt performs dead code elimination in his implementation by doing a single\nbackwards pass over the program. This is a very simple and effective\noptimization, and I implemented it in my implementation as well. The dead code\nelimination pass is very simple: It starts by marking the result operation as\nused. Then it goes backwards over the program. If the current operation is\nused, its arguments are marked as used as well. Afterwards, all the operations\nthat are not marked as used are removed from the program. The PyPy JIT actually\nperforms dead code elimination on traces in exactly the same way (and I don't\nthink we ever explained how this works on the blog), so I thought it was worth\nmentioning.\nMatt also performs register allocation as part of the backwards pass, but I\ndidn't implement it because I wasn't too interested in that aspect.\nRandom testing of the optimizer\nTo make sure I didn't break anything in the optimizer, I implemented a\ntest that generates random input programs and checks that the output of the\noptimizer is equivalent to the input program. The test generates random\noperations, random intervals for the operations and a random input value within\nthat interval. It then runs the optimizer on the input program and checks that\nthe output program has the same result as the input program. This is again\nimplemented with hypothesis. Hypothesis' test case minimization feature is\nsuper useful for finding optimizer bugs. It's just not fun to analyze a problem\non a many-thousand-operation input file, but Hypothesis often generated reduced\ntest cases that were only a few operations long.\nVisualizing programs\nIt's actually surprisingly annoying to visualize prospero.vm well, because\nit's quite a bit too large to just feed it into Graphviz. I made the problem\nslightly easier by grouping several operations together, where only the first\noperation in a group is used as the argument for more than one operation\nfurther in the program. This made it slightly more manageable for Graphviz. But\nit still wasn't a big enough improvement to be able to visualize all of\nprospero.vm in its unoptimized form at the top of the octree.\nHere's a visualization of the optimized prospero.vm at one of the octree\nlevels:\n\nThe result is on top, every node points to its arguments. The min and max\noperations form a kind of \"spine\" of the expression tree, because they are\nunions and intersection in the constructive solid geometry sense.\nI also wrote a function to visualize the octree recursion itself, the output\nlooks like this:\n\n\nGreen nodes are where the interval analysis determined that the output must be\nentirely outside the shape. Yellow nodes are where the octree recursion\nbottomed out.\nC implementation\nTo achieve even faster performance, I decided to rewrite the implementation in\nC. While RPython is great for prototyping, it can be challenging to control\nlow-level aspects of the code. The rewrite in C allowed me to experiment with\nseveral techniques I had been curious about:\n\nmusttail optimization for the interpreter.\nSIMD (Single Instruction, Multiple Data): Using Clang's\n  ext_vector_type, I process eight pixels at once using AVX (or some other\n  SIMD magic that I don't properly understand).\nEfficient struct packing: I packed the operations struct into just 8\n  bytes by limiting the maximum number of operations to 65,536, with the idea\n  of making the optimizer faster.\n\nI didn't rigorously study the performance impact of each of these techniques\nindividually, so it's possible that some of them might not have contributed\nsignificantly. However, the rewrite was a fun exercise for me to explore these\ntechniques. The code can be found\nhere.\nTesting the C implementation\nAt various points I had bugs in the C implementation, leading to a fun glitchy\nversion of prospero:\n\nTo find these bugs, I used the same random testing approach as in the\nRPython version. I generated random input programs as strings in Python and\nchecked that the output of the C implementation was equivalent to the output of\nthe RPython implementation (simply by calling out to the shell and reading the\ngenerated image, then comparing pixels). This helped ensure that the C\nimplementation was\ncorrect and didn't introduce any bugs. It was surprisingly tricky to get this\nright, for reasons that I didn't expect. At lot of them are related to the fact\nthat in C I used float and Python uses double for its (Python) float\ntype. This made the random tester find weird floating point corner cases where\nrounding behaviour between the widths was different.\nI solved those by using double in C when running the random tests by means of\nan IFDEF.\nIt's super fun to watch the random program generator produce random images, here are a few:\n\n\nPerformance\nSome very rough performance results on my laptop (an AMD Ryzen 7 PRO 7840U with\n32 GiB RAM running Ubuntu 24.04), comparing the RPython version, the C version\n(with and without demanded info), and Fidget (in vm mode, its JIT made things\nworse for me), both for 1024x1024 and 4096x4096 images:\n\n\n\nImplementation\n1024x1024\n4096x4096\n\n\n\n\nRPython\n26.8ms\n75.0ms\n\n\nC (no demanded info)\n24.5ms\n45.0ms\n\n\nC (demanded info)\n18.0ms\n37.0ms\n\n\nFidget\n10.8ms\n57.8ms\n\n\n\nThe demanded info seem to help quite a bit, which was nice to see.\nConclusion\nThat's it! I had lots of fun with the challenge and have a whole bunch of other\nideas I want to try out, thanks Matt for this interesting puzzle.",
      "tags": "toy-optimizer",
      "url": "https://www.pypy.org/posts/2025/04/prospero-in-rpython.html"
    },
    {
      "title": "PyPy v7.3.19 release",
      "text": "PyPy v7.3.19: release of python 2.7, 3.10 and 3.11 beta\nThe PyPy team is proud to release version 7.3.19 of PyPy. This is primarily a\nbug-fix release fixing JIT-related problems and follows quickly on the heels of\nthe previous release on Feb 6, 2025.\nThis release includes a python 3.11 interpreter. There were bugs in the first\nbeta that could prevent its wider use, so we are continuing to call this\nrelease \"beta\". In the next release we will drop 3.10 and remove the \"beta\"\nlabel.\nThe release includes three different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.10, which is an interpreter supporting the syntax and the features of\nPython 3.10, including the stdlib for CPython 3.10.16.\nPyPy3.11, which is an interpreter supporting the syntax and the features of\nPython 3.11, including the stdlib for CPython 3.11.11.\n\nThe interpreters are based on much the same codebase, thus the triple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. It follows after 7.3.17 release on August 28, 2024.\nWe recommend updating. You can find links to download the releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear\nabout it and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with\nmaking RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy. In any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython\nIt's fast (PyPy and CPython performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64) and macos (macos_arm64).\n\nPyPy supports Windows 32-bit, Linux PPC64 big- and little-endian, Linux ARM\n32 bit, RISC-V RV64IMAFD Linux, and s390x Linux but does not release binaries.\nPlease reach out to us if you wish to sponsor binary releases for those\nplatforms. Downstream packagers provide binary builds for debian, Fedora,\nconda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.19 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2025/02/pypy-v7319-release.html"
    },
    {
      "title": "Low Overhead Allocation Sampling with VMProf in PyPy's GC",
      "text": "Introduction\nThere are many time-based statistical profilers around (like VMProf or py-spy\njust to name a few). They allow the user to pick a trade-off between profiling\nprecision and runtime overhead.\nOn the other hand there are memory profilers\nsuch as memray. They can be handy for\nfinding leaks or for discovering functions that allocate a lot of memory.\nMemory profilers typlically save every single allocation a program does. This\nresults in precise profiling, but larger overhead.\nIn this post we describe our experimental approach to low overhead statistical\nmemory profiling. Instead of saving every single allocation a program does, it\nonly saves every nth allocated byte. We have tightly integrated VMProf and the\nPyPy Garbage Collector to achieve this. The main technical insight is that the\ncheck whether an allocation should be sampled can be made free. This is done by\nfolding it into the bump pointer allocator check that the PyPy\u2019s GC uses to\nfind out if it should start a minor collection. In this way the fast path with\nand without memory sampling are exactly the same.\nBackground\nTo get an insight how the profiler and GC interact, lets take a brief look at\nboth of them first.\nVMProf\nVMProf is a statistical time-based profiler for PyPy. VMProf samples the stack of currently running Python functions a certain user-configured number of times per second. By adjusting\nthis number, the overhead of profiling can be modified to pick the correct trade-off between overhead and precision of the profile. In the resulting profile, functions with huge runtime stand out the most, functions with shorter runtime less so. If you want to get a little more introduction to VMProf and how to use it with PyPy, you may look\nat this blog post\nPyPy\u2019s GC\nPyPy uses a generational incremental copying collector. That means there are two spaces for allocated objects, the nursery and the old-space. Freshly allocated objects will be allocated into the nursery. When the nursery is full at some point, it will be collected and all objects that survive will be tenured i.e. moved into the old-space. The old-space is much larger than the nursery and is collected less frequently and incrementally (not completely\ncollected in one go, but step-by-step). The old space collection is not relevant for the rest of the post though. We will now take a look at nursery allocations and how the nursery is collected.\nBump Pointer Allocation in the Nursery\nThe nursery (a small continuous memory area) utilizes two pointers to keep track from where on the nursery is free and where it ends. They are called nursery_free and nursery_top. When memory is allocated, the GC checks if there is enough space in the nursery left. If there is enough space, the nursery_free pointer will be returned as the start address for the newly allocated memory, and nursery_free will be moved forward by the amount of allocated memory.\n\ndef allocate(totalsize):\n  # Save position, where the object will be allocated to as result\n  result = gc.nursery_free\n  # Move nursery_free pointer forward by totalsize\n  gc.nursery_free = result + totalsize\n  # Check if this allocation would exceed the nursery\n  if gc.nursery_free > gc.nursery_top:\n      # If it does => collect the nursery and allocate afterwards\n      result = collect_and_reserve(totalsize)\n  # result is a pointer into the nursery, obj will be allocated there\n  return result\n\ndef collect_and_reserve(size_of_allocation):\n    # do a minor collection and return the start of the nursery afterwards\n    minor_collection()\n    return gc.nursery_free\n\n\nUnderstanding this is crucial for our allocation sampling approach, so let us go through this step-by-step.\nWe already saw an example on how an allocation into a non-full nursery will look like. But what happens, if the nursery is (too) full?\n\nAs soon as an object doesn't fit into the nursery anymore, it will be collected. A nursery collection will move all surviving objects into the old-space, so that the nursery is free afterwards, and the requested allocation can be made.\n\n(Note that this is still a bit of a simplification.)\nSampling Approach\nThe last section described how the nursery allocation works normally. Now we'll talk how we integrate the new allocation sampling approach into it.\nTo decide whether the GC should trigger a sample, the sampling logic is integrated into the bump pointer allocation logic. Usually, when there is not enough space in the nursery left to fulfill an allocation request, the nursery will be collected and the allocation will be done afterwards. We reuse that mechanism for sampling, by introducing a new pointer called sample_point that is calculated by sample_point = nursery_free + sample_n_bytes where sample_n_bytes is the number of bytes allocated before a sample is made (i.e. our sampling rate).\nImagine we'd have a nursery of 2MB and want to sample every 512KB allocated, then you could imagine our nursery looking like that:\n\nWe use the sample point as nursery_top, so that allocating a chunk of 512KB would exceed the nursery top and start a nursery collection. But of course we don't want to do a minor collection just then, so before starting a collection, we need to check if the nursery is actually full or if that is just an exceeded sample point. The latter will then trigger a VMprof stack sample. Afterwards we don't actually do a minor collection, but change nursery_top and immediately return to the caller.\nThe last picture is a conceptual simplification. Only one sampling point exists at any given time. After we created the sampling point, it will be used as nursery top, if exceeded at some point, we will just add sample_n_bytes to that sampling point, i.e. move it forward.\nHere's how the updated collect_and_reserve function looks like:\ndef collect_and_reserve(size_of_allocation):\n    # Check if we exceeded a sample point or if we need to do a minor collection\n    if gc.nursery_top == gc.sample_point:\n        # One allocation could exceed multiple sample points\n        # Sample, move sample_point forward\n        vmprof.sample_now()\n        gc.sample_point += sample_n_bytes\n\n        # Set sample point as new nursery_top if it fits into the nursery\n        if sample_point <= gc.real_nursery_top:\n            gc.nursery_top = sample_point\n        # Or use the real nursery top if it does not fit\n        else:\n            gc.nursery_top = gc.real_nursery_top\n\n        # Is there enough memory left inside the nursery\n        if gc.nursery_free + size_of_allocation <= gc.nursery_top:\n            # Yes => move nursery_free forward\n            gc.nursery_free += size_of_allocation\n            return gc.nursery_free\n\n    # We did not exceed a sampling point and must do a minor collection, or\n    # we exceeded a sample point but we needed to do a minor collection anyway\n    minor_collection()\n    return gc.nursery_free\n\n\nWhy is the Overhead \u2018low\u2019\nThe most important property of our approach is that the bump-pointer fast path is not changed at all. If sampling is turned off, the slow path in collect_and_reserve has three extra instructions for the if at the beginning, but are only a very small amount of overhead, compared to doing a minor collection.\nWhen sampling is on, the extra logic in collect_and_reserve gets executed. Every time an allocation exceeds the sample_point, collect_and_reserve will sample the Python functions currently executing. The resulting overhead is directly controlled by sample_n_bytes. After sampling, the sample_point and nursery_top must be set accordingly. This will be done once after sampling in collect_and_reserve. At some point a nursery collection will free the nursery and set the new sample_point afterwards.\nThat means that the overhead mostly depends on the sampling rate and the rate at which the user program allocates memory, as the combination of those two factors determines the amount of samples.\nSince the sampling rate can be adjusted from as low as 64 Byte to a theoretical maximum of ~4 GB (at the moment), the tradeoff between number of samples (i.e. profiling precision) and overhead can be completely adjusted.\nWe also suspect linkage between user program stack depth and overhead (a deeper stack takes longer to walk, leading to higher overhead), especially when walking the C call stack to.\nSampling rates bigger than the nursery size\nThe nursery usually has a size of a few megabytes, but profiling long-runningor larger applications with tons of allocations could result in very high number of samples per second (and thus overhead). To combat that it is possible to use sampling rates higher than the nursery size.\nThe sampling point is not limited by the nursery size, but if it is 'outside' the nursery (e.g. because sample_n_bytes is set to twice the nursery size) it won't be used as nursery_top until it 'fits' into the nursery.\n\nAfter every nursery collection, we'd usually set the sample_point to nursery_free + sample_n_bytes, but if it is larger than the nursery, then the amount of collected memory during the last nursery collection is subtracted from sample_point.\n\nAt some point the sample_point will be smaller than the nursery size, then it will be used as nursery_top again to trigger a sample when exceeded.\nDifferences to Time-Based Sampling\nAs mentioned in the introduction, time-based sampling \u2018hits\u2019 functions with high runtime, and allocation-sampling \u2018hits\u2019 functions allocating much memory. But are those always different functions? The answer is: sometimes. There can be functions allocating lots of memory, that do not have a (relative) high runtime.\nAnother difference to time-based sampling is that the profiling overhead does not solely depend on the sampling rate (if we exclude a potential stack-depth - overhead correlation for now) but also on the amount of memory the user code allocates.\nLet us look at an example:\nIf we\u2019d sample every 1024 Byte and some program A allocates 3 MB and runs for 5 seconds, and program B allocates 6 MB but also runs for 5 seconds, there will be ~3000 samples when profiling A, but ~6000 samples when profiling B. That means we cannot give a \u2018standard\u2019 sampling rate like time-based profilers use to do (e.g. vmprof uses ~1000 samples/s for time sampling), as the number of resulting samples, and thus overhead, depends on sampling rate and amount of memory allocated by the program.\nFor testing and benchmarking, we usually started with a sampling rate of 128Kb and then halved or doubled that (multiple times) depending on sample counts, our need for precision (and size of the profile).\nEvaluation\nOverhead\nNow let us take a look at the allocation sampling overhead, by profiling some benchmarks. \nThe x-axis shows the sampling rate, while the y-axis shows the overhead, which is computed as runtime_with_sampling / runtime_without_sampling.\nAll benchmarks were executed five times on a PyPy with JIT and native profiling enabled, so that every dot in the plot is one run of a benchmark.\n\nAs you probably expected, the Overhead drops with higher allocation sampling rates.\nReaching from as high as ~390% for 32kb allocation sampling to as low as < 10% for 32mb.\nLet me give one concrete example: One run of the microbenchmark at 32kb sampling took 15.596 seconds and triggered 822050 samples.\nThat makes a ridiculous amount of 822050 / 15.596 = ~52709 samples per second. \nThere is probably no need for that amount of samples per second, so that for 'real' application profiling a much higher sampling rate would be sufficient.\nLet us compare that to time sampling.\nThis time we ran those benchmarks with 100, 1000 and 2000 samples per second.\n\nThe overhead varies with the sampling rate. Both with allocation and time sampling, you can reach any amount of overhead and any level of profiling precision you want. The best approach probably is to just try out a sampling rate and choose what gives you the right tradeoff between precision and overhead (and disk usage).\nThe benchmarks used are:\nmicrobenchmark \n\nhttps://github.com/Cskorpion/microbenchmark\npypy microbench.py 65536\n\ngcbench \n\nhttps://github.com/pypy/pypy/blob/main/rpython/translator/goal/gcbench.py\nprint statements removed\npypy gcbench.py 1\n\npypy translate step\n\nfirst step of the pypy translation (annotation step)\npypy path/to/rpython --opt=0 --cc=gcc --dont-write-c-files --gc=incminimark --annotate path/to/pypy/goal/targetpypystandalone.py\n\ninterpreter pystone\n\npystone benchmark on top of an interpreted pypy on top of a translated pypy\npypy path/to/pypy/bin/pyinteractive.py -c \"import test.pystone; test.pystone.main(1)\"\n\nAll benchmarks executed on:\n\nKubuntu 24.04\nAMD Ryzen 7 5700U\n24gb DDR4 3200MHz (dual channel)\n\nSSD benchmarking at read: 1965 MB/s, write: 227 MB/s\n\nSequential 1MB 1 Thread 8 Queues\n\n\n\nSelf built PyPy with allocation sampling features\n\nhttps://github.com/Cskorpion/pypy/tree/gc_allocation_sampling_u_2.7\n\n\n\nModified VMProf with allocation sampling support\n\nhttps://github.com/Cskorpion/vmprof-python/tree/pypy_gc_allocation_sampling\n\n\n\nExample\nWe have also modified vmprof-firefox-converter to show the allocation samples in the Firefor Profiler UI. With the techniques from this post, the output looks like this:\n\nWhile this view is interesting, it would be even better if we could also see what types of objects are being allocated in these functions. We will take about how to do this in a future blog post.\nConclusion\nIn this blog post we introduced allocation sampling for PyPy by going through the technical aspects and the corresponding overhead. In a future blog post, we are going to dive into the actual usage of allocation sampling with VMProf, and show an example case study. That will be accompanied by some new improvements and additional features, like extracting the type of an object that triggered a sample.\nSo far all this work is still experimental and happening on PyPy branches but\nwe hope to get the technique stable enough to merge it to main and ship it with\nPyPy eventually.\n-- Christoph Jung and CF Bolz-Tereick",
      "tags": "gc,profiling,vmprof",
      "url": "https://www.pypy.org/posts/2025/02/pypy-gc-sampling.html"
    },
    {
      "title": "PyPy v7.3.18 release",
      "text": "PyPy v7.3.18: release of python 2.7, 3.10 and 3.11 beta\nThe PyPy team is proud to release version 7.3.18 of PyPy.\nThis release includes a python 3.11 interpreter. We are labelling it \"beta\"\nbecause it is the first one. In the next release we will drop 3.10 and remove\nthe \"beta\" label. There are a particularly large set of bugfixes in this\nrelease thanks to @devdanzin using fusil on the 3.10 builds, originally written\nby Victor Stinner. Other significant changes:\n\nWe have updated libffi shipped in our portable builds. We also now statically\nlink to libffi where possible which reduces the number of\nshared object dependencies.\nWe have added code to be able to show the native function names when\nprofiling with VMProf. So far only Linux supports this feature.\nWe have added a PEP 768-inspired remote debugging facility.\nThe HPy backend has been updated to latest HPy HEAD\n\nThe release includes three different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.10, which is an interpreter supporting the syntax and the features of\nPython 3.10, including the stdlib for CPython 3.10.16.\nPyPy3.11, which is an interpreter supporting the syntax and the features of\nPython 3.11, including the stdlib for CPython 3.11.11.\n\nThe interpreters are based on much the same codebase, thus the triple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. It follows after 7.3.17 release on August 28, 2024.\nWe recommend updating. You can find links to download the releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear\nabout it and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with\nmaking RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy. In any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nVMProf Native Symbol Names\nWhen running VMProf profiling with native profiling enabled, PyPy did so far\nnot produce function names for C functions. The output looked like this:\npypy -m vmprof ~/projects/gitpypy/lib-python/2.7/test/pystone.py\nPystone(1.1) time for 50000 passes = 0.0109887\nThis machine benchmarks at 4.55011e+06 pystones/second\n vmprof output:\n %:      name:                location:\n 100.0%  entry_point          <builtin>/app_main.py:874\n 100.0%  run_command_line     <builtin>/app_main.py:601\n 100.0%  run_toplevel         <builtin>/app_main.py:93\n 100.0%  _run_module_as_main  /home/user/bin/pypy-c-jit-170203-99a72243b541-linux64/lib-python/2.7/runpy.py:150\n 100.0%  _run_code            /home/user/bin/pypy-c-jit-170203-99a72243b541-linux64/lib-python/2.7/runpy.py:62\n 100.0%  <module>             /home/user/bin/pypy-c-jit-170203-99a72243b541-linux64/site-packages/vmprof/__main__.py:1\n 100.0%  main                 /home/user/bin/pypy-c-jit-170203-99a72243b541-linux64/site-packages/vmprof/__main__.py:30\n 100.0%  run_path             /home/user/bin/pypy-c-jit-170203-99a72243b541-linux64/lib-python/2.7/runpy.py:238\n 100.0%  _run_module_code     /home/user/bin/pypy-c-jit-170203-99a72243b541-linux64/lib-python/2.7/runpy.py:75\n 100.0%  <module>             /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:3\n 100.0%  main                 /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:60\n 100.0%  pystones             /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:67\n 100.0%  Proc0                /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:79\n 76.9%   <unknown code>\n 69.2%   <unknown code>\n 53.8%   <unknown code>\n 53.8%   <unknown code>\n 46.2%   <unknown code>\n 46.2%   <unknown code>\n 38.5%   <unknown code>\n 38.5%   Proc8                /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:212\n 30.8%   <unknown code>\n ...\nWe can now symbolify these C functions and give function names and which\nshared library they come from, at least on Linux:\nPystone(1.1) time for 50000 passes = 0.218967\nThis machine benchmarks at 228345 pystones/second\n vmprof output:\n %:      name:                                           location:\n 100.0%  entry_point                                     <builtin>/app_main.py:889\n 100.0%  run_command_line                                <builtin>/app_main.py:616\n 100.0%  run_toplevel                                    <builtin>/app_main.py:95\n 100.0%  _run_module_as_main                             /home/user/projects/gitpypy/lib-python/2.7/runpy.py:150\n 100.0%  _run_code                                       /home/user/projects/gitpypy/lib-python/2.7/runpy.py:62\n 100.0%  <module>                                        /home/user/projects/gitpypy/site-packages/vmprof/__main__.py:1\n 100.0%  main                                            /home/user/projects/gitpypy/site-packages/vmprof/__main__.py:30\n 100.0%  run_module                                      /home/user/projects/gitpypy/lib-python/2.7/runpy.py:179\n 100.0%  _run_module_code                                /home/user/projects/gitpypy/lib-python/2.7/runpy.py:75\n 100.0%  <module>                                        /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:3\n 100.0%  main                                            /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:60\n 100.0%  pystones                                        /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:67\n 100.0%  Proc0                                           /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:79\n 95.5%   n:pypy_g_execute_frame:0:pypy-c\n 91.4%   n:pypy_g_PyFrame_dispatch:0:pypy-c\n 63.8%   n:pypy_g_PyFrame_dispatch_bytecode:0:pypy-c\n 49.8%   Proc1                                           /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:137\n 17.6%   copy                                            /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:53\n 13.6%   n:pypy_g_PyFrame_CALL_FUNCTION:0:pypy-c\n 10.4%   Proc8                                           /home/user/projects/gitpypy/lib-python/2.7/test/pystone.py:212\n 8.6%    n:pypy_g_STORE_ATTR_slowpath:0:pypy-c\nThis becomes even more useful when using the VMProf Firefox converter, which\nuses the Firefox Profiler Web UI to visualize profiling output:\n\n\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython\nIt's fast (PyPy and CPython performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64) and macos (macos_arm64).\n\nPyPy supports Windows 32-bit, Linux PPC64 big- and little-endian, Linux ARM\n32 bit, RISC-V RV64IMAFD Linux, and s390x Linux but does not release binaries.\nPlease reach out to us if you wish to sponsor binary releases for those\nplatforms. Downstream packagers provide binary builds for debian, Fedora,\nconda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.18 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2025/02/pypy-v7318-release.html"
    },
    {
      "title": "Musings on Tracing in PyPy",
      "text": "Last summer, Shriram Krishnamurthi asked on\nTwitter:\n\n\"I'm curious what the current state of tracing JITs is. They used to be all the\nrage for a while, then I though I heard they weren't so effective, then I\nhaven't heard of them at all. Is the latter because they are ubiquitous, or\nbecause they proved to not work so well?\"\n\nI replied with my personal (pretty subjective) opinions about the\nquestion in a lengthy Twitter thread (which also spawned an even lengthier\ndiscussion). I wanted to turn what I wrote there into a blog post to make it\nmore widely available (Twitter is no longer easily consumable without an\naccount), and also because I'm mostly not using Twitter anymore. The blog post\ni still somewhat terse, I've written a small background section and tried to at\nleast add links to further information. Please ask in the comments if something\nis particularly unclear.\nBackground\nI'll explain a few of the central terms of the rest of the post. JIT compilers\nare compilers that do their work at runtime, interleaved (or concurrent with)\nthe execution of the program. There are (at least) two common general styles of\nJIT compiler architectures. The most common one is that of a method-based JIT,\nwhich will compile one method or function at a time. Then there are tracing JIT\ncompilers, which generate code by tracing the execution of the user's program.\nThey often focus on loops as their main unit of compilation.\nThen there is the distinction between a \"regular\" JIT compiler and that of a\nmeta-JIT. A regular JIT is built to compile one specific source language to\nmachine code. A meta-JIT is a framework for building JIT compilers for a\nvariety of different languages, reusing as much machinery as possible between\nthe different implementation.\nPersonal and Project Context\nSome personal context: my perspective is informed by nearly two\ndecades\nof work on PyPy. PyPy's implementation language, RPython, has support for a\nmeta-JIT, which allows it to reuse its JIT infrastructure for the various\nPython versions that we support (currently we do releases of PyPy2.7 and\nPyPy3.10 together). Our meta-JIT infrastructure has been used for some\nexperimental different languages like:\n\nPyPy's regular expression engine\nRPySom, a tiny Smalltalk\nRuby\nPHP\nProlog,\nRacket,\na database (SQLite)\nLox, the language of Crafting Interpreters\nan ARM and RISC-V emulator\nand many more\n\nThose implementations had various degrees of maturity and many of them are\nresearch software and aren't maintained any more.\nPyPy gives itself the goal to try to be extremely compatible with all the\nquirks of the Python language. We don't change the Python language to make\nthings easier to compile and we support the introspection and debugging\nfeatures of Python. We try very hard to have no opinions on language design.\nThe CPython core developers come up with the semantics, we somehow deal with\nthem.\nMeta-tracing\nPyPy started using a tracing\nJIT approach\nnot because we thought method-based just-in-time compilers are bad.\nHistorically we had\ntried\nto implement a method-based meta-JIT that was using partial evaluation (we wrote\nthree or four method-based prototypes that all weren't as good as we hoped).\nAfter all those experiments\nfailed\nwe switched to the tracing\napproach, and only at this\npoint did our meta-JIT start producing interesting performance.\nIn the meta-JIT context tracing has good properties, because tracing has\nrelatively understandable behavior and its easy(ish) to tweak how things work\nwith extra annotations in the interpreter\nsource.\nAnother reason why meta-tracing often works well for PyPy is that it can often\nslice through the complicated layers of Python quite effectively and remove a\nlot of overhead. Python is often described as simple, but I think that's\nactually a misconception. On the implementation level it's a very big and\ncomplicated language, and it is also continuously getting new features every\nyear (the language is quite a bit more complicated than Javascript, for\nexample1).\nTruffle\nLater Truffle came along\nand made a method-based meta-JIT using partial evaluation work. However Truffle\n(and Graal) has had significantly more people working on it and much more\nmoney invested. In addition, it at first required a quite specific style of\nAST-based interpreters (in\nthe last few years they have also added support for bytecode-based\ninterpreters).\nIt's still my impression that getting similar results with Truffle is more\nwork for language\nimplementers\nthan with RPython, and the warmup of\nTruffle can often pretty bad. But Truffle is definitely an existence proof that\nmeta-JITs don't have to be based on tracing.\nTracing, the good\nLet's now actually get to he heart of Shriram's question and discuss some of\nthe advantages of tracing that go beyond the ease of using tracing for a\nmeta-JIT.\nTracing allows for doing very aggressive partial\ninlining,\nFollowing just the hot path through lots of layers of abstraction is obviously\noften really useful for generating fast code.\nIt's definitely possible to achieve the same effect in a method-based context\nwith path splitting. But it\nrequires a lot more implementation work and is not trivial, because the path\nexecution counts of inlined\nfunctions can often be very call-site dependent. Tracing, on the other hand,\ngives you call-site dependent path splitting \"for free\".\n(The aggressive partial inlining and path splitting is even more important in\nthe meta-tracing context of PyPy, where some of inlined layers are a part of\nthe language runtime, and where rare corner cases that are never executed in\npractice are everywhere.)\nAnother advantage of tracing is that it makes a number of optimizations\nreally easy to implement, because there are (to first approximation) no control\nflow merges. This makes all the optimizations that we do (super-)local\noptimizations,\nthat operate on a single (very long) basic block. This allows the JIT to do the\noptimizations in exactly one forwards and one backwards pass. An example is our\nallocation removal/partial\nescape analysis pass, which is quite\nsimple,\nwhereas the version for general control\nflow has\na lot more complexity, particularly in its handling of loops.\nThis ease of implementation of optimizations allowed us to implement some\npretty decent optimizations. Our allocation removal, the way PyPy's JIT can\nreason about the heap, about dictionary accesses, about properties of functions\nof the runtime, about the range and known bits of integer\nvariables is all quite\nsolid.\nTracing, the bad\nTracing also comes with a significant number of downsides. Probably the biggest\none is that it tends to have big performance cliffs (PyPy certainly has them,\nand other tracing JITs such as TraceMonkey had them too). In my experience the\n'good' cases of tracing are really good, but if something goes wrong you are\nannoyed and performance can become a lot slower. With a simple method JIT the\nperformance is often much more \"even\".\nAnother set of downsides is that tracing has a number of corner cases and\n\"weird\" behaviour in certain situations. Questions such as:\n- When do you stop inlining?\n- What happens when you trace recursion?\n- What happens if your traces are consistently too long, even without inlining?\n- and so on...\nSome of these problems can be solved by adding heuristics to the tracing JIT,\nbut doing so loses a lot of the simplicity of tracing again.\nThere are also some classes of programs that tend to generally perform quite\npoorly when they are executed by a tracing JIT, bytecode interpreters in\nparticularly, and other extremely unpredictably branchy code. This is because\nthe core assumption of the tracing jit \"loops take similar control flow paths\"\nis just really wrong in the case of interpreters.\nDiscussion\nThe Twitter thread spawned quite a bit of discussion, please look at the\noriginal thread for all of the comments. Here are three that I wanted to\nhighlight:\n\n\"This is a really great summary. Meta-tracing is probably the one biggest\nsuccess story. I think it has to do with how big and branchy the bytecode\nimplementations are for typical dynamic languages; the trace captures latent\ntype feedback naturally.\nThere is an upper limit, tho.\"\n\nBen Titzer\nI agree with this completely. The complexity of Python bytecodes is a big\nfactor for why meta tracing works well for us. But also in Python there are\nmany builtin types (collection types, types that form the meta-object\nprotocol of\nPython, standard library modules implemented in C/RPython) and tracing\noperations on them is very important too, for good performance.\n\n\n\"I think Mozilla had a blog post talking more about the difficulty with\nTraceMonkey, could only find this one:\nhttps://blog.mozilla.org/nnethercote/category/jagermonkey/\"\n\nStefan Marr\n\n\"imo doing tracing for JS is really hard mode, because the browser is so\nincredibly warmup-sensitive. IIRC tracemonkey used a really low loop trip count\n(single-digit?) to decide when to start tracing (pypy uses >1000). the JS\ninterpreters of the time were also quite slow.\"\n\nme\nIn the meantime there were some more reminiscences about tracing in Javascript\nby Shu-Yu Guo in a panel\ndiscussion and by Jason\nOrendorff on Mastodon.\n\n\n\"There are a number of corner cases you have to deal with in a tracing JIT. It's\nunfortunately not as simple and easy as the initial papers would have you\nbelieve. One example is how would you deal with a loop inside a loop? Is your\ntracing now recursive?\nThere's been some research work on trace stitching to deal with trace explosion\nbut it does add complexity. With the increase in complexity, I think most\nindustrial VM developers would rather pick tried-and-true method-based JITs\nthat are well understood.\"\n\nMaxime Chevalier\nConclusion\nGiven access to enough developers and in the context of \"normal\" jitting (ie\nnot meta-jitting) it's very unclear to me that you should use tracing. It makes\nmore sense to rather spend effort on a solid control-flow-graph-based baseline\nand then try to get some of the good properties of tracing on top (path\nsplitting, partial inlining, partial escape analysis, etc).\nFor PyPy with its meta-JIT (and the fact that we don't have particularly much\nfunding nor people) I still think tracing was/is a relatively pragmatic choice.\nWhen I talked with Sam Tobin-Hochstadt about this\ntopic recently he characterized it like this: \"tracing is a labor-saving device\nfor compiler authors\".\nPerformance-wise PyPy is still quite hard to beat in the cases where it works\nwell (i.e. pure Python code that doesn't use too many C modules, which are\nsupported but slow in\nPyPy).\nIn general, there are very few JITs for Python (particularly with the\nconstraint of not being \"allowed\" to change the language), the most competitive\nother ones are GraalPy, also based on a\nmeta-JIT approach. Instagram is running on\nCinder and also CPython has\ngrown a JIT\nrecently which\nwas part of the recent 3.13 release, but only as an off-by-default build\noption,\nso I'm very excited about how Python's performance will develop in the next\nyears!\n\n\n\n\n(A side point: people who haven't worked on Python tend to\nunderestimate its complexity and pace of development. A pet peeve of mine\nis C++ compiler devs/static analysis/Javascript people/other well-meaning\ncommunities coming with statements like \"why don't you just...\"  \ud83e\udd37\u200d\u2640\ufe0f)\u00a0\u21a9",
      "tags": "",
      "url": "https://www.pypy.org/posts/2025/01/musings-tracing.html"
    },
    {
      "title": "Towards PyPy3.11 - an update",
      "text": "We1 are steadily working towards a Python 3.11 interpreter, which will be part\nof the upcoming PyPy 7.3.18 release. Along with that, we also recently updated \nspeed.pypy.org to compare PyPy's performance to CPython\n3.11 (it used to be CPython 3.7). \n\nWhy is there no PyPy for Python 3.11?\nTL;DR: we are working on it and hopefully will have a beta version soon\nWe started by merging the exception groups\nwork by Nico Rittinghaus, merging the CPython stdlib for Python 3.11.9, and\nupdating the regex engine to handle atomic groupings. I think these were the\nlargest changes needed to support Python3.11, maybe I missed something else?\nWe then created a milestone with\nmany of the changes that might not be caught in testing. As of today that\nmilestone stands at 24/40 issues closed, 16 open. This is in addition to the\nv7.3.18 milestone, which has 11/23\nissues closed, 12 open.\nWe also updated our infrastructure to run nightly buildbot test of the\npy3.11 branch, including adding py3.11 to our benchmarking site speed.pypy.org.\nThen the real work started: fixing these milestone issues and failing stdlib\ntests. Some of the changes were cosmetic changes to error messages, some were\nmore involved changes to the interpreter to behave more like CPython. For instance,\nhex(x) where x is an int calls long.format(x, \"#x\") on CPython where in\nPyPy we used x.__format__(\"#x\"). This subtle difference caused a failure in\nthe repr of IntEnum. Tracking down problems like these takes time. We are\nnow down to about 250 failing stdlib tests, with thousands passing. For comparison, \nPyPy3.10, first released in June 2023, still has around 100 failing stdlib\ntests.\nC-extension support\nPyPy supports the Python C-API via a cpyext compatibility layer. We \"mangle\"\nthe CPython symbols to add an extra Py to prevent loading CPython c-extension\nmodules into PyPy, since the ABI is different. So a function like\nPyLong_FromLong will be exported from the c shared object as\nPyPyLong_FromLong. One of my long-standing goals is to remove this mangling,\nbut it then requires that our c declarations, inlined functions, and macros in\nthe headers match 1:1 the CPython headers. We can get by with not declaring and\nimplementing parts of the interfaces, but what is declared must be identical.\nThis is a long-term project, with each release the headers get closer to the\nCPython versions. I hope to concentrate on the PyUnicode interfaces for this\nrelease.\nNote that the time to do this is before a new version release. Once the version\nis released, we cannot change the headers significantly.\nSo what is left?\nSummarizing the milestones and other things to do:\n\nUpdate to the recently released 3.11.11 stdlib\nMake sure vmprof works\nUpdate the time module to use more MONOTONIC_CLOCK, implement\n  time.sleep differently, and clean up the many duplicate time like\n  interfaces we have across the codebase. We have the time module, some time\n  routines in the _threading moduleand RPython threading support in therpython/rlib` code. We should also make sure we are using 64-bit time\n  interfaces.\nDecide whether zero-cost exceptions gain us in performance and whether we\n  should implement them even if they do not improve performance.\nUpdate our hpy backend to the latest HEAD, which\n  would allow running the hpy numpy\n  fork\nReintegrate the pure-python pyrepl libbrary from CPython 3.13.\n\nWhat else did I forget?\nWhy did the benchmark results get worse on speed.pypy.org?\nTL;DR: running a benchmark site is hard. Something changed in the benchmark\nrunner, and suddenly benchmarks got 10-15% slower.\nPyPy run an instance of\ncodepseed with a\nvery old benchmarking suite\nthat can still run Python2 (remember: that is the language of\nRPython underlying PyPy). The site\nruns on PSA infrastructure and the benchmarking machine is generously sponsored\nby Baroque Software. On Nov 9, there was a\nsudden jump in benchmarking times. For instance\nhere\nis the result for the float benchmark. This happened across various benchmark\nruns: both PyPy2.7 and PyPy3.11alpha, with and without the JIT. After spending\nsome time rerunning various benchmarks, I could only conclude the machine\nitself had gotten slower, maybe due to some security update in the linux\nkernel, maybe some change in the hosting platform. This \"broke\" the front-page\ncomparsison: suddenly \"latest\" is much slower than the historic benchmarks run\nprevious to the changes in the machine.\nThat page also recently (as of last week) uses CPython 3.11 as a baseline for\ncomparison, where previously it used CPython3.7. It is common knowledge that\nthe newer CPython versions are faster, and we see this now quite clearly.\nDiving into individual benchmarks, we can see that ones where PyPy-with-a-jit\nwas comparable to CPython3.7 seem to be the ones that CPython3.11 improved\ngreatly. Looking at a comparison of the\nruns\nthis can be seen in benchmarks like deltablue and the sqlalchemy family. So the\nnew graph has more lines that extend past 1.5 than the old graph.\n\nNew graph with PyPy3.11\n\n\n\nOlder graph with PyPy3.10\n\n.\n\n\n\n\nThese days most of the work is done by CF\nBolz-Tereick and me on a\nvolunteer basis. Want to get involved? Reach out, we would love to expand the\nteam. Have an idea for funding the work? Fantastic, let's talk.\u00a0\u21a9",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2025/01/towards-pypy311-an-update.html"
    },
    {
      "title": "Guest Post: Final Encoding in RPython Interpreters",
      "text": "Introduction\nThis post started as a quick note summarizing a recent experiment I carried\nout upon a small RPython interpreter by rewriting it in an uncommon style. It\nis written for folks who have already written some RPython and want to take a\ndeeper look at interpreter architecture.\nSome experiments are about finding solutions to problems. This experiment is\nabout taking a solution which is already well-understood and applying it in\nthe context of RPython to find a new approach. As we will see, there is no\nreal change in functionality or the number of clauses in the interpreter; it's\nmore like a comparison between endo- and exoskeletons, a different arrangement\nof equivalent bones and plates.\nOverview\nAn RPython interpreter for a programming language generally does three or four\nthings, in order:\n\nRead and parse input programs\nEncode concrete syntax as abstract syntax\nOptionally, optimize or reduce the abstract syntax\nEvaluate the abstract syntax: read input data, compute, print output data,\n   etc.\n\nToday we'll look at abstract syntax. Most programming languages admit a\nconcrete parse tree which is\nreadily abstracted to provide an abstract syntax\ntree (AST). The AST is\nusually encoded with the initial style of encoding. An initial encoding can\nbe transformed into any other encoding for the same AST, looks like a\nhierarchy of classes, and is implemented as a static structure on the heap.\nIn contrast, there is also a final encoding. A final encoding can be\ntransformed into by any other encoding, looks like an interface for the\nactions of the interpreter, and is implemented as an unwinding structure on\nthe stack. From the RPython perspective, Python builtin modules like os or\nsys are final encodings for features of the operating system; the underlying\nimplementation is different when translated or untranslated, but the interface\nused to access those features does not change.\nIn RPython, an initial encoding is built from a hierarchy of classes. Each\nclass represents a type of tree nodes, corresponding to a parser production in\nthe concrete parse tree. Each class instance therefore represents an\nindividual tree node. The fields of a class, particularly those filled during\n.__init__(), store pre-computed properties of each node; methods can be used\nto compute node properties on demand. This seems like an obvious and simple\napproach; what other approaches could there be? We need an example.\nFinal Encoding of Brainfuck\nWe will consider Brainfuck, a simple\nTuring-complete programming language. An example Brainfuck program might be:\n[-]\n\n\nThis program is built from a loop and a decrement, and sets a cell to zero. In\nan initial encoding which follows the algebraic semantics of\nBrainfuck, the program could\nbe expressed by applying class constructors to build a structure on the heap:\nLoop(Plus(-1))\n\n\nA final encoding is similar, except that class constructors are replaced by\nmethods, the structure is built on the stack, and we are parameterized over\nthe choice of class:\nlambda cls: cls.loop(cls.plus(-1))\n\n\nIn ordinary Python, transforming between these would be trivial, and mostly is\na matter of passing around the appropriate class. Indeed, initial and final\nencodings are equivalent; we'll return to that fact later. However, in RPython,\nall of the types must line up, and classes must be determined before\ntranslation. We'll need to monomorphize our final encodings, using some\nRPython tricks later on. Before that, let's see what an actual Brainfuck\ninterface looks like, so that we can cover all of the difficulties with final\nencoding.\nBefore we embark, please keep in mind that local code doesn't know what cls\nis. There's no type-safe way to inspect an arbitrary semantic domain. In the\ninitial-encoded version, we can ask isinstance(bf, Loop) to see whether an\nAST node is a loop, but there simply isn't an equivalent for final-encoded\nASTs. So, there is an implicit challenge to think about: how do we evaluate a\nprogram in an arbitrary semantic domain? For bonus points, how do we optimize\na program without inspecting the types of its AST nodes?\nWhat follows is a dissection of\nthis\nmodule at the given revision. Readers may find it satisfying to read the\nentire interpreter top to bottom first; it is less than 300 lines.\nCore Functionality\nFinal encoding is given as methods on an interface. These five methods\ncorrespond precisely to the summands of the algebra of Brainfuck.\nclass BF(object):\n    # Other methods elided\n    def plus(self, i): pass\n    def right(self, i): pass\n    def input(self): pass\n    def output(self): pass\n    def loop(self, bfs): pass\n\n\nNote that the .loop() method takes another program as an argument.\nInitial-encoded ASTs have other initial-encoded ASTs as fields on class\ninstances; final-encoded ASTs have other final-encoded ASTs as parameters\nto interface methods. RPython infers all of the types, so the reader has to\nknow that i is usually an integer while bfs is a sequence of Brainfuck\noperations.\nWe're using a class to implement this functionality. Later, we'll treat it as\na mixin, rather than a superclass, to avoid typing problems.\nMonoid\nIn order to optimize input programs, we'll need to represent the underlying\nmonoid of Brainfuck programs. To do\nthis, we add the signature for a monoid:\nclass BF(object):\n    # Other methods elided\n    def unit(self): pass\n    def join(self, l, r): pass\n\n\nThis is technically a unital\nmagma, since RPython doesn't\nsupport algebraic laws, but we will enforce the algebraic laws later on during\noptimization. We also want to make use of the folklore that free\nmonoids are lists, allowing\ncallers to pass a list of actions which we'll reduce with recursion:\nclass BF(object):\n    # Other methods elided\n    def joinList(self, bfs):\n        if not bfs: return self.unit()\n        elif len(bfs) == 1: return bfs[0]\n        elif len(bfs) == 2: return self.join(bfs[0], bfs[1])\n        else:\n            i = len(bfs) >> 1\n            return self.join(self.joinList(bfs[:i]), self.joinList(bfs[i:]))\n\n\n.joinList() is a little bulky to implement, but Wirth's principle applies:\nthe interpreter is shorter with it than without it.\nIdioms\nFinally, our interface includes a few high-level idioms, like the zero program\nshown earlier, which are defined in terms of low-level behaviors. In an\ninitial encoding, these could be defined as module-level functions; here, we\ndefine them on the mixin class BF.\nclass BF(object):\n    # Other methods elided\n    def zero(self): return self.loop(self.plus(-1))\n    def move(self, i): return self.scalemove(i, 1)\n    def move2(self, i, j): return self.scalemove2(i, 1, j, 1)\n    def scalemove(self, i, s):\n        return self.loop(self.joinList([\n            self.plus(-1), self.right(i), self.plus(s), self.right(-i)]))\n    def scalemove2(self, i, s, j, t):\n        return self.loop(self.joinList([\n                self.plus(-1), self.right(i), self.plus(s), self.right(j - i),\n                self.plus(t), self.right(-j)]))\n\n\nInterface-oriented Architecture\nApplying Interfaces\nNow, we hack at RPython's object model until everything translates. First,\nconsider the task of pretty-printing. For Brainfuck, we'll simply regurgitate\nthe input program as a Python string:\nclass AsStr(object):\n    import_from_mixin(BF)\n    def unit(self): return \"\"\n    def join(self, l, r): return l + r\n    def plus(self, i): return '+' * i if i > 0 else '-' * -i\n    def right(self, i): return '>' * i if i > 0 else '<' * -i\n    def loop(self, bfs): return '[' + bfs + ']'\n    def input(self): return ','\n    def output(self): return '.'\n\n\nVia rlib.objectmodel.import_from_mixin, no stressing with covariance of\nreturn types is required. Instead, we shift from a Java-esque view of classes\nand objects, to an OCaml-ish view of prebuilt classes and constructors.\nAsStr is monomorphic, and any caller of it will have to create their own\ncovariance somehow. For example, here are the first few lines of the parsing\nfunction:\n@specialize.argtype(1)\ndef parse(s, domain):\n    ops = [domain.unit()]\n    # Parser elided to preserve the reader's attention\n\n\nBy invoking rlib.objectmodel.specialize.argtype, we make copies of the\nparsing function, up to one per call site, based on our choice of semantic\ndomain. Oleg calls these \"symantics\"\nbut I prefer \"domain\" in code. Also, note how the parsing stack starts with\nthe unit of the monoid, which corresponds to the empty input string; the\nparser will repeatedly use the monoidal join to build up a parsed expression\nwithout inspecting it. Here's a small taste of that:\nwhile i < len(s):\n    char = s[i]\n    if char == '+': ops[-1] = domain.join(ops[-1], domain.plus(1))\n    elif char == '-': ops[-1] = domain.join(ops[-1], domain.plus(-1))\n    # and so on\n\n\nThe reader may feel justifiably mystified; what breaks if we don't add these\nmagic annotations? Well, the translator will throw UnionError because the\nlow-level types don't match. RPython only wants to make one copy of functions\nlike parse() in its low-level representation, and each copy of parse()\nwill be compiled to monomorphic machine code. In this interpreter, in order to\nsupport parsing to an optimized string and also parsing to an evaluator, we\nneed two copies of parse(). It is okay to not fully understand this at\nfirst.\nComposing Interfaces\nEarlier, we noted that an interpreter can optionally optimize input programs\nafter parsing. To support this, we'll precompose a peephole\noptimizer onto an\narbitrary domain. We could also postcompose with a parser instead, but that\nsounds more difficult. Here are the relevant parts:\ndef makePeephole(cls):\n    domain = cls()\n    def stripDomain(bfs): return domain.joinList([t[0] for t in bfs])\n    class Peephole(object):\n        import_from_mixin(BF)\n        def unit(self): return []\n        def join(self, l, r): return l + r\n        # Actual definition elided... for now...\n    return Peephole, stripDomain\n\n\nDon't worry about the actual optimization yet. What's important here is the\npattern of initialization of semantic domains. makePeephole is an\nSML-style functor on semantic\ndomains: given a final encoding of Brainfuck, it produces another final\nencoding of Brainfuck which incorporates optimizations. The helper\nstripDomain is a finalizer which performs the extraction from the\noptimizer's domain to the underlying cls that was passed in at translation\ntime. For example, let's optimize pretty-printing:\nAsStr, finishStr = makePeephole(AsStr)\n\n\nNow, it only takes one line to parse and print an optimized AST without ever\nbuilding it on the heap. To be pedantic, fragments of the output string will\nbe heap-allocated, but the AST's node structure will only ever be\nstack-allocated. Further, to be shallow, the parser is written to prevent\nmalicious input from causing a stack overflow, and this forces it to maintain\na heap-allocated RPython list of intermediate operations inside loops.\nprint finishStr(parse(text, AsStr()))\n\n\nPerformance\nBut is it fast? Yes. It's faster than the prior version, which was\ninitial-encoded, and also faster than Andrew Brown's classic version (part\n1,\npart\n2).\nSince Brown's interpreter does not perform much optimization, we will focus on\nhow final encoding can outperform initial encoding.\nJIT\nFirst, why is it faster than the same interpreter with initial encoding? Well,\nit still has initial encoding from the JIT's perspective! There is an Op\nclass with a hierarchy of subclasses implementing individual behaviors. A\nsincere tagless-final student, or those who remember Stop Writing Classes\n(2012, Pycon\nUS), will\nrecognize that the following classes could be plain functions, and should\nthink of the classes as a concession to RPython's lack of support for lambdas\nwith closures rather than an initial encoding. We aren't ever going to\ndirectly typecheck any Op, but the JIT will generate typechecking guards\nanyway, so we effectively get a fully-promoted AST inlined into each JIT\ntrace. First, some simple behaviors:\nclass Op(object): _immutable_ = True\n\nclass _Input(Op):\n    _immutable_ = True\n    def runOn(self, tape, position):\n        tape[position] = ord(os.read(0, 1)[0])\n        return position\nInput = _Input()\n\nclass _Output(Op):\n    _immutable_ = True\n    def runOn(self, tape, position):\n        os.write(1, chr(tape[position]))\n        return position\nOutput = _Output()\n\nclass Add(Op):\n    _immutable_ = True\n    _immutable_fields_ = \"imm\",\n    def __init__(self, imm): self.imm = imm\n    def runOn(self, tape, position):\n        tape[position] += self.imm\n        return position\n\n\nThe JIT does technically have less information than before; it no longer knows\nthat a sequence of immutable operations is immutable enough to be worth\nunrolling, but a bit of rlib.jit.unroll_safe fixes that:\nclass Seq(Op):\n    _immutable_ = True\n    _immutable_fields_ = \"ops[*]\",\n    def __init__(self, ops): self.ops = ops\n    @unroll_safe\n    def runOn(self, tape, position):\n        for op in self.ops: position = op.runOn(tape, position)\n        return position\n\n\nFinally, the JIT entry point is at the head of each loop, just like with prior\ninterpreters. Since Brainfuck doesn't support mid-loop jumps, there's no\npenalty for only allowing merge points at the head of the loop.\nclass Loop(Op):\n    _immutable_ = True\n    _immutable_fields_ = \"op\",\n    def __init__(self, op): self.op = op\n    def runOn(self, tape, position):\n        op = self.op\n        while tape[position]:\n            jitdriver.jit_merge_point(op=op, position=position, tape=tape)\n            position = op.runOn(tape, position)\n        return position\n\n\nThat's the end of the implicit challenge. There's no secret to it; just\nevaluate the AST. Here's part of the semantic domain for evaluation, as well\nas the \"functor\" to optimize it. In AsOps.join() are the only\nisinstance() calls in the entire interpreter! This is acceptable because\nSeq is effectively a type wrapper for an RPython list, so that a list of\noperations is also an operation; its list is initial-encoded and available for\ninspection.\nclass AsOps(object):\n    import_from_mixin(BF)\n    def unit(self): return Shift(0)\n    def join(self, l, r):\n        if isinstance(l, Seq) and isinstance(r, Seq):\n            return Seq(l.ops + r.ops)\n        elif isinstance(l, Seq): return Seq(l.ops + [r])\n        elif isinstance(r, Seq): return Seq([l] + r.ops)\n        return Seq([l, r])\n    # Other methods elided!\n\nAsOps, finishOps = makePeephole(AsOps)\n\n\nAnd finally here is the actual top-level code to evaluate the input program.\nAs before, once everything is composed, the actual invocation only takes one\nline.\ntape = bytearray(\"\\x00\" * cells)\nfinishOps(parse(text, AsOps())).runOn(tape, 0)\n\n\nPeephole Optimization\nOur peephole optimizer is an abstract\ninterpreter with one\ninstruction of lookahead/rewrite buffer. It implements the aforementioned\nalgebraic laws of the Brainfuck monoid. It also implements idiom recognition\nfor loops. First, the abstract interpreter. The abstract domain has six\nelements:\nclass AbstractDomain(object): pass\nmeh, aLoop, aZero, theIdentity, anAdd, aRight = [AbstractDomain() for _ in range(6)]\n\n\nWe'll also tag everything with an integer, so that anAdd or aRight can be\nexact annotations. This is the actual Peephole.join() method:\ndef join(self, l, r):\n    if not l: return r\n    rv = l[:]\n    bfHead, adHead, immHead = rv.pop()\n    for bf, ad, imm in r:\n        if ad is theIdentity: continue\n        elif adHead is aLoop and ad is aLoop: continue\n        elif adHead is theIdentity:\n            bfHead, adHead, immHead = bf, ad, imm\n        elif adHead is anAdd and ad is aZero:\n            bfHead, adHead, immHead = bf, ad, imm\n        elif adHead is anAdd and ad is anAdd:\n            immHead += imm\n            if immHead: bfHead = domain.plus(immHead)\n            elif rv: bfHead, adHead, immHead = rv.pop()\n            else:\n                bfHead = domain.unit()\n                adHead = theIdentity\n        elif adHead is aRight and ad is aRight:\n            immHead += imm\n            if immHead: bfHead = domain.right(immHead)\n            elif rv: bfHead, adHead, immHead = rv.pop()\n            else:\n                bfHead = domain.unit()\n                adHead = theIdentity\n        else:\n            rv.append((bfHead, adHead, immHead))\n            bfHead, adHead, immHead = bf, ad, imm\n    rv.append((bfHead, adHead, immHead))\n    return rv\n\n\nIf this were to get much longer, then implementing a\nDSL would be worth it,\nbut this is a short-enough method to inline. The abstract interpretation is\nassumed by induction for the left-hand side of the join, save for the final\ninstruction, which is loaded into a rewrite register. Each instruction on the\nright-hand side is inspected exactly once. The logic for anAdd followed by\nanAdd is exactly the same as for aRight followed by aRight because they\nboth have underlying Abelian\ngroups given by the integers.\nThe rewrite register is carefully pushed onto and popped off from the\nleft-hand side in order to cancel out theIdentity, which itself is merely a\nunifier for anAdd or aRight of 0.\nNote that we generate a lot of garbage. For example, parsing a string of n\n'+' characters will cause the peephole optimizer to allocate n instances of\nthe underlying domain.plus() action, from domain.plus(1) up to\ndomain.plus(n). An older initial-encoded version of this interpreter used\nhash consing to avoid ever\nbuilding an op more than once, even loops. It appears more efficient to\ngenerate lots of immutable garbage than to repeatedly hash inputs and search\nmutable hash tables, at least for optimizing Brainfuck incrementally during\nparsing.\nFinally, let's look at idiom recognition. RPython lists are initial-coded, so\nwe can dispatch based on the length of the list, and then inspect the abstract\ndomains of each action.\ndef isConstAdd(bf, i): return bf[1] is anAdd and bf[2] == i\n\ndef oppositeShifts(bf1, bf2):\n    return bf1[1] is bf2[1] is aRight and bf1[2] == -bf2[2]\n\ndef oppositeShifts2(bf1, bf2, bf3):\n    return (bf1[1] is bf2[1] is bf3[1] is aRight and\n            bf1[2] + bf2[2] + bf3[2] == 0)\n\ndef loop(self, bfs):\n    if len(bfs) == 1:\n        bf, ad, imm = bfs[0]\n        if ad is anAdd and imm in (1, -1):\n            return [(domain.zero(), aZero, 0)]\n    elif len(bfs) == 4:\n        if (isConstAdd(bfs[0], -1) and\n            bfs[2][1] is anAdd and\n            oppositeShifts(bfs[1], bfs[3])):\n            return [(domain.scalemove(bfs[1][2], bfs[2][2]), aLoop, 0)]\n        if (isConstAdd(bfs[3], -1) and\n            bfs[1][1] is anAdd and\n            oppositeShifts(bfs[0], bfs[2])):\n            return [(domain.scalemove(bfs[0][2], bfs[1][2]), aLoop, 0)]\n    elif len(bfs) == 6:\n        if (isConstAdd(bfs[0], -1) and\n            bfs[2][1] is bfs[4][1] is anAdd and\n            oppositeShifts2(bfs[1], bfs[3], bfs[5])):\n            return [(domain.scalemove2(bfs[1][2], bfs[2][2],\n                                       bfs[1][2] + bfs[3][2],\n                                       bfs[4][2]), aLoop, 0)]\n        if (isConstAdd(bfs[5], -1) and\n            bfs[1][1] is bfs[3][1] is anAdd and\n            oppositeShifts2(bfs[0], bfs[2], bfs[4])):\n            return [(domain.scalemove2(bfs[0][2], bfs[1][2],\n                                       bfs[0][2] + bfs[2][2],\n                                       bfs[3][2]), aLoop, 0)]\n    return [(domain.loop(stripDomain(bfs)), aLoop, 0)]\n\n\nThis ends the bonus question. How do we optimize an unknown semantic domain?\nWe must maintain an abstract context which describes elements of the domain.\nIn initial encoding, we ask an AST about itself. In final encoding, we already\nknow everything relevant about the AST.\nThe careful reader will see that I didn't really answer that opening question\nin the JIT section. Because the JIT still ranges over the same operations as\nbefore, it can't really be slower; but why is it now faster? Because the\noptimizer is now slightly better in a few edge cases. It performs the same\noptimizations as before, but the rigor of abstract interpretation causes it to\nemit slightly better operations to the JIT backend.\nConcretely, improving the optimizer can shorten pretty-printed programs. The\nBusy Beaver Gauge measures the length of programs\nwhich search for solutions to mathematical problems. After implementing and\ndebugging the final-encoded interpreter, I found that two of my entries on the\nBusy Beaver Gauge for Brainfuck had\nbecome shorter by about 2%. (Most other entries are already hand-optimized\naccording to the standard algebra and have no optimization opportunities.)\nDiscussion\nGiven that initial and final encodings are equivalent, and noting that\nRPython's toolchain is written to prefer initial encodings, what did we\nactually gain? Did we gain anything?\nOne obvious downside to final encoding in RPython is interpreter size. The\nexample interpreter shown here is a rewrite of an initial-encoded interpreter\nwhich can be seen\nhere\nfor comparison. Final encoding adds about 20% more code in this case.\nFinal encoding is not necessarily more code than initial encoding, though. All\nAST encodings in interpreters are subject to the Expression\nProblem, which states that\nthere is generally a quadratic amount of code required to implement multiple\nbehaviors for an AST with multiple types of nodes; specifically, n behaviors\nfor m types of nodes require n \u00d7 m methods. Initial encodings improve the\ncost of adding new types of nodes; final encodings improve the cost of adding\nnew behaviors. Final encoding may tend to win in large codebases for mature\nlanguages, where the language does not change often but new behaviors are added\nfrequently and maintained for long periods.\nOptimizations in final encoding require a bit of planning. The\nabstract-interpretation approach is solid but relies upon the monoid and its\nalgebraic laws. In the worst case, an entire class hierarchy could be required\nto encode the abstraction.\nIt is remarkable to find a 2% improvement in residual program size merely\nby reimplementing an optimizer as an abstract interpreter respecting the\nalgebraic laws. This could be the most important lesson for compiler\nengineers, if it happens to generalize.\nFinal encoding was popularized via the tagless-final movement in OCaml and\nScala, including famously in a series of tutorials by Kiselyov et\nal. A \"tag\", in this jargon, is a\nruntime identifier for an object's type or class; a tagless encoding\neffectively doesn't allow isinstance() at all. In the above presentation,\ntags could be hacked in, but were not materially relevant to most steps. Tags\nwere required for the final evaluation step, though, and the tagless-final\ninsight is that certain type systems can express type-safe evaluation without\nthose tags. We won't go further in this direction because tags also\ncommunicate valuable information to the JIT.\nSummarizing Table\n\n\n\nInitial Encoding\nFinal Encoding\n\n\n\n\nhierarchy of classes\nsignature of interfaces\n\n\nclass constructors\nmethod calls\n\n\nbuilt on the heap\nbuilt on the stack\n\n\ntraversals allocate stack\ntraversals allocate heap\n\n\ntags are available with isinstance()\ntags are only available through hacks\n\n\ncost of adding a new AST node: one class\ncost of adding a new AST node: one method on every other class\n\n\ncost of adding a new behavior: one method on every other class\ncost of adding a new behavior: one class\n\n\n\nCredits\nThanks to folks in #pypy on Libera Chat: arigato for the idea, larstiq for\npushing me to write it up, and cfbolz and mattip for reviewing and finding\nmistakes. The original IRC discussion leading to this blog post is available\nhere.\nThis interpreter is part of the rpypkgs\nsuite, a Nix flake for RPython interpreters. Readers with Nix installed can\nrun this interpreter directly from the flake:\n$ nix-prefetch-url https://github.com/MG-K/pypy-tutorial-ko/raw/refs/heads/master/mandel.b\n$ nix run github:rpypkgs/rpypkgs#bf -- /nix/store/ngnphbap9ncvz41d0fkvdh61n7j2bg21-mandel.b",
      "tags": "guestpost",
      "url": "https://www.pypy.org/posts/2024/11/guest-post-final-encoding-in-rpython.html"
    },
    {
      "title": "A DSL for Peephole Transformation Rules of Integer Operations in the PyPy JIT",
      "text": "As is probably apparent from the sequence of blog posts about the topic in the\nlast year, I have been thinking about and working on integer optimizations in the JIT\ncompiler a lot. This work was mainly motivated by Pydrofoil, where integer\noperations matter a lot more than for your typical Python program.\nIn this post I'll describe my most recent change, which is a new small domain\nspecific language that I implemented to specify peephole optimizations on\ninteger operations in the JIT.\nIt uses pattern matching to specify how (sequences of) integer operations\nshould be simplified and optimized. The rules are then compiled to\nRPython code that then becomes part of the JIT's optimization passes.\nTo make it less likely to introduce incorrect optimizations into the JIT, the\nrules are automatically proven correct with Z3 as part of the build process (for\na more hands-on intro to how that works you can look at the knownbits post).\nIn this blog post I want to motivate why I introduced the DSL and give an\nintroduction to how it works.\n\nMotivation\nThis summer, after I wrote my scripts to mine JIT traces for missed optimization\nopportunities, I started implementing a few of the integer peephole rewrite that\nthe script identified. Unfortunately, doing so led to the problem that the way\nwe express these rewrites up to now is very imperative and verbose. Here's a\nsnippet of RPython code that shows some rewrites for integer multiplication\n(look at the comments to see what the different parts actually do). You don't\nneed to understand the code in detail, but basically it's in very imperative\nstyle and there's quite a lot of boilerplate.\ndef optimize_INT_MUL(self, op):\n    arg0 = get_box_replacement(op.getarg(0))\n    b0 = self.getintbound(arg0)\n    arg1 = get_box_replacement(op.getarg(1))\n    b1 = self.getintbound(arg1)\n\n    if b0.known_eq_const(1):\n        # 1 * x == x\n        self.make_equal_to(op, arg1)\n    elif b1.known_eq_const(1):\n        # x * 1 == x\n        self.make_equal_to(op, arg0)\n    elif b0.known_eq_const(0) or b1.known_eq_const(0):\n        # 0 * x == x * 0 == 0\n        self.make_constant_int(op, 0)\n    else:\n        for lhs, rhs in [(arg0, arg1), (arg1, arg0)]:\n            lh_info = self.getintbound(lhs)\n            if lh_info.is_constant():\n                x = lh_info.get_constant_int()\n                if x & (x - 1) == 0:\n                    # x * (2 ** c) == x << c\n                    new_rhs = ConstInt(highest_bit(lh_info.get_constant_int()))\n                    op = self.replace_op_with(op, rop.INT_LSHIFT, args=[rhs, new_rhs])\n                    self.optimizer.send_extra_operation(op)\n                    return\n                elif x == -1:\n                    # x * -1 == -x\n                    op = self.replace_op_with(op, rop.INT_NEG, args=[rhs])\n                    self.optimizer.send_extra_operation(op)\n                    return\n            else:\n                # x * (1 << y) == x << y\n                shiftop = self.optimizer.as_operation(get_box_replacement(lhs), rop.INT_LSHIFT)\n                if shiftop is None:\n                    continue\n                if not shiftop.getarg(0).is_constant() or shiftop.getarg(0).getint() != 1:\n                    continue\n                shiftvar = get_box_replacement(shiftop.getarg(1))\n                shiftbound = self.getintbound(shiftvar)\n                if shiftbound.known_nonnegative() and shiftbound.known_lt_const(LONG_BIT):\n                    op = self.replace_op_with(\n                            op, rop.INT_LSHIFT, args=[rhs, shiftvar])\n                    self.optimizer.send_extra_operation(op)\n                    return\n        return self.emit(op)\n\nAdding more rules to these functions is very tedious and gets super confusing\nwhen the functions get bigger. In addition I am always worried about making\nmistakes when writing this kind of code, and there is no feedback at all about\nwhich of these rules are actually applied a lot in real programs.\nTherefore I decided to write a small domain specific language with the goal of\nexpressing these rules in a more declarative way. In the rest of the post I'll\ndescribe the DSL (most of that description is adapted from the documentation\nabout it that I wrote).\n\n\nThe Peephole Rule DSL\n\nSimple transformation rules\nThe rules in the DSL specify how integer operation can be transformed into\ncheaper other integer operations. A rule always consists of a name, a pattern,\nand a target. Here's a simple rule:\nadd_zero: int_add(x, 0)\n    => x\nThe name of the rule is add_zero. It matches operations in the trace of the\nform int_add(x, 0), where x will match anything and 0 will match only the\nconstant zero. After the => arrow is the target of the rewrite, i.e. what the\noperation is rewritten to, in this case x.\nThe rule language has a list of which of the operations are commutative, so add_zero\nwill also optimize int_add(0, x) to x.\nVariables in the pattern can repeat:\nsub_x_x: int_sub(x, x)\n    => 0\nThis rule matches against int_sub operations where the two arguments are the\nsame (either the same box, or the same constant).\nHere's a rule with a more complicated pattern:\nsub_add: int_sub(int_add(x, y), y)\n    => x\nThis pattern matches int_sub operations, where the first argument was\nproduced by an int_add operation. In addition, one of the arguments of the\naddition has to be the same as the second argument of the subtraction.\nThe constants MININT, MAXINT and LONG_BIT (which is either 32 or 64,\ndepending on which platform the JIT is built for) can be used in rules, they\nbehave like writing numbers but allow bit-width-independent formulations:\nis_true_and_minint: int_is_true(int_and(x, MININT))\n    => int_lt(x, 0)\nIt is also possible to have a pattern where some arguments needs to be a\nconstant, without specifying which constant. Those patterns look like this:\nsub_add_consts: int_sub(int_add(x, C1), C2) # incomplete\n    # more goes here\n    => int_sub(x, C)\nVariables in the pattern that start with a C match against constants only.\nHowever, in this current form the rule is incomplete, because the variable C\nthat is being used in the target operation is not defined anywhere. We will see\nhow to compute it in the next section.\n\n\nComputing constants and other intermediate results\nSometimes it is necessary to compute intermediate results that are used in the\ntarget operation. To do that, there can be extra assignments between the rule head\nand the rule target.:\nsub_add_consts: int_sub(int_add(x, C1), C2) # incomplete\n    C = C1 + C2\n    => int_sub(x, C)\nThe right hand side of such an assignment is a subset of Python syntax,\nsupporting arithmetic using +, -, *, and certain helper functions.\nHowever, the syntax allows you to be explicit about unsignedness for some\noperations. E.g. >>u exists for unsigned right shifts (and I plan to add\n>u, >=u, <u, <=u for comparisons).\nHere's an example of a rule that uses >>u:\nurshift_lshift_x_c_c: uint_rshift(int_lshift(x, C), C)\n    mask = (-1 << C) >>u C\n    => int_and(x, mask)\n\n\nChecks\nSome rewrites are only true under certain conditions. For example,\nint_eq(x, 1) can be rewritten to x, if x is known to store a boolean value. This can\nbe expressed with checks:\neq_one: int_eq(x, 1)\n    check x.is_bool()\n    => x\nA check is followed by a boolean expression. The variables from the pattern can\nbe used as IntBound instances in checks (and also in assignments) to find out\nwhat the abstract interpretation of the JIT knows about the value of a trace variable\n(IntBound is the name of the abstract domain that the JIT uses for integers,\ndespite the fact that it also stores knownbits information nowadays).\nHere's another example:\nmul_lshift: int_mul(x, int_lshift(1, y))\n    check y.known_ge_const(0) and y.known_le_const(LONG_BIT)\n    => int_lshift(x, y)\nIt expresses that x * (1 << y) can be rewritten to x << y but checks that\ny is known to be between 0 and LONG_BIT.\nChecks and assignments can be repeated and combined with each other:\nmul_pow2_const: int_mul(x, C)\n    check C > 0 and C & (C - 1) == 0\n    shift = highest_bit(C)\n    => int_lshift(x, shift)\nIn addition to calling methods on IntBound instances, it's also possible to\naccess their attributes, like in this rule:\nand_x_c_in_range: int_and(x, C)\n    check x.lower >= 0 and x.upper <= C & ~(C + 1)\n    => x\n\n\nRule Ordering and Liveness\nThe generated optimizer code will give preference to applying rules that\nproduce a constant or a variable as a rewrite result. Only if none of those\nmatch do rules that produce new result operations get applied. For example, the\nrules sub_x_x and sub_add are tried before trying sub_add_consts,\nbecause the former two rules optimize to a constant and a variable\nrespectively, while the latter produces a new operation as the result.\nThe rule sub_add_consts has a possible problem, which is that if the\nintermediate result of the int_add operation in the rule head is used by\nsome other operations, then the sub_add_consts rule does not actually\nreduce the number of operations (and might actually make things slightly worse\ndue to increased register pressure). However, currently it would be extremely\nhard to take that kind of information into account in the optimization pass of\nthe JIT, so we optimistically apply the rules anyway.\n\n\nChecking rule coverage\nEvery rewrite rule should have at least one unit test where it triggers. To\nensure this, the unit test file that mainly checks integer optimizations in the\nJIT has an assert at the end of a test run, that every rule fired at least once.\n\n\nPrinting rule statistics\nThe JIT can print statistics about which rule fired how often in the\njit-intbounds-stats logging category, using the PYPYLOG mechanism. For\nexample, to print the category to stdout at the end of program execution, run\nPyPy like this:\nPYPYLOG=jit-intbounds-stats:- pypy ...\nThe output of that will look something like this:\nint_add\n    add_reassoc_consts 2514\n    add_zero 107008\nint_sub\n    sub_zero 31519\n    sub_from_zero 523\n    sub_x_x 3153\n    sub_add_consts 159\n    sub_add 55\n    sub_sub_x_c_c 1752\n    sub_sub_c_x_c 0\n    sub_xor_x_y_y 0\n    sub_or_x_y_y 0\nint_mul\n    mul_zero 0\n    mul_one 110\n    mul_minus_one 0\n    mul_pow2_const 1456\n    mul_lshift 0\n...\n\n\nTermination and Confluence\nRight now there are unfortunately no checks that the rules actually rewrite\noperations towards \"simpler\" forms. There is no cost model, and also nothing\nthat prevents you from writing a rule like this:\nneg_complication: int_neg(x) # leads to infinite rewrites\n    => int_mul(-1, x)\nDoing this would lead to endless rewrites if there is also another rule that\nturns multiplication with -1 into negation.\nThere is also no checking for confluence (yet?), i.e. the property that all\nrewrites starting from the same input trace always lead to the same output\ntrace, no matter in which order the rules are applied.\n\n\nProofs\nIt is very easy to write a peephole rule that is not correct in all corner\ncases. Therefore all the rules are proven correct with Z3 before compiled into\nactual JIT code, by default. When the proof fails, a (hopefully minimal)\ncounterexample is printed. The counterexample consists of values for all the\ninputs that fulfil the checks, values for the intermediate expressions, and\nthen two different values for the source and the target operations.\nE.g. if we try to add the incorrect rule:\nmul_is_add: int_mul(a, b)\n    => int_add(a, b)\nWe get the following counterexample as output:\nCould not prove correctness of rule 'mul_is_add'\nin line 1\ncounterexample given by Z3:\ncounterexample values:\na: 0\nb: 1\noperation int_mul(a, b) with Z3 formula a*b\nhas counterexample result vale: 0\nBUT\ntarget expression: int_add(a, b) with Z3 formula a + b\nhas counterexample value: 1\nIf we add conditions, they are taken into account and the counterexample will\nfulfil the conditions:\nmul_is_add: int_mul(a, b)\n    check a.known_gt_const(1) and b.known_gt_const(2)\n    => int_add(a, b)\nThis leads to the following counterexample:\nCould not prove correctness of rule 'mul_is_add'\nin line 46\ncounterexample given by Z3:\ncounterexample values:\na: 2\nb: 3\noperation int_mul(a, b) with Z3 formula a*b\nhas counterexample result vale: 6\nBUT\ntarget expression: int_add(a, b) with Z3 formula a + b\nhas counterexample value: 5\nSome IntBound methods cannot be used in Z3 proofs because their control\nflow is too complex. If that is the case, they can have Z3-equivalent\nformulations defined (in every case this is done, it's a potential proof hole if\nthe Z3 friendly reformulation and the real implementation differ from each\nother, therefore extra care is required to make very sure they are equivalent).\nIt's possible to skip the proof of individual rules entirely by adding\nSORRY_Z3 to its body (but we should try not to do that too often):\neq_different_knownbits: int_eq(x, y)\n    SORRY_Z3\n    check x.known_ne(y)\n    => 0\n\n\nChecking for satisfiability\nIn addition to checking whether the rule yields a correct optimization, we also\ncheck whether the rule can ever apply. This ensures that there are some\nruntime values that would fulfil all the checks in a rule. Here's an example of\na rule violating this:\nnever_applies: int_is_true(x)\n    check x.known_lt_const(0) and x.known_gt_const(0) # impossible condition, always False\n    => x\nRight now the error messages if this goes wrong are not completely easy to\nunderstand. I hope to be able to improve this later:\nRule 'never_applies' cannot ever apply\nin line 1\nZ3 did not manage to find values for variables x such that the following condition becomes True:\nAnd(x <= x_upper,\n    x_lower <= x,\n    If(x_upper < 0, x_lower > 0, x_upper < 0))\n\n\nImplementation Notes\nThe implementation of the DSL is done in a relatively ad-hoc manner. It is\nparsed using rply, there's a small type checker that tries to find common\nproblems in how the rules are written. Z3 is used via the Python API, like in\nthe previous blog posts that are using it. The\npattern matching RPython code is generated using an approach inspired by Luc\nMaranget's paper Compiling Pattern Matching to Good Decision Trees. See\nthis blog post for an approachable introduction.\n\n\n\nConclusion\nNow that I've described the DSL, here are the rules that are equivalent to the\nimperative code in the motivation section:\nmul_zero: int_mul(x, 0)\n    => 0\n\nmul_one: int_mul(x, 1)\n    => x\n\nmul_minus_one: int_mul(x, -1)\n    => int_neg(x)\n\nmul_pow2_const: int_mul(x, C)\n    check C > 0 and C & (C - 1) == 0\n    shift = highest_bit(C)\n    => int_lshift(x, shift)\n\nmul_lshift: int_mul(x, int_lshift(1, y))\n    check y.known_ge_const(0) and y.known_le_const(LONG_BIT)\n    => int_lshift(x, y)\nThe current status of the DSL is that it got merged to PyPy's main branch. I\nrewrote a part of the integer rewrites into the DSL, but some are still in the\nold imperative style (mostly for complicated reasons, the easily ported ones are\nall done). Since I've only been porting optimizations that had existed prior to\nthe existence of the DSL, performance numbers of benchmarks didn't change.\nThere are a number of features that are still missing and some possible\nextensions that I plan to work on in the future:\n\nAll the integer operations that the DSL handles so far are the variants that\ndo not check for overflow (or where overflow was proven to be impossible to\nhappen). In regular Python code the overflow-checking variants int_add_ovf\netc are much more common, but the DSL doesn't support them yet. I plan to fix\nthis, but don't completely understand how the correctness proofs for them\nshould be done correctly.\nA related problem is that I don't understand what it means for a rewrite to be\ncorrect if some of the operations are only defined for a subset of the input\nvalues. E.g. division isn't defined if the divisor is zero. In theory, a\ndivision operation in the trace should always be preceded by a check that the\ndivisor isn't zero. But sometimes other optimization move the check around and\nthe connection to the division gets lost or muddled. What optimizations can we\nstill safely perform on the division? There's lots of prior work on this\nquestion, but I still don't understand what the correct approach in our\ncontext would be.\nOrdering comparisons like int_lt, int_le and their unsigned variants are\nnot ported to the DSL yet. Comparisons are an area where the JIT is not super\ngood yet at optimizing away operations. This is a pretty big topic and I've\nstarted a project with Nico Rittinghaus to try to improve the situation a bit\nmore generally.\nA more advanced direction of work would be to implement a simplified form of\ne-graphs (or ae-graphs). The JIT has like half of an e-graph data\nstructure already, and we probably can't afford a full one in terms of compile\ntime costs, but maybe we can have two thirds or something?\n\n\n\nAcknowledgements\nThank you to Max Bernstein and Martin Berger for super helpful feedback on\ndrafts of the post!",
      "tags": "jit,z3",
      "url": "https://www.pypy.org/posts/2024/10/jit-peephole-dsl.html"
    },
    {
      "title": "Guest Post: How PortaOne uses PyPy for high-performance processing, connecting over 1B of phone calls every month",
      "text": "The PyPy project is always happy to hear about industrial use  and deployments\nof PyPy. For the GC bug\nfinding\ntask earlier this year, we collaborated with PortaOne and we're super happy\nthat Serhii Titov, head of the QA department at PortaOne, was up to writing\nthis guest post to describe their use and experience with the project.\n\nWhat does PortaOne do?\nWe at PortaOne Inc. allow telecom operators to\nlaunch new services (or provide existing services more efficiently) using our\nVoIP platform (PortaSIP) and our real-time charging system (PortaBilling),\nwhich provides additional features for cloud PBX, such as call transfer,\nqueues, interactive voice response (IVR) and more. At this moment our support\nteam manages several thousand servers with our software installed in 100\ncountries, through which over 500 telecommunication service providers connect\nmillions of end users every day. The unique thing about PortaOne is that we\nsupply the source code of our product to our customers - something unheard of\nin the telecom world! Thus we attract \"telco innovators\", who use our APIs to\nbuild around the system and the source code to create unique tweaks of\nfunctionality, which produces amazing products.\nAt the core of PortaSIP is the middle-ware component (the proper name for it is\n\"B2BUA\", but that probably does not say much to anyone outside of experts in\nVoIP), which implements the actual handling of SIP calls, messages, etc. and\nall added features (for instance, trying to send a call via telco operators\nthrough which the cost per minute is lower). It has to be fast (since even a\nsmall delay in establishing a call is noticed by a customer), reliable\n(everyone hates when a call drops or cannot be completed) and yet easily\nexpandable with new functionality. This is why we decided to use Python as\nopposed to C/C++ or similar programming languages, which are often used in\ntelecom equipment.\nThe B2BUA component is a batch of similar Python processes that are looped\ninside a\nasyncore.dispatcher\nwrapper. The load balancing between these Python processes is done by our\nstateless SIP proxy server written in C++. All our sockets are served by this\nB2BUA. We have our custom client-wrappers around pymysql, redis,\ncassandra-driver and requests to communicate with external services. Some\nof the Python processes use cffi\nwrappers around C-code to improve their performance (examples: an Oracle DB\ndriver, a client to a radius server, a custom C logger).\nThe I/O operations that block the main thread of the Python processes are\nprocessed in sub-threads. We have custom wrappers  around threading.Thread\nand also asyncore.dispatcher. The results of such operations are returned to\nthe main thread.\nImproving our performance with PyPy\nWe started with CPython and then in 2014 switched to PyPy because it was\nfaster. Here's an exact quote from our first testing notes: \"PyPy gives\nsignificant performance boost, ~50%\". Nowadays, after years of changes in all\nthe software involved, PyPy still gives us +50% boost compared to CPython.\nTaking care of real time traffic for so many people around the globe is\nsomething we're really proud of. I hope the PyPy team can be proud of it as\nwell, as the PyPy product is a part of this solution.\nFinding a garbage collector bug: stage 1, the GC hooks\nHowever our path with PyPy wasn't perfectly smooth. There were very rare cases\nof crashes on PyPy that we weren't able to catch. That's because to make\ncoredump useful we needed to switch to PyPy with debug, but we cannot let it\nrun in that mode on a production system for an extended period of time, and we\ndid not have any STR (steps-to-reproduce) to make PyPy crash again in our lab.\nThat's why we kept (and still keep) both interpreters installed just in case,\nand we would switch to CPython if we noticed it happening.\nAt the time of updating PyPy from 3.5 to 3.6 our QA started noticing those\ncrashes more often, but we still had no luck with STR or collecting proper\ncoredumps with debug symbols. Then it became even worse after our development\nplayed with the Garbage Collector's\noptions to increase performance\nof our middleware component. The crashes started to affect our regular\nperformance testing (controlled by QA manager Yevhenii Bovda). At that point it\nwas decided that we can no longer live like that and so we started an intense\ninvestigation.\nDuring the first stage of our investigation (following the best practice of\ntroubleshooting) we narrowed down the issue as much as we could. So, it was not\nour code, it was definitely somewhere in PyPy. Eventually our SIP software\nengineer Yevhenii Yatchenko found out\nthat this bug is connected with the use of our custom hooks in the\nGC. Yevhenii created\nticket #4899 and within 2-3 days we\ngot a fix from a member of the PyPy team, in true open-source fashion.\nFinding a garbage collector bug: stage 2, the real bug\nThen came stage 2. In parallel with the previous ticket, Yevhenii created\n#4900 that we still see failing\nwith coredumps quite often, and they are not connected to GC custom hooks. In a\nnutshell, it took us dozens of back and forward emails, three Zoom sessions and\nfour versions of a patch to solve the issue. During the last iteration we got a\nnew set of options to try and a new version of the patch. Surprisingly, that\nhelped! What a relief! So, the next logical step was to remove all debug\noptions and run PyPy only with the patch. Unfortunately, it started to fail\nagain and we came to the obvious conclusion that what will help us is not a\npatch, but one of options we were testing out. At that point we found out that\nPYPY_GC_MAX_PINNED=0\nis a necessary and sufficient condition to solve our issue. This points to\nanother bug in the garbage collector, somehow related to object pinning.\nHere's our current state: we have to add PYPY_GC_MAX_PINNED=0, but we do not\nface the crashes anymore.\nConclusion and next steps\nGratitude is extended to Carl for his invaluable assistance in resolving the\nnasty bugss, because it seems we're the only ones who suffered from the last\none and we really did not want to fall back to CPython due to its performance\ndisadvantage.\nSerhii Titov, head of the QA department at PortaOne Inc.\nP.S. If you are a perfectionist and at this point you have mixed feelings and\nyou are still bothered by the question \"But there might still be a bug in the\nGC, what about that?\" - Carl has some ideas about it and he will sort it out\n(we will help with the testing/verification part).",
      "tags": "casestudy,guestpost",
      "url": "https://www.pypy.org/posts/2024/08/portaone.html"
    },
    {
      "title": "PyPy v7.3.17 release",
      "text": "PyPy v7.3.17: release of python 2.7 and 3.10\nThe PyPy team is proud to release version 7.3.17 of PyPy.\nThis release includes a new RISC-V JIT backend, an improved REPL based on\nwork by the CPython team, and better JIT optimizations of integer\noperations. Special shout-outs to Logan Chien for the RISC-V backend\nwork, to Nico Rittinghaus for better integer optimization in the JIT, and\nthe CPython team that has worked on the repl.\nThe release includes two different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.10, which is an interpreter supporting the syntax and the features of\nPython 3.10, including the stdlib for CPython 3.10.14.\n\nThe interpreters are based on much the same codebase, thus the dual\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. It follows after 7.3.16 release on April 23, 2024.\nWe recommend updating. You can find links to download the releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear\nabout it and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with\nmaking RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy. In any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nRISC-V backend for the JIT\nPyPy's JIT has added support for generating 64-bit RISC-V machine code at\nruntime (RV64-IMAD, specifically). So far we are not releasing binaries for any\nRISC-V platforms, but there are instructions on how to cross-compile binaries.\n\n\nREPL Improvements\nThe biggest user-visible change of the release is new features in the repl of\nPyPy3.10. CPython 3.13 has adopted and extended PyPy's pure-Python repl, adding\na number of features and fixing a number or bugs in the process. We have\nbackported and added the following features:\n\nPrompts and tracebacks use terminal colors, as well as terminal hyperlinks\nfor file names.\nBracketed paste enable pasting several lines of input into the terminal\nwithout auto-indentation getting in the way.\nA special interactive help browser (F1), history browser (F2), explicit paste\nmode (F3).\nSupport for Ctrl-<left/right> to jump over whole words at a time.\n\nSee the CPython documentation for further details. Thanks to \u0141ukasz Langa,\nPablo Galindo Salgado and the other CPython devs involved in this work.\n\n\nBetter JIT optimizations of integer operations\nThe optimizers of PyPy's JIT have become much better at reasoning about and\noptimizing integer operations. This is done with a new \"knownbits\" abstract\ndomain. In many programs that do bit-manipulation of integers, some of the\nbits of the integer variables of the program can be statically known. Here's a\nsimple example:\nx = a | 1\n...\nif x & 1:\n    ...\nelse:\n    ...\n\nWith the new abstract domain, the JIT can optimize the if-condition to\nTrue, because it already knows that the lowest bit of x must be set.\nThis optimization applies to all Python-integers that fit into a machine word\n(PyPy optimistically picks between two different representations for int,\ndepending on the size of the value). Unfortunately there is very little impact\nof this change on almost all Python code, because intensive bit-manipulation is\nrare in Python. However, the change leads to significant performance\nimprovements in Pydrofoil (the RPython-based RISC-V/ARM emulators that are\nautomatically generated from high-level Sail specifications of the respective\nISAs, and that use the RPython JIT to improve performance).\n\n\nPyPy versions and speed.pypy.org\nThe keen-eyed will have noticed no mention of Python version 3.9 in the\nreleases above. Typically we will maintain only one version of Python3, but due\nto PyPy3.9 support on conda-forge we maintained multiple versions from the\nfirst release of PyPy3.10 in PyPy v7.3.12 (Dec 2022). Conda-forge is\nsunsetting its PyPy support, which means we can drop PyPy3.9. Since that was\nthe major driver of benchmarks at https://speed.pypy.org, we revamped the site\nto showcase PyPy3.9, PyPy3.10, and various versions of cpython on the home\npage. For historical reasons, the \"baseline\" for comparison is still cpython\n3.7.19.\nWe will keep the buildbots building PyPY3.9 until the end of August, these\nbuilds will still be available on the nightly builds tab of the buildbot.\n\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython\nIt's fast (PyPy and CPython performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64) and macos (macos_arm64).\n\nPyPy supports Windows 32-bit, Linux PPC64 big- and little-endian, Linux ARM\n32 bit, RISC-V RV64IMAFD Linux, and s390x Linux but does not release binaries.\nPlease reach out to us if you wish to sponsor binary releases for those\nplatforms. Downstream packagers provide binary builds for debian, Fedora,\nconda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.17 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2024/08/pypy-v7317-release.html"
    },
    {
      "title": "Conda-forge proposes sunsetting support for PyPy",
      "text": "Conda-forge has kindly been providing support for PyPy since 2019. The\nconda-forge team has been very patient and generous with resources, but it\nseems the uptake of PyPy has not justified the effort. Major packages still\nare not available on PyPy,\nothers find it hard to update\nversions. We don't\nget much feedback at all about people using PyPy, and even less about PyPy on\nconda-forge. The conda-forge team has proposed sunsetting\nPyPy going\nforward, which means current packages would remain but no new packages would be\nbuilt. If you have an opinion, you can comment on that PR, or on this blog post.\nSince conda-forge supports PyPy3.9 but not PyPy3.10, we have continued\nreleasing PyPy3.9 even though we typically support only one version of PyPy3.\nWith the sunsetting proposal, we will not release any more updates to PyPy3.9.\nI opened a poll about the\nintention to drop PyPy3.9. If you have an opinion, please chime in.",
      "tags": "conda-forge",
      "url": "https://www.pypy.org/posts/2024/08/conda-forge-proposes-dropping-support-for-pypy.html"
    },
    {
      "title": "A Knownbits Abstract Domain for the Toy Optimizer, Correctly",
      "text": "After Max' introduction to abstract interpretation for the toy optimizer in the\nlast post, I want to present a more complicated abstract domain in this post.\nThis abstract domain reasons about the individual bits of a variable in a trace.\nEvery bit can be either \"known zero\", \"known one\" or \"unknown\". The abstract\ndomain is useful for optimizing integer operations, particularly the bitwise operations.\nThe abstract domain follows quite closely the tristate abstract domain of the\neBPF verifier in the Linux\nKernel, as\ndescribed by the paper\nSound, Precise, and Fast Abstract Interpretation with Tristate\nNumbers by Harishankar Vishwanathan, Matan\nShachnai, Srinivas Narayana, and Santosh Nagarakatte.\nThe presentation in this post will still be in the context of the\ntoy optimizer. We'll spend a significant part of\nthe post convincing ourselves that the abstract domain transfer functions that\nwe're writing are really correct, using both property-based testing and\nautomated proofs (again using Z3).\nPyPy has implemented and merged a more complicated version of the same abstract\ndomain for the \"real\" PyPy JIT. A more thorough explanation of that real world\nimplementation will follow.\nI'd like to thank Max Bernstein and Armin Rigo for lots of great feedback on\ndrafts of this post. The PyPy implementation was mainly done by Nico\nRittinghaus and me.\nContents:\n\n\nMotivation\nThe Knownbits Abstract Domain\nTransfer Functions\nProperty-based Tests with Hypothesis\nWhen are Transfer Functions Correct? How do we test them?\nImplementing Binary Transfer Functions\nAddition and Subtraction\nProving correctness of the transfer functions with Z3\nCases where this style of Z3 proof doesn't work\nMaking Statements about Precision\nUsing the Abstract Domain in the Toy Optimizer for Generalized Constant Folding\nUsing the KnownBits Domain for Conditional Peephole Rewrites\nConclusion\n\n\nMotivation\nIn many programs that do bit-manipulation of integers, some of the bits of the\ninteger variables of the program can be statically known. Here's a simple\nexample:\nx = a | 1\n...\nif x & 1:\n    ...\nelse:\n    ...\n\n\nAfter the assignment x = a | 1, we know that the lowest bit of x must be 1\n(the other bits are unknown) and an optimizer could remove the condition x & 1 by\nconstant-folding it to 1.\nAnother (more complicated) example is:\nassert i & 0b111 == 0 # check that i is a multiple of 8\nj = i + 16\nassert j & 0b111 == 0\n\n\nThis kind of code could e.g. happen in a CPU\nemulator, where i and j are\nintegers that represent emulated pointers, and the asserts are alignment\nchecks. The first assert implies that the lowest three bits of i must be 0.\nAdding 16 to such a number produces a result where the lowest three bits are\nagain all 0, therefore the second assert is always true. So we would like a\ncompiler to remove the second assert.\nBoth of these will optimizations are doable with the help of the knownbits\nabstract domain that we'll discuss in the rest of the post.\nThe Knownbits Abstract Domain\nAn abstract value of the knownbits domain needs to be able to store, for every\nbit of an integer variable in a program, whether it is known 0, known 1, or\nunknown. To represent\nthree different states, we need 2 bits, which we will call one and unknown.\nHere's the encoding:\n\n\n\none\nunknown\nknownbit\n\n\n\n\n0\n0\n0\n\n\n1\n0\n1\n\n\n0\n1\n?\n\n\n1\n1\nillegal\n\n\n\nThe unknown bit is set if we don't know the value of the bit (\"?\"), the one\nbit is set if the bit is known to be a 1. Since two bits are enough to encode\nfour different states, but we only need three, the combination of a set one\nbit and a set unknown is not allowed.\nWe don't just want to encode a single bit, however. Instead, we want to do this\nfor all the bits of an integer variable. Therefore the instances of the abstract\ndomain get two integer fields ones and unknowns, where each pair of\ncorresponding bits encodes the knowledge about the corresponding bit of the\ninteger variable in the program.\nWe can start implementing a Python class that works like this:\nfrom dataclasses import dataclass\n\n@dataclass(eq=False)\nclass KnownBits:\n    ones : int\n    unknowns : int\n\n    def __post_init__(self):\n        if isinstance(self.ones, int):\n            assert self.is_well_formed()\n\n    def is_well_formed(self):\n        # a bit cannot be both 1 and unknown\n        return self.ones & self.unknowns == 0\n\n    @staticmethod\n    def from_constant(const : int):\n        \"\"\" Construct a KnownBits corresponding to a constant, where all bits\n        are known.\"\"\"\n        return KnownBits(const, 0)\n\n    def is_constant(self):\n        \"\"\" Check if the KnownBits instance represents a constant. \"\"\"\n        # it's a constant if there are no unknowns\n        return self.unknowns == 0\n\n\nWe can also add some convenience properties. Sometimes it is easier to work\nwith an integer where all the known bits are set, or one where the positions\nof all the known zeros have a set bit:\nclass KnownBits:\n    ...\n\n    @property\n    def knowns(self):\n        \"\"\" return an integer where the known bits are set. \"\"\"\n        # the knowns are just the unknowns, inverted\n        return ~self.unknowns\n\n    @property\n    def zeros(self):\n        \"\"\" return an integer where the places that are known zeros have a bit\n        set. \"\"\"\n        # it's a 0 if it is known, but not 1\n        return self.knowns & ~self.ones\n\n\nAlso, for debugging and for writing tests we want a way to print the known bits\nin a human-readable form, and also to have a way to construct a KnownBits\ninstance from a string. It's not important to understand the details of\n__str__ or from_str for the rest of the post, so I'm putting them into a fold:\n\nKnownBits from and to string conversions\n\n\nclass KnownBits:\n    ...\n\n    def __repr__(self):\n        if self.is_constant():\n            return f\"KnownBits.from_constant({self.ones})\"\n        return f\"KnownBits({self.ones}, {self.unknowns})\"\n\n    def __str__(self):\n        res = []\n        ones, unknowns = self.ones, self.unknowns\n        # construct the string representation right to left\n        while 1:\n            if not ones and not unknowns:\n                break # we leave off the leading known 0s\n            if ones == -1 and not unknowns:\n                # -1 has all bits set in two's complement, so the leading\n                # bits are all 1\n                res.append('1')\n                res.append(\"...\")\n                break\n            if unknowns == -1:\n                # -1 has all bits set in two's complement, so the leading bits\n                # are all ?\n                assert not ones\n                res.append(\"?\")\n                res.append(\"...\")\n                break\n            if unknowns & 1:\n                res.append('?')\n            elif ones & 1:\n                res.append('1')\n            else:\n                res.append('0')\n            ones >>= 1\n            unknowns >>= 1\n        if not res:\n            res.append('0')\n        res.reverse()\n        return \"\".join(res)\n\n    @staticmethod\n    def from_str(s):\n        \"\"\" Construct a KnownBits instance that from a string. String can start\n        with ...1 to mean that all higher bits are 1, or ...? to mean that all\n        higher bits are unknown. Otherwise it is assumed that the higher bits\n        are all 0. \"\"\"\n        ones, unknowns = 0, 0\n        startindex = 0\n        if s.startswith(\"...?\"):\n            unknowns = -1\n            startindex = 4\n        elif s.startswith(\"...1\"):\n            ones = -1\n            startindex = 4\n        for index in range(startindex, len(s)):\n            ones <<= 1\n            unknowns <<= 1\n            c = s[index]\n            if c == '1':\n                ones |= 1\n            elif c == '?':\n                unknowns |= 1\n        return KnownBits(ones, unknowns)\n\n    @staticmethod\n    def all_unknown():\n        \"\"\" convenience constructor for the \"all bits unknown\" abstract value\n        \"\"\"\n        return KnownBits.from_str(\"...?\")\n\n\n\n\n\n\nAnd here's a pytest-style unit test for str:\ndef test_str():\n    assert str(KnownBits.from_constant(0)) == '0'\n    assert str(KnownBits.from_constant(5)) == '101'\n    assert str(KnownBits(5, 0b10)) == '1?1'\n    assert str(KnownBits(~0b1111, 0b10)) == '...100?0'\n    assert str(KnownBits(1, ~0b1)) == '...?1'\n\n\nAn instance of KnownBits represents a set of integers, namely those that match\nthe known bits stored in the instance. We can write a method contains that\ntakes a concrete int value and returns True if the value matches the\npattern of the known bits:\nclass KnownBits:\n    ...\n\n    def contains(self, value : int):\n        \"\"\" Check whether the KnownBits instance contains the concrete integer\n        `value`. \"\"\"\n        # check whether value matches the bit pattern. in the places where we\n        # know the bits, the value must agree with ones.\n        return value & self.knowns == self.ones\n\n\nand a test:\ndef test_contains():\n    k1 = KnownBits.from_str('1?1')\n    assert k1.contains(0b111)\n    assert k1.contains(0b101)\n    assert not k1.contains(0b110)\n    assert not k1.contains(0b011)\n\n    k2 = KnownBits.from_str('...?1') # all odd numbers\n    for i in range(-101, 100):\n        assert k2.contains(i) == (i & 1)\n\n\nTransfer Functions\nNow that we have implemented the basics of the KnownBits class, we need to\nstart implementing the transfer functions. They are for computing what we know\nabout the results of an operation, given the knowledge we have about the bits\nof the arguments.\nWe'll start with a simple unary operation, invert(x) (which is ~x in Python\nand C syntax), which flips all the bits of at integer. If we know some bits of\nthe arguments, we can compute the corresponding bits of the result. The unknown\nbits remain unknown.\nHere's the code:\nclass KnownBits:\n    ...\n\n    def abstract_invert(self):\n        # self.zeros has bits set where the known 0s are in self\n        return KnownBits(self.zeros, self.unknowns)\n\n\nAnd a unit-test:\ndef test_invert():\n    k1 = KnownBits.from_str('01?01?01?')\n    k2 = k1.abstract_invert()\n    assert str(k2) == '...10?10?10?'\n\n    k1 = KnownBits.from_str('...?')\n    k2 = k1.abstract_invert()\n    assert str(k2) == '...?'\n\n\nBefore we continue with further transfer functions, we'll think about\ncorrectness of the transfer functions and build up some test infrastructure. To\ntest transfer functions, it's quite important to move being simple example-style\nunit tests. The state-space for more complicated binary transfer functions is\nextremely large and it's too easy to do something wrong in a corner case.\nTherefore we'll look at property-based-test for KnownBits next.\nProperty-based Tests with Hypothesis\nWe want to do property-based tests of KnownBits, to try\nmake it less likely that we'll get a corner-case in the implementation wrong.\nWe'll use Hypothesis for that.\nI can't give a decent introduction to Hypothesis here, but want to give a few\nhints about the API. Hypothesis is a way to run unit tests with randomly\ngenerated input. It provides strategies to describe the data that the test\nfunctions expects. Hypothesis provides primitive strategies (for things like\nintegers, strings, floats, etc) and ways to build composite strategies out of\nthe primitive ones.\nTo be able to write the tests, we need to generate random KnownBits instances,\nand we also want an int instance that is a member of the KnownBits instance.\nWe generate tuples of (KnownBits, int) together, to ensure this property.\nWe'll ask Hypothesis to generate us a random concrete int as the concrete\nvalue, and then we'll also generate a second random int to use as the\nunknown masks (i.e. which bits of the concrete int we don't know in the\nKnownBits instance). Here's a function that takes two such ints and builds the\ntuple:\ndef build_knownbits_and_contained_number(concrete_value : int, unknowns : int):\n    # to construct a valid KnownBits instance, we need to mask off the unknown\n    # bits\n    ones = concrete_value & ~unknowns\n    return KnownBits(ones, unknowns), concrete_value\n\n\nWe can turn this function into a hypothesis strategy to generate input data\nusing the strategies.builds function:\nfrom hypothesis import strategies, given, settings\n\nints = strategies.integers()\n\nrandom_knownbits_and_contained_number = strategies.builds(\n    build_knownbits_and_contained_number,\n    ints, ints\n)\n\n\nOne important special case of KnownBits are the constants, which contain only\na single concrete value. We'll also generate some of those specifically, and\nthen combine the random_knownbits_and_contained_number strategy with it:\nconstant_knownbits = strategies.builds(\n    lambda value: (KnownBits.from_constant(value), value),\n    ints\n)\n\nknownbits_and_contained_number = constant_knownbits | random_knownbits_and_contained_number\n\n\nNow we can write the first property-based tests, for the KnownBits.contains\nmethod:\n@given(knownbits_and_contained_number)\ndef test_contains(t):\n    k, n = t\n    assert k.contains(t)\n\n\nThe @given decorator is used to tell Hypothesis which strategy to use to\ngenerate random data for the test function. Hypothesis will run the test with a\nnumber of random examples (100 by default). If it finds an error, it will try to\nminimize the example needed that demonstrates the problem, to try to make it\neasier to understand what is going wrong. It also saves all failing cases into\nan example database and tries them again on subsequent runs.\nThis test is as much a check for whether we got the strategies right as it is\nfor the logic in KnownBits.contains. Here's an example output of random\nconcrete and abstract values that we are getting here:\n110000011001101 ...?0???1\n...1011011 ...1011011\n...1001101110101000010010011111011 ...1001101110101000010010011111011\n...1001101110101000010010011111011 ...100110111010100001?010?1??1??11\n1000001101111101001011010011111101000011000111011001011111101 1000001101111101001011010011111101000011000111011001011111101\n1000001101111101001011010011111101000011000111011001011111101 1000001101111101001011010011111101000011000111????01?11?????1\n1111100000010 1111100000010\n1111100000010 ...?11111?00000??\n110110 110110\n110110 ...?00?00????11??10\n110110 ??0??0\n...100010111011111 ...?100?10111??111?\n...1000100000110001 ...?000?00000??000?\n110000001110 ...?0?0??000?00?0?0000000?00???0000?????00???000?0?00?01?000?0??1??\n110000001110 ??000000???0\n1011011010000001110101001111000010001001011101010010010001000000010101010010001101110101111111010101010010101100110000011110000 1011011010000001110101001111000010001001011101010010010001000000010101010010001101110101111111010101010010101100110000011110000\n...1011010010010100 ...1011010010010100\n...1011111110110011 ...1011111110110011\n101000011110110 101000011?10?1?\n100101 ?00?0?\n\n\nThat looks suitably random, but we might want to bias our random numbers a\nlittle bit towards common error values like small constants, powers of two, etc.\nLike this:\nINTEGER_WIDTH = 64\n# some small integers\nints_special = set(range(100))\n# powers of two\nints_special = ints_special.union(1 << i for i in range(INTEGER_WIDTH - 2))\n# powers of two - 1\nints_special = ints_special.union((1 << i) - 1 for i in range(INTEGER_WIDTH - 2))\n# negative versions of what we have so far\nints_special = ints_special.union(-x for x in ints_special)\n# bit-flipped versions of what we have so far\nints_special = ints_special.union(~x for x in ints_special)\nints_special = list(ints_special)\n# sort them (because hypothesis simplifies towards earlier elements in the list)\nints_special.sort(key=lambda element: (abs(element), element < 0))\n\nints = strategies.sampled_from(ints_special) | strategies.integers()\n\n\nNow we get data like this:\n1110 1110\n...10000000000000000001 ...10000??0??0000??00?1\n1 ??0??0000??00?1\n1 ?\n...10101100 ...10101100\n110000000011001010111011111111111111011110010001001100110001011 ...?0?101?\n110000000011001010111011111111111111011110010001001100110001011 ??00000000??00?0?0???0??????????????0????00?000?00??00??000?0??\n...1011111111111111111111111111 ...?11?11??\n...1011111111111111111111111111 ...?0??????????????????????????\n0 ...?0??????????????????????????\n101101 101101\n111111111111111111111111111111111111111111111 111111111111111111111111111111111111111111111\n10111 10111\n...101100 ...1?111011?0\n101000 ?001010?0\n101000 ?0?000\n110010 110010\n...100111 ...100111\n1111011010010 1111011010010\n...1000000000000000000000000000000000000 ...1000000000000000000000000000000000000\n\n\nWe can also write a test that checks that the somewhat tricky logic in\n__str__ and from_str is correct, by making sure that the two functions\nround-trip (ie converting a KnownBits to a string and then back to a\nKnownBits instance produces the same abstract value).\n@given(knownbits_and_contained_number)\ndef test_hypothesis_str_roundtrips(t1):\n    k1, n1 = t1\n    s = str(k1)\n    k2 = KnownBits.from_str(s)\n    assert k1.ones == k2.ones\n    assert k1.unknowns == k2.unknowns\n\n\nNow let's actually apply this infrastructure to test abstract_invert.\nWhen are Transfer Functions Correct? How do we test them?\nAbstract values, i.e. instances of KnownBits represent sets of concrete\nvalues. We want the transfer functions to compute overapproximations of the\nconcrete values. So if we have an arbitrary abstract value k, with a concrete\nnumber n that is a member of the abstract values (i.e.\nk.contains(n) == True) then the result of the concrete operation op(n)\nmust be a member of the result of the abstract operation k.abstract_op()\n(i.e. k.abstract_op().contains(op(n)) == True).\nChecking the correctness/overapproximation property is a good match for\nhypothesis. Here's what the test for abstract_invert looks like:\n@given(knownbits_and_contained_number)\ndef test_hypothesis_invert(t):\n    k1, n1 = t1\n    n2 = ~n1 # compute the real result\n    k2 = k1.abstract_invert() # compute the abstract result\n    assert k2.contains(n2) # the abstract result must contain the real result\n\n\nThis is the only condition needed for abstract_invert to be correct. If\nabstract_invert fulfils this property for every combination of abstract and\nconcrete value then abstract_invert is correct. Note however, that this test\ndoes not actually check whether abstract_invert gives us precise results. A\ncorrect (but imprecise) implementation of abstract_invert would simply return\na completely unknown result, regardless of what is known about the input\nKnownBits.\nThe \"proper\" CS term for this notion of correctness is called soundness. The\ncorrectness condition on the transfer functions is called a Galois\nconnection. I won't go into any mathematical/technical details here, but\nwanted to at least mention the terms. I found Martin\nKellogg's\nslides\nto be quite an approachable introduction to the Galois connection and how to\nshow soundness.\nImplementing Binary Transfer Functions\nNow we have infrastructure in place for testing transfer functions with random\ninputs. With that we can start thinking about the more complicated case, that of\nbinary operations. Let's start with the simpler ones, and and or. For and,\nwe can know a 0 bit in the result if either of the input bits are known 0;\nor we can know a 1 bit in the result if both input bits are known 1.\nOtherwise the resulting bit is unknown. Let's look at all the combinations:\nand\ninput1: 000111???\ninput2: 01?01?01? \nresult: 00001?0??\n\n\nclass KnownBits:\n    ...\n\n    def abstract_and(self, other):\n        ones = self.ones & other.ones # known ones\n        knowns = self.zeros | other.zeros | ones\n        return KnownBits(ones, ~knowns)\n\n\nHere's an example unit-test and a property-based test for and:\ndef test_and():\n    # test all combinations of 0, 1, ? in one example\n    k1 = KnownBits.from_str('01?01?01?')\n    k2 = KnownBits.from_str('000111???')\n    res = k1.abstract_and(k2)     # should be: 0...00001?0??\n    assert str(res) ==   \"1?0??\"\n\n@given(knownbits_and_contained_number, knownbits_and_contained_number)\ndef test_hypothesis_and(t1, t2):\n    k1, n1 = t1\n    k2, n2 = t2\n    k3 = k1.abstract_and(k2)\n    n3 = n1 & n2\n    assert k3.contains(n3)\n\n\nTo implement or is pretty similar. The result is known 1 where either of the\ninputs is 1. The result is known 0 where both inputs are known 0, and ?\notherwise.\nor\ninput1: 000111???\ninput2: 01?01?01? \nresult: 01?111?1?\n\n\nclass KnownBits:\n    ...\n\n    def abstract_or(self, other):\n        ones = self.ones | other.ones\n        zeros = self.zeros & other.zeros\n        knowns = ones | zeros\n        return KnownBits(ones, ~knowns)\n\n\nHere's an example unit-test and a property-based test for or:\ndef test_or():\n    k1 = KnownBits.from_str('01?01?01?')\n    k2 = KnownBits.from_str('000111???')\n    res = k1.abstract_or(k2)     # should be:  0...01?111?1?\n    assert str(res) ==   \"1?111?1?\"\n\n@given(knownbits_and_contained_number, knownbits_and_contained_number)\ndef test_hypothesis_or(t1, t2):\n    k1, n1 = t1\n    k2, n2 = t2\n    k3 = k1.abstract_or(k2)\n    n3 = n1 | n2\n    assert k3.contains(n3)\n\n\nImplementing support for abstract_xor is relatively simple, and left as an\nexercise :-).\nAddition and Subtraction\ninvert, and, and or are relatively simple transfer functions to write,\nbecause they compose over the individual bits of the integers. The arithmetic\nfunctions add and sub are significantly harder, because of carries and\nborrows. Coming up with the formulas for them and gaining an intuitive\nunderstanding is quite tricky and involves carefully going through a few\nexamples with pen and paper. When implementing this in PyPy, Nico and I didn't\ncome up with the implementation ourselves, but instead took them from the\nTristate Numbers paper. Here's the code,\nwith example tests and hypothesis tests:\nclass KnownBits:\n    ...\n\n    def abstract_add(self, other):\n        sum_ones = self.ones + other.ones\n        sum_unknowns = self.unknowns + other.unknowns\n        all_carries = sum_ones + sum_unknowns\n        ones_carries = all_carries  sum_ones\n        unknowns = self.unknowns | other.unknowns | ones_carries\n        ones = sum_ones & ~unknowns\n        return KnownBits(ones, unknowns)\n\n    def abstract_sub(self, other):\n        diff_ones = self.ones - other.ones\n        val_borrows = (diff_ones + self.unknowns)  (diff_ones - other.unknowns)\n        unknowns = self.unknowns | other.unknowns | val_borrows\n        ones = diff_ones & ~unknowns\n        return KnownBits(ones, unknowns)\n\n\ndef test_add():\n    k1 = KnownBits.from_str('0?10?10?10')\n    k2 = KnownBits.from_str('0???111000')\n    res = k1.abstract_add(k2)\n    assert str(res) ==   \"?????01?10\"\n\ndef test_sub():\n    k1 = KnownBits.from_str('0?10?10?10')\n    k2 = KnownBits.from_str('0???111000')\n    res = k1.abstract_sub(k2)\n    assert str(res) ==   \"...?11?10\"\n    k1 = KnownBits.from_str(    '...1?10?10?10')\n    k2 = KnownBits.from_str('...10000???111000')\n    res = k1.abstract_sub(k2)\n    assert str(res) ==   \"111?????11?10\"\n\n@given(knownbits_and_contained_number, knownbits_and_contained_number)\ndef test_hypothesis_add(t1, t2):\n    k1, n1 = t1\n    k2, n2 = t2\n    k3 = k1.abstract_add(k2)\n    n3 = n1 + n2\n    assert k3.contains(n3)\n\n@given(knownbits_and_contained_number, knownbits_and_contained_number)\ndef test_hypothesis_sub(t1, t2):\n    k1, n1 = t1\n    k2, n2 = t2\n    k3 = k1.abstract_sub(k2)\n    n3 = n1 - n2\n    assert k3.contains(n3)\n\n\nNow we are in a pretty good situation, and have implemented abstract versions\nfor a bunch of important arithmetic and binary functions. What's also surprising\nis that the implementation of all of the transfer functions is quite efficient.\nWe didn't have to write loops over the individual bits at all, instead we found\nclosed form expressions using primitive operations on the underlying integers\nones and unknowns. This means that computing the results of abstract\noperations is quite efficient, which is important when using the abstract domain\nin the context of a JIT compiler.\nProving correctness of the transfer functions with Z3\nAs one can probably tell from my recent posts, I've been thinking about\ncompiler correctness a lot. Getting the transfer functions absolutely\ncorrect is really crucial, because a bug in them would lead to miscompilation of\nPython code when the abstract domain is added to the JIT. While the randomized\ntests are great, it's still entirely possible for them to miss bugs. The state\nspace for the arguments of a binary transfer function is 3**64 * 3**64, and if\nonly a small part of that contains wrong behaviour it would be really unlikely\nfor us to find it with random tests by chance. Therefore I was reluctant to\nmerge the PyPy branch that contained the new abstract domain for a long time.\nTo increase our confidence in the correctness of the transfer functions further,\nwe can use Z3 to prove their correctness, which gives us much stronger\nguarantees (not 100%, obviously). In this subsection I will show how to do that.\nHere's an attempt to do this manually in the Python repl:\n>>>> import z3\n>>>> solver = z3.Solver()\n>>>> # like last blog post, proof by failing to find counterexamples\n>>>> def prove(cond): assert solver.check(z3.Not(cond)) == z3.unsat\n>>>>\n>>>> # let's set up a z3 bitvector variable for an arbitrary concrete value\n>>>> n1 = z3.BitVec('concrete_value', 64)\n>>>> n1\nconcrete_value\n>>>> # due to operator overloading we can manipulate z3 formulas\n>>>> n2 = ~n1\n>>>> n2\n~concrete_value\n>>>> \n>>>> # now z3 bitvector variables for the ones and zeros fields\n>>>> ones = z3.BitVec('abstract_ones', 64)\n>>>> unknowns = z3.BitVec('abstract_unknowns', 64)\n>>>> # we construct a KnownBits instance with the z3 variables\n>>>> k1 = KnownBits(ones, unknowns)\n>>>> # due to operator overloading we can call the methods on k1:\n>>>> k2 = k1.abstract_invert()\n>>>> k2.ones\n~abstract_unknowns & ~abstract_ones\n>>>> k2.unknowns\nabstract_unknowns\n>>>> # here's the correctness condition that we want to prove:\n>>>> k2.contains(n2)\n~concrete_value & ~abstract_unknowns ==\n~abstract_unknowns & ~abstract_ones\n>>>> # let's try\n>>>> prove(k2.contains(n2))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 1, in prove\nAssertionError\n>>>> # it doesn't work! let's look at the counterexample to see why:\n>>>> solver.model()\n[abstract_unknowns = 0,\n abstract_ones = 0,\n concrete_value = 1]\n>>>> # we can build a KnownBits instance with the values in the\n>>>> # counterexample:\n>>>> ~1 # concrete result\n-2\n>>>> counter_example_k1 = KnownBits(0, 0)\n>>>> counter_example_k1\nKnownBits.from_constant(0)\n>>>> counter_example_k2 = counter_example_k1.abstract_invert()\n>>>> counter_example_k2\nKnownBits.from_constant(-1)\n>>>> # let's check the failing condition\n>>>> counter_example_k2.contains(~1)\nFalse\n\n\nWhat is the problem here? We didn't tell Z3 that n1 was supposed to be a\nmember of k1. We can add this as a precondition to the solver, and then the\nprove works:\n>>>> solver.add(k1.contains(n1))\n>>>> prove(k2.contains(n2)) # works!\n\n\nThis is super cool! It's really a proof about the actual implementation, because\nwe call the implementation methods directly, and due to the operator overloading\nthat Z3 does we can be sure that we are actually checking a formula that\ncorresponds to the Python code. This eliminates one source of errors in formal\nmethods.\nDoing the proof manually on the Python REPL is kind of annoying though, and we\nalso would like to make sure that the proofs are re-done when we change the\ncode. What we would really like to do is writing the proofs as a unit-test that\nwe can run while developing and in CI. Doing this is possible, and the unit\ntests that really perform proofs look pleasingly similar to the\nHypothesis-based ones.\nFirst we need to set up a bit of infrastructure:\nINTEGER_WIDTH = 64\n\ndef BitVec(name):\n    return z3.BitVec(name, INTEGER_WIDTH)\n\ndef BitVecVal(val):\n    return z3.BitVecVal(val, INTEGER_WIDTH)\n\ndef z3_setup_variables():\n    # instantiate a solver\n    solver = z3.Solver()\n\n    # a Z3 variable for the first concrete value\n    n1 = BitVec(\"n1\")\n    # a KnownBits instances that uses Z3 variables as its ones and unknowns,\n    # representing the first abstract value\n    k1 = KnownBits(BitVec(\"n1_ones\"), BitVec(\"n1_unkowns\"))\n    # add the precondition to the solver that the concrete value n1 must be a\n    # member of the abstract value k1\n    solver.add(k1.contains(n1))\n\n    # a Z3 variable for the second concrete value\n    n2 = BitVec(\"n2\")\n    # a KnownBits instances for the second abstract value\n    k2 = KnownBits(BitVec(\"n2_ones\"), BitVec(\"n2_unkowns\"))\n    # add the precondition linking n2 and k2 to the solver\n    solver.add(k2.contains(n2))\n    return solver, k1, n1, k2, n2\n\ndef prove(cond, solver):\n    z3res = solver.check(z3.Not(cond))\n    if z3res != z3.unsat:\n        assert z3res == z3.sat # can't be timeout, we set no timeout\n        # make the model with the counterexample global, to make inspecting the\n        # bug easier when running pytest --pdb\n        global model\n        model = solver.model()\n        print(f\"n1={model.eval(n1)}, n2={model.eval(n2)}\")\n        counter_example_k1 = KnownBits(model.eval(k1.ones).as_signed_long(),\n                                       model.eval(k1.unknowns).as_signed_long())\n        counter_example_k2 = KnownBits(model.eval(k2.ones).as_signed_long(),\n                                       model.eval(k2.unknowns).as_signed_long())\n        print(f\"k1={counter_example_k1}, k2={counter_example_k2}\")\n        print(f\"but {cond=} evaluates to {model.eval(cond)}\")\n        raise ValueError(solver.model())\n\n\nAnd then we can write proof-unit-tests like this:\ndef test_z3_abstract_invert():\n    solver, k1, n1, _, _ = z3_setup_variables()\n    k2 = k1.abstract_invert()\n    n2 = ~n1\n    prove(k2.contains(n2), solver)\n\ndef test_z3_abstract_and():\n    solver, k1, n1, k2, n2 = z3_setup_variables()\n    k3 = k1.abstract_and(k2)\n    n3 = n1 & n2\n    prove(k3.contains(n3), solver)\n\ndef test_z3_abstract_or():\n    solver, k1, n1, k2, n2 = z3_setup_variables()\n    k3 = k1.abstract_or(k2)\n    n3 = n1 | n2\n    prove(k3.contains(n3), solver)\n\ndef test_z3_abstract_add():\n    solver, k1, n1, k2, n2 = z3_setup_variables()\n    k3 = k1.abstract_add(k2)\n    n3 = n1 + n2\n    prove(k3.contains(n3), solver)\n\ndef test_z3_abstract_sub():\n    solver, k1, n1, k2, n2 = z3_setup_variables()\n    k3 = k1.abstract_sub(k2)\n    n3 = n1 - n2\n    prove(k3.contains(n3), solver)\n\n\nIt's possible to write a bit more Python-metaprogramming-magic and unify the\nHypothesis and Z3 tests into the same test definition.1\nCases where this style of Z3 proof doesn't work\nUnfortunately the approach described in the previous section only works for a\nvery small number of cases. It breaks down as soon as the KnownBits methods\nthat we're calling contain any if conditions (including hidden ones like\nthe short-circuiting and and or in Python). Let's look at an example and\nimplement abstract_eq. eq is supposed to be an operation that compares two\nintegers and returns 0 or 1 if they are different or equal, respectively.\nImplementing this in knownbits looks like this (with example and hypothesis\ntests):\nclass KnownBits:\n    ...\n\n    def abstract_eq(self, other):\n        # the result is a 0, 1, or ?\n\n        # if they are both the same constant, they must be equal\n        if self.is_constant() and other.is_constant() and self.ones == other.ones:\n            return KnownBits.from_constant(1)\n        # check whether we have known disagreeing bits, then we know the result\n        # is 0\n        if self._disagrees(other):\n            return KnownBits.from_constant(0)\n        return KnownBits(0, 1) # an unknown boolean\n\n    def _disagrees(self, other):\n        # check whether the bits disagree in any place where both are known\n        both_known = self.knowns & other.knowns\n        return self.ones & both_known != other.ones & both_known\n\ndef test_eq():\n    k1 = KnownBits.from_str('...?')\n    k2 = KnownBits.from_str('...?')\n    assert str(k1.abstract_eq(k2)) == '?'\n    k1 = KnownBits.from_constant(10)\n    assert str(k1.abstract_eq(k1)) == '1'\n    k1 = KnownBits.from_constant(10)\n    k2 = KnownBits.from_constant(20)\n    assert str(k1.abstract_eq(k2)) == '0'\n\n@given(knownbits_and_contained_number, knownbits_and_contained_number)\ndef test_hypothesis_eq(t1, t2):\n    k1, n1 = t1\n    k2, n2 = t2\n    k3 = k1.abstract_eq(k2)\n    assert k3.contains(int(n1 == n2))\n\n\nTrying to do the proof in the same style as before breaks:\n>>>> k3 = k1.abstract_eq(k2)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"knownbits.py\", line 246, in abstract_eq\n    if self._disagrees(other):\n  File \"venv/site-packages/z3/z3.py\", line 381, in __bool__\n    raise Z3Exception(\"Symbolic expressions cannot be cast to concrete Boolean values.\")\nz3.z3types.Z3Exception: Symbolic expressions cannot be cast to concrete Boolean values.\n\n\nWe cannot call abstract_eq on a KnownBits with Z3 variables as fields,\nbecause once we hit an if statement, the whole approach of relying on the\noperator overloading breaks down. Z3 doesn't actually parse the Python code or\nanything advanced like that, we rather build an expression only by running the\ncode and letting the Z3 formulas build up.\nTo still prove the correctness of abstract_eq we need to manually transform\nthe control flow logic of the function into a Z3 formula that uses the z3.If\nexpression, using a small helper function:\ndef z3_cond(b, trueval=1, falseval=0):\n    return z3.If(b, BitVecVal(trueval), BitVecVal(falseval))\n\ndef z3_abstract_eq(k1, k2):\n    # follow the *logic* of abstract_eq, we can't call it due to the ifs in it\n    case1cond = z3.And(k1.is_constant(), k2.is_constant(), k1.ones == k2.ones)\n    case2cond = k1._disagrees(k2)\n\n    # ones is 1 in the first case, 0 otherwise\n    ones = z3_cond(case1cond, 1, 0)\n\n    # in the first two cases, unknowns is 0, 1 otherwise\n    unknowns = z3_cond(z3.Or(case1cond, case2cond), 0, 1)\n    return KnownBits(ones, unknowns)\n\ndef test_z3_abstract_eq_logic():\n    solver, k1, n1, k2, n2 = z3_setup_variables()\n    n3 = z3_cond(n1 == n2) # concrete result\n    k3 = z3_abstract_eq(k1, k2)\n    prove(k3.contains(n3), solver)\n\n\nThis proof works. It is a lot less satisfying than the previous ones though,\nbecause we could have done an error in the manual transcription from Python code\nto Z3 formulas (there are possibly more heavy-handed approaches where we do\nthis transformation more automatically using e.g. the ast module to analyze\nthe source code, but that's a much more complicated researchy project). To\nlessen this problem somewhat we can factor out the parts of the logic that don't\nhave any conditions into small helper methods (like _disagrees in this\nexample) and use them in the manual conversion of the code to Z3 formulas.2\nThe final condition that Z3 checks, btw, is this one:\nIf(n1 == n2, 1, 0) &\n~If(Or(And(n1_unkowns == 0,\n           n2_unkowns == 0,\n           n1_ones == n2_ones),\n       n1_ones & ~n1_unkowns & ~n2_unkowns !=\n       n2_ones & ~n1_unkowns & ~n2_unkowns),\n    0, 1) ==\nIf(And(n1_unkowns == 0, n2_unkowns == 0, n1_ones == n2_ones),\n   1, 0)\n\n\nMaking Statements about Precision\nSo far we have only used Z3 to prove statements about correctness, i.e. that\nour abstract operations overapproximate what can happen with concrete values.\nWhile proving this property is essential if we want to avoid miscompilation,\ncorrectness alone is not a very strong constraint on the implementation of our\nabstract transfer functions. We could simply return Knownbits.unknowns() for\nevery abstract_* method and the resulting overapproximation would be correct,\nbut useless in practice.\nIt's much harder to make statements about whether the transfer functions are\nmaximally precise. There are two aspects of precision I want to discuss in this\nsection, however.\nThe first aspect is that we would really like it if the transfer functions\ncompute the maximally precise results for singleton sets. If all abstract\narguments of an operations are constants, i.e. contain only a single concrete\nelement, then we know that the resulting set also has only a single element. We\ncan prove that all our transfer functions have this property:\ndef test_z3_prove_constant_folding():\n    solver, k1, n1, k2, n2 = z3_setup_variables()\n    k3 = k1.abstract_invert()\n    prove(z3.Implies(k1.is_constant(),\n                     k3.is_constant()), solver)\n\n    k3 = k1.abstract_and(k2)\n    prove(z3.Implies(z3.And(k1.is_constant(), k2.is_constant()),\n                     k3.is_constant()), solver)\n\n    k3 = k1.abstract_or(k2)\n    prove(z3.Implies(z3.And(k1.is_constant(), k2.is_constant()),\n                     k3.is_constant()), solver)\n\n    k3 = k1.abstract_sub(k2)\n    prove(z3.Implies(z3.And(k1.is_constant(), k2.is_constant()),\n                     k3.is_constant()), solver)\n\n    k3 = z3_abstract_eq(k1, k2)\n    prove(z3.Implies(z3.And(k1.is_constant(), k2.is_constant()),\n                     k3.is_constant()), solver)\n\n\nProving with Z3 that the transfer functions are maximally precise for\nnon-constant arguments seems to be relatively hard. I tried a few completely\nrigorous approaches and failed. The paper Sound, Precise, and Fast Abstract\nInterpretation with Tristate Numbers\ncontains an optimality proof for the transfer functions of addition and\nsubtraction, so we can be certain that they are as precise as is\npossible.\nI still want to show an approach for trying to find concrete examples of\nabstract values that are less precise than they could be, using a combination\nof Hypothesis and Z3. The idea is to use hypothesis to pick random abstract\nvalues. Then we compute the abstract result using our transfer function.\nAfterwards we can ask Z3 to find us an abstract result that is better than the\none our transfer function produced. If Z3 finds a better abstract result, we\nhave a concrete example of imprecision for our transfer function. Those tests\naren't strict proofs, because they rely on generating random abstract values,\nbut they can still be valuable (not for the transfer functions in this blog\npost, which are all optimal).\nHere is what the code looks like (this is a little bit bonus content, I'll not\nexplain the details and can only hope that the comments are somewhat helpful):\n@given(random_knownbits_and_contained_number, random_knownbits_and_contained_number)\n@settings(deadline=None)\ndef test_check_precision(t1, t2):\n    k1, n1 = t1\n    k2, n2 = t2\n    # apply transfer function\n    k3 = k1.abstract_add(k2)\n    example_res = n1 + n2\n\n    # try to find a better version of k3 with Z3\n    solver = z3.Solver()\n    solver.set(\"timeout\", 8000)\n\n    var1 = BitVec('v1')\n    var2 = BitVec('v2')\n\n    ones = BitVec('ones')\n    unknowns = BitVec('unknowns')\n    better_k3 = KnownBits(ones, unknowns)\n    print(k1, k2, k3)\n\n    # we're trying to find an example for a better k3, so we use check, without\n    # negation:\n    res = solver.check(z3.And(\n        # better_k3 should be a valid knownbits instance\n        better_k3.is_well_formed(),\n        # it should be better than k3, ie there are known bits in better_k3\n        # that we don't have in k3\n        better_k3.knowns & ~k3.knowns != 0,\n        # now encode the correctness condition for better_k3 with a ForAll:\n        # for all concrete values var1 and var2, it must hold that if\n        # var1 is in k1 and var2 is in k2 it follows that var1 + var2 is in\n        # better_k3\n        z3.ForAll(\n        [var1, var2],\n        z3.Implies(\n            z3.And(k1.contains(var1), k2.contains(var2)),\n            better_k3.contains(var1 + var2)))))\n    # if this query is satisfiable, we have found a better result for the\n    # abstract_add\n    if res == z3.sat:\n        model = solver.model()\n        rk3 = KnownBits(model.eval(ones).as_signed_long(), model.eval(unknowns).as_signed_long())\n        print(\"better\", rk3)\n        assert 0\n    if res == z3.unknown:\n        print(\"timeout\")\n\n\nIt does not actually fail for abstract_add (nor the other abstract\nfunctions). To see the test failing we can add some imprecision to the\nimplementation of abstract_add to see Hypothesis and Z3 find examples of\nvalues that are not optimally precise (for example by setting some bits\nof unknowns in the implementation of abstract_add unconditionally).\nUsing the Abstract Domain in the Toy Optimizer for Generalized Constant Folding\nNow after all this work we can finally actually use the knownbits abstract\ndomain in the toy optimizer. The code for this follows Max' intro post about\nabstract interpretation\nquite closely.\nFor completeness sake, in the fold there's the basic infrastructure classes\nthat make up the IR again (they are identical or at least extremely close to\nthe previous toy posts).\n\ntoy infrastructure\n\nclass Value:\n    def find(self):\n        raise NotImplementedError(\"abstract\")\n\n\n@dataclass(eq=False)\nclass Operation(Value):\n    name : str\n    args : list[Value]\n\n    forwarded : Optional[Value] = None\n\n    def find(self) -> Value:\n        op = self\n        while isinstance(op, Operation):\n            next = op.forwarded\n            if next is None:\n                return op\n            op = next\n        return op\n\n    def arg(self, index):\n        return self.args[index].find()\n\n    def make_equal_to(self, value : Value):\n        self.find().forwarded = value\n\n\n@dataclass(eq=False)\nclass Constant(Value):\n    value : object\n\n    def find(self):\n        return self\n\n\nclass Block(list):\n    def __getattr__(self, opname):\n        def wraparg(arg):\n            if not isinstance(arg, Value):\n                arg = Constant(arg)\n            return arg\n        def make_op(*args):\n            op = Operation(opname,\n                [wraparg(arg) for arg in args])\n            self.append(op)\n            return op\n        return make_op\n\n\ndef bb_to_str(l : Block, varprefix : str = \"var\"):\n    def arg_to_str(arg : Value):\n        if isinstance(arg, Constant):\n            return str(arg.value)\n        else:\n            return varnames[arg]\n\n    varnames = {}\n    res = []\n    for index, op in enumerate(l):\n        # give the operation a name used while\n        # printing:\n        var =  f\"{varprefix}{index}\"\n        varnames[op] = var\n        arguments = \", \".join(\n            arg_to_str(op.arg(i))\n                for i in range(len(op.args))\n        )\n        strop = f\"{var} = {op.name}({arguments})\"\n        res.append(strop)\n    return \"\\n\".join(res)\n\n\n\n\n\n\nNow we can write some first tests, the first one simply checking constant\nfolding:\ndef test_constfold_two_ops():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.int_add(5, 4)\n    var2 = bb.int_add(var1, 10)\n    var3 = bb.int_add(var2, var0)\n\n    opt_bb = simplify(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = int_add(19, optvar0)\"\"\"\n\n\nCalling the transfer functions on constant KnownBits produces a constant\nresults, as we have seen. Therefore \"regular\" constant folding should hopefully\nbe achieved by optimizing with the KnownBits abstract domain too.\nThe next two tests are slightly more complicated and can't be optimized by\nregular constant-folding. They follow the motivating examples from the start of\nthis blog post, a hundred years ago:\ndef test_constfold_via_knownbits():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.int_or(var0, 1)\n    var2 = bb.int_and(var1, 1)\n    var3 = bb.dummy(var2)\n\n    opt_bb = simplify(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = int_or(optvar0, 1)\noptvar2 = dummy(1)\"\"\"\n\ndef test_constfold_alignment_check():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.int_invert(0b111)\n    # mask off the lowest three bits, thus var2 is aligned\n    var2 = bb.int_and(var0, var1)\n    # add 16 to aligned quantity\n    var3 = bb.int_add(var2, 16)\n    # check alignment of result\n    var4 = bb.int_and(var3, 0b111)\n    var5 = bb.int_eq(var4, 0)\n    # var5 should be const-folded to 1\n    var6 = bb.dummy(var5)\n\n    opt_bb = simplify(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = int_and(optvar0, -8)\noptvar2 = int_add(optvar1, 16)\noptvar3 = dummy(1)\"\"\"\n\n\nHere is simplify to make these tests pass:\ndef unknown_transfer_functions(*abstract_args):\n    return KnownBits.all_unknown()\n\n\ndef simplify(bb: Block) -> Block:\n    abstract_values = {} # dict mapping Operation to KnownBits\n\n    def knownbits_of(val : Value):\n        if isinstance(val, Constant):\n            return KnownBits.from_constant(val.value)\n        return abstract_values[val]\n\n    opt_bb = Block()\n    for op in bb:\n        # apply the transfer function on the abstract arguments\n        name_without_prefix = op.name.removeprefix(\"int_\")\n        method_name = f\"abstract_{name_without_prefix}\"\n        transfer_function = getattr(KnownBits, method_name, unknown_transfer_functions)\n        abstract_args = [knownbits_of(arg.find()) for arg in op.args]\n        abstract_res = abstract_values[op] = transfer_function(*abstract_args)\n        # if the result is a constant, we optimize the operation away and make\n        # it equal to the constant result\n        if abstract_res.is_constant():\n            op.make_equal_to(Constant(abstract_res.ones))\n            continue\n        # otherwise emit the op\n        opt_bb.append(op)\n    return opt_bb\n\n\nThe code follows the approach from the previous blog post very closely. The\nonly difference is that we apply the transfer function first, to be able to\ndetect whether the abstract domain can tell us that the result has to always be\na constant. This code makes all three tests pass.\nUsing the KnownBits Domain for Conditional Peephole Rewrites\nSo far we are only using the KnownBits domain to find out that certain\noperations have to produce a constant. We can also use the KnownBits domain\nto check whether certain operation rewrites are correct. Let's use one of the\nexamples from the Mining JIT traces for missing optimizations with\nZ3\npost, where Z3 found the inefficiency (x << 4) & -0xf == x << 4 in PyPy JIT\ntraces. We don't have shift operations, but we want to generalize this optimization\nanyway. The general form of this rewrite is that under some circumstances x &\ny == x, and we can use the KnownBits domain to detect situations where this\nmust be true.\nTo understand when x & y == x is true, we can think about individual pairs of\nbits a and b. If a == 0, then a & b == 0 & b == 0 == a. If b == 1\nthen a & b == a & 1 == a. So if either a == 0 or b == 1 is true,\na & b == a follows. And if either of these conditions is true for all the\nbits of x and y, we can know that x & y == x.\nWe can write a method on KnownBits to check for this condition:\nclass KnownBits:\n    ...\n\n    def is_and_identity(self, other):\n        \"\"\" Return True if n1 & n2 == n1 for any n1 in self and n2 in other.\n        (or, equivalently, return True if n1 | n2 == n2)\"\"\"\n        return self.zeros | other.ones == -1\n\n\nSince my reasoning about this feels ripe for errors, let's check that our\nunderstanding is correct with Z3:\ndef test_prove_is_and_identity():\n    solver, k1, n1, k2, n2 = z3_setup_variables()\n    prove(z3.Implies(k1.is_and_identity(k2), n1 & n2 == n1), solver)\n\n\nNow let's use this in the toy optimizer. Here are two tests for this rewrite:\ndef test_remove_redundant_and():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.int_invert(0b1111)\n    # mask off the lowest four bits\n    var2 = bb.int_and(var0, var1)\n    # applying the same mask is not redundant\n    var3 = bb.int_and(var2, var1)\n    var4 = bb.dummy(var3)\n\n    opt_bb = simplify(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = int_and(optvar0, -16)\noptvar2 = dummy(optvar1)\"\"\"\n\ndef test_remove_redundant_and_more_complex():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.getarg(1)\n    # var2 has bit pattern ????\n    var2 = bb.int_and(var0, 0b1111)\n    # var3 has bit pattern ...?1111\n    var3 = bb.int_or(var1, 0b1111)\n    # var4 is just var2\n    var4 = bb.int_and(var2, var3)\n    var5 = bb.dummy(var4)\n\n    opt_bb = simplify(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = getarg(1)\noptvar2 = int_and(optvar0, 15)\noptvar3 = int_or(optvar1, 15)\noptvar4 = dummy(optvar2)\"\"\"\n\n\nThe first test could also be made to pass by implementing a reassociation\noptimization that turns (x & c1) & c2 into x & (c1 & c2) and then constant-folds the second and. But here we want to\nuse KnownBits and conditionally rewrite int_and to its first argument. So to make the tests pass,\nwe can change simplify like this:\ndef simplify(bb: Block) -> Block:\n    abstract_values = {} # dict mapping Operation to KnownBits\n\n    def knownbits_of(val : Value):\n        ...\n\n    opt_bb = Block()\n    for op in bb:\n        # apply the transfer function on the abstract arguments\n        name_without_prefix = op.name.removeprefix(\"int_\")\n        method_name = f\"abstract_{name_without_prefix}\"\n        transfer_function = getattr(KnownBits, method_name, unknown_transfer_functions)\n        abstract_args = [knownbits_of(arg.find()) for arg in op.args]\n        abstract_res = abstract_values[op] = transfer_function(*abstract_args)\n        # if the result is a constant, we optimize the operation away and make\n        # it equal to the constant result\n        if abstract_res.is_constant():\n            op.make_equal_to(Constant(abstract_res.ones))\n            continue\n        # <<<< new code\n        # conditionally rewrite int_and(x, y) to x\n        if op.name == \"int_and\":\n            k1, k2 = abstract_args\n            if k1.is_and_identity(k2):\n                op.make_equal_to(op.arg(0))\n                continue\n        # >>>> end changes\n        opt_bb.append(op)\n    return opt_bb\n\n\nAnd with that, the new tests pass as well. A real implementation would also\ncheck the other argument order, but we leave that out for the sake of brevity.\nThis rewrite also generalizes the rewrites int_and(0, x) -> 0 and\nint_and(-1, x) -> x, let's add a test for those:\ndef test_remove_and_simple():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.getarg(1)\n    var2 = bb.int_and(0, var0) # == 0\n    var3 = bb.int_invert(var2) # == -1\n    var4 = bb.int_and(var1, var3) # == var1\n    var5 = bb.dummy(var4)\n\n    opt_bb = simplify(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = getarg(1)\noptvar2 = dummy(optvar1)\"\"\"\n\n\nThis test just passes. And that's it for this post!\nConclusion\nIn this post we've seen the implementation, testing and proofs about a 'known\nbits' abstract domain, as well as its use in the toy optimizer to generalize\nconstant folding, and to implement conditional peephole rewrites.\nIn the next posts I'll write about the real implementation of a knownbits\ndomain in PyPy's JIT, its combination with the existing interval abstract\ndomain, how to deal with gaining information from conditions in the program,\nand some lose ends.\nSources:\n\nKnown bits in LLVM\nTristate numbers for known bits in Linux eBPF\nSound, Precise, and Fast Abstract Interpretation with Tristate Numbers\nVerifying the Veri\ufb01er: eBPF Range Analysis Veri\ufb01cation\nBit-Twiddling: Addition with Unknown\n  Bits\n  is a super readable blog post by Dougall J. I've taken the ones and\n  unknowns naming from this post, which I find significantly clearer than\n  value and mask, which the Linux kernel uses.\nBits, Math and Performance(?), a fantastic\n  blog by Harold Aptroot. There are a\n  lot of relevant posts about known bits, range analysis etc. Harold is also\n  the author of Haroldbot, a website that can be used\n  for bitvector calculations, and also checks bitvector identities.\nSharpening Constraint Programming approaches for Bit-Vector Theory\nDeriving Abstract Transfer Functions for Analyzing Embedded Software\nSynthesizing Abstract Transformers\n\n\n\n\n\nThere's a subtletly about the Z3 proofs that I'm sort of\nglossing over here. Python integers are of arbitrary width, and the\nKnownBits code is actually carefully written to work for integers of any\nsize. This property is tested by the Hypothesis tests, which don't limit\nthe sizes of the generated random integers. However, the Z3 proofs only\ncheck bitvectors of a fixed bitwidth of 64. There are various ways to deal\nwith this situation. For most \"real\" compilers, the bitwidth of integers\nwould be fixed anyway. Then the components ones and unknowns of the\nKnownBits class would use the number of bits the corresponding integer\nvariable has, and the Z3 proofs would use the same width. This is what we\ndo in the PyPy JIT.\u00a0\u21a9\n\n\nThe less close connection between implementation and proof\nfor abstract_eq is one of the reasons why it makes sense to do\nunit-testing in addition to proofs. For a more detailed explanation of\nwhy both tests and proofs are good to\nhave, see Jeremy Siek's blog\npost,\nas well as the Knuth\nquote.\u00a0\u21a9",
      "tags": "toy-optimizer,z3",
      "url": "https://www.pypy.org/posts/2024/08/toy-knownbits.html"
    },
    {
      "title": "Abstract interpretation in the Toy Optimizer",
      "text": "This is a cross-post\nfrom Max Bernstein from his excellent blog where he writes about programming\nlanguages, compilers, optimizations, virtual machines. He's looking for a\n(dynamic language runtime or compiler related) job too.\n\nCF Bolz-Tereick wrote some excellent posts in which they introduce a small IR\nand optimizer and extend it with allocation\nremoval. We also did a live stream together in which\nwe did some more heap optimizations.\nIn this blog post, I'm going to write a small abstract interpreter for the Toy\nIR and then show how we can use it to do some simple optimizations. It assumes\nthat you are familiar with the little IR, which I have reproduced unchanged in\na GitHub Gist.\nAbstract interpretation is a general framework for efficiently computing\nproperties that must be true for all possible executions of a program. It's a\nwidely used approach both in compiler optimizations as well as offline static\nanalysis for finding bugs. I'm writing this post to pave the way for CF's next\npost on proving abstract interpreters correct for range analysis and known bits\nanalysis inside PyPy.\nBefore we begin, I want to note a couple of things:\n\nThe Toy IR is in SSA form, which means that every variable is defined exactly\n  once. This means that abstract properties of each variable are easy to track.\nThe Toy IR represents a linear trace without control flow, meaning we won't\n  talk about meet/join or fixpoints. They only make sense if the IR has a\n  notion of conditional branches or back edges (loops).\n\nAlright, let's get started.\nWelcome to abstract interpretation\nAbstract interpretation means a couple different things to different people.\nThere's rigorous mathematical formalism thanks to Patrick and Radhia Cousot,\nour favorite power couple, and there's also sketchy hand-wavy stuff like what\nwill follow in this post. In the end, all people are trying to do is reason\nabout program behavior without running it.\nIn particular, abstract interpretation is an over-approximation of the\nbehavior of a program. Correctly implemented abstract interpreters never lie,\nbut they might be a little bit pessimistic. This is because instead of using\nreal values and running the program---which would produce a concrete result and\nsome real-world behavior---we \"run\" the program with a parallel universe of\nabstract values. This abstract run gives us information about all possible\nruns of the program.1\nAbstract values always represent sets of concrete values. Instead of literally\nstoring a set (in the world of integers, for example, it could get pretty\nbig...there are a lot of integers), we group them into a finite number of named\nsubsets.2\nLet's learn a little about abstract interpretation with an example program and\nexample abstract domain. Here's the example program:\nv0 = 1\nv1 = 2\nv2 = add(v0, v1)\n\n\nAnd our abstract domain is \"is the number positive\" (where \"positive\" means\nnonnegative, but I wanted to keep the words distinct):\n       top\n    /       \\\npositive    negative\n    \\       /\n      bottom\n\n\nThe special top value means \"I don't know\" and the special bottom value\nmeans \"empty set\" or \"unreachable\". The positive and negative values\nrepresent the sets of all positive and negative numbers, respectively.\nWe initialize all the variables v0, v1, and v2 to bottom and then walk\nour IR, updating our knowledge as we go.\n# here\nv0:bottom = 1\nv1:bottom = 2\nv2:bottom = add(v0, v1)\n\n\nIn order to do that, we have to have transfer functions for each operation.\nFor constants, the transfer function is easy: determine if the constant is\npositive or negative. For other operations, we have to define a function that\ntakes the abstract values of the operands and returns the abstract value of the\nresult.\nIn order to be correct, transfer functions for operations have to be compatible\nwith the behavior of their corresponding concrete implementations. You can\nthink of them having an implicit universal quantifier forall in front of\nthem.\nLet's step through the constants at least:\nv0:positive = 1\nv1:positive = 2\n# here\nv2:bottom = add(v0, v1)\n\n\nNow we need to figure out the transfer function for add. It's kind of tricky\nright now because we haven't specified our abstract domain very well. I keep\nsaying \"numbers\", but what kinds of numbers? Integers? Real numbers? Floating\npoint? Some kind of fixed-width bit vector (int8, uint32, ...) like an\nactual machine \"integer\"?\nFor this post, I am going to use the mathematical definition of integer, which\nmeans that the values are not bounded in size and therefore do not overflow.\nActual hardware memory constraints aside, this is kind of like a Python int.\nSo let's look at what happens when we add two abstract numbers:\n\n\n\n\ntop\npositive\nnegative\nbottom\n\n\n\n\ntop\ntop\ntop\ntop\nbottom\n\n\npositive\ntop\npositive\ntop\nbottom\n\n\nnegative\ntop\ntop\nnegative\nbottom\n\n\nbottom\nbottom\nbottom\nbottom\nbottom\n\n\n\nAs an example, let's try to add two numbers a and b, where a is positive\nand b is negative. We don't know anything about their values other than their\nsigns. They could be 5 and -3, where the result is 2, or they could be\n1 and -100, where the result is -99. This is why we can't say anything\nabout the result of this operation and have to return top.\nThe short of this table is that we only really know the result of an addition\nif both operands are positive or both operands are negative. Thankfully, in\nthis example, both operands are known positive. So we can learn something about\nv2:\nv0:positive = 1\nv1:positive = 2\nv2:positive = add(v0, v1)\n# here\n\n\nThis may not seem useful in isolation, but analyzing more complex programs even\nwith this simple domain may be able to remove checks such as if (v2 < 0) { ... }.\nLet's take a look at another example using an sample absval (absolute value)\nIR operation:\nv0 = getarg(0)\nv1 = getarg(1)\nv2 = absval(v0)\nv3 = absval(v1)\nv4 = add(v2, v3)\nv5 = absval(v4)\n\n\nEven though we have no constant/concrete values, we can still learn something\nabout the states of values throughout the program. Since we know that absval\nalways returns a positive number, we learn that v2, v3, and v4 are all\npositive. This means that we can optimize out the absval operation on v5:\nv0:top = getarg(0)\nv1:top = getarg(1)\nv2:positive = absval(v0)\nv3:positive = absval(v1)\nv4:positive = add(v2, v3)\nv5:positive = v4\n\n\nOther interesting lattices include:\n\nConstants (where the middle row is pretty wide)\nRange analysis (bounds on min and max of a number)\nKnown bits (using a bitvector representation of a number, which bits are\n  always 0 or 1)\n\nFor the rest of this blog post, we are going to do a very limited version of\n\"known bits\", called parity. This analysis only tracks the least significant\nbit of a number, which indicates if it is even or odd.\nParity\nThe lattice is pretty similar to the positive/negative lattice:\n    top\n  /     \\\neven    odd\n  \\     /\n   bottom\n\n\nLet's define a data structure to represent this in Python code:\nclass Parity:\n    def __init__(self, name):\n        self.name = name\n\n    def __repr__(self):\n        return self.name\n\n\nAnd instantiate the members of the lattice:\nTOP = Parity(\"top\")\nEVEN = Parity(\"even\")\nODD = Parity(\"odd\")\nBOTTOM = Parity(\"bottom\")\n\n\nNow let's write a forward flow analysis of a basic block using this lattice.\nWe'll do that by assuming that a method on Parity is defined for each IR\noperation. For example, Parity.add, Parity.lshift, etc.\ndef analyze(block: Block) -> None:\n    parity = {v: BOTTOM for v in block}\n\n    def parity_of(value):\n        if isinstance(value, Constant):\n            return Parity.const(value)\n        return parity[value]\n\n    for op in block:\n        transfer = getattr(Parity, op.name)\n        args = [parity_of(arg.find()) for arg in op.args]\n        parity[op] = transfer(*args)\n\n\nFor every operation, we compute the abstract value---the parity---of the\narguments and then call the corresponding method on Parity to get the\nabstract result.\n\nWe need to special case Constants due to a quirk of how the Toy IR is\nconstructed: the constants don't appear in the instruction stream and instead\nare free-floating.\nLet's start by looking at the abstraction function for concrete\nvalues---constants:\nclass Parity:\n    # ...\n    @staticmethod\n    def const(value):\n        if value.value % 2 == 0:\n            return EVEN\n        else:\n            return ODD\n\n\nSeems reasonable enough. Let's pause on operations for a moment and consider an\nexample program:\nv0 = getarg(0)\nv1 = getarg(1)\nv2 = lshift(v0, 1)\nv3 = lshift(v1, 1)\nv4 = add(v2, v3)\nv5 = dummy(v4)\n\n\nThis function (which is admittedly a little contrived) takes two inputs, shifts\nthem left by one bit, adds the result, and then checks the least significant\nbit of the addition result. It then passes that result into a dummy function,\nwhich you can think of as \"return\" or \"escape\".\nTo do some abstract interpretation on this program, we'll need to implement the\ntransfer functions for lshift and add (dummy will just always return\nTOP). We'll start with add. Remember that adding two even numbers returns\nan even number, adding two odd numbers returns an even number, and mixing even\nand odd returns an odd number.\nclass Parity:\n    # ...\n    def add(self, other):\n        if self is BOTTOM or other is BOTTOM:\n            return BOTTOM\n        if self is TOP or other is TOP:\n            return TOP\n        if self is EVEN and other is EVEN:\n            return EVEN\n        if self is ODD and other is ODD:\n            return EVEN\n        return ODD\n\n\nWe also need to fill in the other cases where the operands are top or\nbottom. In this case, they are both \"contagious\"; if either operand is\nbottom, the result is as well. If neither is bottom but either operand is top,\nthe result is as well.\nNow let's look at lshift. Shifting any number left by a non-zero number of\nbits will always result in an even number, but we need to be careful about the\nzero case! Shifting by zero doesn't change the number at all. Unfortunately,\nsince our lattice has no notion of zero, we have to over-approximate here:\nclass Parity:\n    # ...\n    def lshift(self, other):\n        # self << other\n        if other is ODD:\n            return EVEN\n        return TOP\n\n\nThis means that we will miss some opportunities to optimize, but it's a\ntradeoff that's just part of the game. (We could also add more elements to our\nlattice, but that's a topic for another day.)\nNow, if we run our abstract interpretation, we'll collect some interesting\nproperties about the program. If we temporarily hack on the internals of\nbb_to_str, we can print out parity information alongside the IR operations:\nv0:top = getarg(0)\nv1:top = getarg(1)\nv2:even = lshift(v0, 1)\nv3:even = lshift(v1, 1)\nv4:even = add(v2, v3)\nv5:top = dummy(v4)\n\n\nThis is pretty awesome, because we can see that v4, the result of the\naddition, is always even. Maybe we can do something with that information.\nOptimization\nOne way that a program might check if a number is odd is by checking the least\nsignificant bit. This is a common pattern in C code, where you might see code\nlike y = x & 1. Let's introduce a bitand IR operation that acts like the\n& operator in C/Python. Here is an example of use of it in our program:\nv0 = getarg(0)\nv1 = getarg(1)\nv2 = lshift(v0, 1)\nv3 = lshift(v1, 1)\nv4 = add(v2, v3)\nv5 = bitand(v4, 1)  # new!\nv6 = dummy(v5)\n\n\nWe'll hold off on implementing the transfer function for it---that's left as an\nexercise for the reader---and instead do something different.\nInstead, we'll see if we can optimize operations of the form bitand(X, 1). If\nwe statically know the parity as a result of abstract interpretation, we can\nreplace the bitand with a constant 0 or 1.\nWe'll first modify the analyze function (and rename it) to return a new\nBlock containing optimized instructions:\ndef simplify(block: Block) -> Block:\n    parity = {v: BOTTOM for v in block}\n\n    def parity_of(value):\n        if isinstance(value, Constant):\n            return Parity.const(value)\n        return parity[value]\n\n    result = Block()\n    for op in block:\n        # TODO: Optimize op\n        # Emit\n        result.append(op)\n        # Analyze\n        transfer = getattr(Parity, op.name)\n        args = [parity_of(arg.find()) for arg in op.args]\n        parity[op] = transfer(*args)\n    return result\n\n\nWe're approaching this the way that PyPy does things under the hood, which is\nall in roughly a single pass. It tries to optimize an instruction away, and if\nit can't, it copies it into the new block.\nNow let's add in the bitand optimization. It's mostly some gross-looking\npattern matching that checks if the right hand side of a bitwise and\noperation is 1 (TODO: the left hand side, too). CF had some neat ideas on how\nto make this more ergonomic, which I might save for later.3\nThen, if we know the parity, optimize the bitand into a constant.\ndef simplify(block: Block) -> Block:\n    parity = {v: BOTTOM for v in block}\n\n    def parity_of(value):\n        if isinstance(value, Constant):\n            return Parity.const(value)\n        return parity[value]\n\n    result = Block()\n    for op in block:\n        # Try to simplify\n        if isinstance(op, Operation) and op.name == \"bitand\":\n            arg = op.arg(0)\n            mask = op.arg(1)\n            if isinstance(mask, Constant) and mask.value == 1:\n                if parity_of(arg) is EVEN:\n                    op.make_equal_to(Constant(0))\n                    continue\n                elif parity_of(arg) is ODD:\n                    op.make_equal_to(Constant(1))\n                    continue\n        # Emit\n        result.append(op)\n        # Analyze\n        transfer = getattr(Parity, op.name)\n        args = [parity_of(arg.find()) for arg in op.args]\n        parity[op] = transfer(*args)\n    return result\n\n\nRemember: because we use union-find to rewrite instructions in the optimizer\n(make_equal_to), later uses of the same instruction get the new\noptimized version \"for free\" (find).\nLet's see how it works on our IR:\nv0 = getarg(0)\nv1 = getarg(1)\nv2 = lshift(v0, 1)\nv3 = lshift(v1, 1)\nv4 = add(v2, v3)\nv6 = dummy(0)\n\n\nHey, neat! bitand disappeared and the argument to dummy is now the constant\n0 because we know the lowest bit.\nWrapping up\nHopefully you have gained a little bit of an intuitive understanding of\nabstract interpretation. Last year, being able to write some code made me more\ncomfortable with the math. Now being more comfortable with the math is helping\nme write the code. It's nice upward spiral.\nThe two abstract domains we used in this post are simple and not very useful in\npractice but it's possible to get very far using slightly more complicated\nabstract domains. Common domains include: constant propagation, type inference,\nrange analysis, effect inference, liveness, etc. For example, here is a a\nsample lattice for constant propagation:\n\n\n    \n    \n\n\nIt has multiple levels to indicate more and less precision. For example, you\nmight learn that a variable is either 1 or 2 and be able to encode that as\nnonnegative instead of just going straight to top.\nCheck out some real-world abstract interpretation in open source projects:\n\nKnown bits in LLVM\nConstant range in LLVM\nBut I am told that the ranges don't form a lattice (see Interval Analysis and Machine Arithmetic: Why Signedness Ignorance Is Bliss)\nTristate numbers for known bits in Linux eBPF\nRange analysis in Linux eBPF\nGDB prologue analysis\n  of assembly to understand the stack and find frame pointers without using\n  DWARF (some\n  docs)\n\nIf you have some readable examples, please share them so I can add.\nAcknowledgements\nThank you to CF Bolz-Tereick for the toy optimizer and\nhelping edit this post!\n\n\n\n\nIn the words of abstract interpretation researchers Vincent Laviron\nand Francesco Logozzo in their paper Refining Abstract\nInterpretation-based Static Analyses with Hints (APLAS 2009):\n\nThe three main elements of an abstract interpretation are: (i) the\nabstract elements (\"which properties am I interested in?\"); (ii) the\nabstract transfer functions (\"which is the abstract semantics of basic\nstatements?\"); and (iii) the abstract operations (\"how do I combine the\nabstract elements?\").\n\nWe don't have any of these \"abstract operations\" in this post because\nthere's no control flow but you can read about them elsewhere!\u00a0\u21a9\n\n\nThese abstract values are arranged in a lattice, which is a\nmathematical structure with some properties but the most important ones are\nthat it has a top, a bottom, a partial order, a meet operation, and values\ncan only move in one direction on the lattice.\nUsing abstract values from a lattice promises two things:\n\nThe analysis will terminate\nThe analysis will be correct for any run of the program, not just one\n  sample run\n\n\u21a9\n\n\nSomething about __match_args__ and @property...\u00a0\u21a9",
      "tags": "toy-optimizer",
      "url": "https://www.pypy.org/posts/2024/07/toy-abstract-interpretation.html"
    },
    {
      "title": "Mining JIT traces for missing optimizations with Z3",
      "text": "In my last post I've described how to use Z3 to find simple local peephole\noptimization patterns\nfor the integer operations in PyPy's JIT. An example is int_and(x, 0) ->\n0. In this post I want to scale up the problem of identifying possible\noptimizations to much bigger instruction sequences, also using Z3. For that, I\nam starting with the JIT traces of real benchmarks, after they have been\noptimized by the optimizer of PyPy's JIT. Then we can ask Z3 to find\ninefficient integer operations in those traces.\nStarting from the optimized traces of real programs has some big\nadvantages over the \"classical\" superoptimization approach of generating and\nthen trying all possible sequences of instructions. It avoids the\ncombinatorial explosion that happens with the latter approach. Also, starting\nfrom the traces of benchmarks or (even better) actual programs makes sure that\nwe actually care about the missing optimizations\nthat are found in this way. And because the traces are analyzed after they have\nbeen optimized by PyPy's optimizer, we only get reports for missing\noptimizations, that the JIT isn't able to do (yet).\nThe techniques and experiments I describe in this post are again the result of\na bunch of discussions with John Regehr at a conference a few weeks ago, as\nwell as reading his blog posts and papers. Thanks John! Also thanks to Max\nBernstein for super helpful feedback on the drafts\nof this blog post (and for poking me to write things in general).\nHigh-Level Approach\nThe approach that I took works as follows:\n\nRun benchmarks or other interesting programs and then dump the IR of the JIT\n  traces into a file. The traces have at that point been already optimized by\n  the PyPy JIT's optimizer.\nFor every trace, ignore all the operations on non-integer variables.\nTranslate every integer operation into a Z3 formula.\nFor every operation, use Z3 to find out whether the operation is redundant\n  (how that is done is described below).\nIf the operation is redundant, the trace is less efficient than it could have\n  been, because the optimizer could also have removed the operation. Report the\n  inefficiency.\nMinimize the inefficient programs by removing as many operations as possible\n  to make the problem easier to understand.\n\nIn the post I will describe the details and show some pseudocode of the\napproach. I'll also make the proper code public eventually (but it needs a\nhealthy dose of cleanups first).\nDumping PyPy Traces\nPyPy will write its JIT traces into the file out if the environment variable\nPYPYLOG is set as follows:\nPYPYLOG=jit-log-opt:out pypy <program.py>\n\n\nThis environment variable works for PyPy, but also for other virtual machines\nbuilt with RPython.\n(This is really a side point for the rest of the blog post, but since the\nquestion came up I wanted to clarify it: Operations on integers in the Python\nprogram that the JIT is running don't all correspond 1-to-1 with the int_...\noperations in the traces. The int_... trace operations always operate on\nmachine words. The Python int type supports arbitrarily large integers. PyPy\nwill optimistically try to lower the operations on Python integers into machine\nword operations, but adds the necessary guards into the trace to make sure that\noverflow outside of the range of machine words is caught. In case one of these\nguards fails the interpreter switches to a big integer heap-allocated\nrepresentation.)\nEncoding Traces as Z3 formulas\nThe last blog post already contained the code to encode the results of\nindividual trace operations into Z3 formulas, so we don't need to repeat that\nhere. To encode traces of operations we introduce a Z3 variable for every\noperation in the trace and then call the z3_expression function for every\nsingle one of the operations in the trace.\nFor example, for the following trace:\n[i1]\ni2 = uint_rshift(i1, 32)\ni3 = int_and(i2, 65535)\ni4 = uint_rshift(i1, 48)\ni5 = int_lshift(i4, 16)\ni6 = int_or(i5, i3)\njump(i6, i2) # equal\n\n\nWe would get the Z3 formula:\nz3.And(i2 == LShR(i1, 32),\n       i3 == i2 & 65535,\n       i4 == LShR(i1, 48),\n       i5 == i4 << 16)\n\n\nUsually we won't ask for the formula of the whole trace at once. Instead we go\nthrough the trace operation by operation and try to find inefficiencies in the\ncurrent one we are looking at. Roughly like this (pseudo-)code:\ndef newvar(name):\n    return z3.BitVec(name, INTEGER_WIDTH)\n\ndef find_inefficiencies(trace):\n    solver = z3.Solver()\n    var_to_z3var = {}\n    for input_argument in trace.inputargs:\n        var_to_z3var[input_argument] = newz3var(input_argument)\n    for op in trace:\n        var_to_z3var[op] = z3resultvar = newz3var(op.resultvarname)\n        arg0 = op.args[0]\n        z3arg0 = var_to_z3var[arg0]\n        if len(op.args) == 2:\n            arg1 = op.args[1]\n            z3arg1 = var_to_z3var[arg1]\n        else:\n            z3arg1 = None\n        res, valid_if = z3_expression(op.name, z3arg0, z3arg1)\n        # checking for inefficiencies, see the next sections\n        ...\n        if ...:\n            return \"inefficient\", op\n\n        # not inefficient, assert op into the solver and continue with the next op\n        solver.add(z3resultvar == res)\n    return None # no inefficiency found\n\n\nIdentifying constant booleans with Z3\nTo get started finding inefficiencies in a trace, we can\nfirst focus on boolean variables. For every operation in the trace that\nreturns a bool we can ask Z3 to prove that this variable must be always True or\nalways False. Most of the time, neither of these proofs will succeed. But if Z3\nmanages to prove one of them, we know have found an ineffiency: instead of\ncomputing the boolean result (eg by executing a comparison) the JIT's optimizer\ncould have replaced the operation with the corresponding boolean constant.\nHere's an example of an inefficiency found that way: if x < y and y < z are\nboth true, PyPy's JIT could conclude that x < z must also\nbe true. However, currently the JIT cannot make that conclusion because it\nonly reasons about the concrete ranges (lower and upper bounds) for every\ninteger variable, but it has no way to remember anything about relationships\nbetween different variables. This kind of reasoning would quite often be useful\nto remove list/string bounds checks. Here's a talk about how LLVM does\nthis (but it might be\ntoo heavyweight for a JIT setting).\nHere are some more examples found that way:\n\nx - 1 == x is always False\nx - (x == -1) == -1 is always False. The pattern x - (x == -1) happens a\n  lot in PyPy's hash computations: To be compatible with the CPython hashes we\n  need to make sure that no object's hash is -1 (CPython uses -1 as an error\n  value on the C level).\n\nHere's pseudo-code for how to implement checking boolean operations for\ninefficiencies:\ndef find_inefficiencies(trace):\n    ...\n    for op in trace:\n        ...\n        res, valid_if = z3_expression(op.name, z3arg0, z3arg1)\n        # check for boolean constant result\n        if op.has_boolean_result():\n            if prove(solver, res == 0):\n                return \"inefficient\", op, 0\n            if prove(solver, res == 1):\n                return \"inefficient\", op, 1\n        # checking for other inefficiencies, see the next sections\n        ...\n\n        # not inefficient, add op to the solver and continue with the next op\n        solver.add(z3resultvar == res)\n    return None # no inefficiency found\n\n\nIdentifying redundant operations\nA more interesting class of redundancy is to try to find two operations in a\ntrace that compute the same result. We can do that by asking Z3 to prove for\neach pair of different operations in the trace to prove that the result is\nalways the same. If a previous operation returns the same result, the JIT could\nhave reused that result instead of re-computing it, saving time. Doing this\nsearch for equivalent operations with Z3 is quadratic in the number of\noperations, but since traces have a maximum length it is not too bad in\npractice.\nThis is the real workhorse of my script so far, it's what finds most of the\ninefficiencies. Here's a few examples:\n\nThe very first and super useful example the script found is int_eq(b, 1) ==\n  b if b is known to be a boolean (ie and integer 0 or 1). I have already\n  implemented this optimization in the JIT.\nSimilarly, int_and(b, 1) == b for booleans.\n(x << 4) & -0xf == x << 4\n((x >> 63) << 1) << 2) >> 3 == x >> 63. In general the JIT is quite bad at\n  optimizing repeated shifts (the infrastructure for doing better with that is\n  already in place, so this will be a relatively easy fix).\n(x & 0xffffffff) | ((x >> 32) << 32) == x. Having the JIT optimize this\n  would maybe require first recognizing that (x >> 32) << 32 can be expressed\n  as a mask: (x & 0xffffffff00000000), and then using (x & c1) | (x & c2) ==\n  x & (c1 | c2)\nA commonly occurring pattern is variations of this one:\n  ((x & 1345)  2048) - 2048 == x & 1345 (with different constants, of\n  course). xor is add without carry, and x & 1345 does not have the bit\n  2048 set. Therefore the  2048 is equivalent to + 2048, which the -\n  2048 cancels. More generally, if a & b == 0, then a + b == a | b == a  b.\n  I don't understand at all why this appears so often in the traces, but I\n  see variations of it a lot. LLVM can optimize this, but GCC\n  can't, thanks to\n  Andrew Pinski for filing the\n  bug!\n\nAnd here's some implementation pseudo-code again:\ndef find_inefficiencies(trace):\n    ...\n    for op in trace:\n        ...\n        res, valid_if = z3_expression(op.name, z3arg0, z3arg1)\n        # check for boolean constant result\n        ...\n        # searching for redundant operations\n        for previous_op in trace:\n            if previous_op is op:\n                break # done, reached the current op\n            previous_op_z3var = var_to_z3var[previous_op]\n            if prove(solver, previous_op_z3var == res):\n                return \"inefficient\", op, previous_op\n        ...\n        # more code here later\n        ...\n\n        # not inefficient, add op to the solver and continue with the next op\n        solver.add(z3resultvar == res)\n    return None # no inefficiency found\n\n\nSynthesizing more complicated constants with exists-forall\nTo find out whether some integer operations always return a constant result, we\ncan't simply use the same trick as for those operations that return boolean\nresults, because enumerating 2\u2076\u2074 possible constants and checking them all\nwould take too long. Like in the last post, we can use z3.ForAll to find out\nwhether Z3 can synthesize a constant for the result of an operation for us.\nIf such a constant exists, the JIT could have removed the operation,\nand replaced it with the constant that Z3 provides.\nHere a few examples of inefficiencies found this way:\n\n(x  1)  x == 1 (or, more generally: (x  y)  x == y)\nif x | y == 0, it follows that x == 0 and y == 0\nif x != MAXINT, then x + 1 > x\n\nImplementing this is actually slightly annoying. The solver.add calls for\nnon-inefficient ops add assertions to the solver, which are now confusing the\nz3.ForAll query. We could remove all assertion from the solver, then do the\nForAll query, then add the assertions back. What I ended doing instead was\ninstantiating a second solver object that I'm using for the ForAll queries,\nthat remains empty the whole time.\ndef find_inefficiencies(trace):\n    solver = z3.Solver()\n    empty_solver = z3.Solver()\n    var_to_z3var = {}\n    ...\n    for op in trace:\n        ...\n        res, valid_if = z3_expression(op.name, z3arg0, z3arg1)\n        # check for boolean constant result\n        ...\n        # searching for redundant operations\n        ...\n        # checking for constant results\n        constvar = z3.BitVec('find_const', INTEGER_WIDTH)\n        condition = z3.ForAll(\n            var_to_z3var.values(),\n            z3.Implies(\n                *solver.assertions(),\n                expr == constvar\n            )\n        )\n        if empty_solver.check(condition) == z3.sat:\n            model = empty_solver.model()\n            const = model[constvar].as_signed_long()\n            return \"inefficient\", op, const\n\n        # not inefficient, add op to the solver and continue with the next op\n        solver.add(z3resultvar == res)\n    return None # no inefficiency found\n\n\nMinimization\nAnalyzing an inefficiency by hand in the context of a larger trace is quite\ntedious. Therefore I've implemented a (super inefficient) script to try to make\nthe examples smaller. Here's how that works:\n\nFirst throw out all the operations that occur after the inefficient operation\n  in the trace.\nThen we remove all \"dead\" operations, ie operations that don't have their\n  results used (all the operations that we can analyze with Z3 are without side\n  effects).\nNow we try to remove every guard in the trace one by one and check\n  afterwards, whether the resulting trace still has an inefficiency.\nWe also try to replace every single operation with a new argument to the\n  trace, to see whether the inefficiency is still present.\n\nThe minimization process is sort of inefficient and I should probably be using\n shrinkray or\n C-Reduce instead. However, it\n seems to work well in practice and the runtime isn't too bad.\nResults\nSo far I am using the JIT traces of three programs: 1) Booting Linux on the\nPydrofoil RISC-V emulator, 2) booting Linux on the Pydrofoil ARM emulator, and 3)\nrunning the PyPy bootstrap process on top of PyPy.\nI picked these programs because most Python programs don't contain interesting\namounts of integer operations, and the traces of the emulators\ncontain a lot of them. I also used the bootstrap process because I still wanted\nto try a big Python program and personally care about the runtime of this\nprogram a lot.\nThe script identifies 94\ninefficiencies in the traces, a lot of them come from repeating\npatterns. My next steps will be to manually inspect them all, categorize them, and\nimplement easy optimizations identified that way. I also want a way to sort the\nexamples by execution count in the benchmarks, to get a feeling for which of\nthem are most important.\nI didn't investigate the full set of Python\nbenchmarks that PyPy uses yet, because I don't expect\nthem to contain interesting amounts of integer operations, but maybe I am wrong\nabout that? Will have to try eventually.\nConclusion\nThis was again much easier to do than I would have expected! Given that I had\nthe translation of trace ops to Z3 already in place, it was a matter of about a\nday's of programming to use this infrastructure to find the first problems and\nminimizing them.\nReusing the results of existing operations or replacing operations by constants\ncan be seen as \"zero-instruction superoptimization\". I'll probably be rather\nbusy for a while to add the missing optimizations identified by my simple\nscript. But later extensions to actually synthesize one or several operations\nin the attempt to optimize the traces more and find more opportunities should\nbe possible.\nFinding inefficiencies in traces with Z3 is significantly less\nannoying and also less error-prone than just manually inspecting traces and\ntrying to spot optimization opportunities.\nRandom Notes and Sources\nAgain, John's blog posts:\n\nLet\u2019s Work on an LLVM Superoptimizer\nEarly Superoptimizer Results\nA Few Synthesizing Superoptimizer Results\nSynthesizing Constants\n\nand papers:\n\nA Synthesizing Superoptimizer\nHydra: Generalizing Peephole Optimizations with Program Synthesis\n\nI remembered recently that I had seen the approach of optimizing the traces of\na tracing JIT with Z3 a long time ago, as part of the (now long dead, I think)\nSPUR\nproject.\nThere's a workshop\npaper\nfrom 2010 about this. SPUR was trying to use Z3 built into the actual JIT (as\nopposed to using Z3 only to find places where the regular optimizers could be\nimproved). In addition to bitvectors, SPUR also used the Z3 support for arrays\nto model the C# heap and remove redundant stores. This is still another future\nextension for all the Z3 work I've been doing in the context of the PyPy JIT.",
      "tags": "jit,z3",
      "url": "https://www.pypy.org/posts/2024/07/mining-jit-traces-missing-optimizations-z3.html"
    },
    {
      "title": "Finding Simple Rewrite Rules for the JIT with Z3",
      "text": "In June I was at the PLDI conference in\nCopenhagen to present a paper\nI co-authored with Max Bernstein. I also finally\nmet John Regehr, who I'd been talking on social\nmedia for ages but had never met. John has been working on compiler correctness\nand better techniques for building compilers and optimizers since a very long\ntime. The blog post Finding JIT Optimizer Bugs using SMT Solvers and\nFuzzing\nwas heavily inspired by this work. We talked a lot about his and his groups\nwork on using Z3 for\nsuperoptimization and for\nfinding missing optimizations. I have applied some of the things John told me\nabout to the traces of PyPy's JIT, and wanted to blog about that. However, my\ndraft felt quite hard to understand. Therefore I have now written this current\npost, to at least try to provide a somewhat gentler on-ramp to the topic.\nIn this post we will use the Python-API to Z3 to find local peephole rewrite\nrules for the operations in the intermediate representation of PyPy's tracing\nJIT. The code for this is simple enough that we can go through all of it.\nThe PyPy JIT produces traces of machine level instructions, which are optimized\nand then turned into machine code. The optimizer uses a number of approaches to\nmake the traces more efficient. For integer operations it applies a number of\narithmetic simplification rules rules, for example int_add(x, 0) -> x. When\nimplementing these rules in the JIT there are two problems: How do we know\nthat the rules are correct? And how do we know that we haven't forgotten any\nrules? We'll try to answer both of these, but the first one in particular.\nWe'll be using Z3, a satisfiability module theories (SMT) solver which has good\nbitvector support and most importantly an excellent Python API. We can use the\nsolver to reason about bitvectors, which are how we will model machine\nintegers.\nTo find rewrite rules, we will consider the binary operations (i.e. those\ntaking two arguments) in PyPy traces that take and produce integers. The\ncompletely general form op(x, y) is not simplifiable on its own. But if\neither x == y\nor if one of the arguments is a constant, we can potentially simplify the\noperation into a simpler form. The results are either the variable x, or a\n(potentially different) constant. We'll ignore constant-folding where both\narguments of the binary operation are constants. The possible results for a\nsimplifiable binary operation are the variable x or another constant. This\nleaves the following patterns as possibilities:\n\nop(x, x) == x\nop(x, x) == c1\nop(x, c1) == x\nop(c1, x) == x\nop(x, c1) == c2\nop(c1, x) == c2\n\nOur approach will be to take every single supported binary integer operation,\ninstantiate all of these patterns, and try to ask Z3 whether the resulting\nsimplification is valid for all values of x.\nQuick intro to the Z3 Python-API\nHere's a terminal session showing the use of the Z3 Python API:\n>>>> import z3\n>>>> # construct a Z3 bitvector variable of width 8, with name x:\n>>>> x = z3.BitVec('x', 8)\n>>>> # construct a more complicated formula by using operator overloading:\n>>>> x + x\nx + x\n>>>> x + 1\nx + 1\n\n\nZ3 checks the \"satisfiability\" of a formula. This means that it tries to find\nan example set of concrete values for the variables that occur in a formula,\nsuch that the formula becomes true. Examples:\n>>>> solver = z3.Solver()\n>>>> solver.check(x * x == 3)\nunsat\n>>>> # meaning no x fulfils this property\n>>>>\n>>>> solver.check(x * x == 9)\nsat\n>>>> model = solver.model()\n>>>> model\n[x = 253]\n>>>> model[x].as_signed_long()\n-3\n>>>> # 253 is the same as -3 in two's complement arithmetic with 8 bits\n\n\nIn order to use Z3 to prove something, we can ask Z3 to find counterexamples\nfor the statement, meaning concrete values that would make the negation of the\nstatement true:\n>>>> solver.check(z3.Not(x  -1 == ~x))\nunsat\n\n\nThe result unsat means that we just proved that x  -1 == ~x is true for\nall x, because there is no value for x that makes not (x  -1 == ~x)\ntrue (this works because -1 has all the bits set).\nIf we try to prove something incorrect in this way, the following happens:\n>>>> solver.check(z3.Not(x  -1 == x))\nsat\n\n\nsat shows that x  -1 == x is (unsurprisingly) not always true, and we can\nask for a counterexample:\n>>>> solver.model()\n[x = 0]\n\n\nThis way of proving this works because the check calls try to solve an\n(implicit) \"exists\" quantifier, over all the Z3 variables used in the formula.\ncheck will either return z3.unsat, which means that no concrete values make\nthe formula true; or z3.sat, which means that you can get some concrete\nvalues that make the formula true by calling solver.model().\nIn math terms we prove things using check by de-Morgan's rules for quantifiers:\n$$ \\lnot \\exists x: \\lnot f(x) \\implies \\forall x: f(x) $$\nNow that we've seen the basics of using the Z3 API on a few small examples,\nwe'll use it in a bigger program.\nEncoding the integer operations of RPython's JIT into Z3 formulas\nNow we'll use the API to reason about the integer operations of the PyPy JIT\nintermediate representation (IR). The binary integer operations are:\nopnames2 = [\n\"int_add\",\n\"int_sub\",\n\"int_mul\",\n\"int_and\",\n\"int_or\",\n\"int_xor\",\n\"int_eq\",\n\"int_ne\",\n\"int_lt\",\n\"int_le\",\n\"int_gt\",\n\"int_ge\",\n\"uint_lt\",\n\"uint_le\",\n\"uint_gt\",\n\"uint_ge\",\n\"int_lshift\",\n\"int_rshift\",\n\"uint_rshift\",\n\"uint_mul_high\",\n\"int_pydiv\",\n\"int_pymod\",\n]\n\n\nThere's not much special about the integer operations. Like in LLVM, most of\nthem are signedness-independent: int_add, int_sub, int_mul, ... work\ncorrectly for unsigned integers but also for\ntwo's-complement signed\nintegers. Exceptions for that are order comparisons like int_lt etc. for\nwhich we have unsigned variants uint_lt etc. All operations that produce a\nboolean result return a full-width integer 0 or 1 (the PyPy JIT supports\nonly word-sized integers in its intermediate representation)\nIn order to reason about the IR operations, some ground work:\nimport z3\n\nINTEGER_WIDTH = 64\nsolver = z3.Solver()\nsolver.set(\"timeout\", 10000) # milliseconds, ie 10s\nxvar = z3.BitVec('x', INTEGER_WIDTH)\nconstvar = z3.BitVec('const', INTEGER_WIDTH)\nconstvar2 = z3.BitVec('const2', INTEGER_WIDTH)\nTRUEBV = z3.BitVecVal(1, INTEGER_WIDTH)\nFALSEBV = z3.BitVecVal(0, INTEGER_WIDTH)\n\n\nAnd here's the a function to turn an integer IR operation of PyPy's JIT into Z3\nformulas:\ndef z3_expression(opname, arg0, arg1=None):\n    \"\"\" computes a tuple of (result, valid_if) of Z3 formulas. `result` is the\n    formula representing the result of the operation, given argument formulas\n    arg0 and arg1. `valid_if` is a pre-condition that must be true for the\n    result to be meaningful. \"\"\"\n    result = None\n    valid_if = True # the precondition is mostly True, with few exceptions\n    if opname == \"int_add\":\n        result = arg0 + arg1\n    elif opname == \"int_sub\":\n        result = arg0 - arg1\n    elif opname == \"int_mul\":\n        result = arg0 * arg1\n    elif opname == \"int_and\":\n        result = arg0 & arg1\n    elif opname == \"int_or\":\n        result = arg0 | arg1\n    elif opname == \"int_xor\":\n        result = arg0  arg1\n    elif opname == \"int_eq\":\n        result = cond(arg0 == arg1)\n    elif opname == \"int_ne\":\n        result = cond(arg0 != arg1)\n    elif opname == \"int_lt\":\n        result = cond(arg0 < arg1)\n    elif opname == \"int_le\":\n        result = cond(arg0 <= arg1)\n    elif opname == \"int_gt\":\n        result = cond(arg0 > arg1)\n    elif opname == \"int_ge\":\n        result = cond(arg0 >= arg1)\n    elif opname == \"uint_lt\":\n        result = cond(z3.ULT(arg0, arg1))\n    elif opname == \"uint_le\":\n        result = cond(z3.ULE(arg0, arg1))\n    elif opname == \"uint_gt\":\n        result = cond(z3.UGT(arg0, arg1))\n    elif opname == \"uint_ge\":\n        result = cond(z3.UGE(arg0, arg1))\n    elif opname == \"int_lshift\":\n        result = arg0 << arg1\n        valid_if = z3.And(arg1 >= 0, arg1 < INTEGER_WIDTH)\n    elif opname == \"int_rshift\":\n        result = arg0 << arg1\n        valid_if = z3.And(arg1 >= 0, arg1 < INTEGER_WIDTH)\n    elif opname == \"uint_rshift\":\n        result = z3.LShR(arg0, arg1)\n        valid_if = z3.And(arg1 >= 0, arg1 < INTEGER_WIDTH)\n    elif opname == \"uint_mul_high\":\n        # zero-extend args to 2*INTEGER_WIDTH bit, then multiply and extract\n        # highest INTEGER_WIDTH bits\n        zarg0 = z3.ZeroExt(INTEGER_WIDTH, arg0)\n        zarg1 = z3.ZeroExt(INTEGER_WIDTH, arg1)\n        result = z3.Extract(INTEGER_WIDTH * 2 - 1, INTEGER_WIDTH, zarg0 * zarg1)\n    elif opname == \"int_pydiv\":\n        valid_if = arg1 != 0\n        r = arg0 / arg1\n        psubx = r * arg1 - arg0\n        result = r + (z3.If(arg1 < 0, psubx, -psubx) >> (INTEGER_WIDTH - 1))\n    elif opname == \"int_pymod\":\n        valid_if = arg1 != 0\n        r = arg0 % arg1\n        result = r + (arg1 & z3.If(arg1 < 0, -r, r) >> (INTEGER_WIDTH - 1))\n    elif opname == \"int_is_true\":\n        result = cond(arg0 != FALSEBV)\n    elif opname == \"int_is_zero\":\n        result = cond(arg0 == FALSEBV)\n    elif opname == \"int_neg\":\n        result = -arg0\n    elif opname == \"int_invert\":\n        result = ~arg0\n    else:\n        assert 0, \"unknown operation \" + opname\n    return result, valid_if\n\ndef cond(z3expr):\n    \"\"\" helper function to turn a Z3 boolean result z3expr into a 1 or 0\n    bitvector, using z3.If \"\"\"\n    return z3.If(z3expr, TRUEBV, FALSEBV)\n\n\nWe map the semantics of a PyPy JIT operation to Z3 with the z3_expression\nfunction. It takes the name of a JIT operation and its two (or one) arguments\ninto a pair of Z3 formulas, result and valid_if. The resulting formulas are\nconstructed with the operator overloading of Z3 variables/formulas.\nThe first element result of the result of z3_expression represents the result\nof performing the operation. valid_if is a bool that represents a condition that\nneeds to be True in order for the result of the operation to be defined. E.g.\nint_pydiv(a, b) is only valid if b != 0. Most operations are always valid,\nso they return True as that condition (we'll ignore valid_if for a bit, but it\nwill become more relevant further down in the post).\nWe can define a helper function to prove things by finding counterexamples:\ndef prove(cond):\n    \"\"\" Try to prove a condition cond by searching for counterexamples of its negation. \"\"\"\n    z3res = solver.check(z3.Not(cond))\n    if z3res == z3.unsat:\n        return True\n    elif z3res == z3.unknown: # eg on timeout\n        return False\n    elif z3res == z3.sat:\n        return False\n    assert 0, \"should be unreachable\"\n\n\nFinding rewrite rules\nNow we can start finding our first rewrite rules, following the first pattern\nop(x, x) -> x. We do this by iterating over all the supported binary\noperation names, getting the z3 expression for op(x, x) and then asking Z3 to\nprove op(x, x) == x.\nfor opname in opnames2:\n    result, valid_if = z3_expression(opname, xvar, xvar)\n    if prove(result == xvar):\n        print(f\"{opname}(x, x) -> x, {result}\")\n\n\nThis yields the simplifications:\nint_and(x, x) -> x\nint_or(x, x) -> x\n\n\nSynthesizing constants\nSupporting the next patterns is harder: op(x, x) == c1, op(x, c1) == x, and\nop(c1, x) == x. We don't know which constants to pick to try to get Z3 to\nprove the equality. We could iterate over common constants like 0, 1,\nMAXINT, etc, or even over all the 256 values for a bitvector of length 8.\nHowever, we will instead ask Z3 to find the constants for us too.\nThis can be done by using quantifiers, in this case z3.ForAll. The query we\npose to Z3 is \"does there exist a constant c1 such that for all x the\nfollowing is true: op(x, c1) == x? Note that the constant c1 is not\nnecessarily unique, there could be many of them. We generate several matching\nconstant, and add that they must be different to the condition of the second\nand further queries.\nWe can express this in a helper function:\ndef find_constant(z3expr, number_of_results=5):\n    condition = z3.ForAll(\n        [xvar],\n        z3expr\n    )\n    for i in range(number_of_results):\n        checkres = solver.check(condition)\n        if checkres == z3.sat:\n            # if a solver check succeeds, we can ask for a model, which is\n            # concrete values for the variables constvar\n            model = solver.model()\n            const = model[constvar].as_signed_long()\n            yield const\n            # make sure we don't generate the same constant again on the\n            # next call\n            condition = z3.And(constvar != const, condition)\n        else:\n            # no (more) constants found\n            break\n\n\nWe can use this new function for the three mentioned patterns:\n# try to find constants for op(x, x) == c\nfor opname in opnames2:\n    result, valid_if = z3_expression(opname, xvar, xvar)\n    for const in find_constant(result == constvar):\n        print(f\"{opname}(x, x) -> {const}\")\n# try to find constants for op(x, c) == x and op(c, x) == x\nfor opname in opnames2:\n    result, valid_if = z3_expression(opname, xvar, constvar)\n    for const in find_constant(result == xvar):\n        print(f\"{opname}(x, {const}) -> x\")\n    result, valid_if = z3_expression(opname, constvar, xvar)\n    for const in find_constant(result == xvar):\n        print(f\"{opname}({const}, x) -> x\")\n# this code is not quite correct, we'll correct it later\n\n\nTogether this yields the following new simplifications:\n# careful, these are not all correct!\nint_sub(x, x) -> 0\nint_xor(x, x) -> 0\nint_eq(x, x) -> 1\nint_ne(x, x) -> 0\nint_lt(x, x) -> 0\nint_le(x, x) -> 1\nint_gt(x, x) -> 0\nint_ge(x, x) -> 1\nuint_lt(x, x) -> 0\nuint_le(x, x) -> 1\nuint_gt(x, x) -> 0\nuint_ge(x, x) -> 1\nuint_rshift(x, x) -> 0\nint_pymod(x, x) -> 0\nint_add(x, 0) -> x\nint_add(0, x) -> x\nint_sub(x, 0) -> x\nint_mul(x, 1) -> x\nint_mul(1, x) -> x\nint_and(x, -1) -> x\nint_and(-1, x) -> x\nint_or(x, 0) -> x\nint_or(0, x) -> x\nint_xor(x, 0) -> x\nint_xor(0, x) -> x\nint_lshift(x, 0) -> x\nint_rshift(x, 0) -> x\nuint_rshift(x, 0) -> x\nint_pydiv(x, 1) -> x\nint_pymod(x, 0) -> x\n\n\nMost of these look good at first glance, but the last one reveals a problem:\nwe've been ignoring the valid_if expression up to now. We can stop doing that by\nchanging the code like this, which adds z3.And(valid_if, ...) to the argument of\nthe calls to find_constant:\n# try to find constants for op(x, x) == c, op(x, c) == x and op(c, x) == x\nfor opname in opnames2:\n    result, valid_if = z3_expression(opname, xvar, xvar)\n    for const in find_constant(z3.And(valid_if, result == constvar)):\n        print(f\"{opname}(x, x) -> {const}\")\n# try to find constants for op(x, c) == x and op(c, x) == x\nfor opname in opnames2:\n    result, valid_if = z3_expression(opname, xvar, constvar)\n    for const in find_constant(z3.And(result == xvar, valid_if)):\n        print(f\"{opname}(x, {const}) -> x\")\n    result, valid_if = z3_expression(opname, constvar, xvar)\n    for const in find_constant(z3.And(result == xvar, valid_if)):\n        print(f\"{opname}({const}, x) -> x\")\n\n\nAnd we get this list instead:\nint_sub(x, x) -> 0\nint_xor(x, x) -> 0\nint_eq(x, x) -> 1\nint_ne(x, x) -> 0\nint_lt(x, x) -> 0\nint_le(x, x) -> 1\nint_gt(x, x) -> 0\nint_ge(x, x) -> 1\nuint_lt(x, x) -> 0\nuint_le(x, x) -> 1\nuint_gt(x, x) -> 0\nuint_ge(x, x) -> 1\nint_add(x, 0) -> x\nint_add(0, x) -> x\nint_sub(x, 0) -> x\nint_mul(x, 1) -> x\nint_mul(1, x) -> x\nint_and(x, -1) -> x\nint_and(-1, x) -> x\nint_or(x, 0) -> x\nint_or(0, x) -> x\nint_xor(x, 0) -> x\nint_xor(0, x) -> x\nint_lshift(x, 0) -> x\nint_rshift(x, 0) -> x\nuint_rshift(x, 0) -> x\nint_pydiv(x, 1) -> x\n\n\nSynthesizing two constants\nFor the patterns op(x, c1) == c2 and op(c1, x) == c2 we need to synthesize\ntwo constants. We can again write a helper method for that:\ndef find_2consts(z3expr, number_of_results=5):\n    condition = z3.ForAll(\n        [xvar],\n        z3expr\n    )\n    for i in range(number_of_results):\n        checkres = solver.check(condition)\n        if checkres == z3.sat:\n            model = solver.model()\n            const = model[constvar].as_signed_long()\n            const2 = model[constvar2].as_signed_long()\n            yield const, const2\n            condition = z3.And(z3.Or(constvar != const, constvar2 != const2), condition)\n        else:\n            return\n\n\nAnd then use it like this:\nfor opname in opnames2:\n    # try to find constants c1, c2 such that op(c1, x) -> c2\n    result, valid_if = z3_expression(opname, constvar, xvar)\n    consts = find_2consts(z3.And(valid_if, result == constvar2))\n    for const, const2 in consts:\n        print(f\"{opname}({const}, x) -> {const2}\")\n    # try to find constants c1, c2 such that op(x, c1) -> c2\n    result, valid_if = z3_expression(opname, xvar, constvar)\n    consts = find_2consts(z3.And(valid_if, result == constvar2))\n    for const, const2 in consts:\n        print(\"%s(x, %s) -> %s\" % (opname, const, const2))\n\n\nWhich yields some straightforward simplifications:\nint_mul(0, x) -> 0\nint_mul(x, 0) -> 0\nint_and(0, x) -> 0\nint_and(x, 0) -> 0\nuint_lt(x, 0) -> 0\nuint_le(0, x) -> 1\nuint_gt(0, x) -> 0\nuint_ge(x, 0) -> 1\nint_lshift(0, x) -> 0\nint_rshift(0, x) -> 0\nuint_rshift(0, x) -> 0\nuint_mul_high(0, x) -> 0\nuint_mul_high(1, x) -> 0\nuint_mul_high(x, 0) -> 0\nuint_mul_high(x, 1) -> 0\nint_pymod(x, 1) -> 0\nint_pymod(x, -1) -> 0\n\n\nA few require a bit more thinking:\nint_or(-1, x) -> -1\nint_or(x, -1) -> -1\n\n\nThe are true because in two's complement, -1 has all bits set.\nThe following ones require recognizing that -9223372036854775808 == -2**63 is\nthe most negative signed 64-bit integer, and 9223372036854775807 == 2 ** 63 -\n1 is the most positive one:\nint_lt(9223372036854775807, x) -> 0\nint_lt(x, -9223372036854775808) -> 0\nint_le(-9223372036854775808, x) -> 1\nint_le(x, 9223372036854775807) -> 1\nint_gt(-9223372036854775808, x) -> 0\nint_gt(x, 9223372036854775807) -> 0\nint_ge(9223372036854775807, x) -> 1\nint_ge(x, -9223372036854775808) -> 1\n\n\nThe following ones are true because the bitpattern for -1 is the largest\nunsigned number:\nuint_lt(-1, x) -> 0\nuint_le(x, -1) -> 1\nuint_gt(x, -1) -> 0\nuint_ge(-1, x) -> 1\n\n\nStrength Reductions\nAll the patterns so far only had a variable or a constant on the target of the\nrewrite. We can also use the machinery to do strengh-reductions where we\ngenerate a single-argument operation op1(x) for input operations op(x, c1)\nor op(c1, x). To achieve this, we try all combinations of binary and unary\noperations. (We won't consider strength reductions where a binary operation\ngets turned into a \"cheaper\" other binary operation here.)\nopnames1 = [\n\"int_is_true\",\n\"int_is_zero\",\n\"int_neg\",\n\"int_invert\",\n]\n\nfor opname in opnames2:\n    for opname1 in opnames1:\n        result, valid_if = z3_expression(opname, xvar, constvar)\n        # try to find a constant op(x, c) == g(x)\n        result1, valid_if1 = z3_expression(opname1, xvar)\n        consts = find_constant(z3.And(valid_if, valid_if1, result == result1))\n        for const in consts:\n            print(f\"{opname}(x, {const}) -> {opname1}(x)\")\n\n        # try to find a constant op(c, x) == g(x)\n        result, valid_if = z3_expression(opname, constvar, xvar)\n        result1, valid_if1 = z3_expression(opname1, xvar)\n        consts = find_constant(z3.And(valid_if, valid_if1, result == result1))\n        for const in consts:\n            print(f\"{opname}({const}, x) -> {opname1}(x)\")\n\n\nWhich yields the following new simplifications:\nint_sub(0, x) -> int_neg(x)\nint_sub(-1, x) -> int_invert(x)\nint_mul(x, -1) -> int_neg(x)\nint_mul(-1, x) -> int_neg(x)\nint_xor(x, -1) -> int_invert(x)\nint_xor(-1, x) -> int_invert(x)\nint_eq(x, 0) -> int_is_zero(x)\nint_eq(0, x) -> int_is_zero(x)\nint_ne(x, 0) -> int_is_true(x)\nint_ne(0, x) -> int_is_true(x)\nuint_lt(0, x) -> int_is_true(x)\nuint_lt(x, 1) -> int_is_zero(x)\nuint_le(1, x) -> int_is_true(x)\nuint_le(x, 0) -> int_is_zero(x)\nuint_gt(x, 0) -> int_is_true(x)\nuint_gt(1, x) -> int_is_zero(x)\nuint_ge(x, 1) -> int_is_true(x)\nuint_ge(0, x) -> int_is_zero(x)\nint_pydiv(x, -1) -> int_neg(x)\n\n\nConclusions\nWith not very little code we managed to generate a whole lot of local\nsimplifications for integer operations in the IR of PyPy's JIT. The rules\ndiscovered that way are \"simple\", in the sense that they only require looking\nat a single instruction, and not where the arguments of that instruction came\nfrom. They also don't require any knowledge about the properties of the\narguments of the instructions (e.g. that they are positive).\nThe rewrites in this post have mostly been in PyPy's JIT already. But now we\nmechanically confirmed that they are correct. I've also added the remaining\nuseful looking ones, in particular int_eq(x, 0) -> int_is_zero(x) etc.\nIf we wanted to scale this approach up, we would have to work much harder!\nThere are a bunch of problems that come with generalizing the approach to\nlooking at sequences of instructions:\n\n\nCombinatorial explosion: if we look at sequences of instructions, we very\n  quickly get a combinatorial explosion and it becomes untractable to try all\n  combinations.\n\n\nFinding non-minimal patterns: Some complicated simplifications can be\n  instances of simpler ones. For example, because int_add(x, 0) -> x, it's\n  also true that int_add(int_sub(x, y), 0) -> int_sub(x, y). If we simply\n  generate all possible sequences, we will find the latter simplification rule,\n  which we would usually not care about.\n\n\nUnclear usefulness: if we simply generate all rewrites up to a certain number\n  of instructions, we will get a lot of patterns that are useless in the sense\n  that they typically aren't found in realistic programs. It would be much\n  better to somehow focus on the patterns that real benchmarks are using.\n\n\nIn the next blog post I'll discuss an alternative approach to simply generating\nall possible sequences of instructions, that tries to address these problems.\nThis works by analyzing the real traces of benchmarks and mining those for\ninefficiencies, which only shows problems that occur in actual programs.\nSources\nI've been re-reading a lot of blog posts from John's blog:\n\nLet\u2019s Work on an LLVM Superoptimizer\nEarly Superoptimizer Results\nA Few Synthesizing Superoptimizer Results\nSynthesizing Constants\n\nbut also papers:\n\nA Synthesizing Superoptimizer\nHydra: Generalizing Peephole Optimizations with Program Synthesis\n\nAnother of my favorite blogs has been Philipp Zucker's\nblog in the last year or two, lots of excellent\nposts about/using Z3 on there.",
      "tags": "jit,z3",
      "url": "https://www.pypy.org/posts/2024/07/finding-simple-rewrite-rules-jit-z3.html"
    },
    {
      "title": "Profiling PyPy using the Firefox profiler user interface",
      "text": "Introduction\nIf you ever wanted to profile your Python code on PyPy, you probably came across VMProf \u2014 a statistical profiler for PyPy.\nVMProf's console output can already give some insights into where your code spends time, \nbut it is far from showing all the information captured while profiling.\nThere have been some tools around to visualize VMProf's output.\nUnfortunately the vmprof.com user interface is no longer available and vmprof-server is not as easy to use, you may want to take a look at a local viewer or converter.\nThose so far could give you some general visualizations of your profile, but do not show any PyPy related context like PyPy's log output (PyPyLog, which is output when using the PYPYLOG environment variable to log JIT actions).\nTo bring all of those features together in one tool, you may take a look at the vmprof-firefox-converter.\nCreated in the context of my bachelor's thesis, the vmprof-firefox-converter is a tool for analyzing VMProf profiles with the Firefox profiler user interface. \nInstead of building a new user interface from scratch, this allows us to reuse the user interface work Mozilla put into the Firefox profiler.\nThe Firefox profiler offers a timeline where you can zoom into profiles and work with different visualizations like a flame graph or a stack chart.\nTo understand why there is time spent inside a function, you can revisit the source code and even dive into the intermediate representation of functions executed by PyPy's just-in-time compiler.\nAdditionally, there is a visualization for PyPy's log output, to keep track whether PyPy spent time inside the interpreter, JIT or GC throughout the profiling time.\nProfiling word count\nIn this blog post, I want to show an example of how to use the vmprof-firefox-converter for a simple Python program.\nBased on Ben Hoyt's blog Performance comparison: counting words in Python, Go, C++, C, AWK, Forth, and Rust we will profile two python versions of a word counter running on PyPy. One being a bit more optimized. For this, VMProf will be used, but instead of just going with the console output, we will use the Firefox profiler user interface.\nAt first, we are going to look at a simple way of counting words with Collections.Counter.\nThis will read one line from the standard input at a time and count the words with counter.update()\ncounts = collections.Counter()\nfor line in sys.stdin:\n    words = line.lower().split()\n    counts.update(words)\n\nfor word, count in counts.most_common():\n    print(word, count)\n\n\nTo start profiling, simply execute:\npypy -m vmprofconvert -run simple.py <kjvbible_x10.txt\nThis will run the above code with vmprof, automatically capture and convert the results and finally open the Firefox profiler. \nThe input file is the king James version of the bible concatenated ten times.\nTo get started, we take a look at the call stack.\n\nHere we see that most of the time is spent in native code (marked as blue) e.g., the counter.update() or split() C implementation.\nNow let's proceed with the more optimized version.\nThis time we read 64 Kb of data from the standard input and count the words with counter.update().\ncounts = collections.Counter()\nremaining = ''\nwhile True:\n    chunk = remaining + sys.stdin.read(64*1024)\n    if not chunk:\n        break\n    last_lf = chunk.rfind('\\n')  # process to last LF character\n    if last_lf == -1:\n        remaining = ''\n    else:\n        remaining = chunk[last_lf+1:]\n        chunk = chunk[:last_lf]\n    counts.update(chunk.lower().split())\n\nfor word, count in counts.most_common():\n    print(word, count)\n\n\nAs we did before, we are going to take a peek at the call stack.\n \nNow there is more time spent in native code, caused by larger chunks of text passed to  counter.update().\nThis becomes even more clear by comparing the stack charts.\n\nHere, in the unoptimized case, we only read in one line at each loop iteration.\nThis results in small \"spikes\" in the stack chart. \nBut let's take an even closer look.\n\nZoomed in, we see the call stack alternating between _count_elements() and (unfortunately unsymbolized) native calls coming from reading and splitting the input text (e.g., decode()).\nLet us now take a look at the optimized case.\n\nAnd if we look closer at the same interval as before, we see some spikes, but slightly different.\n\nEven though we do not want to compare the (amount of) milliseconds directly, we clearly see that the spikes are wider, i.e. the time spent in those function calls is longer.\nYou may already know where this comes from.\nWe read a 64 Kb chunk of data from std in and pass that to counter.update(), so both these tasks do more work and take longer.\nBigger chunks mean there is less alternating between reading and counting, so there is more time spent doing work than \"doing\" loop iterations.\nGetting started\nYou can get the converter from GitHub.\nBoth VMProf and the vmprof-firefox-converter were created for profiling PyPy, but you can also use them with CPython. \nThis project is still somewhat experimental, so if you want to try it out, please let us know whether it worked for you.",
      "tags": "profiling,vmprof",
      "url": "https://www.pypy.org/posts/2024/05/vmprof-firefox-converter.html"
    },
    {
      "title": "PyPy v7.3.16 release",
      "text": "PyPy v7.3.16: release of python 2.7, 3.9, and 3.10\nThe PyPy team is proud to release version 7.3.16 of PyPy.\nThis release includes security fixes from upstream CPython, and bugfixes to the\ngarbage collector, described in a gc bug-hunt blog post.\nThe release includes three different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.9, which is an interpreter supporting the syntax and the features of\nPython 3.9, including the stdlib for CPython 3.9.19.\nPyPy3.10, which is an interpreter supporting the syntax and the features of\nPython 3.10, including the stdlib for CPython 3.10.14.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. It follows after 7.3.15 release on Jan 15, 2024\nWe recommend updating. You can find links to download the v7.3.16 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear\nabout it and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with\nmaking RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy. In any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython\nIt's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64).\nApple M1 arm64 machines (macos_arm64).\ns390x running Linux\n\n\nPyPy support Windows 32-bit, Linux PPC64 big- and little-endian, and Linux ARM\n32 bit, but does not release binaries. Please reach out to us if you wish to\nsponsor binary releases for those platforms. Downstream packagers provide\nbinary builds for debian, Fedora, conda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.16 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2024/04/pypy-v7316-release.html"
    },
    {
      "title": "Fixing a Bug in PyPy's Incremental GC",
      "text": "Introduction\nSince last summer, I've been looking on and off into a weird and hard to\nreproduce crash bug in PyPy. It was\nmanifesting only on CI, and it seemed to always happen in the AST rewriting\nphase of pytest, the symptoms being that PyPy would crash\nwith a segfault. All my attempts to reproduce it locally failed, and my\nattempts to try to understand the problem by dumping the involved ASTs lead\nnowhere.\nA few weeks ago, we got two more\nbug reports, the last one by\nthe authors of the nanobind binding\ngenerator, with the same symptoms: crash in AST rewriting, only on CI. I\ndecided to make a more serious push to try to find the bug this time.\nUltimately the problem turned out to be several bugs in PyPy's garbage\ncollector (GC) that had been there since its inception in\n2013.\nUnderstanding the\nsituation turned out to be quite involved, additionally complicated by this\nbeing the first time that I was working on this particular aspect of PyPy's GC.\nSince the bug was so much work to find, I thought I'd write a blog post about\nit.\nThe blog post consists of three parts: first a chronological description of\nwhat I did to find the bug, a technical explanation of what goes wrong, some\nreflections on the bug (and then a bonus bug I also found in the process).\nFinding the Bug\nI started from the failing nanobind CI\nruns\nthat ended with a segfault of the PyPy interpreter. This was only an\nintermittent problem, not every run was failing. When I tried to just run the\ntest suite locally, I couldn't get it to fail. Therefore at first I tried to\nlearn more about what was happening by looking on the CI runners.\nRunning on CI\nI forked the nanobind repo and hacked the CI script in order to get it to use a\nPyPy build with full debug information and more assertions turned on. In order\nto increase the probability of seeing the crash I added an otherwise unused\nmatrix\nvariable to the CI script that just contained 32 parameters. This means every\nbuild is done 32 times (sorry Github for wasting your CPUs \ud83d\ude15). With that\namount of repetition, I got at least one job of every build that was crashing.\nThen I added the -Xfaulthandler option to the PyPy command which will use the\nfaulthandler module\ntry to print a Python stacktrace if the VM segfaults to confirm that PyPy was\nindeed crashing in the AST\nrewriting\nphase\nof pytest, which pytest uses for nicer\nassertions.\nI experimented with hacking our faulthandler implementation to also give me a\nC-level callstack, but that didn't work as well as I hoped.\nThen I tried to run gdb on CI to try to get it\nto print a C callstack at the crash point. You can get gdb to execute commands\nas if typed at the prompt with the -ex commandline option, I used something\nlike this:\ngdb -ex \"set confirm off\" -ex \"set pagination off\" -ex \\\n    \"set debuginfod enabled off\" -ex run -ex where -ex quit \\\n    --args <command> <arguments>\n\n\nBut unfortunately the crash never occurred when running in gdb.\nAfterwards I tried the next best thing, which was configuring the CI runner to\ndump a core file and upload it as a build\nartifact, which worked. Looking\nat the cores locally only sort of worked, because I am running a different\nversion of Ubuntu than the CI runners. So I used\ntmate to be able to log into the\nCI runner after a crash and interactively used gdb there. Unfortunately what I\nlearned from that was that the bug was some kind of memory corruption,\nwhich is always incredibly unpleasant to debug. Basically the header word of a\nPython object had been corrupted somehow at the point of the crash, which means\nthat it's vtable wasn't\nusable any more.\n(Sidenote: PyPy doesn't really use a vtable\npointer,\ninstead it uses half a word in the header for the vtable, and the other half\nfor flags that the GC needs to keep track of the state of the object.\nCorrupting all this is still bad.)\nReproducing Locally\nAt that point it was clear that I had to push to reproduce the problem on my\nlaptop, to allow me to work on the problem more directly and not to always have\nto go via the CI runner. Memory corruption bugs often have a lot of randomness\n(depending on which part of memory gets modified, things might crash or more\nlikely just happily keep running). Therefore I decided to try to brute-force\nreproducing the crash by simply running the tests many many times. Since the\ncrash happened in the AST rewriting phase of pytest, and that happens only if\nno pyc\nfiles\nof the bytecode-compiled rewritten ASTs exist, I made sure to delete them\nbefore every test run.\nTo repeat the test runs I used\nmultitime, which is a simple program\nthat runs a command repeatedly. It's meant for lightweight benchmarking\npurposes, but it also halts the execution of the command if that command exits\nwith an error (and it sleeps a small random time between runs, which might help\nwith randomizing the situation, maybe). Here's a demo:\n\n\n(Max pointed out\nautoclave to me when reviewing\nthis post, which is a more dedicated tool for this job.)\nThankfully, running the tests repeatedly eventually lead to a crash, solving my\n\"only happens on CI\" problem. I then tried various variants to exclude possible\nsources of errors. The first source of errors to exclude in PyPy bugs is the\njust-in-time compiler, so I reran the tests with --jit off to see whether I\ncould still get it to crash, and thankfully I eventually could (JIT bugs are\noften very annoying).\nNext source of bugs to exclude where C-extensions. Since those were the tests\nof nanobind, a framework for creating C-extension modules I was a bit worried\nthat the bug might be in our emulation of CPython's C-API. But running PyPy\nwith the -v option (which will print all the imports as they happen)\nconfirmed that at the point of crash no C-extension had been imported yet.\nUsing rr\nI still couldn't get the bug to happen in GDB, so the tool I tried next was\nrr, the \"reverse debugger\". rr can record the execution of a program and\nlater replay it arbitrarily often. This gives you a time-traveling debugger\nthat allows you to execute the program backwards in addition to forwards.\nEventually I managed to get the crash to happen when running the tests with\nrr record --chaos (--chaos randomizes some decisions that rr takes, to try to\nincrease the chance of reproducing bugs).\nUsing rr well is quite hard, and I'm not very good at it. The main approach I\nuse with rr to debug memory corruption is to replay the crash, then set a\nwatchpoint\nfor the corrupted memory location, then use the command reverse-continue to\nfind the place in the code that mutated the memory location. reverse-continue\nis like continue, except that it will execute the program backwards from the\ncurrent point. Here's a little demo of this:\n\n\nDoing this for my bug revealed that the object that was being corrupted was\nerroneously collected by the garbage collector. For some reason the GC had\nwrongly decided that the object was no longer reachable and therefore put the\nobject into a freelist by writing a pointer to the next entry in the freelist\ninto the first word of the object, overwriting the object's header. The next\ntime the object was used things crashed.\nSide-quest: wrong GC assertions\nAt this point in the process, I got massively side-tracked. PyPy's GC has a\nnumber of debug modes that you can optionally turn on. Those slow down the\nprogram execution a lot, but they should in theory help to understand why the\nGC goes wrong. When I turned them on, I was getting a failing assertion really\nearly in the test execution, complaining about an invariant violation in the GC\nlogic. At first this made me very happy. I thought that this would help me fix\nthe bug more quickly.\nExtremely frustratingly, after two days of work I concluded that the assertion\nlogic itself was wrong. I have fixed that in the meantime too, the details\nof that are in the bonus section at the end of the post.\nUsing GDB scripting to find the real bug\nAfter that disaster I went back to the earlier rr recording without GC assertions\nand tried to understand in more detail why the GC decided to free an object\nthat was still being referenced. To be able to do that I used the GDB Python\nscripting\nAPI to\nwrite some helper commands to understand the state of the GC heap (rr is an\nextension of GDB, so the GDB scripting API works in rr too).\nThe first (small) helper command I wrote with the GDB scripting API was a way\nto pretty-print the currently active GC flags of a random PyPy object, starting\njust from the pointer. The more complex command I wrote was an object tracer,\nwhich follows pointers to GC objects starting from a root object to explore the\nobject graph. The object tracer isn't complete, it doesn't deal with all the\ncomplexities of PyPy's GC. But it was good enough to help me with my problem, I\nfound out that the corrupted object was stored in an array.\nAs an example, here's a function that uses the GDB API to walk one of the\nhelper data structures of the GC, a stack of pointers:\ndef walk_addr_stack(obj):\n    \"\"\" walk an instance of the AddressStack class (which is a linked list of\n    arrays of 1019 pointers).\n\n    the first of the arrays is only partially filled with used_in_last_chunk\n    items, all the other chunks are full.\"\"\"\n    if obj.type.code == gdb.TYPE_CODE_PTR:\n        obj = obj.dereference()\n    used_in_last_chunk = lookup(obj, \"used_in_last_chunk\")\n    chunk = lookup(obj, \"inst_chunk\").dereference()\n    while 1:\n        items = lookup(chunk, \"items\")\n        for i in range(used_in_last_chunk):\n            yield items[i]\n        chunk = lookup(chunk, \"next\")\n        if not chunk:\n            break\n        chunk = chunk.dereference()\n        used_in_last_chunk = 1019\n\n\nThe full file of supporting code I wrote can be found in this\ngist. This is\npretty rough throw-away code, however.\nIn the following recording I show a staged debugging session with some of the\nextra commands I wrote with the Python API. The details aren't important, I\njust wanted to give a bit of a flavor of what inspecting objects looks like:\n\n\nThe next step was to understand why the array content wasn't being correctly\ntraced by the GC, which I eventually managed with some conditional\nbreakpoints,\nmore watchpoints, and using reverse-continue. It turned out to be a bug that\noccurs when the content of one array was memcopied into another array. The\ntechnical details of why the array wasn't traced correctly are described in\ndetail in the next section.\nWriting a unit test\nTo try to make sure I really understood the bug correctly I then wrote a GC\nunit test that shows the problem. Like most of PyPy, our GC is written in\nRPython, a (somewhat strange) subset/dialect of Python2, which can be compiled\nto C code. However, since it is also valid Python2 code, it can be unit-tested\non top of a Python2\nimplementation\n(which is one of the reasons why we keep maintaining PyPy2).\nIn the GC unit tests you have a lot of control about what order things happen\nin, e.g. how objects are allocated, when garbage collection phases happen, etc.\nAfter some trying I managed to write a test that crashes with the same kind of\nmemory corruption that my original crash exhibited: an object that is still\nreachable via an array is collected by the GC. To give you a flavor of what\nthis kind of test looks like, here's an (edited for clarity) version of the\ntest I eventually managed to write\ndef test_incrementality_bug_arraycopy(self):\n    source = self.malloc(VAR, 8) # first array\n    # the stackroots list emulates the C stack\n    self.stackroots.append(source)\n    target = self.malloc(VAR, 8) # second array\n    self.stackroots.append(target)\n    node = self.malloc(S) # unrelated object, will be collected\n    node.x = 5\n    # store reference into source array, calling the write barrier\n    self.writearray(source, 0, node)\n    val = self.gc.collect_step()\n    source = self.stackroots[0] # reload arrays, they might have moved\n    target = self.stackroots[1]\n    # this GC step traces target\n    val = self.gc.collect_step()\n\n    # emulate what a memcopy of arrays does\n    res = self.gc.writebarrier_before_copy(source, target, 0, 0, 2)\n    assert res\n    target[0] = source[0] # copy two elements of the arrays\n    target[1] = source[1]\n    # now overwrite the reference to node in source\n    self.writearray(source, 0, lltype.nullptr(S))\n    # this GC step traces source\n    self.gc.collect_step()\n    # some more collection steps, crucially target isn't traced again\n    # but node is deleted\n    for i in range(3):\n        self.gc.collect_step()\n    # used to crash, node got collected\n    assert target[0].x == 5\n\n\nOne of the good properties of testing our GC that way is that all the memory is\nemulated. The crash in the last line of the test isn't a segfault at all,\ninstead you get a nice exception saying that you tried to access a freed chunk\nof memory and you can then debug this with a python2 debugger.\nFixing the Bug\nWith the unit test in hand, fixing the test was relatively straightforward (the\ndiff in its simplest form is anyway only a single line\nchange).\nAfter this first version of my fix, I\ntalked to Armin\nRigo who\nhelped me find different case that was still wrong, in the same area of the\ncode.\nI also got help by the developers at PortaOne\nwho are using PyPy on their servers and had seen some mysterious PyPy\ncrashes\nrecently, that looked related to the GC. They did test deployments of my fixes\nin their various stages to their servers to try to see whether stability\nimproved for them. Unfortunately in the end it turned out that their crashes\nare an unrelated GC bug related to object pinning, which we haven't resolved\nyet.\nWriting a GC fuzzer/property based test\nFinding bugs in the GC is always extremely disconcerting, particularly since\nthis one managed to hide for so long (more than ten years!). Therefore I wanted\nto use these bugs as motivation to try to find more problems in PyPy's GC. Given\nthe ridiculous effectiveness of fuzzing, I used\nhypothesis to write a\nproperty-based test. Every test performs a sequence of randomly chosen steps\nfrom the following list:\n\nallocate an object\nread a random field from a random object\nwrite a random reference into a random object\ndrop a random stack reference\nperform one GC step\nallocate an array\nread a random index from a random array\nwrite to an array\nmemcopy between two arrays\n\nThis approach of doing a sequence of steps is pretty close to the stateful\ntesting approach of\nhypothesis, but I just implemented it manually with the data\nstrategy.\nEvery one of those steps is always performed on both the tested GC, and on some\nregular Python objects. The Python objects provide the \"ground truth\" of what\nthe heap should look like, so we can compare the state of the GC objects\nwith the state of the Python objects to find out whether the GC made a mistake.\nIn order to check whether the test is actually useful, I reverted my bug fixes\nand made sure that the test re-finds both the spurious GC assertion error and the\nproblems with memcopying an array.\nIn addition, the test also found corner cases in my fix. There was a situation\nthat I hadn't accounted for, which the test found after eventually.\nI also plan on adding a bunch of other GC features as steps in the\ntest to stress them too (for example weakrefs, identity hashes, pinning, maybe\nfinalization).\nAt the point of publishing this post, the fixes got merged to the 2.7/3.9/3.10\nbranches of PyPy, and will be part of the next release (v7.3.16).\nThe technical details of the bug\nIn order to understand the technical details of the bug, I need to give some\nbackground explanations about PyPy's GC.\nPyPy's incremental GC\nPyPy uses an incremental generational mark-sweep GC. It's\ngenerational\nand therefore has minor collections (where only young objects get collected)\nand major collections (collecting long-lived objects eventually, using a\nmark-and-sweep\nalgorithm). Young objects are allocated in a nursery using a\nbump-pointer allocator, which makes allocation quite efficient. They are moved\nout of the nursery by minor collections. In order to find references from old\nto young objects the GC uses a write barrier to detect writes into old objects.\nThe GC is also\nincremental,\nwhich means that its major collections aren't done all at once (which would\nlead to long pauses). Instead, major collections are sliced up into small\nsteps, which are done directly after a minor collection (the GC isn't\nconcurrent though, which would mean that the GC does work in a separate\nthread).\nThe incremental GC uses tri-color\nmarking\nto reason about the reachable part of the heap during the marking phase, where\nevery old object can be:\n\nblack: already marked, reachable, definitely survives the collection\ngrey: will survive, but still needs to be marked\nwhite: potentially dead\n\nThe color of every object is encoded by setting flags\nin the object header.\nThe GC maintains the invariant that black objects must never point to white\nobjects. At the start of a major collection cycle the stack roots are turned\ngray. During the mark phase of a major collection cycle, the GC will trace gray\nobjects, until\nnone are left. To trace a gray object, all the objects it references have to be\nmarked grey if they are white so far. After a grey object is traced, it can be\nmarked black (because all the referenced objects are now either black or gray).\nEventually, there are no gray objects left. At that point (because no white\nobject can be reached from a black one) all the white objects are known to be\nunreachable and can therefore be freed.\nThe GC is incremental because every collection step will only trace a limited\nnumber of gray objects, before giving control back to the program. This leads to\na problem: if an already traced (black) object is changed between two marking\nsteps of the GC, the program can mutate that object and write a new reference\ninto one of its fields. This could lead to an invariant violation, if the\nreferenced object is white. Therefore, the GC uses the write barrier (which it\nneeds anyway to find references from old to young objects) to mark all black\nobjects that are modified gray, and then trace them again at one of the\nlater collection steps.\nThe special write barrier of memcopy\nArrays use a different kind of write barrier than normal objects. Since they\ncan be arbitrarily large, tracing them can take a long time. Therefore it's\npotentially wasteful to trace them fully at a minor collection. To fix this,\nthe array write barrier keeps more granular information about which parts of\nthe array have been modified since the last collection step. Then only the\nmodified parts of the array need to be traced, not the whole array.\nIn addition, there is another optimization for arrays, which is that memcopy is\ntreated specially by the GC. If memcopy is implemented by simply writing a loop\nthat copies the content of one array to the other, that will invoke the write\nbarrier every single loop iteration for the write of every array element,\ncosting a lot of overhead. Here's some pseudo-code:\ndef arraycopy(source, dest, source_start, dest_start, length):\n    for i in range(length):\n        value = source[source_start + i]\n        dest[dest_start + i] = value # <- write barrier inserted here\n\n\nTherefore the GC has a special memcopy-specific\nwrite barrier that will perform the GC logic once before the memcopy loop, and\nthen use a regular (typically SIMD-optimized) memcopy implementation from\nlibc. Roughly like this:\ndef arraycopy(source, dest, source_start, dest_start, length):\n    gc_writebarrier_before_array_copy(source, dest, source_start, dest_start, length)\n    raw_memcopy(cast_to_voidp(source) + source_start,\n                cast_to_voidp(dest) + dest_start,\n                sizeof(itemtype(source)) * length)\n\n\n(this is really a rough sketch. The real\ncode\nis much more complicated.)\nThe bug\nThe bugs turned out to be precisely in this memcopy write barrier. When we\nimplemented the current GC, we adapted our previous GC, which was a\ngenerational mark-sweep GC but not incremental. We started with most of the\nprevious GC's code, including the write barriers. The regular write barriers\nwere adapted to the new incremental assumptions, in particular the need for the\nwrite barrier to also turn black objects back to gray when they are modified\nduring a marking phase. This was simply not done at all for the memcopy write\nbarrier, at least in two of the code paths. Fixing this problem fixes the unit\ntests and stops the crashes.\nReflections\nThe way the bug was introduced is really typical. A piece of code (the memcopy\nwrite barrier) was written under a set of assumptions. Then those assumptions\nchanged later. Not all the code pieces that relied on these assumptions to be\ncorrect were updated. It's pretty hard to prevent this in all situations.\nI still think we could have done more to prevent the bug occurring. Writing a\nproperty-based test for the GC would have been a good idea given the complexity\nof the GC, and definitely something we did in other parts of our code at the\ntime (just using the random module mostly, we started using hypothesis\nlater).\nIt's a bit of a mystery to me why this bug managed to be undetected for so\nlong. Memcopy happens in a lot of pretty core operations of e.g. lists in\nPython (list.extend, to name just one example). To speculate, I would suspect\nthat all the other preconditions for the bug occurring made it pretty rare:\n\nthe content of an old list that is not yet marked needs to be copied into\n  another old list that is marked already\nthe source of the copy needs to also store an object that has no other\n  references\nthe source of the copy then needs to be overwritten with other data\nthen the next collection steps need to be happening at the right points\n...\n\nGiven the complexity of the GC logic I also wonder whether some lightweight\nformal methods would have been a good idea. Formalizing some of the core\ninvariants in B or\nTLA+ and then model\nchecking them up to some number\nof\nobjects would have found this problem pretty quickly. There are also correctness\nproofs for GC algorithms in some research papers, but I don't have a good\noverview of the literature to point to any that are particularly good or bad.\nGoing such a more formal route might have fixed this and probably a whole bunch\nof other bugs, but of course it's a pretty expensive (and tedious) approach.\nWhile it was super annoying to track this down, it was definitely good to learn\na bit more about how to use rr and the GDB scripting interface.\nBonus Section: The Wrong Assertion\nSome more technical information about the wrong assertion is in this section.\nBackground: pre-built objects\nPyPy's VM-building bootstrapping process can \"freeze\" a bunch of heap objects\ninto the final binary. This allows the VM to start up quickly, because those\nfrozen objects are loaded by the OS as part of the binary.\nThose frozen pre-built objects are parts of the 'roots' of the garbage\ncollector and need to be traced. However, tracing all the pre-built objects at\nevery collection would be very expensive, because there are a lot of them\n(about 150,000 in a PyPy 3.10 binary). Tracing them all is also not necessary,\nbecause most of them are never modified. Unmodified pre-built objects can only reference\nother pre-built objects, which can never be deallocated anyway. Therefore we\nhave an optimization that uses the write barrier (which we need anyway to find\nold-to-young pointers) to notice when a pre-built object gets modified for the\nvery first time. If that happens, it gets added to the set of pre-built objects\nthat gets counted as a root, and is traced as a root at collections\nfrom then on.\nThe wrong assertion\nThe assertion that triggered when I turned on the GC debug mode was saying that\nthe GC found a reference from a black to a white object, violating its\ninvariant. Unmodified pre-built objects count as black, and they aren't roots,\nbecause they can only ever reference other pre-built objects. However, when a\npre-built object gets modified for the first time, it becomes part of the root\nset and will be marked gray. This logic works fine.\nThe wrong assertion triggers if a pre-built object is mutated for the very\nfirst time in the middle of an incremental marking phase. While the pre-built\nobject gets added to the root set just fine, and will get traced before the\nmarking phase ends, this is encoded slightly differently for pre-built objects,\ncompared to \"regular\" old objects. Therefore, the invariant checking code\nwrongly reported a black->white pointer in this situation.\nTo fix it I also wrote a unit test checking the problem, made sure that the GC\nhypothesis test also found the bug, and then fixed the wrong assertion to take\nthe color encoding of pre-built objects into account.\nThe bug managed to be invisible because we don't tend to turn on the GC\nassertions very often. We only do that when we find a GC bug, which is of\ncourse also when we need it the most to be correct.\nAcknowledgements\nThanks to Matti Picus, Max Bernstein, Wouter van Heyst for giving me feedback on drafts of the\npost. Thanks to Armin Rigo for reviewing the code and pointing out holes in my\nthinking. Thanks to the original reporters of the various forms of the bug,\nincluding Lily Foote, David Hewitt, Wenzel Jakob.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2024/03/fixing-bug-incremental-gc.html"
    },
    {
      "title": "PyPy v7.3.15 release",
      "text": "PyPy v7.3.15: release of python 2.7, 3.9, and 3.10\nThe PyPy team is proud to release version 7.3.15 of PyPy.\nThis is primarily a bug-fix release, and includes work done to migrate PyPy to\nGit and Github.\nThe release includes three different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.9, which is an interpreter supporting the syntax and the features of\nPython 3.9, including the stdlib for CPython 3.9.18.\nPyPy3.10, which is an interpreter supporting the syntax and the features of\nPython 3.10, including the stdlib for CPython 3.10.13.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. It follows after 7.3.14 release on Dec 25, 2023\nWe recommend updating. You can find links to download the v7.3.15 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with\nmaking RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy. In any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython\nIt's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64).\nApple M1 arm64 machines (macos_arm64).\ns390x running Linux\n\n\nPyPy support Windows 32-bit, Linux PPC64 big- and little-endian, and Linux ARM\n32 bit, but does not release binaries. Please reach out to us if you wish to\nsponsor binary releases for those platforms. Downstream packagers provide\nbinary builds for debian, Fedora, conda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.15 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2024/01/pypy-v7315-release.html"
    },
    {
      "title": "PyPy has moved to Git, GitHub",
      "text": "PyPy has moved its canonical repo and issue tracker from\nhttps://foss.heptapod.net/pypy/pypy to https://github.com/pypy/pypy. Obviously,\nthis means development will now be tracked in Git rather than Mercurial.\nMotivation\nWe still feel Mercurial is a better version control system. The named branch\nmodel and user interface are superior. But\n\n\nfoss.heptapod.net is not well indexed in google/bing/duckduckgo\n  search, so people find it harder to search for issues in the project.\n\n\nSince Heptapod has tightened its spam control, we get reports that\n  users create issues only to have them flagged as spam.\n\n\nOpen Source has become synonymous with GitHub, and we are too small to\n  change that.\n\n\nMuch of the current development comes as a reaction to fixing issues.\n  Tracking interlocking issues is easier if all the code is on the same\n  platform.\n\n\nThe FAQ\n  presents two arguments against the move. Github notes\n  solves much of point (1): the difficulty of discovering provenance of\n  commits, although not entirely. But the main problem is point (2), it turns\n  out that not moving to GitHub is an impediment to contribution and issue\n  reporting.\n\n\nPeople who wish to continue to use Mercurial can use the same method below to\n  push to GitHub.\n\n\nGitHub is more resource rich than foss.heptapod.net. We could add CI\n  jobs to replace some of our aging buildbot\n  infrastructure.\n\n\nMethod\nThe migration required two parts: migrating the code and then migrating the\nissues and merge requests.\nCode migration 1: code and notes\nI used a fork of git-remote-hg to\ncreate a local Git repo with all the changesets. Then I wanted to add a Git\nnote to each commit with the branch it came from. So I prepared a file with two\ncolumns: the Git commit hash, and the corresponding branch from Mercurial.\nMercurial can describe each commit in two ways: either the commit hash or by a\nnumber index. I used hg log to convert an index i to the Mercurial hash,\nand then git-hg-helper from git-remote-hg to convert the Mercurial hash to\na Git hash:\n$(cd pypy-git; git-hg-helper git-rev $(cd ../pypy-hg; hg log -r $i -T\"{node}\\n\"))\n\n\nThen I used hg log again to print the Mercurial branch for the index i:\n$(cd pypy-hg; hg log -r $i -T'{branch}\\n')\n\n\nPutting these two together, I could loop over all the commits by their\nnumerical index to prepare the file. Then I iterated over each line in the\nfile, and added the Git note. Since the git note add command works on the\ncurrent HEAD, I needed to checkout each commit in turn and then add the note:\ngit checkout -q <hash> && git notes --ref refs/notes/branch add -m branch:<branch>\n\n\nI could then use git push --all to push to GitHub.\nCode migration 2: prepare the branches\nPyPy has almost 500 open branches. The code migration created all the branch\nHEADs, but git push --all did not push them. I needed to check them out and\npush each one. So I created a file with all the branch names\ncd pypy-hg; hg branches | cut -f1 -d\" \" > branches.txt\n\n\nand then push each one to the GitHub repo\nwhile read branch; do git checkout branches/$branch && git push origin branches/$branch; done < branches.txt\n\n\nNote that the branches were named branches/XXX by the migration, not branch/XXX. This confuses the merge request migration, more about that later.\nIssue and merge request migration\nI used the solution from\nnode-gitlab-2-github which\nworked almost perfectly. It is important to do the conversion on a private\nrepo otherwise every mention of a successfully mapped user name notifies\nthe user about the transfer. This can be quite annoying for a repo the size of\nPyPy with 600 merge requests and over 4000 issues. Issues transferred without a\nproblem: the script properly retained the issue numbers. However the script\ndoes not convert the Mercurial hashes to Git hashes, so the bare hashes in\ncomments show up without a link to the commit. Merge requests are more of a problem:\n\nThe Mercurial named branch \"disappears\" once it is merged, so a merge request\n  to a merged branch does not find the target branch name in Git. The\n  conversion creates an issue instead with the label gitlab merge request.\nFor some reason, the branches created by git-remote-hg are called\n  branches/XXX and not branch/XXX as expected by GitLab. This messes up the\n  merge request/PR conversion. For some of the branches (open PRs and main\n  target branches) I manually created additional branches without the es. The\n  net result is that open merge requests became open PRs, merged merge requests\n  became issues, and closed-not-merged merge requests were not migrated.\n\nLayered conversions\nPyPy already migrated once from Bitbucket to Heptapod. Many of the issues\nreflect the multiple transitions: they have lines like \"Created originally on\nBitbucket by XXX\" from the first transition, and an additional line \"In\nHeptapod\" from this transition.\nCredits\nWe would like to express our gratitude to the Octobus\nteam who support Heptapod. The transition from Bitbucket was quite an effort,\nand they have generously hosted our development since then. We wish them all\nthe best, and still believe that Mercurial should have \"won\".\nNext steps\nWhile the repo at GitHub is live, there are still a few more things we need to\ndo:\n\nDocumentation needs an update for the new repo and the build automation from\n  readthedocs must be adjusted.\nThe wiki should be copied from Heptapod.\nbuildbot.pypy.org should also look at the new repo. I hope the code is up to\n  the task of interacting with a Git repo.\nspeed.pypy.org tracks changes, it too needs to reference the new location\nTo keep tracking branches with Git notes on new commits, I activated a\n  github action by Julian to\n  add a Git branch note to each commit. Please see the README there for\n  directions on using Git notes.\nSome of the merge requests were not migrated. If someone wants to, they could\n  migrate those once they figure out the branch naming problems.\n\nAdditionally, now is the time for all of you to prove the move is worthwhile:\n\nStar the repo, let others know how to find it,\nHelp fix some of the open issues or file new ones,\nTake advantage of the more familiar workflow to get involved in the project,\nSuggest ways to improve the migration: are there things I missed or could\n  have done better?\n\nHow will development change?\nHeptapod did not allow personal forks, so we were generous with a commit bit to\nthe main repo. Additionally, we (well, me) have been using a\ncommit-directly-to-main workflow. We will now be adopting a more structured\nworkflow. Please fork the repo and submit a pull request for any changes. We\ncan now add some pre-merge CI to check that the PR at least passes the first\nstage of translation. The live and active branches will be:\n\nmain: what was \"default\" in Mercurial, it is the Python2.7 interpreter and\n  the base of the RPython interpreter,\npy3.9: the Python3.9 interpreter, which also includes all RPython changes\n  from main. This is exactly like on Mercurial, and\npy3.10: the Python3.10 interpreter, which also includes all RPython changes\n  from main and all bugfixes from py3.9. This is exactly like on Mercurial.\n\nWorking between the repos\nFinding commits\nIf you want to figure out how a Mercurial commit relates to a Git commit, you\ncan use git-hg-helper. You run it in the Git repo. It takes the full long\nhash from one repo and gives you the corresponding hash of the other repo:\n$ git-hg-helper git-rev d64027c4c2b903403ceeef2c301f5132454491df\n4527e62ad94b0e940a5b0f9f20d29428672f93f7\n$ git-hg-helper hg-rev 4527e62ad94b0e940a5b0f9f20d29428672f93f7\nd64027c4c2b903403ceeef2c301f5132454491df\n\n\nFinding branches\nBranches migrated from Mercurial will have a branches prefix, not branch.\nWhile GitLab uses branch for its prefix, the git-remote-hg script uses\nbranches. New work should be in a PR targeting main, py3.9 or py3.10.\nThanks for helping to make PyPy better.\nMatti\nUpdate\nIn the meantime we found out that unfortunately something went wrong in the\nmigration of the issues. The old issue\n3655 got lost in the\nmigration. This means that after number 3655 the numbers are different between\ngithub and heptapod, with heptapod being one larger. E.g. issue 3700 on\nheptapod is issue 3699 on\ngithub. We are investigating\noptions.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2023/12/pypy-moved-to-git-github.html"
    },
    {
      "title": "PyPy v7.3.14 release",
      "text": "PyPy v7.3.14: release of python 2.7, 3.9, and 3.10\nThe PyPy team is proud to release version 7.3.14 of PyPy.\nHighlights of this release are compatibility with HPy-0.9, cffi 1.16,\nadditional C-API interfaces, and more python3.10 fixes.\nThe release includes three different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.9, which is an interpreter supporting the syntax and the features of\nPython 3.9, including the stdlib for CPython 3.9.18.\nPyPy3.10, which is an interpreter supporting the syntax and the features of\nPython 3.10, including the stdlib for CPython 3.10.13.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. It follows after 7.3.13 release on Sept 29, 2023.\nWe recommend updating. You can find links to download the v7.3.14 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. Since the last release we have contributions from three new\ncontributors.  PyPy has many layers and we need help with all of them: bug\nfixes, PyPy and RPython documentation improvements, or general help\nwith making RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy. In any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython\nIt's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64).\nApple M1 arm64 machines (macos_arm64).\ns390x running Linux\n\n\nPyPy support Windows 32-bit, Linux PPC64 big- and little-endian, and Linux ARM\n32 bit, but does not release binaries. Please reach out to us if you wish to\nsponsor binary releases for those platforms. Downstream packagers provide\nbinary builds for debian, Fedora, conda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.14 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2023/12/pypy-v7314-release.html"
    },
    {
      "title": "PyPy v7.3.13 release",
      "text": "PyPy v7.3.13: release of python 2.7, 3.9, and 3.10\nThe PyPy team is proud to release version 7.3.13 of PyPy.\nThis is primarily a security/bug-fix release. CPython released security\npatches, and this release also improves the ability to use type\nspecifications via PyType_FromSpec and friends. There are also some\nsmall speed-ups.\nThe release includes three different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.9, which is an interpreter supporting the syntax and the features of\nPython 3.9, including the stdlib for CPython 3.9.18.\nPyPy3.10, which is an interpreter supporting the syntax and the features of\nPython 3.10, including the stdlib for CPython 3.10.13. Note it requires at\nleast cython 0.29.35 or cython 3.0.0b3.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. It follows after 7.3.12 release on June 16, 2023.\nWe recommend updating. You can find links to download the v7.3.13 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with making\nRPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy. In any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython\nIt's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64).\nApple M1 arm64 machines (macos_arm64).\ns390x running Linux\n\n\nPyPy support Windows 32-bit, Linux PPC64 big- and little-endian, and Linux ARM\n32 bit, but does not release binaries. Please reach out to us if you wish to\nsponsor binary releases for those platforms. Downstream packagers provide\nbinary builds for debian, Fedora, conda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.13 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2023/09/pypy-v7313-release.html"
    },
    {
      "title": "PyPy v7.3.12 release",
      "text": "PyPy v7.3.12: release of python 2.7, 3.9, and 3.10.\nThe PyPy team is proud to release version 7.3.12 of PyPy.\nThis release includes a new string-to-int algorithm (also appearing in CPython\n3.12) that is faster than the older one; support for symlinks in Windows; and\nour first Python3.10 version.\nThe release includes three different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.9, which is an interpreter supporting the syntax and the features of\nPython 3.9, including the stdlib for CPython 3.9.17.\nPyPy3.10, which is an interpreter supporting the syntax and the features of\nPython 3.10, including the stdlib for CPython 3.10.12. This is our first\nrelease of 3.10, but based on past experience we are quite confident in\nits compatibility with upstream. Of course, we recommend testing your code\nwith this new version before putting it into production. Note it does\nrequire at least cython 0.29.35 or cython 3.0.0b3\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. It follows after 7.3.11 release on Dec 29, 2022\nWe recommend updating. You can find links to download the v7.3.12 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with making\nRPython's JIT even better. Since the previous release, we have accepted\ncontributions from one new contributor, thanks for pitching in, and welcome\nto the project!\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy. In any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.9 and\n3.10. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64).\nApple M1 arm64 machines (macos_arm64).\ns390x running Linux\n\n\nPyPy support Windows 32-bit, Linux PPC64 big- and little-endian, and Linux ARM\n32 bit, but does not release binaries. Please reach out to us if you wish to\nsponsor binary releases for those platforms. Downstream packagers provide\nbinary builds for debian, Fedora, conda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.12 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2023/06/pypy-v7312-release.html"
    },
    {
      "title": "RPython-based emulator speeds up RISC-V simulation over 15x",
      "text": "In cooperation with RISC-V International, who funded a part of this project,\nwe recently created a workflow to\nuse RPython to take a Sail RISC-V model and automatically create a RISC-V ISA\nemulator from it, which we call Pydrofoil. The simulator sped up booting a\nlinux emulator from 35 minutes (using the standard Sail-generated emulator in\nC) to 2 minutes, a speedup of 17.5x. More details about the process are in the\nRISC-V blog post.\nA few take-aways from the project:\n\nWhile PyPy has shown it can speed up generic python code about 4x, the\ntechnology behind PyPy can really shine in other areas.\nRPython is malleable and can be molded to many tasks, the RPython meta-JIT is\nvery flexible.\nA JIT is well-suited for the problem of emulation, because it can\nperform dynamic binary translation.\n\nPyPy can solve real world performance problems, even somewhat unusual ones.\nPlease get in touch and let us know how we can help you solve yours!",
      "tags": "casestudy,performance",
      "url": "https://www.pypy.org/posts/2023/05/rpython-used-to-speed-up-risc-v-simulation-over-15x.html"
    },
    {
      "title": "Repeated string concatenation is quadratic in PyPy (and CPython)",
      "text": "This is a super brief blog post responding to an issue that we got on the PyPy\nissue tracker. I am moving my response to the blog (with permission of the\nsubmitter) to have a post to point to, since it's a problem that comes up with\nsome regularity. It's also documented on our page of differences between PyPy\nand CPython but I thought an additional blog post might be good.\nThe issue pointed out that a small program that operates on strings is much\nslower on PyPy compared to CPython. The program is a solution for 2016's\nAdvent of Code Day 16 and looks like this:\ndef dragon(a):\n    b = a[::-1].replace('0','r').replace('1','0').replace('r','1')\n    return a+'0'+b\n\ndef diffstr(a):\n    b = \"\"\n    for i in range(0,len(a),2):\n        b += ['0','1'][a[i] == a[i+1]]\n    return b\n\ndef iterdiff(a):\n    b = a\n    while(len(b) % 2 == 0):\n        b = diffstr(b)\n    return b\n\nsize = 35651584\ninitstate = '10010000000110000'\nwhile(len(initstate) < size):\n    initstate = dragon(initstate)\ninitstate = initstate[:size]\nprint(iterdiff(initstate))\n\nThe submitter pointed out, that the program is fast on CPython (~8s on my\nlaptop) and slow (didn't finish) on PyPy.\nThe reason for the performance difference is that += on strings in a loop\nhas quadratic complexity in PyPy, which is what diffstr does. To see the\nquadraticness, consider that to add a character at the end of the string, the\nbeginning of the string needs to be copied into a new chunk of memory. If the\nloop runs n times, that means there are\n1 + 2 + 3 + ... + n = n * (n + 1) // 2\ncharacter copies.\nRepeated string concatenations are in principle also quadratic in CPython, but\nCPython has an optimization that makes them sometimes not quadratic, which is\nwhat makes this program not too slow in CPython.\nIn order to fix the problem on PyPy it's best to use a list for the string\nparts, which has the right amortized O(1) complexity for .append calls, and\nthen use str.join after the loop:\ndef diffstr(a):\n    b = []\n    for i in range(0,len(a),2):\n        b.append(['0','1'][a[i] == a[i+1]])\n    return \"\".join(b)\n\nWith this change the program becomes a little bit faster on CPython for me, and\non PyPy it stops being quadratic and runs in ~3.5s.\nIn general, it's best not to rely on the presence of this optimization in\nCPython either. Sometimes, a small innocent looking changes will break CPython's\noptimization. E.g. this useless change makes CPython also take ages:\ndef diffstr(a):\n    b = \"\"\n    for i in range(0,len(a),2):\n        b += ['0','1'][a[i] == a[i+1]]\n        c = b\n    return b\n\nThe reason why this change breaks the optimization in CPython is that it only\ntriggers if the reference count of b is 1, in which case it uses realloc\non the string. The change is unrealistic of course, but you could imagine a\nrelated that keeps an extra reference to b for a sensible reason.\nAnother situation in which the optimization doesn't work is discussed in this\nStackOverflow question with an answer by Tim Peters.\nIt's unlikely that PyPy will fix this. We had a prototype how to do it, but it\nseems very little \"production\" code uses += on strings in a loop, and the fix\nmakes the strings implementation quite a bit more complex.\nSo, in summary, don't use repeated concatenations in a loop!",
      "tags": "performance",
      "url": "https://www.pypy.org/posts/2023/01/string-concatenation-quadratic.html"
    },
    {
      "title": "PyPy v7.3.11 release",
      "text": "PyPy v7.3.11: release of python 2.7, 3.8, and 3.9\nThe PyPy team is proud to release version 7.3.11 of PyPy. As could be expected,\nthe first release of macOS arm64 impacted the macOS x86-64 build, so this is\na bug release to restore the ability of macOS users to run PyPy on\nmacOS < 11.0. It also incorporates the latest CPython stdlib updates\nreleased the day after 7.3.10 went out, and a few more bug fixes. The release\nincludes three different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.8, which is an interpreter supporting the syntax and the features of\nPython 3.8, including the stdlib for CPython 3.8.16. Note we intend to drop\nsupport for this version in an upcoming release as soon as we release\nPyython 3.10.\nPyPy3.9, which is an interpreter supporting the syntax and the features of\nPython 3.9, including the stdlib for CPython 3.9.16.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases and follows quickly on the heals of the 7.3.10 release on Dec 6.\nWe recommend updating. You can find links to download the v7.3.11 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with making\nRPython's JIT even better. Since the previous release, we have accepted\ncontributions from one new contributor, thanks for pitching in, and welcome\nto the project!\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy.\nIn any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.8 and\n3.9. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64).\nApple M1 arm64 machines (macos_arm64).\ns390x running Linux\n\n\nPyPy support Windows 32-bit, Linux PPC64 big- and little-endian, and Linux ARM\n32 bit, but does not release binaries. Please reach out to us if you wish to\nsponsor binary releases for those platforms. Downstream packagers provide\nbinary builds for debian, Fedora, conda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.11 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2022/12/pypy-v7311-release.html"
    },
    {
      "title": "Finding JIT Optimizer Bugs using SMT Solvers and Fuzzing",
      "text": "In this blog post I want to describe a recent bug finding technique that I've\nadded to the PyPy JIT testing infrastructure. This technique uses the Z3\ntheorem prover to find bugs in the optimizer of PyPy's JIT, in particular its\ninteger operation optimizations. The approach is\nbased on things I have learned from John Regehr's blog (this post is a\ngood first one to read), Twitter, and on\nhis (et al) paper Alive2: Bounded Translation Validation for LLVM. The work\nwas triggered by a recent miscompilation bug my current bachelor student Nico\nRittinghaus found.\n\nBackground: Python Integers in the PyPy JIT\nThe optimizer of PyPy's JITs operates on traces, which are linear sequences of\ninstructions with guards. The instructions in the traces operate on different\nmachine-level data types, machine integers, doubles, pointers, bools, etc. In\nthis post we'll be mostly concerned with machine integers.\nTo given some wider context I'll explain a bit how Python ints in the user code\nrelate to the types that are used in traces when the PyPy Python implementation\nis used.\nWhen PyPy turns a regular Python 3 function into a trace, there is a lot of work\nhappening in the JIT frontend to try to observe and infer the types that the\nPython function concretely uses at runtime. The traces are generated under these\ntyping assumptions. Therefore, code that uses ints in the Python code can\ntypically be translated into traces that operate on machine integers. In order\nto make sure that the Python integer semantics are upheld, many of the\noperations in the traces need to check that the integer results of some\noperations still fit into a machine integer. If that is not the case (a rare\nsituation for most programs), the trace is left via a guard, execution falls\nback to the interpreter, and there a big integer representation is chosen for\nthe too big value (the big integer representation is done via a pointer and\nsome storage on the heap).\nAll of this machinery is not going to be too relevant for the rest of the\npost. For the post it's important to know that trace instructions operate on\nmachine integers and other low-level types, and some of the operations can\noptionally check whether the\nresults still fit into a machine integer. These trace operations are improved by\nthe optimizer, which tries to transform the trace into one that behaves the\nsame, but is less costly to execute.\n\n\nBackground: Bounds Analysis in PyPy's JIT\nThe optimizer of PyPy's JIT has an analysis based on abstract interpretation\nthat tries to find out whether the integer values stored in a variable are\nactually not using the full 64 bit (or 32 bit) range, but instead fit into some\nsmaller range. This means that for every integer variable x in a trace, the\nJIT compiler tracks upper and lower bounds of the runtime value of that\nvariable: a range [a, b] such that for every concrete runtime value v\nthat gets stored in variable x, a <= v <= b must be true.\na and b start out\nas the most general MININT and MAXINT, but sometimes there is extra\ninformation that makes it possible to improve these known bounds, and that is\noften useful to optimize the code.\nA typical example is that the JIT knows that the length of a string is\nnon-negative, so for this kind of code: x = len(s) where s is a string,\nx gets a range [0, MAXINT] assigned. With this information we could for\nexample remove a check x + 10 < 0 completely, because it can never be true.\nThe bounds information is useful for optimization, but the analysis of the\nbounds is also a source of bugs in the JIT, because the reasoning is often\nsubtle and easy to get wrong in corner cases. We already use a number of testing\ntechniques to try to make sure that it is correct. A simple one is\nproperty-based testing using Hypothesis on the operations on bounds. Even\nthough Hypothesis is fantastic, it unfortunately does not catch\nabsolutely all the bugs even if we'd like it too, as we'll see in the next\nsection.\n\n\nMotivation: A JIT Miscompilation\nI am currently supervising a Bachelor thesis by Nico Rittinghaus, who is\nextending the integer analysis in the JIT. He'll probably write a separate blog\npost about that soon. In the process of his work, the current bounds analysis\ncode got a lot of scrutiny, and we found out that one of the unit tests of the\nbounds analysis was actually incorrect, and the example code in that unit test\nwas optimized incorrectly. This case of incorrect optimization is not a big deal\nfor regular Python code, because it involved a \"wrapping integer addition\noperation\", i.e. one where overflowing results just wrap around to negative\nvalues. All the additions and other arithmetic operations that the PyPy Python\nfrontend generates actually have\noverflow checks (to be able to switch to a big integer representation if\nneeded).\nHowever, it's still possible to trigger the problem with the\n__pypy__.intop.int_add API which is a function that exposes wraparound\narithmetic on Python ints.\nHere's the miscompilation. The JIT optimizes the following function:\nimport __pypy__\n\ndef wrong(x):\n    a = __pypy__.intop.int_add(x, 10)\n    if a < 15:\n        if x < 6:\n            return 0\n        return 1\n    return 2\n\nInto the following code:\nimport __pypy__\n\ndef wrong(x):\n    a = __pypy__.intop.int_add(x, 10)\n    if a < 15:\n        return 0\n    return 2\n\nBasically the faulty reasoning of the JIT looks like this: if int_add(x, 10) < 15\nthen it must follow that x < 5, which is stronger than x < 6, so the\nsecond if is always true. This sounds good, but is actually wrong\nif the addition + 10 wrapped around. So if x == MAXINT, then\nint_add(x, 10) == MININT + 9 < 15. But MAXINT < 5 is not\ncorrect.\nNote how the same reasoning with overflow-checking addition is correct! If x +\n10 < 15 and the + didn't overflow, then indeed x < 6. And if your\nmind bends starting to think about all this, you understand some of the\ndifficulty of getting the JIT correct in this area.\n\n\nHow could we have avoided this bug?\nOne exercise I try to do after finding bugs is to reflect on ways that the\nbug could have been avoided. I think this is particularly important in the JIT,\nwhere bugs are potentially really annoying to find and can cause very strange\nbehaviour in basically arbitrary Python code.\nIt's easy to always answer this question with \"try to think more carefully\nwhen working\", but that approach cannot be relied on in complicated situations,\nbecause humans don't concentrate perfectly for long stretches of time.\nA situation-specific problem I identified was the bad design of the range analysis API.\nA range is not just represented by two numbers, instead it's two numbers\nand two bools that are supposed to represent that some operation did or did not\nunderflow/overflow. The meaning of these bools was quite hard to grasp and easy\nto get wrong, so probably they should never have been introduced in the first\nplace (and my bugfix indeed removed them).\nBut in the rest of this blog post I want to talk about another, systematic\napproach that can be applied to the problem of mis-optimizations of integer\noperations, and that is done by applying an SMT solver to the problem.\nAn SMT solver (Satisfyability Modulo Theories) is a tool that can be used to\nfind out whether mathematical formulas are \"satisfiable\", i.e. whether\nsome chosen set of inputs exists that will make the formulas evaluate to true. SMT solvers are\ncommonly used in a wide range of CS applications including program correctness\nproofs, program synthesis, etc. The most widely known one is probably Z3 by\nMicrosoft Research which has the nice advantage of coming with an easy-to-use\nPython binding.\nGoing into this I basically knew next to nothing about SMT solvers (despite\nhaving been embedded in a formal methods research group for years!) so it was an\ninteresting new world to learn about.\nAs briefly mentioned in the introduction, the approach I took followed a similar\n(but much more properly executed) one applied to LLVM operations, called\nAlive2. Krister Waldfridsson has done similar work for GCC recently,\ndescribed on his blog.\n\n\nZ3 Proof of Concept\nThe first thing I did was to try to get Z3 find the above bug, by encoding the\ninput program into an SMT formula by hand and trying to get Z3 to prove the condition\nthat the JIT thinks is always true. The Z3 code for this looks as follows:\nfrom z3 import BitVec, Implies, prove\nx = BitVec('x', 64)\na = x + 10\ncond1 = a < 15\ncond2 = x < 6\nprove(Implies(cond1, cond2))\n\nHere, x is defined to be a bit vector variable of width 64, which is a\ndatatype that can be used to represent bounded machine integers. Addition on\nbit vectors performs wraparound arithmetic, like the __pypy__.intop.int_add\ncall in the original code. The JIT optimized the second condition away, so\nessentially it was convinced that the first condition implies the second one.\nThe above snippet tries to get Z3 to confirm this.\nWhen run, the above program prints:\ncounterexample\n[x = 9223372036854775803]\nWhich shows the bug. As a small side-note, I thought it was cool that the\nprocess of \"proving\" something in Z3 basically means trying to find an example\nfor the negation of the formula. If no counterexample can be found for the\nnegation, the original formula is true. If the original formula turns out to be\nfalse (like here) we get a nice example that shows the problem to go with it.\nIt's not realistic to hand-translate all the hundreds of\nunit-tests into Z3 formulas and then ask Z3 to prove the optimizations. Instead,\nwe want to have a program that does this for us.\n\n\nSMT Checking of the JIT Optimizer\nWhat we want from this program is the following: given an unoptimized trace and\nits optimized version, we want to use Z3 to check whether the optimized trace\nbehaves identically to the unoptimized one. One question is what \"behaves\nidentically\" means. What we care about is the outputs of the trace being the\nsame values, no matter how they are computed. Also, for every guard we want to\nmake sure that it fails in identical ways in the optimized and unoptimized\nversions. A guard is only allowed to be optimized away if it can never fail.\nThe code that comes after a guard can assume that the guard has not failed,\nbecause otherwise execution would have left the trace. All of this should be\ntrue regardless for the values of the input variables of the trace.\nSo in order to check that the two traces are behaving identically, we do the\nfollowing:\n\nWe create Z3 variables for every input variable. We use the same input\nvariables both for the unoptimized as well as the optimized trace.\nWe align the two traces at the corresponding guards. Thankfully the optimizer\nkeeps track of which optimized guard corresponds to which unoptimized input\nguard.\nAll the operations before a guard are translated into Z3 formulas, for both\nversions of the trace.\nFor two corresponding guards, we ask Z3 to prove that the guard conditions are\nidentical.\nFor a guard that was optimized away we ask Z3 to prove that the condition is\nalways true.\nAfter a guard, we tell Z3 that from now on it can assume that the guard\ncondition is true.\nWe repeat this, guard for guard, until we reach the end of the trace. There,\nwe ask Z3 to prove that the output variables in the unoptimized trace and the\noptimized trace are identical (every trace can return one or many values).\n\nI implemented this, it's not a lot of code, basically a couple of hundred lines\nof (somewhat hacky) Python code. So far I only support integer\noperations. Here are some parts of the code to give you a flavor of what this\nlooks like.\nThis is the code that translates operations into Z3 formulas:\ndef add_to_solver(self, ops, state):\n    for op in ops:\n        if op.type != 'v': # is it an operation with a result\n            res = self.newvar(op)\n        else: # or does it return void\n            res = None\n\n       # ...\n\n        # convert arguments\n        if op.numargs() == 1:\n            arg0 = self.convertarg(op, 0)\n        elif op.numargs() == 2:\n            arg0 = self.convertarg(op, 0)\n            arg1 = self.convertarg(op, 1)\n\n        # compute results\n        if opname == \"int_add\":\n            expr = arg0 + arg1\n        elif opname == \"int_sub\":\n            expr = arg0 - arg1\n        elif opname == \"int_mul\":\n            expr = arg0 * arg1\n        elif opname == \"int_and\":\n            expr = arg0 & arg1\n        elif opname == \"int_or\":\n            expr = arg0 | arg1\n        elif opname == \"int_xor\":\n            expr = arg0  arg1\n\n        # ...  more operations, some shown below\n\n        self.solver.add(res == expr)\n\nNew Z3 variables are defined by the helper function newvar, which adds the\noperation to a dictionary box_to_z3 mapping boxes (=variables) to Z3\nvariables. Due to the SSA property that traces have, a variable must be defined\nbefore its first use.\nHere's what newvar looks like (LONG_BIT is a constant that is either\n64 or 32, depending on the target architecture):\ndef newvar(self, box, repr=None):\n    # ... some logic around making the string representation\n    # somewhat nicer omitted\n    result = z3.BitVec(repr, LONG_BIT)\n    self.box_to_z3[box] = result\n    return result\n\nThe convert method turns an operation argument (either a constant or a\nvariable) into a Z3 formula (either a constant bit vector or an already defined\nZ3 variable). convertarg is a helper function that takes an operation, reads\nits nth argument and converts it.\ndef convert(self, box):\n    if isinstance(box, ConstInt):\n        return z3.BitVecVal(box.getint(), LONG_BIT)\n    return self.box_to_z3[box]\n\ndef convertarg(self, box, arg):\n    return self.convert(box.getarg(arg))\n\nThe lookup of variables in box_to_z3 that convert does cannot fail,\nbecause the variable must have been defined before use.\nComparisons return the bit vector 0 or bit vector 1, we use a helper function\ncond to turn the Z3 truth value of the comparison into a bit vector:\ndef cond(self, z3expr):\n    return z3.If(z3expr, TRUEBV, FALSEBV)\n\n\ndef add_to_solver(self, ops, state):\n        # ... start as above\n\n        # more cases\n        elif opname == \"int_eq\":\n            expr = self.cond(arg0 == arg1)\n        elif opname == \"int_ne\":\n            expr = self.cond(arg0 != arg1)\n        elif opname == \"int_lt\":\n            expr = self.cond(arg0 < arg1)\n        elif opname == \"int_le\":\n            expr = self.cond(arg0 <= arg1)\n        elif opname == \"int_gt\":\n            expr = self.cond(arg0 > arg1)\n        elif opname == \"int_ge\":\n            expr = self.cond(arg0 >= arg1)\n        elif opname == \"int_is_true\":\n            expr = self.cond(arg0 != FALSEBV)\n        elif opname == \"uint_lt\":\n            expr = self.cond(z3.ULT(arg0, arg1))\n        elif opname == \"uint_le\":\n            expr = self.cond(z3.ULE(arg0, arg1))\n        elif opname == \"uint_gt\":\n            expr = self.cond(z3.UGT(arg0, arg1))\n        elif opname == \"uint_ge\":\n            expr = self.cond(z3.UGE(arg0, arg1))\n        elif opname == \"int_is_zero\":\n            expr = self.cond(arg0 == FALSEBV)\n\n        # ... rest as above\n\nSo basically for every trace operation that operates on integers I had to give a\ntranslation into Z3 formulas, which is mostly straightforward.\nGuard operations get converted into a Z3 boolean by their own helper function,\nwhich looks like this:\ndef guard_to_condition(self, guard, state):\n    opname = guard.getopname()\n    if opname == \"guard_true\":\n        return self.convertarg(guard, 0) == TRUEBV\n    elif opname == \"guard_false\":\n        return self.convertarg(guard, 0) == FALSEBV\n    elif opname == \"guard_value\":\n        return self.convertarg(guard, 0) == self.convertarg(guard, 1)\n\n    # ... some more exist, shown below\n\nSome operations are a bit trickier. An important example in the context of\nthis blog post are integer operations that check for overflow. The overflow\noperations return a result, but also a boolean whether the operation overflowed\nor not.\ndef add_to_solver(self, ops, state):\n\n        # ... more cases\n\n        elif opname == \"int_add_ovf\":\n            expr = arg0 + arg1\n            m = z3.SignExt(LONG_BIT, arg0) + z3.SignExt(LONG_BIT, arg1)\n            state.no_ovf = m == z3.SignExt(LONG_BIT, expr)\n        elif opname == \"int_sub_ovf\":\n            expr = arg0 - arg1\n            m = z3.SignExt(LONG_BIT, arg0) - z3.SignExt(LONG_BIT, arg1)\n            state.no_ovf = m == z3.SignExt(LONG_BIT, expr)\n        elif opname == \"int_mul_ovf\":\n            expr = arg0 * arg1\n            m = z3.SignExt(LONG_BIT, arg0) * z3.SignExt(LONG_BIT, arg1)\n            state.no_ovf = m == z3.SignExt(LONG_BIT, expr)\n\n        # ...\n\nThe boolean is computed by comparing the result of the bit vector operation with\nthe result of converting the input bit vectors into an abstract (arbitrary\nprecision) integer and the result back to bit vectors. Let's go through the\naddition case step by step, the other cases work analogously.\nThe addition in the first elif that computes expr is an addition on bit\nvectors, therefore it is performing wraparound arithmetic.\nz3.SignExt(LONG_BIT, arg0) sign-extends arg0 from a bit vector of\nLONG_BIT bits to an abstract, arbitrary precision integer. The addition in\nthe second line is therefore an addition between abstract integers, so it will\nnever overflow and just compute the correct result as an integer.\nThe condition to check for overflow is now: if the results of the two different\nways to do the addition are the same, then overflow did not occur. So in order\nto compute state.no_ovf in the addition case the\ncode converts the result of the bit vector wraparound addition to\nan abstract integer (using SignExt again), and then compares that to the integer\nresult.\nThis boolean can then be checked by the guard operations guard_no_overflow\nand guard_overflow.\ndef guard_to_condition(self, guard, state):\n\n    # ... more cases\n\n    elif opname == \"guard_no_overflow\":\n        assert state.no_ovf is not None\n        return state.no_ovf\n    elif opname == \"guard_overflow\":\n        assert state.no_ovf is not None\n        return z3.Not(state.no_ovf)\n\n    # ... more cases\n\n\n\nFinding the Bug, Again\nLet's actually make all of this more concrete by applying it to the trace of our\noriginal bug. The input trace and the incorrectly optimized trace for that look\nlike this (differences highlighted):\n# input                       # optimized\n[i0]                          [i0]\ni1 = int_add(i0, 10)          i1 = int_add(i0, 10)\ni2 = int_lt(i1, 15)           i2 = int_lt(i1, 15)\nguard_true(i2)                guard_true(i2)\ni3 = int_lt(i0, 6)            jump(0)\nguard_true(i3)\njump(0)\n\nNote that the trace represents just one of the paths through the control flow\ngraph of the original function, which is typical for tracing JITs (the other\npaths could incrementally get added later).\nThe first guards in both these traces correspond to each other, so the first\nchunks to check are the first three operations (lines 1-4). Those operations\ndon't get changed by the optimizer at all.\nThese two identical traces get translated to the following Z3 formulas:\ni1unoptimized == input_i0 + 10\ni2unoptimized == If(i1unoptimized < 15, 1, 0)\ni1optimized == input_i0 + 10\ni2optimized == If(i1optimized < 15, 1, 0)\n\nTo check that the two corresponding guards are the same, the solver is asked to\nprove that (i2unoptimized == 1) == (i2optimized == 1). This is\ncorrect, because the formulas for i2unoptimized and i2optimized are\ncompletely identical.\nAfter checking that the guards behave the same, we add the knowledge to the\nsolver that the guards passed. So the Z3 formulas become:\ni1unoptimized == input_i0 + 10\ni2unoptimized == If(i1unoptimized < 15, 1, 0)\ni1optimized == input_i0 + 10\ni2optimized == If(i1optimized < 15, 1, 0)\ni1optimized == 1\ni2optimized == 1\n\nNow we continue with the remaining operations of the two traces (lines 6-8).\nWe start by adding the int_lt operation in the unoptimized trace to the Z3\nformulas:\n...\ni3unoptimized == If(input_i0 < 6, 1, 0)\n\nBecause the second guard was optimized away, we need to ask Z3 to prove that\ni3unoptimized == 1 is always true, which fails and gives the following\ncounterexample:\ninput_i0 = 9223372036854775800\ni1unoptimized = 9223372036854775810\ni2unoptimized = 0\ni1optimized = 9223372036854775810\ni2optimized = 1\ni3unoptimized = 0\n\nThus demonstrating the bug. The fact that the Z3-based equivalence check also\nmanaged to find the original motivating bug without manually translating it to\na formula is a good confirmation that the approach works.\n\n\nSecond bug\nSo with this code I applied the Z3-based equivalence check to all our optimizer\nunit tests. In addition to the bug we've been discussing the whole post, it also\nfound another buggy test! I had found it too by hand by staring at all the tests\nin the process of writing all the Z3 infrastructure, but it was still a good\nconfirmation that the process worked. This bug was in the range analysis for\nint_neg, integer negation. It failed to account that -MININT == MININT\nand therefore did a mis-optimization along the following lines:\nimport __pypy__\n\ndef wrong(x):\n    a = __pypy__.intop.int_sub(0, x)\n    if a < 0:\n        if x > 0:\n            return 0\n        return 1\n    return 2\n\nWhich was wrongly optimized into:\nimport __pypy__\n\ndef wrong(x):\n    a = __pypy__.intop.int_sub(0, x)\n    if a < 0:\n        return 0\n    return 2\n\nThis is wrong precisely for x == MININT.\n\n\nGenerating Random Traces\nThese two bugs were the only two that the Z3 checker found for existing unit\ntests. To try to find some more bugs I combined PyPy's existing random trace\ngenerator with the Z3 optimization checker. The random trace generator has so\nfar been mostly used to find bugs in the machine code backends, particularly\nalso in the register allocator. So far we haven't used it with our optimizer,\nbut my experiments show that we should have!\nI'm going to describe a little bit how the random trace generator works. It's\nactually not that complicated, but there's one neat trick to it.\nThe basic idea is straightforward, it starts out with an empty trace with a\nrandom number of input variables. Then it adds some number of operations to the\ntrace, either regular operations or guards. Every operation takes already\nexisting variables as input.\nThe neat trick is that our random trace generator keeps a concrete random\nexample value for every one of the input variables, and an example result for\nevery operation. In this way, it is possible to generate guards that are\nconsistent with the example values to ensure that running the trace to its end\nis possible with at least one set of values.\nHere's an example random trace that is generated, together with the random\nexample inputs and the results of every operation at the end of every line:\n[i0, i1, i2, i3, i4, i5] # example values: 9, 11, -8, -95, 46, 57\ni6 = int_add_ovf(i3, i0) # -86\nguard_no_overflow()\ni7 = int_sub(i2, -35/ci) # 27\ni8 = uint_ge(i3, i5) # 1\nguard_true(i8)\ni9 = int_lt(i7, i8) # 0\ni10 = int_mul_ovf(34/ci, i7) # 918\nguard_no_overflow()\ni11 = int_and(i10, 63/ci) # 22\ni12 = int_rshift(i3, i11) # -1\ni13 = int_is_zero(i7) # 0\ni14 = int_is_true(i13) # 0\nguard_false(i13)\ni15 = int_lt(i8, i4) # 1\ni16 = int_and(i6, i0) # 8\ni17 = uint_ge(i6, -6/ci) # 0\nfinish()\nNote how every guard generated is true for the example values.\nI have been running this combination of random trace generation and Z3 checking\nfor many nights and it has found some bugs, which I'll describe in the next\nsection. It should probably be run for a lot longer, but still a useful\nexercise already.\nIn this mode, I'm giving every Z3 call a time limit to make sure that the random\ntests don't just take arbitrarily long. This means that asking Z3 to prove\nsomething can have three outcomes, either it's proved, or Z3 finds a\ncounterexample, or Z3 times out.\n\n\nBugs Found\nIn addition to the two bugs I've already described, I'll briefly list the\nadditional bugs that were found by optimizing random traces and then trying to\nprove the equivalence with Z3.\nMost of the bugs were actually identified by optimizing random traces alone, not\nby the Z3 component. They manifested as assert failures in the JIT compiler.\n\nThe JIT concluded after 12 == int_mul(x, 12) that x == 1, which is\nincorrect if overflow occurred (a counterexample is 0x8000000000000001).\nAn amusing bug, where from 0 == int_lshift(0x1000000000000000, x) with\nx <= 0 <= 15, the JIT concluded that 0x1000000000000000 == 0,\ntriggering an assert. This wrong conclusion was again caused by not taking the\npossibility of overflow into account.\nA corner case in an optimization for chained integer additions with a\nconstant, where in complex enough expressions, the wrong IR API was used\n(which works correctly in simple cases). Again, this triggered an assert.\n\nThis shows that we should have been fuzzing our JIT optimizer already (not a\nsurprising  observation in hindsight, fuzz all the things!).\nThankfully, there was also one further bug that really failed in the Z3\nverifier. It's a bug in common subexpression elimination / arithmetic\nsimplification, which again does not take overflow correctly into account.\nThe buggy trace looks like this (unfortunately it's not easily possible to show\nthis bug in Python code).\n[a, b]\nc = int_add(a, b)\nr = int_sub_ovf(c, b)\nguard_no_ovf()\nfinish(r)\n\nThis was optimized to:\n[a, b]\nfinish(a)\n\nWhich is incorrect, because the guard can fail given the right inputs.\nBut the optimizer concluded that the subtraction is safe, because its the\ninverse of an earlier addition, not taking into account that this earlier\naddition can have overflowed.\nNote that a related optimization is actually correct. Given this code:\n[a, b]\nc = int_add_ovf(a, b)\nguard_no_ovf()\nr = int_sub(c, b)\nfinish(r)\n\nIt can be optimized to:\n[a, b]\nc = int_add_ovf(a, b)\nguard_no_ovf()\nfinish(a)\n\n\n\nFuture Work and Conclusion\nIn the current form the Z3 checker is only a start, even though it has already\nbeen concretely useful. There are various directions into which we could extend\nit. In addition to generate random tests completely from scratch, we could also\nstart from the existing manually written unit-tests and randomly mutate those.\nI also want to extend the Z3 checker with support more operations, heap\noperations in particular (but it's not quite clear to me how to model garbage\ncollection).\nI also want to try to switch the code away from the Z3 API and use the more\ngeneral smtlib interface directly, in order to be able to use other SMT\ncheckers than Z3, eg CVC4.\nBut all in all this was a fun and not too hard way to find a bunch of bugs in\nour optimizer! And the infrastructure is now in place, which means that we run\nsome random test cases every time we execute our tests. This is going to be\nparticularly useful when we do further work on the integer reasoning of the JIT\n(like Nico is doing, for example). As of time of writing of this post, all the\nbugs mentioned have been fixed and the Z3 code has landed on the default branch\nand runs as part of PyPy's CI infrastructure.\n\n\nAcknowledgements\nThanks to Saam Barati, Max Bernstein, Joshua Schmidt and Martin\nBerger, for great feedback on drafts of this post!",
      "tags": "jit,testing,z3",
      "url": "https://www.pypy.org/posts/2022/12/jit-bug-finding-smt-fuzzing.html"
    },
    {
      "title": "PyPy v7.3.10 release",
      "text": "PyPy v7.3.10: release of python 2.7, 3.8, and 3.9\nThe PyPy team is proud to release version 7.3.10 of PyPy. We have some nice\nspeedups and bugfixes we wish to share. The release includes three different\ninterpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.8, which is an interpreter supporting the syntax and the features of\nPython 3.8, including the stdlib for CPython 3.8.15.\nPyPy3.9, which is an interpreter supporting the syntax and the features of\nPython 3.9, including the stdlib for CPython 3.9.15. We have gained\nconfidence in the stability of this version, and are removing the \"beta\"\nlabel.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. Highlights of the release, since the release of 7.3.9 in March 2022\ninclude:\n\n\nA release of Apple Silicon M1 arm64 versions. This work was sponsored by\nan anonymous donor and is tested on our buildbots.\nMany improvements to the basic interpreter to make it 15-20% faster\nThe conda-forge community has built over 1000 packages for PyPy3.8 and 3.9,\nmaking it easier than ever to use PyPy.\nUpdate the packaged OpenSSL to 1.1.1s, sqlite3 to 3.39.4, and apply\napplicable security fixes from CPython 3.9.15 to PyPy2.7\nUpdate the HPy backend in PyPy3.8 and PyPy3.9 to 0.0.4\n\n\nWe recommend updating. You can find links to download the v7.3.10 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: bug fixes,\nPyPy and RPython documentation improvements, or general help with making\nRPython's JIT even better. Since the previous release, we have accepted\ncontributions from five new contributors, thanks for pitching in, and welcome\nto the project!\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy.\nIn any case, both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.8 and\n3.9. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nWe provide binary builds for:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS 64 bits, Windows 64 bits)\n64-bit ARM machines running Linux (aarch64).\nApple M1 arm64 machines (macos_arm64).\ns390x running Linux\n\n\nPyPy support Windows 32-bit, Linux PPC64 big- and little-endian, and Linux ARM\n32 bit, but does not release binaries. Please reach out to us if you wish to\nsponsor binary releases for those platforms. Downstream packagers provide\nbinary builds for debian, Fedora, conda, OpenBSD, FreeBSD, Gentoo, and more.\n\n\nWhat else is new?\nFor more information about the 7.3.10 release, see the full changelog.\nPlease update, and continue to help us make pypy better.\nCheers,\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2022/12/pypy-v7310-release.html"
    },
    {
      "title": "PyPy and conda-forge",
      "text": "You can use PyPy as your python interpreter in a conda environment. The\nconda-forge team has graciously provided this service.\nThe conda-forge tips-and-tricks\npage says:\n\nThe conda-forge channel supports creating and installing packages into\nenvironments using the PyPy interpreter. Many packages are already available.\nYou need to enable the conda-forge channel and use the pypy identifier when\ncreating your environment:\n\n  $ conda create -c conda-forge -n my-pypy-env pypy python=3.8\n  $ conda activate my-pypy-env\n\n\n\nCurrently supported python versions are 3.8 and 3.9. Support for pypy3.7 has\nbeen dropped. While you can still create a python 3.7 environment, you you\nwill not be getting updates as new package versions are released (including\npypy itself).\nif you are using defaults as a low priority channel, then you need to use\nstrict channel priority as the metadata in defaults has not been patched yet\nwhich allows cpython extension packages to be installed alongside pypy.\n\n  $ conda config --set channel_priority strict\n\n\nThe work required some out-of-the-box thinking on the part of conda-forge since\nthey needed to add the idea of a pypy identifier to the python version and\nthe whole conda team has been very supportive of the effort needed. Binary\npackages are on offer for the usual platforms:\n\nx86_64 windows, macos, linux\nppc64le and aarch64 linux.\n\nThere are currently over 1000 packages available for download via the\nconda-forge channel, and more are being added as the kind package maintainers\nwork around various differences between CPython and PyPy. Please let us know if\nyour favorite package is not supported.",
      "tags": "extension modules",
      "url": "https://www.pypy.org/posts/2022/11/pypy-and-conda-forge.html"
    },
    {
      "title": "The PyPy Blog Turns 15 Years",
      "text": "Exactly 15 years ago today we wrote the first blog post on the PyPy blog!\nOver the years, we have written 423 posts, from the shortest to the\nlongest. In 2021 we moved from blogger to our own domain.\nThe topics over the years varied widely, we published release announcements;\nroadmaps; JIT, GC and STM updates; benchmarks; sprint, trip and\nconference reports; technical deep dives; case studies; april fool's\njokes; research projects; other languages using RPython; finished PhD\nBachelor and Master, theses; pictures:\n\n\n\nand diagrams:\n\n\n\nQuite a number of blog posts were very early iterations of papers that we\npublished later, here are a few that I can remember:\n\nApplying a Tracing JIT to an Interpreter became Tracing the meta-level:\nPyPy's tracing JIT compiler at ICOOOLPS 2009, by far our most successful\npaper.\nEscape Analysis in PyPy's JIT became Allocation removal by partial\nevaluation in a tracing JIT at PEPM 2010.\nControlling the Tracing of an Interpreter With Hints was a draft of the\npaper Runtime feedback in a meta-tracing JIT for efficient dynamic\nlanguages at ICOOOLPS 2011\nUsing Escape Analysis Across Loop Boundaries for Specialization was the\nnucleus of Loop-aware optimizations in PyPy's tracing JIT at DLS 2012.\nList Strategies was eventually turned into the paper Storage strategies\nfor collections in dynamically typed languages at OOPSLA 2013.\n\n\nGreatest Hits\nIn terms of visitors, the top five posts on the old blog were \u2013 on the new blog\nwe simply don't have stats (yet?):\n\nLet's remove the global interpreter lock\nTutorial: Writing an Interpreter with PyPy, Part 1\nPyPy's new JSON parser\nPyPy gets funding from Mozilla for Python 3.5 support\nHow to make your code 80 times faster\n\nThe number of posts per year developed like this:\n\nThe most prolific authors are:\n\nMaciej Fija\u0142kowski\nCarl Friedrich Bolz-Tereick\nArmin Rigo\nAntonio Cuni\nMatti Picus\n\nSeveral blog posts have made it to the Hacker News front page, three of them to\nnumber 1:\n\nPyPy-STM: first \u201cinteresting\u201d release (discussion)\nLet's Remove the Global Interpreter Lock (discussion)\nInside cpyext: Why emulating CPython C API is so Hard (discussion)\n\n\n\nPersonal Favourites\nWhile looking through the posts, there were a few that stood out to me in some\nway, so here's a subjective list of ones that I had fun looking at again:\n\n2008: Sprint Discussions: JIT Generator Planning\n2009: PyPy gets a new compiler\n2010: Oh, and btw: PyPy gets funding through \"Eurostars\"\n2011: Realtime image processing in Python\n2012: Architecture of Cppyy\n2013: 10 years of PyPy\n2014: PyPy IO Improvements\n2015: Automatic SIMD vectorization support in PyPy\n2016: PyPy Enterprise Edition\n2017: Async HTTP benchmarks on PyPy3\n2018: Improving SyntaxError in PyPy\n2018: The First 15 Years of PyPy \u2014 a Personal Retrospective\n2019: PyPy for low-latency systems\n2020: PyPy and CFFI have moved to Heptapod\n2021: Some Ways that PyPy uses Graphviz\n\nWe'd like to thank our authors, guest authors, commenters, users and readers who\nhave stuck with us through one and a half decades! If there's any particular\ntopics you would like to read something about, or any guest posts you'd like to\nwrite, let us know!",
      "tags": "meta",
      "url": "https://www.pypy.org/posts/2022/10/blog-15-years.html"
    },
    {
      "title": "Allocation Removal in the Toy Optimizer",
      "text": "One of the workhorse optimization of RPython's tracing JIT is allocation\nremoval, which removes short-lived object allocation from traces. Many Python\nprograms create a lot of objects that only live for a short time, and whose\nlifespan is fully predictable (common examples are integer and float boxes, but\nalso tuples, frames, intermediate string results, etc). Allocation removal will\ntry (and very often succeed) to remove these allocations from traces. In\nthis blog post I want to show a toy version of how allocation removal is\nimplemented.\nIn the previous blog post of this series I showed the complete code for\nwriting a toy one-pass optimizer that does constant folding, common\nsubexpression elimination and strength reduction. In this\nsecond post, I want to use allocation removal as a more advanced optimization\npass. The basic optimization framework is the same, we will use the same\ndatastructures for intermediate representation and also keep using the same\nunion find data structure to store equivalences between IR operations. Here's\nthe infrastructure code from the last post:\nimport pytest\nfrom typing import Optional, Any\n\n\nclass Value:\n    def find(self):\n        raise NotImplementedError(\"abstract\")\n\n    def _set_forwarded(self, value):\n        raise NotImplementedError(\"abstract\")\n\n\nclass Operation(Value):\n    def __init__(\n        self, name: str, args: list[Value]\n    ):\n        self.name = name\n        self.args = args\n        self.forwarded = None\n        self.info = None\n\n    def __repr__(self):\n        return (\n            f\"Operation({self.name}, \"\n            f\"{self.args}, {self.forwarded}, \"\n            f\"{self.info})\"\n        )\n\n    def find(self) -> Value:\n        op = self\n        while isinstance(op, Operation):\n            next = op.forwarded\n            if next is None:\n                return op\n            op = next\n        return op\n\n    def arg(self, index):\n        return self.args[index].find()\n\n    def make_equal_to(self, value: Value):\n        self.find()._set_forwarded(value)\n\n    def _set_forwarded(self, value: Value):\n        self.forwarded = value\n\n\nclass Constant(Value):\n    def __init__(self, value: Any):\n        self.value = value\n\n    def __repr__(self):\n        return f\"Constant({self.value})\"\n\n    def find(self):\n        return self\n\n    def _set_forwarded(self, value: Value):\n        assert (\n            isinstance(value, Constant)\n            and value.value == self.value\n        )\n\nclass Block(list):\n    def opbuilder(opname):\n        def wraparg(arg):\n            if not isinstance(arg, Value):\n                arg = Constant(arg)\n            return arg\n        def build(self, *args):\n            # construct an Operation, wrap the\n            # arguments in Constants if necessary\n            op = Operation(opname,\n                [wraparg(arg) for arg in args])\n            # add it to self, the basic block\n            self.append(op)\n            return op\n        return build\n\n    # a bunch of operations we support\n    add = opbuilder(\"add\")\n    mul = opbuilder(\"mul\")\n    getarg = opbuilder(\"getarg\")\n    dummy = opbuilder(\"dummy\")\n    lshift = opbuilder(\"lshift\")\n    # some new one for this post\n    alloc = opbuilder(\"alloc\")\n    load = opbuilder(\"load\")\n    store = opbuilder(\"store\")\n    print = opbuilder(\"print\")\n\ndef bb_to_str(bb: Block, varprefix: str = \"var\"):\n    def arg_to_str(arg: Value):\n        if isinstance(arg, Constant):\n            return str(arg.value)\n        else:\n            return varnames[arg]\n\n    varnames = {}\n    res = []\n    for index, op in enumerate(bb):\n        var = f\"{varprefix}{index}\"\n        varnames[op] = var\n        arguments = \", \".join(\n            arg_to_str(op.arg(i))\n                for i in range(len(op.args))\n        )\n        strop = f\"{var} = {op.name}({arguments})\"\n        res.append(strop)\n    return \"\\n\".join(res)\n\nThere are two changes to the code from the last post: Operation instances\nhave a new .info field, which is set to None by default. We will learn\nhow the info field is used a bit further down. Also, we define some new\noperations.\n\nInterpreter\nIn this post we will mainly concern ourselves with optimizing\nprograms that allocate memory. We assume that our language is garbage collected\nand memory safe. The new operations that we will optimize are alloc\n(allocates some new object), store (stores a value into a fixed field of an\nobject), load (loads the value from a field in the object).\nWe are leaving out a lot of details of a \"real\" system here, usually an\nalloc operation would get some extra information, for example the type of\nthe freshly allocated object or at least its size. load and store would\ntypically have some kind of field offset and maybe some information about the\nfield's type\nHere's a simple program that uses these operations:\nvar0 = getarg(0)\nobj0 = alloc()\nstore(obj0, 0, var0)\nvar1 = load(obj0, 0)\nprint(var1)\nThe code allocates a new object obj0, stores var0 into field 0 of\nthe object, the loads the same field and prints the result of the load.\nBefore we get started in writing the optimizer for these operations, let's try\nto understand the semantics of the new operations a bit better. To do this, we\ncan sketch a small interpreter for basic blocks, supporting only getarg,\nalloc, store, load, print:\ndef test_interpret():\n    bb = Block()\n    var0 = bb.getarg(0)\n    obj = bb.alloc()\n    sto = bb.store(obj, 0, var0)\n    var1 = bb.load(obj, 0)\n    bb.print(var1)\n    assert interpret(bb, 17) == 17\n\nclass Object:\n    def __init__(self):\n        self.contents: dict[int, Any] = {}\n\n    def store(self, idx : int, value : Any):\n        self.contents[idx] = value\n\n    def load(self, idx : int):\n        return self.contents[idx]\n\ndef get_num(op, index=1):\n    assert isinstance(op.arg(index), Constant)\n    return op.arg(index).value\n\ndef interpret(bb : Block, *args : tuple[Any]):\n    def argval(op, i):\n        arg = op.arg(i)\n        if isinstance(arg, Constant):\n            return arg.value\n        else:\n            assert isinstance(arg, Operation)\n            return arg.info\n\n    for index, op in enumerate(bb):\n        if op.name == \"getarg\":\n            res = args[get_num(op, 0)]\n        elif op.name == \"alloc\":\n            res = Object()\n        elif op.name == \"load\":\n            fieldnum = get_num(op)\n            res = argval(op, 0).load(fieldnum)\n        elif op.name == \"store\":\n            obj = argval(op, 0)\n            fieldnum = get_num(op)\n            fieldvalue = argval(op, 2)\n            obj.store(fieldnum, fieldvalue)\n            # no result, only side effect\n            continue\n        elif op.name == \"print\":\n            res = argval(op, 0)\n            print(res)\n            return res\n        else:\n            raise NotImplementedError(\n                f\"{op.name} not supported\")\n        op.info = res\n\nThe interpreter  walks the operations of a block, executing each one in turn. It\nuses the info field to store the result of each already executed\nOperation. In this interpreter sketch we stop at the first print that\nwe execute and return its argument for the simple but bad reason that it makes\ntest_interpret easier to write.\nObjects in the interpreter are represented using a class Object, which\nstores the object's field into a Python dictionary. As written above, this is a\nsimplification, in a real system the alloc operation might for example take\nsome kind of type as an argument, that describes which kinds of fields an\nobject has and how they are laid out in memory, which would allow more\nefficient storage of the content. But we don't want to care about this level of\ndetail in the post, so using a dict in the interpreter is good enough.\n\n\nVersion 1: Naive Attempt\nIn many programs, some allocated objects don't live for very long and have a\ncompletely predictable lifetime. They get allocated, used for a while, and then\nthere is no way to reference them any more, so the garbage collector will\nreclaim them. The very first example block had such an allocation:\nvar0 = getarg(0)\nobj0 = alloc()\nstore(obj0, 0, var0)\nvar1 = load(obj0, 0)\nprint(var1)\nHere obj0 is written to, then read from, and then it's no longer used. We\nwant to optimize such programs to remove this alloc operation. The optimized\nversion of this program would look like this:\nvar0 = getarg(0)\nprint(var0)\nThe alloc, store and load operations have been completely removed.\nThis is a pretty important optimizations for PyPy's JIT: Allocations, memory\nreads and writes are quite costly and occur a lot in Python, so getting rid\nof as many of them as possible is instrumental for performance.\nImplementing the optimization is not a lot of code! However, understanding all\nthe corner cases of the\noptimization and making sure that the resulting program behave correctly is not\ncompletely trivial. Therefore we will develop the optimization step by step, in\na test driven fashion: I will start each section with a new test that shows a\nbug in the version of the optimization that we have so far.\nLet's start in a really naive way. Here's the first test we would like to\npass, using the example program above:\ndef test_remove_unused_allocation():\n    bb = Block()\n    var0 = bb.getarg(0)\n    obj = bb.alloc()\n    sto = bb.store(obj, 0, var0)\n    var1 = bb.load(obj, 0)\n    bb.print(var1)\n    opt_bb = optimize_alloc_removal(bb)\n    # the virtual object looks like this:\n    #  obj\n    # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    # \u2502 0: var0  \u2502\n    # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = print(optvar0)\"\"\"\n\nWe will define a class VirtualObject that is basically identical to\nObject above. But it will not be used by the interpreter, instead we will\nuse it during optimization.\nclass VirtualObject:\n    def __init__(self):\n        self.contents: dict[int, Value] = {}\n\n    def store(self, idx, value):\n        self.contents[idx] = value\n\n    def load(self, idx):\n        return self.contents[idx]\n\nThe structure of the optimizer is going to be like those in the first blog post.\nThe optimizer makes a single pass over all operations. It removes some and\nemits others.\nThis first version of the allocation removal optimizer is going to be extremely\noptimistic. It simply assumes that all the allocations in the program can be\noptimized away. That is not realistic in practice. We will have to\nrefine this approach later, but it's a good way to start. That means whenever\nthe optimizer sees an alloc operation, it removes it and creates a\nVirtualObject object which stores the information that is known during\noptimization about the result of the alloc. Like in the interpreter, the\nVirtualObject is stored in the .info field of the Operation instance\nthat represents the alloc.\nWhen the optimizer sees a store operation, it will also remove it and\ninstead execute the store by calling the VirtualObject.store method.\nHere is one important difference between the interpreter and the optimizer: In\nthe interpreter, the values that were stored into an Object (and thus\nput into the object's .contents dictionary) were runtime values, for\nexample integers or other objects. In the optimizer however, the\nfields of the VirtualObject store Value instances, either Constant\ninstances or Operation instances.\nWhen the optimizer sees a load operation, it also removes it, and replaces\nthe load with the Operation (or Constant) that is stored in the\nVirtualObject at that point:\ndef optimize_alloc_removal(bb):\n    opt_bb = Block()\n    for op in bb:\n        if op.name == \"alloc\":\n            op.info = VirtualObject()\n            continue\n        if op.name == \"load\":\n            info = op.arg(0).info\n            field = get_num(op)\n            op.make_equal_to(info.load(field))\n            continue\n        if op.name == \"store\":\n            info = op.arg(0).info\n            field = get_num(op)\n            info.store(field, op.arg(2))\n            continue\n        opt_bb.append(op)\n    return opt_bb\n\nThis is the first version of the optimization. It doesn't handle all kinds of\ndifficult cases, and we'll have to do something about its optimism.\nBut, already in this minimalistic form, we can write a slightly more complicated\ntest with two allocations, one object pointing to the other. It works correctly\ntoo, both allocations are removed:\ndef test_remove_two_allocations():\n    bb = Block()\n    var0 = bb.getarg(0)\n    obj0 = bb.alloc()\n    sto1 = bb.store(obj0, 0, var0)\n    obj1 = bb.alloc()\n    sto2 = bb.store(obj1, 0, obj0)\n    var1 = bb.load(obj1, 0)\n    var2 = bb.load(var1, 0)\n    bb.print(var2)\n    # the virtual objects look like this:\n    #  obj0\n    # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    # \u2502 0: \u2577 \u2502\n    # \u2514\u2500\u2500\u2500\u2500\u253c\u2500\u2518\n    #      \u2502\n    #      \u25bc\n    #     obj1\n    #   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    #   \u2502 0: var0 \u2502\n    #   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    # therefore\n    # var1 is the same as obj0\n    # var2 is the same as var0\n    opt_bb = optimize_alloc_removal(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = print(optvar0)\"\"\"\n\n\n\nVersion 2: Re-Materializing Allocations\nTo make it easier to talk about how the optimizer operates, let's introduce\nsome terminology. As already seen by the choice\nof the class name VirtualObject, we will call an object virtual if the\noptimizer has optimized away the alloc operation that creates the object.\nOther objects are equivalently not virtual, for example those that have\nexisted before we enter the current code block.\nThe first problem that we need to fix is the assumption that every\nallocation can be removed. So far we only looked at small programs where every\nallocation could be removed, or equivalently, where every object is virtual.\nA program that creates virtual objects, stores into and loads from them, and\nthen forgets the objects. In this simple case removing the allocations is fine.\nAs we saw in the previous section, it's also fine to have a virtual object\nreference another virtual, both allocations can be removed.\nWhat are the cases were we can't remove an allocation?\nThe first version of the optimizer simply assumed that every allocation can be\nremoved. This can't work. We will replace this assumption with the following\nsimple heuristic:\nIf a reference to a virtual object a is stored into an object b\nthat is not virtual, then a will also stop being virtual. If an object a\nthat was virtual stops being virtual, we say that it escapes. \u00b9\nThe simplest test case for this happening looks like this:\ndef test_materialize():\n    bb = Block()\n    var0 = bb.getarg(0)\n    obj = bb.alloc()\n    sto = bb.store(var0, 0, obj)\n    opt_bb = optimize_alloc_removal(bb)\n    #  obj is virtual, without any fields\n    # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    # \u2502 empty \u2502\n    # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    # then we store a reference to obj into\n    # field 0 of var0. Since var0 is not virtual,\n    # obj escapes, so we have to put it back\n    # into the optimized basic block\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = alloc()\noptvar2 = store(optvar0, 0, optvar1)\"\"\"\n    # so far, fails like this:\n    # the line:\n    # info.store(field, op.arg(2))\n    # produces an AttributeError because info\n    # is None\n\nIf the optimizer reaches a point where a virtual object escapes (like the\nstore operation in the test), the optimizer has already removed the alloc\noperation that created the virtual object. If the object escapes, we don't want\nto go back in the operations list and re-insert the alloc operation, that\nsounds potentially very complicated. Instead, we re-insert the alloc\noperation that will recreate the virtual object at the point of escape using a\nhelper function materialize.\ndef materialize(opt_bb, value: Operation) -> None:\n    assert not isinstance(value, Constant)\n    assert isinstance(value, Operation)\n    info = value.info\n    assert isinstance(info, VirtualObject)\n    assert value.name == \"alloc\"\n    # put the alloc operation back into the trace\n    opt_bb.append(value)\n\nI've added a number of fairly strong assertions to materialize to encode our\ncurrent assumptions about the situations in which it expects to be called. We\nwill remove some of them later as we generalize the code.\nNow that we have materialize we need to change optimize_alloc_removal to\nrecognize the case of storing a virtual object into a non-virtual one. We can\nrecognize Operation instances that produced a virtual object by looking at\ntheir .info field. If it is None, the object is not virtual, otherwise\nit is. If we store something into a virtual object, we leave the code as above.\nIf we store a virtual object into an object that is not virtual, we will first\nmaterialize the virtual object, and then emit the store.\ndef optimize_alloc_removal(bb):\n    opt_bb = Block()\n    for op in bb:\n        if op.name == \"alloc\":\n            op.info = VirtualObject()\n            continue\n        if op.name == \"load\":\n            info = op.arg(0).info\n            field = get_num(op)\n            op.make_equal_to(info.load(field))\n            continue\n        if op.name == \"store\":\n            info = op.arg(0).info\n            if info: # virtual\n                field = get_num(op)\n                info.store(field, op.arg(2))\n                continue\n            else: # not virtual\n                # first materialize the\n                # right hand side\n                materialize(opt_bb, op.arg(2))\n                # then emit the store via\n                # the general path below\n        opt_bb.append(op)\n    return opt_bb\n\nThis is the general idea, and it is enough to pass test_materialize. But of\ncourse there are still a number of further problems that we now need to solve.\n\n\nVersion 3: Don't Materialize Twice\nThe first problem is the fact that after we materialize a virtual object, it is\nno longer virtual. So if it escapes a second time, it should not be\nmaterialized a second time. A test for that case could simply repeat the\nstore operation:\ndef test_dont_materialize_twice():\n    # obj is again an empty virtual object,\n    # and we store it into var0 *twice*.\n    # this should only materialize it once\n    bb = Block()\n    var0 = bb.getarg(0)\n    obj = bb.alloc()\n    sto0 = bb.store(var0, 0, obj)\n    sto1 = bb.store(var0, 0, obj)\n    opt_bb = optimize_alloc_removal(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = alloc()\noptvar2 = store(optvar0, 0, optvar1)\noptvar3 = store(optvar0, 0, optvar1)\"\"\"\n    # fails so far: the operations that we get\n    # at the moment are:\n    # optvar0 = getarg(0)\n    # optvar1 = alloc()\n    # optvar2 = store(optvar0, 0, optvar1)\n    # optvar3 = alloc()\n    # optvar4 = store(optvar0, 0, optvar3)\n    # ie the object is materialized twice,\n    # which is incorrect\n\nWe solve the problem by setting the .info field of an object that we\nmaterialize to None to mark it as no longer being virtual.\ndef materialize(opt_bb, value: Operation) -> None:\n    assert not isinstance(value, Constant)\n    assert isinstance(value, Operation)\n    info = value.info\n    if info is None:\n        return # already materialized\n    assert value.name == \"alloc\"\n    # put the alloc operation back into the trace\n    opt_bb.append(value)\n    # but only once\n    value.info = None\n\n# optimize_alloc_removal unchanged\n\nThis fixes the problem, only one alloc is created. This fix also allows\nanother test case to pass, one where we store a non-virtual into another\nnon-virtual, code which we cannot optimize at all:\ndef test_materialize_non_virtuals():\n    # in this example we store a non-virtual var1\n    # into another non-virtual var0\n    # this should just lead to no optimization at\n    # all\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.getarg(1)\n    sto = bb.store(var0, 0, var1)\n    opt_bb = optimize_alloc_removal(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = getarg(1)\noptvar2 = store(optvar0, 0, optvar1)\"\"\"\n\n\n\nVersion 4: Materialization of Constants\nAnother straightforward extension is to support materializing constants. A\nconstant is never virtual, so materializing it should do nothing.\ndef test_materialization_constants():\n    # in this example we store the constant 17\n    # into the non-virtual var0\n    # again, this will not be optimized\n    bb = Block()\n    var0 = bb.getarg(0)\n    sto = bb.store(var0, 0, 17)\n    opt_bb = optimize_alloc_removal(bb)\n    # the previous line fails so far, triggering\n    # the assert:\n    # assert not isinstance(value, Constant)\n    # in materialize\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = store(optvar0, 0, 17)\"\"\"\n\nTo implement that case, we check for value being a constant and return\nearly:\ndef materialize(opt_bb, value: Operation) -> None:\n    if isinstance(value, Constant):\n        return\n    assert isinstance(value, Operation)\n    info = value.info\n    if info is None:\n        return # already materialized\n    assert value.name == \"alloc\"\n    # put the alloc operation back into the trace\n    opt_bb.append(value)\n    # but only once\n    value.info = None\n\n# optimize_alloc_removal unchanged\n\n\n\nVersion 5: Materializing Fields\nNow we need to solve a more difficult problem. So far, the virtual objects that\nwe have materialized have all been empty, meaning they didn't have any fields\nwritten to at the point of materialization. Let's write a test for this:\ndef test_materialize_fields():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.getarg(1)\n    obj = bb.alloc()\n    contents0 = bb.store(obj, 0, 8)\n    contents1 = bb.store(obj, 1, var1)\n    sto = bb.store(var0, 0, obj)\n\n    # the virtual obj looks like this\n    #  obj\n    # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    # \u2502 0: 8 \u2502 1: var1  \u2502\n    # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    # then it needs to be materialized\n    # this is the first example where a virtual\n    # object that we want to materialize has any\n    # content and is not just an empty object\n    opt_bb = optimize_alloc_removal(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = getarg(1)\noptvar2 = alloc()\noptvar3 = store(optvar2, 0, 8)\noptvar4 = store(optvar2, 1, optvar1)\noptvar5 = store(optvar0, 0, optvar2)\"\"\"\n    # fails so far! the operations we get\n    # at the moment are:\n    # optvar0 = getarg(0)\n    # optvar1 = getarg(1)\n    # optvar2 = alloc()\n    # optvar3 = store(optvar0, 0, optvar2)\n    # which is wrong, because the store operations\n    # into optvar1 got lost\n\nTo fix this problem, we need to re-create a store operation for every\nelement of the .contents dictionary of the virtual object we are\nmaterializing. \u00b2\ndef materialize(opt_bb, value: Operation) -> None:\n    if isinstance(value, Constant):\n        return\n    assert isinstance(value, Operation)\n    info = value.info\n    if info is None:\n        return # already materialized\n    assert value.name == \"alloc\"\n    # put the alloc operation back into the trace\n    opt_bb.append(value)\n    # put the content back\n    for idx, val in info.contents.items():\n        # re-create store operation\n        opt_bb.store(value, idx, val)\n    # only materialize once\n    value.info = None\n\n# optimize_alloc_removal unchanged\n\nThis is enough to pass the test.\n\n\nVersion 6: Recursive Materialization\nIn the above example, the fields of the virtual objects contained\nonly constants or non-virtual objects. However, we could have a situation where\na whole tree of virtual objects is built, and then the root of the tree escapes.\nThis makes it necessary to escape the whole tree. Let's write a test for a small\ntree of two virtual objects:\ndef test_materialize_chained_objects():\n    bb = Block()\n    var0 = bb.getarg(0)\n    obj0 = bb.alloc()\n    obj1 = bb.alloc()\n    contents = bb.store(obj0, 0, obj1)\n    const = bb.store(obj1, 0, 1337)\n    sto = bb.store(var0, 0, obj0)\n    #  obj0\n    # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    # \u2502 0: \u2577 \u2502\n    # \u2514\u2500\u2500\u2500\u2500\u253c\u2500\u2518\n    #      \u2502\n    #      \u25bc\n    #     obj1\n    #   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    #   \u2502 0: 1337 \u2502\n    #   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    # now obj0 escapes\n    opt_bb = optimize_alloc_removal(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = alloc()\noptvar2 = alloc()\noptvar3 = store(optvar2, 0, 1337)\noptvar4 = store(optvar1, 0, optvar2)\noptvar5 = store(optvar0, 0, optvar1)\"\"\"\n    # fails in an annoying way! the resulting\n    # basic block is not in proper SSA form\n    # so printing it fails. The optimized\n    # block would look like this:\n    # optvar0 = getarg(0)\n    # optvar1 = alloc()\n    # optvar3 = store(optvar1, 0, optvar2)\n    # optvar4 = store(optvar0, 0, optvar1)\n    # where optvar2 is an ``alloc`` Operation\n    # that is not itself in the output block\n\nTo fix it, materialize needs to call itself recursively for all the field\nvalues of the virtual object:\ndef materialize(opt_bb, value: Operation) -> None:\n    if isinstance(value, Constant):\n        return\n    assert isinstance(value, Operation)\n    info = value.info\n    if info is None:\n        return # already materialized\n    assert value.name == \"alloc\"\n    # put the alloc operation back into the trace\n    opt_bb.append(value)\n    # put the content back\n    for idx, val in sorted(info.contents.items()):\n        # materialize recursively\n        materialize(opt_bb, val)\n        opt_bb.store(value, idx, val)\n    # only materialize once\n    value.info = None\n\n# optimize_alloc_removal unchanged\n\nGetting there, the materialization logic is almost done. We need to fix a\nsubtle remaining problem though.\n\n\nVersion 7: Dealing with Object Cycles\nThe bug we need to fix in this section is a bit tricky, and does not immediately\noccur in a lot of programs. In\nfact, in PyPy a variant of it was hiding out in our optimizer\nuntil we found it much later (despite us being aware of the general problem and\ncorrectly dealing with it in other cases).\nThe problem is this: a virtual object can (directly or indirectly) point to\nitself, and we must carefully deal with that case to avoid infinite recursion in\nmaterialize. Here's the simplest test:\ndef test_object_graph_cycles():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.alloc()\n    var2 = bb.store(var1, 0, var1)\n    var3 = bb.store(var0, 1, var1)\n    #   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    #   \u25bc        \u2502\n    #  obj0      \u2502\n    # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n    # \u2502 0: \u2577 \u2502   \u2502\n    # \u2514\u2500\u2500\u2500\u2500\u253c\u2500\u2518   \u2502\n    #      \u2502     \u2502\n    #      \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n    # obj0 points to itself, and then it is\n    # escaped\n    opt_bb = optimize_alloc_removal(bb)\n    # the previous line fails with an\n    # InfiniteRecursionError\n    # materialize calls itself, infinitely\n\n    # what we want is instead this output:\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = alloc()\noptvar2 = store(optvar1, 0, optvar1)\noptvar3 = store(optvar0, 1, optvar1)\"\"\"\n\nThe fix is not a big change, but a little bit subtle nevertheless.\nWe have to change the\norder in which things are done in materialize. Right after emitting the\nalloc, we set the .info to None, to mark the object as not virtual.\nOnly afterwards do we re-create the stores and call materialize recursively.\nIf a recursive call reaches the same object, it's already marked as non-virtual,\nso materialize won't recurse further:\ndef materialize(opt_bb, value: Operation) -> None:\n    if isinstance(value, Constant):\n        return\n    assert isinstance(value, Operation)\n    info = value.info\n    if info is None:\n        return # already materialized\n    assert value.name == \"alloc\"\n    # put the alloc operation back into the trace\n    opt_bb.append(value)\n    # only materialize once\n    value.info = None\n    # put the content back\n    for idx, val in sorted(info.contents.items()):\n        # materialize recursively\n        materialize(opt_bb, val)\n        opt_bb.store(value, idx, val)\n\n\n\nVersion 8: Loading from non-virtual objects\nNow materialize is done. We need to go back to optimize_alloc_removal and\nimprove it further. The last time we changed it, we added a case analysis to the\ncode dealing with store, distinguishing between storing to a virtual and to\na non-virtual object. We need to add an equivalent distinction to the load\ncase, because right now loading from a non-virtual crashes.\ndef test_load_non_virtual():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.load(var0, 0)\n    bb.print(var1)\n    # the next line fails in the line\n    # op.make_equal_to(info.load(field))\n    # because info is None\n    opt_bb = optimize_alloc_removal(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = load(optvar0, 0)\noptvar2 = print(optvar1)\"\"\"\n\nTo fix it, we split the load code into two cases, leaving the virtual path\nas before, and letting the load from a non-virtual fall through to the\ngeneral code at the end of the function.\ndef optimize_alloc_removal(bb):\n    opt_bb = Block()\n    for op in bb:\n        if op.name == \"alloc\":\n            op.info = VirtualObject()\n            continue\n        if op.name == \"load\":\n            info = op.arg(0).info\n            if info: # virtual\n                field = get_num(op)\n                op.make_equal_to(info.load(field))\n                continue\n            # otherwise not virtual, use the\n            # general path below\n        if op.name == \"store\":\n            info = op.arg(0).info\n            if info: # virtual\n                field = get_num(op)\n                info.store(field, op.arg(2))\n                continue\n            else: # not virtual\n                # first materialize the\n                # right hand side\n                materialize(opt_bb, op.arg(2))\n                # then emit the store via\n                # the general path below\n        opt_bb.append(op)\n    return opt_bb\n\n\n\nVersion 9 (Final): Materialize on Other Operations\nWe're almost at the end now. There's one final generalization left to do. We\nstarted with the heuristic that storing a virtual into a non-virtual would\nescape it. This should be generalized. Every time we pass a virtual into any\noperation where it is not the first argument of a load and a store\nshould also escape it (imagine passing the virtual to some function call).\nLet's test this as usual with our print operation:\ndef test_materialize_on_other_ops():\n    # materialize not just on store\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.alloc()\n    var2 = bb.print(var1)\n    opt_bb = optimize_alloc_removal(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = alloc()\noptvar2 = print(optvar1)\"\"\"\n    # again, the resulting basic block is not in\n    # valid SSA form\n\nTo fix this, we will take the call to materialize out of the store code\npath and instead put it into the generic code path the end of the while\nloop:\n# materialize is unchanged\ndef materialize(opt_bb, value: Value) -> None:\n    if isinstance(value, Constant):\n        return\n    assert isinstance(value, Operation)\n    info = value.info\n    if not info:\n        # Already materialized\n        return\n    assert value.name == \"alloc\"\n    opt_bb.append(value)\n    value.info = None\n    for idx, val in sorted(info.contents.items()):\n        materialize(opt_bb, val)\n        opt_bb.store(value, idx, val)\n\ndef optimize_alloc_removal(bb):\n    opt_bb = Block()\n    for op in bb:\n        if op.name == \"alloc\":\n            op.info = VirtualObject()\n            continue\n        if op.name == \"load\":\n            info = op.arg(0).info\n            if info: # virtual\n                field = get_num(op)\n                op.make_equal_to(info.load(field))\n                continue\n        if op.name == \"store\":\n            info = op.arg(0).info\n            if info: # virtual\n                field = get_num(op)\n                info.store(field, op.arg(2))\n                continue\n        # materialize all the arguments of\n        # operations that are put into the\n        # output basic block\n        for arg in op.args:\n            materialize(opt_bb, arg.find())\n        opt_bb.append(op)\n    return opt_bb\n\nThat's it, we're done. It's not a lot of code, but actually quite a powerful\noptimization. In addition to removing allocations for objects that are only used\nbriefly and in predictable ways, it also has another effect. If an object is\nallocated, used in a number of operations and then escapes further down in the\nblock, the operations in between can often be optimized away. This is\ndemonstrated by the next test (which already passes):\ndef test_sink_allocations():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.alloc()\n    var2 = bb.store(var1, 0, 123)\n    var3 = bb.store(var1, 1, 456)\n    var4 = bb.load(var1, 0)\n    var5 = bb.load(var1, 1)\n    var6 = bb.add(var4, var5)\n    var7 = bb.store(var1, 0, var6)\n    var8 = bb.store(var0, 1, var1)\n    opt_bb = optimize_alloc_removal(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = add(123, 456)\noptvar2 = alloc()\noptvar3 = store(optvar2, 0, optvar1)\noptvar4 = store(optvar2, 1, 456)\noptvar5 = store(optvar0, 1, optvar2)\"\"\"\n\nNote that the addition is not optimized away, because the code from this blog\npost does not contain constant folding and the other optimizations from\nthe last one. Combining them would not be too hard though.\n\n\nConclusion\nThat's it! The core idea of PyPy's allocation removal optimization in one or\ntwo screens of code. The real implementation has a number of refinements,\nbut the core ideas are all here.\nI'm not going to show any benchmark numbers or anything like that here, if you\nare interested in numbers you could look at the evaluation Section 6.\n\"Implementation and Evaluation\" of the paper that describes the work.\nThere's a complementary optimization that improves load and store\noperations for objects that are not virtual. I'll probably not write that\ndown as another post, but Max Bernstein and I developed that together on a\nPyPy Twitch channel channel a few weeks ago, here's the recording:\n\n\nFootnotes\n\u00b9 This is how PyPy uses the terminology, not really used consistently by other\nprojects. The term \"escape\" is fairly standard throughout the escape\nanalysis literature. The term \"virtual\" was used originally in Armin Rigo's\nPsyco but is e.g. also used by the paper Partial Escape Analysis and Scalar\nReplacement for Java.\n\u00b2 The order in which we put the store operations back is relying on\ndictionary iteration order, which is insertion order. That's not a bad\nordering, we could also be explicit and sort the fields in some order (ideally\nthe order in which the object lays them out in memory).",
      "tags": "toy-optimizer",
      "url": "https://www.pypy.org/posts/2022/10/toy-optimizer-allocation-removal.html"
    },
    {
      "title": "PyPy Sponsors and Consultants",
      "text": "Keeping a project as ambitious as PyPy requires resources. Sometimes the\nproblems encountered are general, like updating python versions or supporting\nvarious c-extensions. Sometimes the problems are specific and require precise\nsolutions that may not generalize to all users. Likewise, sponsorship of PyPy\ncan be general or specific.\nGeneral PyPy Sponsorship\nPyPy has had many financial contributors in the\npast. We are grateful to them, and to the following current sponsors:\n\n\nBaroque Software who hosts the buildbot\n  master as well as our benchmark\n  runner\n\n\nThe recurring and once-off fiscal sponsors via\n  OpenCollective\n\n\nOthers providing buildbot workers\n\n\nPyPy Consulting Work\n\n\nBaroque Software is an innovative company that\n  has been doing performance oriented consulting for a variety of biggest\n  players on the market since 2007. Please reach out to their team for\n  help making PyPy fulfill its potential in your application.\n\n\nMatti Picus, the PyPy release manager, has been\n  pushing PyPy into the Python ecosystem since 2016: dealing with\n  packaging, compatibility, and performance. He works at\n  Quansight and is available for projects.",
      "tags": "",
      "url": "https://www.pypy.org/pypy-sponsors.html"
    },
    {
      "title": "D\u00fcsseldorf HPy/PyPy/GraalPy sprint September 19-23rd 2022",
      "text": "The programming language group of the Computer Science department of\nHeinrich-Heine Universit\u00e4t D\u00fcsseldorf is happy to invite everybody to another\nsprint in D\u00fcsseldorf, from the 19th to the 23rd of September 2022. This is a\nfully public sprint, everyone and particularly newcomers are welcome to join\nus! The goal is to bring together people from the HPy, PyPy, GraalPy and\nCPython communities.\n\nTopics and goals\n\nwork on HPy APIs, discussions around next steps for the project\ncontinuing new and ongoing ports to HPy, including Cython, NumPy, Pillow, Matplotlib\n3.10 support on PyPy and GraalPy\npreparing the next PyPy release\ndiscussions around ways to improve collaboration between the different Python\nimplementations\n\n\n\nWhat is a sprint?\nThe experience of the PyPy project has shown the benefits of regular\nsprints. They are focussed one week physical meetings where people pair-program\non new features and discuss future plans. Coming to one is a great way to get\nstarted with a project!\n\n\nLocation\nThe sprint will take place in a seminar room of the computer science\ndepartment.  It is in the building 25.12, room 02.50 (second floor) of the\nuniversity campus. For travel instructions see\n\nhttps://www.cs.hhu.de/lehrstuehle-und-arbeitsgruppen/softwaretechnik-und-programmiersprachen/kontakt/service/lage-und-anreise\n\nWe ask participants to wear masks during the indoor working hours.\n\n\n\n\n\nWiegels, CC BY 3.0, via Wikimedia Commons\n\n\n\n\nExact times\nWork days: starting September 19th (~morning), ending September 23rd (~afternoon).\nWe will do a to-be-planned social activity on Wednesday afternoon.\n\n\nRegistration\nPlease register by editing this file or by opening a pull request:\n\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/extradoc/sprintinfo/ddorf2022/people.txt\n\nor by sending a quick mail to the pypy-dev mailing list:\n\nhttp://mail.python.org/mailman/listinfo/pypy-dev",
      "tags": "sprints",
      "url": "https://www.pypy.org/posts/2022/07/ddorf-sprint-sep-2022.html"
    },
    {
      "title": "M1 support for PyPy",
      "text": "The PyPy team is happy to announce that we can now target the macOS ARM64\nplatform. Much of the work was executed by Maciej Fija\u0142kowski (fijal) and\nfunded via a generous contribution to our OpenCollective. The work is based\non our existing support for aarch64 (arm64 on linux) with some twists\nto support the differences between the CPUs and the operating system. There\nare nightly builds for pypy3.8 and pypy3.9 (look for macos_arm64), and\nthe architecture will be part of our next release.\nPlease try it out and let us know how it is useful for you or how we could\nimprove.\nWe still need help improving our macOS support. We have an open issue to\nhelp our packaging story. Help is welcome.\nThe PyPy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2022/07/m1-support-for-pypy.html"
    },
    {
      "title": "Implementing a Toy Optimizer",
      "text": "In this blog post I want to show the complete code (in Python3) of how a very\nsimple optimizer for sequences of operations can work. These algorithms could\nbe part of a (really simple) compiler, or a JIT. The architecture of the code in\nthis blog post is very similar to that of the trace optimizer of the PyPy JIT:\nAfter a trace is produced, is is optimized before being sent to the machine code\nbackend that produces binary instructions for the CPU architecture that PyPy is\nrunning on.\nTo get started, the first thing we need to do is define how our operations are\nstored. The\nformat that a compiler uses to store the program while it is being optimized\nis usually called its intermediate representation (IR). Many production\ncompilers use IRs that are in the Static Single-Assignment Form (SSA), and\nwe will also use that. SSA form has the property that every variable is\nassigned to exactly once, and every variable is defined before it is used. This\nsimplifies many things.\nLet's make this concrete. If our input program is a complex expressions, such\nas a * (b + 17) + (b + 17) the intermediate representation of that (or at\nleast its text representation) would maybe be something like:\nvar1 = add(b, 17)\nvar2 = mul(a, var1)\nvar3 = add(b, 17)\nvar4 = add(var2, var3)\nThis sequence of instructions is inefficient. The operation add(b, 17) is\ncomputed twice and we can save time by removing the second one and only\ncomputing it once. In this post I want to show an optimizer that can do this\n(and some related) optimizations.\nLooking at the IR we notice that the input expression has been linearized\ninto a sequence of operations, and all the intermedia results have been given\nunique variable names. The value that every variable is assigned is computed\nby the right hand side, which is some operation consisting of an operand and an\narbitrary number of arguments. The arguments of an operation are either\nthemselves variables or constants.\nI will not at all talk about the process of translating the input program\ninto the IR. Instead, I will assume we have some component that does this\ntranslation already. The tests in this blog post will construct small\nsnippets of IR by hand. I also won't talk about what happens after the\noptimization (usually the optimized IR is translated into machine code).\n\nImplementing the Intermediate Representation\nLet's start modelling the intermediate representation with Python classes.\nFirst we define a base class of all values that can be used as arguments in\noperations, and let's also add a class that represents constants:\nimport pytest\nfrom typing import Optional, Any\n\nclass Value:\n    pass\n\nclass Constant(Value):\n    def __init__(self, value: Any):\n        self.value = value\n\n    def __repr__(self):\n        return f\"Constant({self.value})\"\n\nOne consequence of the fact that every variable is assigned to only once is\nthat variables are in a one-to-one correspondence with the right-hand-side of\ntheir unique assignments. That means that we don't need a class that represents\nvariables at all. Instead, it's sufficient to have a class that represents an\noperation (the right-hand side), and that by definition is the same as the variable (left-hand side) that it defines:\nclass Operation(Value):\n    def __init__(self, name: str, args: list[Value]):\n        self.name = name\n        self.args = args\n\n    def __repr__(self):\n        return f\"Operation({self.name}, {self.args})\"\n\n    def arg(self, index: int):\n        return self.args[index]\n\nNow we can instantiate these two classes to represent the example sequence of\noperations above:\ndef test_construct_example():\n    # first we need something to represent\n    # \"a\" and \"b\". In our limited view, we don't\n    # know where they come from, so we will define\n    # them with a pseudo-operation called \"getarg\"\n    # which takes a number n as an argument and\n    # returns the n-th input argument. The proper\n    # SSA way to do this would be phi-nodes.\n\n    a = Operation(\"getarg\", [Constant(0)])\n    b = Operation(\"getarg\", [Constant(1)])\n    # var1 = add(b, 17)\n    var1 = Operation(\"add\", [b, Constant(17)])\n    # var2 = mul(a, var1)\n    var2 = Operation(\"mul\", [a, var1])\n    # var3 = add(b, 17)\n    var3 = Operation(\"add\", [b, Constant(17)])\n    # var4 = add(var2, var3)\n    var4 = Operation(\"add\", [var2, var3])\n\n    sequence = [a, b, var1, var2, var3, var4]\n    # nothing to test really, it shouldn't crash\n\nUsually, complicated programs are represented as a control flow graph in a\ncompiler, which represents all the possible paths that control can take while\nexecuting the program. Every node in the control flow graph is a basic\nblock. A basic block is a linear sequence of operations with no control flow\ninside of it.\nWhen optimizing a program, a compiler usually looks at the whole control flow\ngraph of a function. However, that is still too complicated! So let's\nsimplify further and look at only at optimizations we can do when looking at\na single basic block and its sequence of instructions (they are called local\noptimizations).\nLet's define a class representing basic blocks and let's also add some\nconvenience functions for constructing sequences of operations, because the\ncode in test_construct_example is a bit annoying.\nclass Block(list):\n    def opbuilder(opname):\n        def wraparg(arg):\n            if not isinstance(arg, Value):\n                arg = Constant(arg)\n            return arg\n        def build(self, *args):\n            # construct an Operation, wrap the\n            # arguments in Constants if necessary\n            op = Operation(opname,\n                [wraparg(arg) for arg in args])\n            # add it to self, the basic block\n            self.append(op)\n            return op\n        return build\n\n    # a bunch of operations we support\n    add = opbuilder(\"add\")\n    mul = opbuilder(\"mul\")\n    getarg = opbuilder(\"getarg\")\n    dummy = opbuilder(\"dummy\")\n    lshift = opbuilder(\"lshift\")\n\ndef test_convencience_block_construction():\n    bb = Block()\n    # a again with getarg, the following line\n    # defines the Operation instance and\n    # immediately adds it to the basic block bb\n    a = bb.getarg(0)\n    assert len(bb) == 1\n    assert bb[0].name == \"getarg\"\n\n    # it's a Constant\n    assert bb[0].args[0].value == 0\n\n    # b with getarg\n    b = bb.getarg(1)\n    # var1 = add(b, 17)\n    var1 = bb.add(b, 17)\n    # var2 = mul(a, var1)\n    var2 = bb.mul(a, var1)\n    # var3 = add(b, 17)\n    var3 = bb.add(b, 17)\n    # var4 = add(var2, var3)\n    var4 = bb.add(var2, var3)\n    assert len(bb) == 6\n\nThat's a good bit of infrastructure to make the tests easy to write. One\nthing we are lacking though is a way to print the basic blocks into a nicely\nreadable textual representation. Because in the current form, the repr of a\nBlock is very annoying, the output of pretty-printing bb in the test above\nlooks like this:\n[Operation('getarg', [Constant(0)]),\n Operation('getarg', [Constant(1)]),\n Operation('add',\n           [Operation('getarg',\n                      [Constant(1)]),\n                 Constant(17)]),\n Operation('mul',\n           [Operation('getarg',\n                      [Constant(0)]),\n                 Operation('add',\n                           [Operation('getarg',\n                                      [Constant(1)]),\n                            Constant(17)])]),\n Operation('add',\n           [Operation('getarg',\n                      [Constant(1)]),\n            Constant(17)]),\n Operation('add',\n           [Operation('mul',\n                       [Operation('getarg',\n                                  [Constant(0)]),\n                             Operation('add',\n                                       [Operation('getarg',\n                                                  [Constant(1)]),\n                                        Constant(17)])]),\n                 Operation('add',\n                           [Operation('getarg',\n                                           [Constant(1)]),\n                                 Constant(17)])])]\n\nIt's impossible to see what is going on here, because the Operations in the\nbasic block appear several times, once as elements of the list but then also as\narguments to operations further down in the list. So we need some code that\nturns things back into a readable textual representation, so we have a chance\nto debug.\ndef bb_to_str(bb: Block, varprefix: str = \"var\"):\n    # the implementation is not too important,\n    # look at the test below to see what the\n    # result looks like\n\n    def arg_to_str(arg: Value):\n        if isinstance(arg, Constant):\n            return str(arg.value)\n        else:\n            # the key must exist, otherwise it's\n            # not a valid SSA basic block:\n            # the variable must be defined before\n            # its first use\n            return varnames[arg]\n\n    varnames = {}\n    res = []\n    for index, op in enumerate(bb):\n        # give the operation a name used while\n        # printing:\n        var = f\"{varprefix}{index}\"\n        varnames[op] = var\n        arguments = \", \".join(\n            arg_to_str(op.arg(i))\n                for i in range(len(op.args))\n        )\n        strop = f\"{var} = {op.name}({arguments})\"\n        res.append(strop)\n    return \"\\n\".join(res)\n\ndef test_basicblock_to_str():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.add(5, 4)\n    var2 = bb.add(var1, var0)\n\n    assert bb_to_str(bb) == \"\"\"\\\nvar0 = getarg(0)\nvar1 = add(5, 4)\nvar2 = add(var1, var0)\"\"\"\n\n    # with a different prefix for the invented\n    # variable names:\n    assert bb_to_str(bb, \"x\") == \"\"\"\\\nx0 = getarg(0)\nx1 = add(5, 4)\nx2 = add(x1, x0)\"\"\"\n\n    # and our running example:\n    bb = Block()\n    a = bb.getarg(0)\n    b = bb.getarg(1)\n    var1 = bb.add(b, 17)\n    var2 = bb.mul(a, var1)\n    var3 = bb.add(b, 17)\n    var4 = bb.add(var2, var3)\n\n    assert bb_to_str(bb, \"v\") == \"\"\"\\\nv0 = getarg(0)\nv1 = getarg(1)\nv2 = add(v1, 17)\nv3 = mul(v0, v2)\nv4 = add(v1, 17)\nv5 = add(v3, v4)\"\"\"\n    # Note the re-numbering of the variables! We\n    # don't attach names to Operations at all, so\n    # the printing will just number them in\n    # sequence, can sometimes be a source of\n    # confusion.\n\nThis is much better. Now we're done with the basic infrastructure, we can\ndefine sequences of operations and print them in a readable way. Next we need a\ncentral data structure that is used when actually optimizing basic blocks.\n\n\nStoring Equivalences between Operations Using a Union-Find Data Structure\nWhen optimizing a sequence of operations, we want to make it less costly to\nexecute. For that we typically want to remove operations (and sometimes\nreplace operations with less expensive ones). We can remove operations if\nthey do redundant computation, like case of the duplicate add(v1, 17) in\nthe example. So what we want to do is to turn the running input sequence:\nv0 = getarg(0)\nv1 = getarg(1)\nv2 = add(v1, 17)\nv3 = mul(v0, v2)\nv4 = add(v1, 17)\nv5 = add(v3, v4)\nInto the following optimized output sequence:\noptvar0 = getarg(0)\noptvar1 = getarg(1)\noptvar2 = add(optvar1, 17)\noptvar3 = mul(optvar0, optvar2)\noptvar4 = add(optvar3, optvar2)\nWe left out the second add (which defines v4), and then replaced the\nusage of v4 with v2 in the final operation that defines v5.\nWhat we effectively did was discover that v2 and v4 are equivalent and then\nreplaced v4 with v2. In general, we might discover more such equivalences,\nand we need a data structure to store them. A good data structure to store\nthese equivalences is Union Find (also called Disjoint-set data structure),\nwhich stores a collection of disjoint sets. Disjoint means, that no operation\ncan appear in more than one set. The sets in our concrete case are the sets of\noperations that compute the same result.\nWhen we start out, every operation is in its own singleton set, with no other\nmember. As we discover more equivalences, we will unify sets into larger sets\nof operations that all compute the same result. So one operation the data\nstructure supports is union, to unify two sets, we'll call that\nmake_equal_to in the code below.\nThe other operation the data structure supports is find, which takes an\noperation and returns a \"representative\" of the set of all equivalent\noperations. Two operations are in the same set, if the representative that\nfind returns for them is the same.\nThe exact details of how the data structure works are only sort of important\n(even though it's very cool, I promise!). It's OK to skip over the\nimplementation. We will add the data structure right into our Value,\nConstant and Operation classes:\nclass Value:\n    def find(self):\n        raise NotImplementedError(\"abstract\")\n    def _set_forwarded(self, value):\n        raise NotImplementedError(\"abstract\")\n\n\nclass Operation(Value):\n    def __init__(self, name: str, args: list[Value]):\n        self.name = name\n        self.args = args\n        self.forwarded = None\n\n    def __repr__(self):\n        return (\n            f\"Operation({self.name},\"\n            f\"{self.args}, {self.forwarded})\"\n        )\n\n    def find(self) -> Value:\n        # returns the \"representative\" value of\n        # self, in the union-find sense\n        op = self\n        while isinstance(op, Operation):\n            # could do path compression here too\n            # but not essential\n            next = op.forwarded\n            if next is None:\n                return op\n            op = next\n        return op\n\n    def arg(self, index):\n        # change to above: return the\n        # representative of argument 'index'\n        return self.args[index].find()\n\n    def make_equal_to(self, value: Value):\n        # this is \"union\" in the union-find sense,\n        # but the direction is important! The\n        # representative of the union of Operations\n        # must be either a Constant or an operation\n        # that we know for sure is not optimized\n        # away.\n\n        self.find()._set_forwarded(value)\n\n    def _set_forwarded(self, value: Value):\n        self.forwarded = value\n\n\nclass Constant(Value):\n    def __init__(self, value: Any):\n        self.value = value\n\n    def __repr__(self):\n        return f\"Constant({self.value})\"\n\n    def find(self):\n        return self\n\n    def _set_forwarded(self, value: Value):\n        # if we found out that an Operation is\n        # equal to a constant, it's a compiler bug\n        # to find out that it's equal to another\n        # constant\n        assert isinstance(value, Constant) and \\\n            value.value == self.value\n\ndef test_union_find():\n    # construct three operation, and unify them\n    # step by step\n    bb = Block()\n    a1 = bb.dummy(1)\n    a2 = bb.dummy(2)\n    a3 = bb.dummy(3)\n\n    # at the beginning, every op is its own\n    # representative, that means every\n    # operation is in a singleton set\n    # {a1} {a2} {a3}\n    assert a1.find() is a1\n    assert a2.find() is a2\n    assert a3.find() is a3\n\n    # now we unify a2 and a1, then the sets are\n    # {a1, a2} {a3}\n    a2.make_equal_to(a1)\n    # they both return a1 as the representative\n    assert a1.find() is a1\n    assert a2.find() is a1\n    # a3 is still different\n    assert a3.find() is a3\n\n    # now they are all in the same set {a1, a2, a3}\n    a3.make_equal_to(a2)\n    assert a1.find() is a1\n    assert a2.find() is a1\n    assert a3.find() is a1\n\n    # now they are still all the same, and we\n    # also learned that they are the same as the\n    # constant 6\n    # the single remaining set then is\n    # {6, a1, a2, a3}\n    c = Constant(6)\n    a2.make_equal_to(c)\n    assert a1.find() is c\n    assert a2.find() is c\n    assert a3.find() is c\n\n    # union with the same constant again is fine\n    a2.make_equal_to(c)\n\n\n\nConstant Folding\nNow comes the first actual optimization, a simple constant folding pass. It\nwill remove operations where all the arguments are constants and replace them\nwith the constant result.\nEvery pass has the same structure: we go over all operations in the basic\nblock in order and decide for each operation whether it can be removed. For the\nconstant folding pass, we can remove all the operations with constant\narguments (but we'll implement only the add case here).\nI will show a buggy version of the constant folding pass first. It has a\nproblem that is related to why we need the union-find data structure. We will\nfix it a bit further down.\ndef constfold_buggy(bb: Block) -> Block:\n    opt_bb = Block()\n\n    for op in bb:\n        # basic idea: go over the list and do\n        # constant folding of add where possible\n        if op.name == \"add\":\n            arg0 = op.args[0]\n            arg1 = op.args[1]\n            if isinstance(arg0, Constant) and \\\n                    isinstance(arg1, Constant):\n                # can constant-fold! that means we\n                # learned a new equality, namely\n                # that op is equal to a specific\n                # constant\n                value = arg0.value + arg1.value\n                op.make_equal_to(Constant(value))\n                # don't need to have the operation\n                # in the optimized basic block\n                continue\n        # otherwise the operation is not\n        # constant-foldable and we put into the\n        # output list\n        opt_bb.append(op)\n    return opt_bb\n\n\ndef test_constfold_simple():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.add(5, 4)\n    var2 = bb.add(var1, var0)\n\n    opt_bb = constfold_buggy(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = add(9, optvar0)\"\"\"\n\n@pytest.mark.xfail\ndef test_constfold_buggy_limitation():\n    # this test fails! it shows the problem with\n    # the above simple constfold_buggy pass\n\n    bb = Block()\n    var0 = bb.getarg(0)\n    # this is folded\n    var1 = bb.add(5, 4)\n    # we want this folded too, but it doesn't work\n    var2 = bb.add(var1, 10)\n    var3 = bb.add(var2, var0)\n\n    opt_bb = constfold_buggy(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = add(19, optvar0)\"\"\"\n\nWhy does the test fail? The opt_bb printed output looks like this:\noptvar0 = getarg(0)\noptvar1 = add(9, 10)\noptvar2 = add(optvar1, optvar0)\nThe problem is that when we optimize the second addition in constfold_buggy,\nthe argument of that operation is an Operation not a Constant, so\nconstant-folding is not applied to the second add. However, we have already\nlearned that the argument var1 to the operation var2 is equal to\nConstant(9). This information is stored in the union-find data structure.\nSo what we are missing are suitable find calls in the constant folding pass, to\nmake use of the previously learned equalities.\nHere's the fixed version:\ndef constfold(bb: Block) -> Block:\n    opt_bb = Block()\n\n    for op in bb:\n        # basic idea: go over the list and do\n        # constant folding of add where possible\n        if op.name == \"add\":\n            # >>> changed\n            arg0 = op.arg(0) # uses .find()\n            arg1 = op.arg(1) # uses .find()\n            # <<< end changes\n            if isinstance(arg0, Constant) and \\\n                    isinstance(arg1, Constant):\n                # can constant-fold! that means we\n                # learned a new equality, namely\n                # that op is equal to a specific\n                # constant\n                value = arg0.value + arg1.value\n                op.make_equal_to(Constant(value))\n                # don't need to have the operation\n                # in the optimized basic block\n                continue\n        # otherwise the operation is not\n        # constant-foldable and we put into the\n        # output list\n        opt_bb.append(op)\n    return opt_bb\n\n\ndef test_constfold_two_ops():\n    # now it works!\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.add(5, 4)\n    var2 = bb.add(var1, 10)\n    var3 = bb.add(var2, var0)\n    opt_bb = constfold(bb)\n\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = add(19, optvar0)\"\"\"\n\n\n\nCommon Subexpression Elimination\nThe constfold pass only discovers equalities between Operations and\nConstants. Let's do a second pass that also discovers equalities between\nOperations and other Operations.\nA simple optimization that does that has this property common subexpression\nelimination (CSE), which will finally optimize away the problem in the\nintroductory example code that we had above.\ndef cse(bb: Block) -> Block:\n    # structure is the same, loop over the input,\n    # add some but not all operations to the\n    # output\n\n    opt_bb = Block()\n\n    for op in bb:\n        # only do CSE for add here, but it\n        # generalizes\n        if op.name == \"add\":\n            arg0 = op.arg(0)\n            arg1 = op.arg(1)\n            # Check whether we have emitted the\n            # same operation already\n            prev_op = find_prev_add_op(\n                arg0, arg1, opt_bb)\n            if prev_op is not None:\n                # if yes, we can optimize op away\n                # and replace it with the earlier\n                # result, which is an Operation\n                # that was already emitted to\n                # opt_bb\n                op.make_equal_to(prev_op)\n                continue\n        opt_bb.append(op)\n    return opt_bb\n\n\ndef eq_value(val0, val1):\n    if isinstance(val0, Constant) and \\\n            isinstance(val1, Constant):\n        # constants compare by their value\n        return val0.value == val1.value\n    # everything else by identity\n    return val0 is val1\n\n\ndef find_prev_add_op(arg0: Value, arg1: Value,\n        opt_bb: Block) -> Optional[Operation]:\n    # Really naive and quadratic implementation.\n    # What we do is walk over the already emitted\n    # operations and see whether we emitted an add\n    # with the current arguments already. A real\n    # implementation might use a hashmap of some\n    # kind, or at least only look at a limited\n    # window of instructions.\n    for opt_op in opt_bb:\n        if opt_op.name != \"add\":\n            continue\n        # It's important to call arg here,\n        # for the same reason why we\n        # needed it in constfold: we need to\n        # make sure .find() is called\n        if eq_value(arg0, opt_op.arg(0)) and \\\n                eq_value(arg1, opt_op.arg(1)):\n            return opt_op\n    return None\n\n\ndef test_cse():\n    bb = Block()\n    a = bb.getarg(0)\n    b = bb.getarg(1)\n    var1 = bb.add(b, 17)\n    var2 = bb.mul(a, var1)\n    var3 = bb.add(b, 17)\n    var4 = bb.add(var2, var3)\n\n    opt_bb = cse(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = getarg(1)\noptvar2 = add(optvar1, 17)\noptvar3 = mul(optvar0, optvar2)\noptvar4 = add(optvar3, optvar2)\"\"\"\n\n\n\nStrength Reduction\nNow we have one pass that replaces Operations with Constants and one that\nreplaces Operations with previously existing Operations. Let's now do one\nfinal pass that replaces Operations by newly invented Operations, a simple\nstrength reduction. This one will be simple.\ndef strength_reduce(bb: Block) -> Block:\n    opt_bb = Block()\n    for op in bb:\n        if op.name == \"add\":\n            arg0 = op.arg(0)\n            arg1 = op.arg(1)\n            if arg0 is arg1:\n                # x + x turns into x << 1\n                newop = opt_bb.lshift(arg0, 1)\n                op.make_equal_to(newop)\n                continue\n        opt_bb.append(op)\n    return opt_bb\n\ndef test_strength_reduce():\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.add(var0, var0)\n\n    opt_bb = strength_reduce(bb)\n\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = lshift(optvar0, 1)\"\"\"\n\n\n\nPutting Things Together\nLet's combine the passes into one single pass, so that we are going over all\nthe operations only exactly once, instead of having to look at every operation\nonce for all the different passes.\ndef optimize(bb: Block) -> Block:\n    opt_bb = Block()\n\n    for op in bb:\n        if op.name == \"add\":\n            arg0 = op.arg(0)\n            arg1 = op.arg(1)\n\n            # constant folding\n            if isinstance(arg0, Constant) and \\\n                    isinstance(arg1, Constant):\n                value = arg0.value + arg1.value\n                op.make_equal_to(Constant(value))\n                continue\n\n            # cse\n            prev_op = find_prev_add_op(\n                arg0, arg1, opt_bb)\n            if prev_op is not None:\n                op.make_equal_to(prev_op)\n                continue\n\n            # strength reduce:\n            # x + x turns into x << 1\n            if arg0 is arg1:\n                newop = opt_bb.lshift(arg0, 1)\n                op.make_equal_to(newop)\n                continue\n\n            # and while we are at it, let's do some\n            # arithmetic simplification:\n            # a + 0 => a\n            if eq_value(arg0, Constant(0)):\n                op.make_equal_to(arg1)\n                continue\n            if eq_value(arg1, Constant(0)):\n                op.make_equal_to(arg0)\n                continue\n        opt_bb.append(op)\n    return opt_bb\n\n\ndef test_single_pass():\n    bb = Block()\n    # constant folding\n    var0 = bb.getarg(0)\n    var1 = bb.add(5, 4)\n    var2 = bb.add(var1, 10)\n    var3 = bb.add(var2, var0)\n\n    opt_bb = optimize(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = add(19, optvar0)\"\"\"\n\n    # cse + strength reduction\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.getarg(1)\n    var2 = bb.add(var0, var1)\n    var3 = bb.add(var0, var1) # the same as var3\n    var4 = bb.add(var2, 2)\n    var5 = bb.add(var3, 2) # the same as var4\n    var6 = bb.add(var4, var5)\n\n    opt_bb = optimize(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = getarg(1)\noptvar2 = add(optvar0, optvar1)\noptvar3 = add(optvar2, 2)\noptvar4 = lshift(optvar3, 1)\"\"\"\n\n    # removing + 0\n    bb = Block()\n    var0 = bb.getarg(0)\n    var1 = bb.add(16, -16)\n    var2 = bb.add(var0, var1)\n    var3 = bb.add(0, var2)\n    var4 = bb.add(var2, var3)\n\n    opt_bb = optimize(bb)\n    assert bb_to_str(opt_bb, \"optvar\") == \"\"\"\\\noptvar0 = getarg(0)\noptvar1 = lshift(optvar0, 1)\"\"\"\n\n\n\nConclusion\nThat's it for now. Why is this architecture cool? From a software engineering\npoint of view, sticking everything into a single function like in optimize\nabove is obviously not great, and if you wanted to do this for real you would\ntry to split the cases into different functions that are individually\ndigestible, or even use a DSL that makes the pattern matching much more\nreadable. But the advantage of the architecture is that it's quite efficient,\nit makes it possible to pack a lot of good optimizations into a single pass\nover a basic block.\nOf course this works even better if you are in a tracing context, where\neverything is put into a trace, which is basically one incredibly long basic\nblock. In a JIT context it's also quite important that the\noptimizer itself runs quickly.\nVarious other optimizations are possible in this model. There is a\nfollow-up post that show how to implement what is arguably PyPy's most\nimportant optimization.\n\n\nSome Further Pointers\nThis post is only a short introduction and is taking some shortcuts, I wanted to\nalso give some (non-exhaustive) pointers to more general literature about the\ntouched topics.\nThe approach to CSE described here is usually can be seen as value\nnumbering, it's normally really implemented with a hashmap though. Here's a\npaper that describes various styles of implementing that, even beyond a\nsingle basic block. The paper also partly takes the perspective of discovering\nequivalence classes of operations that compute the same result.\nA technique that leans even more fully into finding equivalences between\noperations is using e-graphs and then applying equality saturation (this is\nsignificantly more advanced that what I described here though). A cool modern\nproject that applies this technique is egg.\nIf you squint a bit, you can generally view a constant folding pass as a very\nsimple form of Partial Evaluation: every operation that has constant\narguments is constant-folded away, and the remaining ones are \"residualized\",\ni.e. put into the output program. This point of view is not super important for\nthe current post, but will become important in the next one.\nAcknowledgements: Thanks to Thorsten Ball for getting me to write\nthis and for his enthusiastic feedback. I also got great feedback from Max\nBernstein, Matti Picus and Per Vognsen. A conversation with Peng Wu that\nwe had many many years ago and that stuck with me made me keep thinking about\nvarious ways to view compiler optimizations.",
      "tags": "toy-optimizer",
      "url": "https://www.pypy.org/posts/2022/07/toy-optimizer.html"
    },
    {
      "title": "How is PyPy Tested?",
      "text": "How is PyPy Tested?\nIn this post I want to give an overview of how the PyPy project does and thinks\nabout testing. PyPy takes testing quite seriously and has done some from the\nstart of the project. Here I want to present the different styles of\ntests that PyPy has, when we use them and how I think about them.\n\nBackground\nTo make the blog post self-contained, I am going to start with a small overview\nabout PyPy's architecture. If you already know what PyPy is and how it works,\nyou can skip this section.\nPyPy means \"Python in Python\". It is an alternative implementation of the Python\nlanguage. Usually, when we speak of \"Python\", we can mean two different things.\nOn the one hand it means \"Python as an abstract programming language\". On the\nother hand, the main implementation of that language is also often called\n\"Python\". To more clearly distinguish the two, the implementation is often also\ncalled \"CPython\", because it is an interpreter implemented in C code.\nNow we can make the statement \"PyPy is Python in Python\" more precise: PyPy is\nan interpreter for Python 3.9, implemented in RPython. RPython (\"Restricted\nPython\") is a subset of Python 2, which is statically typed (using type\ninference, not type annotations) and can be compiled\nto C code. That means we can take our Python 3.9 interpreter, and compile it\ninto a C binary that can run Python 3.9 code. The final binary behaves pretty\nsimilarly to CPython.\nThe main thing that makes PyPy interesting is that during the translation of our\ninterpreter to C, a number of components are automatically inserted into the\nfinal binary. One component is a reasonably good garbage collector.\nThe more exciting component that is inserted into the binary is a just-in-time\ncompiler. The insertion of this component is not fully automatic, instead it is\nguided by a small number of annotations in the source code of the interpreter.\nThe effect of inserting this JIT compiler into the binary is that the resulting\nbinary can run Python code significantly faster than CPython, in many cases.\nHow this works is not important for the rest of the post, if you want to see an\nexample of concretely doing that to a small interpreter you can look at this\nvideo.\n\n\nPyPy Testing History\nA few historical notes on the PyPy project and its relationship to testing: The\nPyPy project was started in 2004. At the time when the project was started,\nExtreme Programming and Agile Software Development were up and coming. On the\nmethodology side, PyPy was heavily influenced by these, and started using\nTest-Driven Development and pair programming right from the start.\nAlso technologically, PyPy has been influential on testing in the Python world.\nOriginally, PyPy had used the unittest testing framework, but pretty soon\nthe developers got frustrated with it. Holger Krekel, one of the original\ndevelopers who started PyPy, started the pytest testing framework soon\nafterwards.\n\n\nInterpreter-Level Tests\nSo, how are tests for PyPy written, concretely? The tests for the interpreter\nare split into two different kinds, which we call \"interpreter level tests\" and\n\"application level tests\". The former are tests that can be used to test the\nobjects and functions that are used in the implementation of the Python\ninterpreter. Since the interpreter is written in Python 2, those tests are also\nwritten in Python 2, using pytest. They tend to be more on the unit test side of\nthings. They are in files with the pattern test_*.py.\nHere is an example that tests the implementation of integers (very slightly\nsimplified):\nclass TestW_IntObject:\n    ...\n\n    def test_hash(self):\n        w_x = W_IntObject(42)\n        w_result = w_x.descr_hash(self.space)\n        assert isinstance(w_result, W_IntObject)\n        assert w_result.intval == 42\n\nThis test checks that if you take an object that represents integers in the\nPython language (using the class W_IntObject, a \"wrapped integer object\")\nwith the value 42, computing the hash of that object returns another instance of\nthe same class, also with the value 42.\nThese tests can be run on top of any Python 2 implementation, either CPython or\nPyPy. We can then test and debug the internals of the PyPy interpreter using\nfamiliar tools like indeed pytest and the Python debuggers. They can be run,\nbecause all the involved code like the tests and the class W_IntObject are\njust completely regular Python 2 classes that behave in the regular way when\nrun on top of a Python interpreter.\nIn CPython, these tests don't really have an equivalent. They would correspond\nto tests that are written in C and that can test the logic of all the C\nfunctions of CPython that execute certain functionality, accessing the internals\nof C structs in the process. \u00b9\n\n\nApplication-Level Tests\nThere is also a second class of tests for the interpreter. Those are tests that\ndon't run on the level of the implementation. Instead, they are executed by\nthe PyPy Python interpreter, thus running on the level of the applications run\nby PyPy. Since the interpreter is running Python 3, the tests are also written\nin Python 3. They are stored in files with the pattern apptest_*.py and\nlook like \"regular\" Python 3 tests. \u00b2\nHere's an example of how you could write a test equivalent to the one above:\ndef test_hash():\n    assert hash(42) == 42\n\nThis style of test looks more \"natural\" and is the preferred one in cases where\nthe test does not need to access the internals of the logic or the objects of\nthe interpreter.\nApplication level tests can be run in two different ways. On the one hand, we\ncan simply run them on CPython 3. This is very useful! Since we want PyPy to\nbehave like CPython, running the tests that we write on CPython is useful to\nmake sure that the tests themselves aren't wrong.\nOn the other hand, the main way to run these tests is on top of PyPy, itself\nrunning on top of a Python 2 implementation. This makes it possible to run the\ntest without first bootstrapping PyPy to C. Since bootstrapping to C is a\nrelatively slow operation (can take up to an hour) it is crucially important to\nbe able to run tests without bootstrapping first. It also again makes it\npossible to debug crashes in the interpreter using the regular Python 2\ndebugger. Of course running tests in this way is unfortunately itself not super\nfast, given that they run on a stack of two different interpreters.\nApplication-level tests correspond quite closely to CPython's tests suite (which\nis using the unittest framework). Of course in CPython it is not possible to run\nthe test suite without building the CPython binary using a C compiler. \u00b3\nSo when do we write application-level tests, and when interpreter-level tests?\nInterpreter-level tests are necessary to test internal data structures that\ntouch data and logic that is not directly exposed to the Python language. If\nthat is not necessary, we try to write application-level tests. App-level tests\nare however by their nature always more on the integration test side of things.\nTo be able to run the test_hash function above, many parts of PyPy need to\nwork correctly, the parser, the bytecode compiler, the bytecode interpreter, the\nhash builtin, calling the __hash__ special method, etc, etc.\nThis observation is also true for CPython! One could argue that CPython has no\nunit tests at all, because in order to be able to even run the tests, most of\nPython needs to be in working order already, so all the tests are really\nimplicitly integration tests.\n\n\nThe CPython Test Suite\nWe also use the CPython Test suite as a final check to see whether our\ninterpreter correctly implements all the features of the Python language. In\nthat sense it acts as some kind of compliance test suite that checks whether we\nimplement the language correctly. The test suite is not perfect for this.\nSince it is written for CPython's purposes during its development, a\nlot of the tests check really specific CPython implementation details. Examples\nfor these are tests that check that __del__ is called immediately after\nobjects go out of scope (which only happens if you use reference counting as a\ngarbage collection strategy, PyPy uses a different approach to garbage\ncollection). Other examples are checking\nfor exception error messages very explicitly. However, the CPython test suite\nhas gotten a lot better in these regards over time, by adding\nsupport.gc_collect() calls to fix the former problem, and by marking some\nvery specific tests with the @impl_detail decorator. Thanks to all the\nCPython developers who have worked on this!\nIn the process of re-implementing CPython's functionality and running CPython's\ntests suite, PyPy can often also be a good way to find bugs in CPython. While we\nthink about the corner cases of some Python feature we occasionally find\nsituations where CPython didn't get everything completely correct either, which\nwe then report back.\n\n\nTesting for Performance Regressions\nAll the tests we described so far are checking behaviour. But one of PyPy's\nimportant goals is to be a fast implementation not \"just\" a correct one. Some\naspects of performance can be tested by regular unit tests, either application-\nor interpreter-level. In order to check whether some performance shortcut is\ntaken in the interpreter, we sometimes can write tests that monkeypatch the slow\ndefault implementation to always error. Then, if the fast path is taken\nproperly, that slow default implementation is never reached.\nBut we also have additional tests that test the correct interaction with the JIT\nexplicitly. For that, we have a special style of test that checks that the JIT\nwill produce the correct machine code for a small snippet of Python code. To\nmake this kind of test somewhat more robust, we don't check the machine code\ndirectly, but instead the architecture independent intermediate\nrepresentation that the JIT uses to produce machine code from.\nAs an example, here is a small test that loading the attribute of a constant\nglobal instance can be completely constant folded away:\ndef test_load_attr(self):\n    src = '''\n        class A(object):\n            pass\n        a = A()\n        a.x = 1\n        def main(n):\n            i = 0\n            while i < n:\n                i = i + a.x\n            return i\n    '''\n    log = self.run(src, [1000])\n    assert log.result == 1000\n    loop, = log.loops_by_filename(self.filepath)\n    assert loop.match(\"\"\"\n        i9 = int_lt(i5, i6)\n        guard_true(i9, descr=...)\n        guard_not_invalidated(descr=...)\n        i10 = int_add(i5, 1)\n        --TICK--\n        jump(..., descr=...)\n    \"\"\")\n\nThe string passed to the loop.match function is a string representation of\nthe intermediate representation code that is generated for the while loop in\nthe main function given in the source. The important part of that\nintermediate representation is that the i = i + a.x addition is optimized\ninto an int_add(x, 1) operation. The second argument for the addition is the\nconstant 1, because the JIT noted that the global a is a constant, and\nthe attribute x of that instance is always 1. The test thus checks that\nthis optimization still works.\nThose tests are again more on the unit test side of things (and can thus\nunfortunately be a bit brittle sometimes and break). The integration test\nequivalent for performance is the PyPy Speed Center which tracks the\nperformance of micro- and macro-benchmarks over time and lets us see when big\nperformance regressions are happening. The speed center is not really an\nautomatic test and does not produce pass/fail outcomes. Instead, it requires\nhuman judgement and intervention in order to interpret the performance changes.\nHaving a real pass/fail mechanism is something that would be great to have\nbut is probably quite tricky in practice.\n\n\nConclusion\nThis concludes my overview of some of the different styles of tests that we use\nto develop the PyPy Python interpreter.\nThere is a whole other set of tests for the development of the RPython language,\nthe garbage collectors it provides as well as the code that does the automatic\nJIT insertion, maybe I'll cover these in a future post.\n\nFootnotes\n\u00b9 CPython has the _testcapimodule.c and related modules, that are used to\nunit-test the C-API. However, these are still driven from Python tests using\nthe unittest framework and wouldn't run without the Python interpreter\nalready working.\n\u00b2 There is also a deprecated different way to write these tests, by putting\nthem in the test_*.py files that interpreter level tests are using and\nthen having a test class with the pattern class AppTest*. We haven't\nconverted all of them to the new style yet, even though the old style is\nquite weird: since the test_*.py files are themselves parsed by\nPython 2, the tests methods in AppTest* classes need to be written in the\nsubset of Python 3 syntax that is also valid Python 2 syntax, leading to a lot\nof confusion.\n\u00b3 Nit-picky side-note: C interpreters are a thing! But not that\nwidely used in practice, or only in very specific situations.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2022/04/how-is-pypy-tested.html"
    },
    {
      "title": "PyPy v7.3.9 security release",
      "text": "PyPy v7.3.9 security release\nThe PyPy team is proud to release version 7.3.9 of PyPy. This is a security\nrelease to match the recent CPython release and updates the portable pypy\ntarballs with bzip2 1.0.8, openssl1.1.1n, and libexpat 2.4.7. Along\nthe way this release fixes some issues discovered after the 7.3.8 release and\nupdates sqlite3 to 3.38.2. It includes:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.7,  which is an interpreter supporting the syntax and the features of\nPython 3.7, including the stdlib for CPython 3.7.13. This will be the last\nrelease of PyPy3.7.\nPyPy3.8, which is an interpreter supporting the syntax and the features of\nPython 3.8, including the stdlib for CPython 3.8.13.\nPyPy3.9, which is an interpreter supporting the syntax and the features of\nPython 3.9, including the stdlib for CPython 3.9.12. We relate to this as\n\"beta\" quality. We welcome testing of this version, if you discover\nincompatibilities, please report them so we can gain confidence in the version.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. Highlights of the release, since the release of 7.3.8 in February 2022,\ninclude:\n\n\nFixed some failing stdlib tests on PyPy3.9\nUpdate the bundled libexpat to 2.4.6 and sqlite3 to 3.38.2\n\n\nWe recommend updating. You can find links to download the v7.3.9 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better. Since the\n7.3.7 release, we have accepted contributions from 6 new contributors,\nthanks for pitching in, and welcome to the project!\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, 3.8 and\n3.9. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 64 bits, OpenBSD, FreeBSD)\n64-bit ARM machines running Linux. A shoutout to Huawei for sponsoring\nthe VM running the tests.\ns390x running Linux\nbig- and little-endian variants of PPC64 running Linux,\n\n\nPyPy support Windows 32-bit, PPC64 big- and little-endian, and ARM 32 bit, but\ndoes not release binaries. Please reach out to us if you wish to sponsor\nreleases for those platforms.\n\n\nKnown Issues with PyPy3.9\n\nWe slightly modified the concurrent future's ProcessExcecutorPool to\nstart all the worker processes when the first task is received (like on\nPython3.8) to avoid an apparent race condition when using fork and\nthreads (issue 3650).\n\n\n\nWhat else is new?\nFor more information about the 7.3.9 release, see the full changelog.\nPlease update, and continue to help us make PyPy better.\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2022/03/pypy-v738-release.html"
    },
    {
      "title": "PyPy v7.3.8: release of python 2.7, 3.7, 3.8, and 3.9",
      "text": "PyPy v7.3.8: release of python 2.7, 3.7, 3.8, and 3.9-beta\nThe PyPy team is proud to release version 7.3.8 of PyPy. It has been only a few\nmonths since our last release, but we have some nice speedups and bugfixes we\nwish to share. The release includes four different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.7,  which is an interpreter supporting the syntax and the features of\nPython 3.7, including the stdlib for CPython 3.7.12. This will be the last\nrelease of PyPy3.7.\nPyPy3.8, which is an interpreter supporting the syntax and the features of\nPython 3.8, including the stdlib for CPython 3.8.12. This is our third\nrelease of this interpreter, and we are removing the \"beta\" tag.\nPyPy3.9, which is an interpreter supporting the syntax and the features of\nPython 3.9, including the stdlib for CPython 3.9.10. As this is our first\nrelease of this interpreter, we relate to this as \"beta\" quality. We\nwelcome testing of this version, if you discover incompatibilities, please\nreport them so we can gain confidence in the version.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. Highlights of the release, since the release of 7.3.7 in late October 2021,\ninclude:\n\n\nPyPy3.9 uses an RPython version of the PEG parser which brought with it a\ncleanup of the lexer and parser in general\nFixed a regression in PyPy3.8 when JITting empty list comprehensions\nTweaked some issues around changing the file layout after packaging to make\nthe on-disk layout of PyPy3.8 more compatible with CPython. This requires\nsetuptools>=58.1.0\nRPython now allows the target executable to have a . in its name, so\nPyPy3.9 will produce a pypy3.9-c and libpypy3.9-c.so. Changing the\nname of the shared object to be version-specific (it used to be\nlibpypy3-c.so) will allow it to live alongside other versions.\nBuilding PyPy3.9+ accepts a --platlibdir argument like CPython.\nImprovement in ssl's use of CFFI buffers to speed up recv and recvinto\nUpdate the packaged OpenSSL to 1.1.1m\n\n\nWe recommend updating. You can find links to download the v7.3.8 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better. Since the\nprevious release, we have accepted contributions from 6 new contributors,\nthanks for pitching in, and welcome to the project!\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a HPy / CFFI / cppyy version of your library that would be performant\non PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, 3.8 and\n3.9. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 64 bits, OpenBSD, FreeBSD)\n64-bit ARM machines running Linux. A shoutout to Huawei for sponsoring\nthe VM running the tests.\ns390x running Linux\nbig- and little-endian variants of PPC64 running Linux,\n\n\nPyPy support Windows 32-bit, PPC64 big- and little-endian, and ARM 32 bit, but\ndoes not release binaries. Please reach out to us if you wish to sponsor\nreleases for those platforms.\n\n\nKnown Issues with PyPy3.9\n\nThere is still a known speed regression around **kwargs handling\nWe slightly modified the concurrent future's ProcessExcecutorPool to\nstart all the worker processes when the first task is received (like on\nPython3.8) to avoid an apparent race condition when using fork and\nthreads (issue 3650).\n\n\n\nWhat else is new?\nFor more information about the 7.3.8 release, see the full changelog.\nPlease update, and continue to help us make PyPy better.\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2022/02/pypy-v738-release.html"
    },
    {
      "title": "Natural Language Processing for Icelandic with PyPy: A Case Study",
      "text": "Natural Language Processing for Icelandic with PyPy: A Case Study\nIcelandic is one\nof the smallest languages of the world, with about 370.000 speakers. It\nis a language in the Germanic family, most similar to Norwegian, Danish\nand Swedish, but closer to the original Old\nNorse spoken throughout\nScandinavia until about the 14th century CE.\nAs with other small languages, there are worries that the language may\nnot\nsurvive\nin a digital world, where all kinds of fancy applications are developed\nfirst - and perhaps only - for the major languages. Voice assistants,\nchatbots, spelling and grammar checking utilities, machine translation,\netc., are increasingly becoming staples of our personal and professional\nlives, but if they don\u2019t exist for Icelandic, Icelanders will gravitate\ntowards English or other languages where such tools are readily\navailable.\nIceland is a technology-savvy country, with world-leading adoption\nrates of the\nInternet,\nPCs and smart devices, and a thriving software industry. So the\ngovernment figured that it would be worthwhile to fund a 5-year\nplan to build natural\nlanguage processing (NLP) resources and other infrastructure for the\nIcelandic language. The project focuses on collecting data and\ndeveloping open source software for a range of core applications, such\nas tokenization, vocabulary lookup, n-gram statistics, part-of-speech\ntagging, named entity recognition, spelling and grammar checking, neural\nlanguage models and speech processing.\n\nMy name is Vilhj\u00e1lmur \u00deorsteinsson, and I\u2019m the founder and CEO of a\nsoftware startup Mi\u00f0eind in Reykjav\u00edk,\nIceland, that employs 10 software engineers and linguists and focuses on\nNLP and AI for the Icelandic language. The company participates in the\ngovernment\u2019s language technology program, and has contributed\nsignificantly to the program\u2019s core tools (e.g., a tokenizer and a\nparser), spelling and grammar checking modules, and a neural machine\ntranslation stack.\nWhen it came to a choice of programming languages and development tools\nfor the government program, the requirements were for a major, well\nsupported, vendor-and-OS-agnostic FOSS platform with a large and diverse\ncommunity, including in the NLP space. The decision to select Python as\na foundational language for the project was a relatively easy one. That\nsaid, there was a bit of trepidation around the well known fact that\nCPython can be slow for inner-core tasks, such as tokenization and\nparsing, that can see heavy workloads in production.\nI first became aware of PyPy in early 2016 when I was developing a\ncrossword game Netskrafl in Python 2.7\nfor Google App Engine. I had a utility program that compressed a\ndictionary into a Directed Acyclic Word Graph and was taking 160\nseconds\u00a0 to run on CPython 2.7, so I tried PyPy and to my amazement saw\na 4x speedup (down to 38 seconds), with literally no effort besides\ndownloading the PyPy runtime.\nThis led me to select PyPy as the default Python interpreter for my\ncompany\u2019s Python development efforts as well as for our production\nwebsites and API servers, a role in which it remains to this day. We\nhave followed PyPy\u2019s upgrades along the way, being just about to migrate\nour minimally required language version from 3.6 to 3.7.\nIn NLP, speed and memory requirements can be quite important for\nsoftware usability. On the other hand, NLP logic and algorithms are\noften complex and challenging to program, so programmer productivity and\ncode clarity are also critical success factors. A pragmatic approach\nbalances these factors, avoids premature optimization and seeks a\ncareful compromise between maximal run-time efficiency and minimal\nprogramming and maintenance effort.\nTurning to our use cases, our Icelandic text\ntokenizer \"Tokenizer\" is fairly light,\nruns tight loops and performs a large number of small, repetitive\noperations. It runs very well on PyPy\u2019s JIT and has not required further\noptimization.\nOur Icelandic parser Greynir\n(known on PyPI as reynir) is,\nif I may say so myself, a piece of work. It parses natural language\ntext according to a\nhand-written context-free\ngrammar,\nusing an Earley-type\nalgorithm as enhanced\nby Scott and\nJohnstone.\nThe CFG contains almost 7,000 nonterminals and 6,000 terminals, and the\nparser handles ambiguity as well as left, right and middle recursion. It\nreturns a packed parse forest for each input sentence, which is then\npruned by a scoring heuristic down to a single best result tree.\nThis parser was originally coded in pure Python and turned out to be\nunusably slow when run on CPython - but usable on PyPy, where it was\n3-4x faster. However, when we started applying it to heavier production\nworkloads, it\u00a0 became apparent that it needed to be faster still. We\nthen proceeded to convert the innermost Earley parsing loop from Python\nto tight\nC++\nand to call it from PyPy via\nCFFI, with callbacks for\ntoken-terminal matching functions (\u201cbusiness logic\u201d) that remained on\nthe Python side. This made the parser much faster (on the order of 100x\nfaster than the original on CPython) and quick enough for our production\nuse cases. Even after moving much of the heavy processing to C++ and using CFFI, PyPy still gives a significant speed boost over CPython.\nConnecting C++ code with PyPy proved to be quite painless using CFFI,\nalthough we had to figure out a few magic incantations in our build\nmodule\nto make it compile smoothly during setup from source on Windows and\nMacOS in addition to Linux. Of course, we build binary PyPy and CPython\nwheels for the most common targets so most users don\u2019t have to worry\nabout setup requirements.\nWith the positive experience from the parser project, we proceeded to\ntake a similar approach for two other core NLP packages: our compressed\nvocabulary package BinPackage\n(known on PyPI as islenska) and our\ntrigrams database package Icegrams.\nThese packages both take large text input (3.1 million word forms with\ninflection data in the vocabulary case; 100 million tokens in the\ntrigrams case) and compress it into packed binary structures. These\nstructures are then memory-mapped at run-time using\nmmap and queried via\nPython functions with a lookup time in the microseconds range. The\nlow-level data structure navigation is done in\nC++,\ncalled from Python via CFFI. The ex-ante preparation, packing,\nbit-fiddling and data structure generation is fast enough with PyPy, so\nwe haven\u2019t seen a need to optimize that part further.\nTo showcase our tools, we host public (and open source) websites such as\ngreynir.is for our parsing, named entity\nrecognition and query stack and\nyfirlestur.is for our spell and grammar\nchecking stack. The server code on these sites is all Python running on\nPyPy using Flask,\nwrapped in gunicorn and hosted on\nnginx. The underlying database is\nPostgreSQL accessed via\nSQLAlchemy and\npsycopg2cffi. This setup\nhas served us well for 6 years and counting, being fast, reliable and\nhaving helpful and supporting communities.\nAs can be inferred from the above, we are avid fans of PyPy and\ncommensurately thankful for the great work by the PyPy team over the\nyears. PyPy has enabled us to use Python for a larger part of our\ntoolset than CPython alone would have supported, and its smooth\nintegration with C/C++ through CFFI has helped us attain a better\ntradeoff between performance and programmer productivity in our\nprojects. We wish for PyPy a great and bright future and also look\nforward to exciting related developments on the horizon, such as\nHPy.",
      "tags": "casestudy,guestpost",
      "url": "https://www.pypy.org/posts/2022/02/nlp-icelandic-case-study.html"
    },
    {
      "title": "Error Message Style Guides of Various Languages",
      "text": "Error Message Style Guides of Various Languages\nPyPy has been trying to produce good SyntaxErrors and other errors for\na long time. CPython has also made an enormous push to improve its\nSyntaxErrors in the last few releases. These improvements are great, but the process\nfeels somewhat arbitrary sometimes. To see what other languages are doing, I\nasked people on Twitter whether they know of error message style guides for\nother programming languages.\nWonderfully, people answered me with lots of helpful links (full list at the\nend of the post), thank you everybody! All those sources are very interesting\nand contain many great points, I recommend reading them directly! In this\npost, I'll try to summarize some common themes or topics that I thought were\nparticularly interesting.\n\nLanguage Use\nAlmost all guides stress the need for plain and simple English, as well as\nconciseness and clarity [Flix, Racket, Rust, Flow]. Flow suggests to put coding\neffort into making the grammar correct, for example in the case of plurals or\nto distinguish between \"a\" and \"an\".\nThe suggested tone should be friendly and neutral, the messages should not\nblame the Programmer [Flow]. Rust and Flix suggest to not use the term\n'illegal' and use something like 'invalid' instead.\nFlow suggests to avoid \"compiler speak\". For example terms like 'token' and\n'identifier' should be avoided and terms that are more familiar to programmers\nbe used (eg \"name\" is better). The Racket guide goes further and has a list of\nallowed technical terms and some prohibited terms.\n\n\nStructure\nSeveral guides (such as Flix and Flow) point out a 80/20 rule: 80% of the times an error message is\nread, the developer knows that message well and knows exactly what to do. For\nthis use case it's important that the message is short. On the other hand, 20%\nof the times this same message will have to be understood by a developer who\nhas never seen it before and is confused, and so the message needs to contain\nenough information\nto allow them to find out what is going on. So the error message needs to strike\na balance between brevity and clarity.\nThe Racket guide proposes to use the following general structure for errors:\n'State the constraint that was violated (\"expected a\"), followed by what was\nfound instead.'\nThe Rust guides says to avoid \"Did you mean?\" and questions in general, and\nwants the compiler to instead be explicit about why something was suggested. The\nexample the Rust guide gives is: 'Compare \"did you mean: Foo\" vs. \"there is a\nstruct with a similar name: Foo\".' Racket goes further and forbids\nsuggestions altogether because \"Students will follow well\u2010meaning\u2010but\u2010wrong\nadvice uncritically, if only because they have no reason to doubt the\nauthoritative voice of the tool.\"\n\n\nFormatting and Source Positions\nThe Rust guide suggests to put all identifiers into backticks (like in\nMarkdown), Flow formats the error messages using full Markdown.\nThe Clang, Flow and Rust guides point out the importance of using precise\nsource code spans to point to errors, which is especially important if the\ncompiler information is used in the context of an IDE to show a red squiggly\nunderline or some other highlighting. The spans should be as small as possible to point out the source of\nthe error [Flow].\n\n\nConclusion\nI am quite impressed how advanced and well-thought out the approaches are. I wonder whether it would makes sense for\nPython to adopt a (probably minimal, to get started) subset of these ideas as guidelines for its own errors.\n\n\nSources\n\nRust: https://rustc-dev-guide.rust-lang.org/diagnostics.html\nClang: https://clang.llvm.org/diagnostics.html\nFlix: https://flix.dev/principles/\nRacket: https://cs.brown.edu/~kfisler/Misc/error-msg-guidelines-racket-studlangs.pdf\nMore about the research that lead to the Racket guidelines (including the referenced papers): https://twitter.com/ShriramKMurthi/status/1451688982761381892\nFlow: https://calebmer.com/2019/07/01/writing-good-compiler-error-messages.html\nElm: https://elm-lang.org/news/compiler-errors-for-humans\nElm's error message catalog: https://github.com/elm/error-message-catalog\nReason: https://reasonml.github.io/blog/2017/08/25/way-nicer-error-messages.html",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/12/error-message-style-guides.html"
    },
    {
      "title": "PyPy v7.3.7: bugfix release of python 3.7 and 3.8",
      "text": "PyPy v7.3.7: bug-fix release of 3.7, 3.8\nWe are releasing a PyPy 7.3.7 to fix the recent 7.3.6 release's binary\nincompatibility with the previous 7.3.x releases. We mistakenly added fields\nto PyFrameObject and PyDateTime_CAPI that broke the promise of binary\ncompatibility, which means that c-extension wheels compiled for 7.3.5 will not\nwork with 7.3.6 and via-versa. Please do not use 7.3.6.\nWe have added a cursory test for binary API breakage to the\nhttps://github.com/pypy/binary-testing repo which hopefully will prevent such\nmistakes in the future.\nAdditionally, a few smaller bugs were fixed:\n\nUse uint for the request argument of fcntl.ioctl (issue 3568)\nFix incorrect tracing of while True` body in 3.8 (issue 3577)\nProperly close resources when using a concurrent.futures.ProcessPool\n(issue 3317)\nFix the value of LIBDIR in _sysconfigdata in 3.8 (issue 3582)\n\nYou can find links to download the v7.3.7 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog site via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a CFFI / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, and\n3.8. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 64 bits, OpenBSD, FreeBSD)\n64-bit ARM machines running Linux.\ns390x running Linux\n\n\nPyPy does support ARM 32 bit and PPC64 processors, but does not release binaries.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2021/10/pypy-v737-release.html"
    },
    {
      "title": "PyPy v7.3.6: release of python 2.7, 3.7, and 3.8",
      "text": "PyPy v7.3.6: release of python 2.7, 3.7, and 3.8-beta\nThe PyPy team is proud to release version 7.3.6 of PyPy, which includes\nthree different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.7,  which is an interpreter supporting the syntax and the features of\nPython 3.7, including the stdlib for CPython 3.7.12.\nPyPy3.8, which is an interpreter supporting the syntax and the features of\nPython 3.8, including the stdlib for CPython 3.8.12. Since this is our\nfirst release of the interpreter, we relate to this as \"beta\" quality. We\nwelcome testing of this version, if you discover incompatibilites, please\nreport them so we can gain confidence in the version.\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. Highlights of the release, since the release of 7.3.5 in May 2021,\ninclude:\n\n\nWe have merged a backend for HPy, the better C-API interface. The backend\nimplements HPy version 0.0.3.\nTranslation of PyPy into a binary, known to be slow, is now about 40%\nfaster. On a modern machine, PyPy3.8 can translate in about 20 minutes.\nPyPy Windows 64 is now available on conda-forge, along with nearly 700\ncommonly used binary packages. This new offering joins the more than 1000\nconda packages for PyPy on Linux and macOS. Many thanks to the conda-forge\nmaintainers for pushing this forward over the past 18 months.\nSpeed improvements were made to io, sum, _ssl and more. These\nwere done in response to user feedback.\nThe 3.8 version of the release contains a beta-quality improvement to the\nJIT to better support compiling huge Python functions by breaking them\nup into smaller pieces.\nThe release of Python3.8 required a concerted effort. We were greatly\nhelped by @isidentical (Batuhan Taskaya) and other new contributors.\nThe 3.8 package now uses the same layout as CPython, and many of the\nPyPy-specific changes to sysconfig, distutils.sysconfig, and\ndistutils.commands.install.py have been removed. The stdlib now\nis located in <base>/lib/pypy3.8 on posix systems, and in\n<base>/Lib on Windows. The include files on windows remain the same.\nOn posix they are in <base>/include/pypy3.8. Note we still use the\npypy prefix to prevent mixing the files with CPython (which uses\npython.\n\n\nWe recommend updating. You can find links to download the v7.3.6 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our blog via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better. Since the\nprevious release, we have accepted contributions from 7 new contributors,\nthanks for pitching in, and welcome to the project!\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a CFFI / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, and\nsoon 3.8. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 64 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n64-bit ARM machines running Linux.\n\n\nPyPy does support Windows 32-bit and ARM 32 bit processors, but does not\nrelease binaries. Please reach out to us if you wish to sponsor releases for\nthose platforms.\n\n\nWhat else is new?\nFor more information about the 7.3.6 release, see the full changelog.\nPlease update, and continue to help us make PyPy better.\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2021/10/pypy-v736-release.html"
    },
    {
      "title": "Better JIT Support for Auto-Generated Python Code",
      "text": "Performance Cliffs\nA common bad property of many different JIT compilers is that of a \"performance\ncliff\": A seemingly reasonable code change, leading to massively reduced\nperformance due to hitting some weird property of the JIT compiler that's not\neasy to understand for the programmer (e.g. here's a blog post about the fix of\na performance cliff when running React on\nV8). Hitting a performance cliff as a\nprogrammer can be intensely frustrating and turn people off from using PyPy\naltogether. Recently we've been working on trying to remove some of PyPy's\nperformance cliffs, and this post describes one such effort.\nThe problem showed up in an issue\nwhere somebody found the performance\nof their website using Tornado a lot\nworse than what various benchmarks suggested. It took some careful digging to\nfigure out what caused the problem: The slow performance was caused by the huge\nfunctions that the Tornado templating engine creates. These functions lead the\nJIT to behave in unproductive ways. In this blog post I'll describe why the\nproblem occurs and how we fixed it.\nProblem\nAfter quite a bit of debugging we narrowed down the problem to the following\nreproducer: If you render a big HTML template\n(example)\nusing the Tornado templating engine, the template rendering is really not any\nfaster than CPython. A small template doesn't show this behavior, and other\nparts of Tornado seem to perform well. So we looked into how the templating\nengine works, and it turns out that the templates are compiled into Python\nfunctions. This means that a big template can turn into a really enormous Python\nfunction (Python version of the\nexample).\nFor some reason really enormous Python functions aren't handled particularly\nwell by the JIT, and in the next section I'll explain some the background that's\nnecessary to understand why this happens.\nTrace Limits and Inlining\nTo understand why the problem occurs, it's necessary to understand how PyPy's\ntrace limit and inlining works. The tracing JIT has a maximum trace length built\nin, the reason for that is some limitation in the compact encoding of traces in\nthe JIT. Another reason is that we don't want to generate arbitrary large chunks\nof machine code. Usually, when we hit the trace limit, it is due to inlining.\nWhile tracing, the JIT will inline many of the functions called from the\noutermost one. This is usually good and improves performance greatly, however,\ninlining can also lead to the trace being too long. If that happens, we\nwill mark a called function as uninlinable. The next time we trace the outer\nfunction we won't inline it, leading to a shorter trace, which hopefully fits\nthe trace limit.\n\nIn the diagram above we trace a function f, which calls a function g, which\nis inlined into the trace. The trace ends up being too long, so the JIT\ndisables inlining of g. The next time we try to trace f the trace will\ncontain a call to g instead of inlining it. The trace ends up being not too\nlong, so we can turn it into machine code when tracing finishes.\nNow we know enough to understand what the problem with automatically generated\ncode is: sometimes, the outermost function itself\ndoesn't fit the trace limit, without any inlining going on at all. This is\nusually not the case for normal, hand-written Python functions. However, it can\nhappen for automatically generated Python code, such as the code that the\nTornado templating engine produces.\nSo, what happens when the JIT hits such a huge function? The function is traced\nuntil the trace is too long. Then the trace limits stops further tracing. Since\nnothing was inlined, we cannot make the trace shorter the next time by disabling\ninlining. Therefore, this happens again and again, the next time we trace the\nfunction we run into exactly the same problem. The net effect is that the\nfunction is even slowed down: we spend time tracing it, then stop tracing and\nthrow the trace away. Therefore, that effort is never useful, so the resulting\nexecution can be slower than not using the JIT at all!\nSolution\nTo get out of the endless cycle of useless retracing we first had the idea of\nsimply disabling all code generation for such huge functions, that produce too long\ntraces even if there is no inlining at all. However, that lead to disappointing\nperformance in the example Tornado program, because important parts of the code\nremain always interpreted.\nInstead, our solution is now as follows: After we have hit the trace limit and\nno inlining has happened so far, we mark the outermost function as a source of huge\ntraces. The next time we trace such a function, we do so in a special mode. In\nthat mode, hitting the trace limit behaves differently: Instead of stopping the\ntracer and throwing away the trace produced so far, we will use the unfinished\ntrace to produce machine code. This trace corresponds to the first part of the\nfunction, but stops at a basically arbitrary point in the middle of the\nfunction.\nThe question is what should happen when execution\nreaches the end of this unfinished trace. We want to be able to cover more of\nthe function with machine code and therefore need to extend the trace\nfrom that point on. But we don't want to do that too\neagerly to prevent lots and lots of machine code being generated. To achieve\nthis behaviour we add a guard to the end of the unfinished trace, which will\nalways fail. This has the right behaviour: a failing guard will transfer control\nto the interpreter, but if it fails often enough, we can patch it to jump to\nmore machine code, that starts from this position. In that way, we can slowly\nexplore the full gigantic function and add all those parts of the control flow\ngraph that are actually commonly executed at runtime.\n\nIn the diagram we are trying to trace a huge function f, which leads to\nhitting the trace limit. However, nothing was inlined into the trace, so\ndisabling inlining won't ensure a successful trace attempt the next time.\nInstead, we mark f as \"huge\". This has the effect that when we trace it again\nand are about to hit the trace limit, we end the trace at an arbitrary point by\ninserting a guard that always fails.\n\nIf this guard failure is executed often enough, we might patch the guard and\nadd a jump to a further part of the function f. This can continue potentially\nseveral times, until the trace really hits and end points (for example by\nclosing the loop and jumping back to trace 1, or by returning from f).\nEvaluation\nSince this is a performance cliff that we didn't observe in any of our\nbenchmarks ourselves, it's pointless to look at the\neffect that this improvement has on existing benchmarks \u2013 there shouldn't and\nindeed there isn't any.\nInstead, we are going to look at a micro-benchmark that came out of the\noriginal bug report, one that simply renders a big artificial Tornado template\n200 times. The code of the micro-benchmark can be found\nhere.\nAll benchmarks were run 10 times in new processes. The means and standard\ndeviations of the benchmark runs are:\n\n\n\nImplementation\nTime taken (lower is better)\n\n\n\n\nCPython 3.9.5\n14.19 \u00b1 0.35s\n\n\nPyPy3 without JIT\n59.48 \u00b1 5.41s\n\n\nPyPy3 JIT old\n14.47 \u00b1 0.35s\n\n\nPyPy3 JIT new\n4.89 \u00b1 0.10s\n\n\n\nWhat we can see is that while the old JIT is very helpful for this\nmicro-benchmark, it only brings the performance up to CPython levels, not\nproviding any extra benefit. The new JIT gives an almost 3x speedup.\nAnother interesting number we can look at is how often the JIT started a trace,\nand for how many traces we produced actual machine code:\n\n\n\nImplementation\nTraces Started\nTraces sent to backend\nTime spent in JIT\n\n\n\n\nPyPy3 JIT old\n216\n24\n0.65s\n\n\nPyPy3 JIT new\n30\n25\n0.06s\n\n\n\nHere we can clearly see the problem: The old JIT would try tracing the\nauto-generated templating code again and again, but would never actually produce\nany machine code, wasting lots of time in the process. The new JIT still traces a\nfew times uselessly, but then eventually converges and stops emitting machine\ncode for all the paths through the auto-generated Python code.\n\n\nRelated Work\nTim Felgentreff pointed me to the fact that\nTruffle also has a\nmechanism\nto slice huge methods into smaller compilation units (and I am sure other JITs\nhave such mechanisms as well).\nConclusion\nIn this post we've described a performance cliff in PyPy's JIT, that of really\nbig auto-generated functions which hit the trace limit without inlining, that we\nstill want to generate machine code for. We achieve this by chunking up the\ntrace into several smaller traces, which we compile piece by piece. This is not\na super common thing to be happening \u2013 otherwise we would have run into and\nfixed it earlier \u2013 but it's still good to have a fix now.\nThe work\ndescribed in this post tiny bit experimental still, but we will release it as\npart of the upcoming 3.8 beta release, to get some more experience with it.\nPlease grab a 3.8 release\ncandidate,\ntry it out and let us know your observations, good and bad!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/09/jit-auto-generated-code.html"
    },
    {
      "title": "#pypy IRC moves to Libera.Chat",
      "text": "Following the example of many other FOSS projects, the PyPy team has\ndecided to move its official #pypy IRC channel from Freenode to\nLibera.Chat: irc.libera.chat/pypy\nThe core devs will no longer be present on the Freenode channel, so we recommend to\njoin the new channel as soon as possible.\nwikimedia.org has a\nnice guide on\nhow to setup your client to migrate from Freenode to Libera.Chat.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/05/pypy-irc-moves-to-libera-chat.html"
    },
    {
      "title": "PyPy v7.3.5: bugfix release of python 2.7 and 3.7",
      "text": "PyPy v7.3.5: release of 2.7 and 3.7\nWe are releasing a PyPy 7.3.5 with bugfixes for PyPy 7.3.4, released April 4.\nPyPy 7.3.4 was the first release that runs on windows 64-bit, so that support\nis still \"beta\". We are releasing it in the hopes that we can garner momentum\nfor its continued support, but are already aware of some problems, for instance\nit errors in the NumPy test suite (issue 3462). Please help out with testing\nthe release and reporting successes and failures, financially supporting our\nongoing work, and helping us find the source of these problems.\n\nThe new windows 64-bit builds improperly named c-extension modules\nwith the same extension as the 32-bit build (issue 3443)\nUse the windows-specific PC/pyconfig.h rather than the posix one\nFix the return type for _Py_HashDouble which impacts 64-bit windows\nA change to the python 3.7 sysconfig.get_config_var('LIBDIR') was wrong,\nleading to problems finding libpypy3-c.so for embedded PyPy (issue 3442).\nInstantiate distutils.command.install schema for PyPy-specific\nimplementation_lower\nDelay thread-checking logic in greenlets until the thread is actually started\n(continuation of issue 3441)\nFour upstream (CPython) security patches were applied:\n\nBPO 42988 to remove pydoc.getfile\nBPO 43285 to not trust the PASV response in ftplib.\nBPO 43075 to remove a possible ReDoS in urllib AbstractBasicAuthHandler\nBPO 43882 to sanitize urls containing ASCII newline and tabs in\nurllib.parse\n\n\nFix for json-specialized dicts (issue 3460)\nSpecialize ByteBuffer.setslice which speeds up binary file reading by a\nfactor of 3\nWhen assigning the full slice of a list, evaluate the rhs before clearing the\nlist (issue 3440)\nOn Python2, PyUnicode_Contains accepts bytes as well as unicode.\nFinish fixing _sqlite3 - untested _reset() was missing an argument\n(issue 3432)\nUpdate the packaged sqlite3 to 3.35.5 on windows. While not a bugfix, this\nseems like an easy win.\n\nWe recommend updating. These fixes are the direct result of end-user bug\nreports, so please continue reporting issues as they crop up.\nYou can find links to download the v7.3.5 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our renovated blog site via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better.\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a CFFI / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, and\nsoon 3.8. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32/64 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n64-bit ARM machines running Linux.\n\n\nPyPy does support ARM 32 bit processors, but does not release binaries.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2021/05/pypy-v735-release.html"
    },
    {
      "title": "Checksums",
      "text": "Here are the checksums\npypy3.11-v7.3.20 sha256:\n9347fe691a07fd9df17a1b186554fb9d9e6210178ffef19520a579ce1f9eb741  pypy3.11-v7.3.20-aarch64.tar.bz2\nd08ce15dd61e9ace5e010b047104f0137110a258184e448ea8239472f10cf99b  pypy3.11-v7.3.20-linux32.tar.bz2\n1410db3a7ae47603e2b7cbfd7ff6390b891b2e041c9eb4f1599f333677bccb3e  pypy3.11-v7.3.20-linux64.tar.bz2\n84a48e09c97f57df62cc9f01b7a6d8c3e306b6270671d871aa8ab8c06945940d  pypy3.11-v7.3.20-macos_arm64.tar.bz2\nbb3ae80cf5fca5044af2e42933e7692c7c5e76a828ce0eb6404a5d5da83b313c  pypy3.11-v7.3.20-macos_x86_64.tar.bz2\n7786dda760003e2ea7409c1037e50200c578ec427ce0245ac4cd758710b206fb  pypy3.11-v7.3.20-src.tar.bz2\n76d93e4f04aa266591e8db1b81c6f70746d4568483489fc7f37ac0f6d7a4a28a  pypy3.11-v7.3.20-src.zip\na8d36f6ceb1d9be6cf24a73b0ba103e7567e396b2f7a33426b05e4a06330755b  pypy3.11-v7.3.20-win64.zip\npypy2.7-v7.3.20 sha256:\nf22a1be607deeaa4f9be6bc63aae09fe4fb5b990d6a23aa4e7c5960dc5d93c96  pypy2.7-v7.3.20-aarch64.tar.bz2\n9d554c5efcb6ef80146bb82965f5d8404d6848e6f04b25c378852a095768a69c  pypy2.7-v7.3.20-linux32.tar.bz2\naa3bb92dbb529fa2d4920895b16d67a810b0c709207857d56cfe4a6e3b41e02a  pypy2.7-v7.3.20-linux64.tar.bz2\nbe3ffbb243316b1ffbf63ac60d72e099d5b64702e4429eeeb18a0608fb3b8dcc  pypy2.7-v7.3.20-macos_arm64.tar.bz2\n9e7cf34e9d4c8d890439fcd10b09a2c0825d8709c63dd4cbf85645711bfb15b7  pypy2.7-v7.3.20-macos_x86_64.tar.bz2\nbf958498445f7bf78338723c8d86bd6711e8792461725d2481df77a9566a3e62  pypy2.7-v7.3.20-src.tar.bz2\naca2a3295ac0263419c29213d2f243ff4a3c54b9c3dfb11c94908873cb7eadf9  pypy2.7-v7.3.20-src.zip\n76d02853601ef8d3c9dd821a9faa84cabf9bc469d6b77797b21ed311b25d5419  pypy2.7-v7.3.20-win64.zip\npypy3.11-v7.3.19 sha256:\n13207dbf81ce24e96da760b1b863627b77bb20b1fb4c95191e02a0b72383df74  pypy3.11-v7.3.19-aarch64.tar.bz2\n5c6cdafd0a0abd14ca59926ed1b6aeb13b228c18b4b46de655aae48734c731ad  pypy3.11-v7.3.19-linux32.tar.bz2\n9177d9e0bb91b05f921c642cb0ff71a0f3653b5d29a42d40d6a078c15b75720f  pypy3.11-v7.3.19-linux64.tar.bz2\n7704e0d5302e53920d32dcfe9afeeb10436d4c94233e8830cf603aa955a861c1  pypy3.11-v7.3.19-macos_arm64.tar.bz2\na2439f9d30dfdae96a5e9101c7dc54a8a68b56c9d7314ea399b0a25d3e87ebb2  pypy3.11-v7.3.19-macos_x86_64.tar.bz2\n4817c044bb469a3274e60aa3645770f81eb4f9166ea7fdc4e6c351345554c8d8  pypy3.11-v7.3.19-src.tar.bz2\nddfdd7ee0ed970a9660e66c17243a83ce6c39f03aca1e517422076938c5095d6  pypy3.11-v7.3.19-src.zip\nb61c7c1dbf879eda6f779c374bfbbeecd3f618ada08404705a1a19d39df48dbd  pypy3.11-v7.3.19-win64.zip\npypy3.10-v7.3.19 sha256:\naf27a589178f11198e2244ab65ca510630ba97c131d7ccc4021eb5bc58de7f57  pypy3.10-v7.3.19-aarch64.tar.bz2\ne63a4fcad2641ee541e852918befb513abf04ce7070f743a50778cae9f9da80e  pypy3.10-v7.3.19-linux32.tar.bz2\nc73ac2cc2380ac9227fd297482bf2a3e17a80618ba46db7544d535515321ec1e  pypy3.10-v7.3.19-linux64.tar.bz2\n3db8a03fc496164801646844d7f3b12baa0adb3ad9a0b7cb124521bc2e168a36  pypy3.10-v7.3.19-macos_arm64.tar.bz2\n2c5e5c2a33ac882551d7f28b98d19d486b8995aa50824a73b4edcc6aeec35c63  pypy3.10-v7.3.19-macos_x86_64.tar.bz2\na7c22930b918f40870865ed8a74147f4434ef84d3d6ca2b3c1eba9355d4929c8  pypy3.10-v7.3.19-src.tar.bz2\n913bc02f17de3a11f984cc2ab76a34418c4d01ec4bef2b1eb349b201a8aa3404  pypy3.10-v7.3.19-src.zip\nc0d07bba6c8fb4e5804f4a8b3f8ef07cc3d89f6ad1db42a45ffb9be60bbb7cc2  pypy3.10-v7.3.19-win64.zip\npypy2.7-v7.3.19 sha256:\nfe89d4fd4af13f76dfe7315975003518cf176520e3ccec1544a88d174f50910e  pypy2.7-v7.3.19-aarch64.tar.bz2\ncc52df02b6926bd8645c1651cd7f6637ce51c2f352d0fb3c6b9330d15194b409  pypy2.7-v7.3.19-linux32.tar.bz2\nd38445508c2eaf14ebb380d9c1ded321c5ebeae31c7e66800173d83cb8ddf423  pypy2.7-v7.3.19-linux64.tar.bz2\n28780e0b908ad6db4b4e096f4237124be79ecc9731946d840d9c8749eb67a759  pypy2.7-v7.3.19-macos_arm64.tar.bz2\n6be28d448d8e64fffc586d9b0ae4d09064a83ccaeb5b8060c651c5cd9ae06878  pypy2.7-v7.3.19-macos_x86_64.tar.bz2\n8703cdcb01f9f82966dd43b6a6018f140399db51ebb43c125c1f9a215e7bb003  pypy2.7-v7.3.19-src.tar.bz2\na7c74f08c4a1d4ce3ad94480b29780db0e87f3096b8e9e17ebd78d88a56f950f  pypy2.7-v7.3.19-src.zip\nfbdcd4fe681981c020a25c1a35225209dc3d651f6117ebe9e4d212b66a2b46ec  pypy2.7-v7.3.19-win64.zip\npypy3.11-v7.3.18 sha256:\n95bc3021cb9be2aa766ecb776c215e97ae8c0dda99dd48840b40b5b87d90bc45  pypy3.11-v7.3.18-aarch64.tar.bz2\n70357a69cdbf4ce449b5743901fcd36208e0a89eadcfb4d2ebdc9b4711a0612c  pypy3.11-v7.3.18-linux32.tar.bz2\ndf5fa534800ff09d990e50ca4fdb02d8a06b9ed948dbcb3a09bea0739ef42de7  pypy3.11-v7.3.18-linux64.tar.bz2\ncc4d6dd621dffd474eb9a2d01e5844d1b6a6f88b404aac2c48a9c267b5370df4  pypy3.11-v7.3.18-macos_arm64.tar.bz2\n4f2274ffb4b787fd5b5f15cd61f7fef5a3710bf0ecbd4293bf0251035b468865  pypy3.11-v7.3.18-macos_x86_64.tar.bz2\nfbc82b8ff67b942e6fce49980dc0f0f83193b005173f9bddd39d6396fb6b939d  pypy3.11-v7.3.18-src.tar.bz2\n59c2515e38c9ac82d4cd4c09569cbeba2661406e7b04e2adb5a275a8ae86ff66  pypy3.11-v7.3.18-src.zip\nccbea242f623700027e713d845dff5c7f7a5806413bbce229d3dcb88715970fd  pypy3.11-v7.3.18-win64.zip\npypy3.10-v7.3.18 sha256:\ne843aecd48eb06b625af67891b99e3440313cfb64c6851fc37df1e5572c8ef9e  pypy3.10-v7.3.18-aarch64.tar.bz2\n34ef09a481254aad0f22bf09fd7c99efb65ffef4f79f5b4222505f55f8d9c22e  pypy3.10-v7.3.18-linux32.tar.bz2\n834ccd4544bb47112a66977add7e47f30619f74061ae990876bcba95d98c27c5  pypy3.10-v7.3.18-linux64.tar.bz2\n1d47da22bc9f7b5329f97c9c7b3ee0b051252ff1e82ca76999ad2d44758be41f  pypy3.10-v7.3.18-macos_arm64.tar.bz2\nb7031cbf815b7ba26ed2503b76ee3e73d7322b55f3671219019e8385042e212f  pypy3.10-v7.3.18-macos_x86_64.tar.bz2\n08eb70e866987e4abe88ae9f54a98f919df92b595f7cdf4041703e8802b11552  pypy3.10-v7.3.18-src.tar.bz2\n646bc0127f402983440ebca0a507e3b576745668d4e1b82cc3cbd6d8e49768d0  pypy3.10-v7.3.18-src.zip\ne7ae89c5d45efcc927425281c870d0ce62cd624628f869cb0a25a0647e39a7be  pypy3.10-v7.3.18-win64.zip\npypy2.7-v7.3.18 sha256:\nd647cad5be915df65f44277fd051c8d52e708d22838b5cb21b2de033530acc80  pypy2.7-v7.3.18-aarch64.tar.bz2\n54990fb1ae2266c260a7ce694b84ab91a8d0d298da440cd5695ac671dc5615e2  pypy2.7-v7.3.18-linux32.tar.bz2\n1da34354e5fa59400609e94c00ba6feccf5aa575abb26fb6caf9c2ac16100ff4  pypy2.7-v7.3.18-linux64.tar.bz2\n48927b1931a4feadea945f7c988c429b42e9fb6567af3810ab86ad95efbe98bc  pypy2.7-v7.3.18-macos_arm64.tar.bz2\ndb9d2bcc8dbe089b34a19083a11116697e8679acc2c47a2862264429810f666e  pypy2.7-v7.3.18-macos_x86_64.tar.bz2\n737435ddfc5afa5b97a7209c87d70d5f1062426c053b9bb8b99a0347cb4891fa  pypy2.7-v7.3.18-src.tar.bz2\n7646a04fd9d3faeb20bef003291258b202c28624971673cb69d2bdef73426e57  pypy2.7-v7.3.18-src.zip\nc1d397743a6727832affddb15296dbeeb4a7602d91ae64114d798a9cc71179ca  pypy2.7-v7.3.18-win64.zip\npypy3.10-v7.3.17 sha256:\n53b6e5907df869c49e4eae7aca09fba16d150741097efb245892c1477d2395f2  pypy3.10-v7.3.17-aarch64.tar.bz2\ne534110e1047da37c1d586c392f74de3424f871d906a2083de6d41f2a8cc9164  pypy3.10-v7.3.17-linux32.tar.bz2\nfdcdb9b24f1a7726003586503fdeb264fd68fc37fbfcea022dcfe825a7fee18b  pypy3.10-v7.3.17-linux64.tar.bz2\na050e25e8d686853dd5afc363e55625165825dacfb55f8753d8225ebe417cfd2  pypy3.10-v7.3.17-macos_arm64.tar.bz2\n6c2c5f2300d7564e711421b4968abd63243cb96f76e363975dd648ebf4a362ee  pypy3.10-v7.3.17-macos_x86_64.tar.bz2\n6ad74bc578e9c6d3a8a1c51503313058e3c58c35df86f7485453c4be6ab24bf7  pypy3.10-v7.3.17-src.tar.bz2\n00857673af7d92144a5e134c723891953a1e99ac002eff440330de23a8147e85  pypy3.10-v7.3.17-src.zip\ncab794a03ddda26238c72942ea6f225612e0dc17c76cac6652da83a95024e6e8  pypy3.10-v7.3.17-win64.zip\npypy2.7-v7.3.17 sha256:\na8df5ce1650f4756933f8780870c91a0a40e7c9870d74629bf241392bcb5c2e3  pypy2.7-v7.3.17-aarch64.tar.bz2\na3aa0867cc837a34941047ece0fbb6ca190410fae6ad35fae4999d03bf178750  pypy2.7-v7.3.17-linux32.tar.bz2\n9f3497f87b3372d17e447369e0016a4bec99a6b4d2a59aba774a25bfe4353474  pypy2.7-v7.3.17-linux64.tar.bz2\n8573172db377ee0831bf20492cdee9bac4e0b194e3dfe8bf7c44ee257a824766  pypy2.7-v7.3.17-macos_arm64.tar.bz2\ne3e1af1d6ad15e51d8d19ea36e1ac65c4c792314cc8b8dc5cf771ec4353b50f8  pypy2.7-v7.3.17-macos_x86_64.tar.bz2\n50e06840f4bbde91448080a4118068a89b8fbcae25ff8da1e2bb1402dc9a0346  pypy2.7-v7.3.17-src.tar.bz2\n593cedd368a59bd5ed5dc8df00961a42a50c5d75d2614a96b1c75d25612dadf1  pypy2.7-v7.3.17-src.zip\n2ce2f4c205819902ee3ea2e80f8fc9ae9b18647bcfc8046ba83fe46b4139f734  pypy2.7-v7.3.17-win64.zip\npypy3.10-v7.3.16 sha256:\nfc720999bc5050e1d3706b3b6445e695cf42bfc71ebc7c88ed6bb88828b1d385  pypy3.10-v7.3.16-aarch64.tar.bz2\n0df48aa780159e879ac89a805d143e4a6cd1b842f98046f5a3f865814bfaa2a4  pypy3.10-v7.3.16-linux32.tar.bz2\n404e6180d6caf9258eaab0c02c72018e9aa8eb03ab9094a0ff17ee5e3b265ac1  pypy3.10-v7.3.16-linux64.tar.bz2\n6c003376667a95c7a228544649677b9927b8210d6444b901817aad24b8719b93  pypy3.10-v7.3.16-macos_arm64.tar.bz2\n490f2c6ba2489f405444f3b4ad42166da6e2eb73489a9535b206067eaaf21737  pypy3.10-v7.3.16-macos_x86_64.tar.bz2\naf97efe498a209ba18c7bc7d084164a9907fb3736588b6864955177e19d5216a  pypy3.10-v7.3.16-s390x.tar.bz2\n4a3a3177d0a1f51d59982bb981d1d485403bda3419d5437b9e077f55f59424ff  pypy3.10-v7.3.16-src.tar.bz2\n8f59b6859d7d49036afce8156ea52f9c6a1e8d1e08af01bd6c70444d092841f5  pypy3.10-v7.3.16-src.zip\ne08415a2f35c6ecf2342b504bdfde11e4c5eca3fc5ef7fd2214ff064a5a54396  pypy3.10-v7.3.16-win64.zip\npypy3.9-v7.3.16 sha256:\nde3f2ed3581b30555ac0dd3e4df78a262ec736a36fb2e8f28259f8539b278ef4  pypy3.9-v7.3.16-aarch64.tar.bz2\n583b6d6dd4e8c07cbc04da04a7ec2bdfa6674825289c2378c5e018d5abe779ea  pypy3.9-v7.3.16-linux32.tar.bz2\n16f9c5b808c848516e742986e826b833cdbeda09ad8764e8704595adbe791b23  pypy3.9-v7.3.16-linux64.tar.bz2\n88f824e7a2d676440d09bc90fc959ae0fd3557d7e2f14bfbbe53d41d159a47fe  pypy3.9-v7.3.16-macos_arm64.tar.bz2\nfda015431621e7e5aa16359d114f2c45a77ed936992c1efff86302e768a6b21c  pypy3.9-v7.3.16-macos_x86_64.tar.bz2\n7a56ebb27dba3110dc1ff52d8e0449cdb37fe5c2275f7faf11432e4e164833ba  pypy3.9-v7.3.16-s390x.tar.bz2\n5b75af3f8e76041e79c1ef5ce22ce63f8bd131733e9302081897d8f650e81843  pypy3.9-v7.3.16-src.tar.bz2\ndef4dae720dd09b868b9b8a7a1255f07f925d88a4543f99cd9ae1aeb0a49ff5e  pypy3.9-v7.3.16-src.zip\n06ec12a5e964dc0ad33e6f380185a4d295178dce6d6df512f508e7aee00a1323  pypy3.9-v7.3.16-win64.zip\npypy2.7-v7.3.16 sha256:\nbe44e65dd8c00d2388b2580dbe2af6a5179f951a8f4979efc74360f92f3c7e96  pypy2.7-v7.3.16-aarch64.tar.bz2\na19712d7a6bd4f6d113e352c5271803c583b5129b76a357d387b1fa85204f8e5  pypy2.7-v7.3.16-linux32.tar.bz2\n04b2fceb712d6f811274825b8a471ee392d3d1b53afc83eb3f42439ce00d8e07  pypy2.7-v7.3.16-linux64.tar.bz2\n9cc13f4d6c4096820e1e0ddabb3959f853e45150ce0166a39aa23867e99f0145  pypy2.7-v7.3.16-macos_arm64.tar.bz2\ne8744c1cef8b9e4eb2d2b6b368ed19a1c5cde482c7ef750f2d9f0807bb77fd1c  pypy2.7-v7.3.16-macos_x86_64.tar.bz2\n09eb70b932e6aac484cf4b5f2de5845f71589f2cbb53e5ed37a497613b43cd53  pypy2.7-v7.3.16-s390x.tar.bz2\n43721cc0c397f0f3560b325c20c70b11f7c76c27910d3df09f8418cec4f9c2ad  pypy2.7-v7.3.16-src.tar.bz2\n54c5f8405bb28e3a48d8962ad1765e8536d53546e1c352bcabab36e5727dd609  pypy2.7-v7.3.16-src.zip\na51ac82cc0374f86b5eba571d4e5f23cdce5ac7cd3bd5b2d2d726c0d98684d7d  pypy2.7-v7.3.16-win64.zip\npypy3.10-v7.3.15 sha256:\n52146fccaf64e87e71d178dda8de63c01577ec3923073dc69e1519622bcacb74  pypy3.10-v7.3.15-aarch64.tar.bz2\n75dd58c9abd8b9d78220373148355bc3119febcf27a2c781d64ad85e7232c4aa  pypy3.10-v7.3.15-linux32.tar.bz2\n33c584e9a70a71afd0cb7dd8ba9996720b911b3b8ed0156aea298d4487ad22c3  pypy3.10-v7.3.15-linux64.tar.bz2\nd927c5105ea7880f7596fe459183e35cc17c853ef5105678b2ad62a8d000a548  pypy3.10-v7.3.15-macos_arm64.tar.bz2\n559b61ba7e7c5a5c23cef5370f1fab47ccdb939ac5d2b42b4bef091abe3f6964  pypy3.10-v7.3.15-macos_x86_64.tar.bz2\n209e57596381e13c9914d1332f359dc4b78de06576739747eb797bdbf85062b8  pypy3.10-v7.3.15-s390x.tar.bz2\n837622130b36603a1893899bd9f529961a8e4a56c9eb67268d72ddf8920c9579  pypy3.10-v7.3.15-src.tar.bz2\n67432b82dd7e436d818bd6cd38115564f13fc226ffd2940f3915ad68b0fc683b  pypy3.10-v7.3.15-src.zip\nb378b3ab1c3719aee0c3e5519e7bff93ff67b2d8aa987fe4f088b54382db676c  pypy3.10-v7.3.15-win64.zip\npypy3.9-v7.3.15 sha256:\n03e35fcba290454bb0ccf7ee57fb42d1e63108d10d593776a382c0a2fe355de0  pypy3.9-v7.3.15-aarch64.tar.bz2\nc6209380977066c9e8b96e8258821c70f996004ce1bc8659ae83d4fd5a89ff5c  pypy3.9-v7.3.15-linux32.tar.bz2\nf062be307200bde434817e1620cebc13f563d6ab25309442c5f4d0f0d68f0912  pypy3.9-v7.3.15-linux64.tar.bz2\n300541c32125767a91b182b03d9cc4257f04971af32d747ecd4d62549d72acfd  pypy3.9-v7.3.15-macos_arm64.tar.bz2\n18ad7c9cb91c5e8ef9d40442b2fd1f6392ae113794c5b6b7d3a45e04f19edec6  pypy3.9-v7.3.15-macos_x86_64.tar.bz2\ndeeb5e54c36a0fd9cfefd16e63a0d5bed4f4a43e6bbc01c23f0ed8f7f1c0aaf3  pypy3.9-v7.3.15-s390x.tar.bz2\n6bb9537d85aa7ad13c0aad2e41ff7fd55080bc9b4d1361b8f502df51db816e18  pypy3.9-v7.3.15-src.tar.bz2\n06dd38124b873343bdf566ca9076ff8e38ad82fd7f2feecd942480c2200a13c0  pypy3.9-v7.3.15-src.zip\na156dad8b58570597eaaabe05663f00f80c60bc11df4a9c46d0953b6c5eb9209  pypy3.9-v7.3.15-win64.zip\npypy2.7-v7.3.15 sha256:\n31b41fca7280636d7818713b7a0fab8f34ece9c82cc88e51d305d43b3e6306d6  pypy2.7-v7.3.15-aarch64.tar.bz2\ncb5c1da62a8ca31050173c4f6f537bc3ff316026895e5f1897b9bb526babae79  pypy2.7-v7.3.15-linux32.tar.bz2\ne857553bdc4f25ba9670a5c173a057a9ff71262d5c5da73a6ddef9d7dc5d4f5e  pypy2.7-v7.3.15-linux64.tar.bz2\n618d33df7ac6570d88a58183e3e15c56f63f862968cecbd2ee896eac6255cea6  pypy2.7-v7.3.15-macos_arm64.tar.bz2\n72dac262fc63115b6ccd2c3c15e7afd1b2e7a65d7e97265c116246d1cf2cdffd  pypy2.7-v7.3.15-macos_x86_64.tar.bz2\neb442279ec3f1eb17da296e38b531d3ca50c6418eab208a020bca4646a1dea46  pypy2.7-v7.3.15-s390x.tar.bz2\na66ddaed39544a35bb7ab7a17dbf673a020c7cb3a614bd2b61a54776888daf2c  pypy2.7-v7.3.15-src.tar.bz2\na424a065d42b49f6e7f3576cdc3acb60778dd578be8d59f04eccd35c2ef15dc8  pypy2.7-v7.3.15-src.zip\nca3c813aec8f9304c7bdc0f69d8ea2a263d4247224ee094e0017338da84c75f2  pypy2.7-v7.3.15-win64.zip\npypy3.10-v7.3.14 sha256:\nfbef65dfc69dcd6006d843553d268b331f1b13dfc3938492bd35f0f477b5bcf4  pypy3.10-v7.3.14-aarch64.tar.bz2\nd37e7c7a03bed5dceca2ab7f821ad7655808cccf6908155f78f0effd811b7f4f  pypy3.10-v7.3.14-linux32.tar.bz2\na83879891dc0a6c1504da0954fba1125b21a2591782897231a8168100ea72b94  pypy3.10-v7.3.14-linux64.tar.bz2\n0f09584e21ed8f45e8ff1e3dd1582f077ebdd23a1068298f45006f67bc692632  pypy3.10-v7.3.14-macos_arm64.tar.bz2\n31ce62b7ea3b5b5bde68241ae9404f0a68f5a7d0094ef651062b7a64caecfd4e  pypy3.10-v7.3.14-macos_x86_64.tar.bz2\n363e87ad3b6547cc68981c665cf049449bed44cf9e49cabbbcc61df73ea2d40b  pypy3.10-v7.3.14-s390x.tar.bz2\na3481af466103fa13740db4e27780e0423dcf8626b3340f60d3d3c28fbc11ae0  pypy3.10-v7.3.14-src.tar.bz2\n95db3e9d22a4820ad9a683d4f6895fa611b16ed02bd709c86a4ac903f9b36721  pypy3.10-v7.3.14-src.zip\n1713edd310f400935fe9a9f8fa0fd9da1a405eaf7b69564d00f437fb498327f8  pypy3.10-v7.3.14-win64.zip\npypy3.9-v7.3.14 sha256:\n14b842f32f60ce2d9d130971f9bcbdb6875824a0e78fac36806d267e0982179c  pypy3.9-v7.3.14-aarch64.tar.bz2\n4ad89a22369a6f2f83a7d8d047e0fc4cf5597f0921fa7afa23499ed05f663503  pypy3.9-v7.3.14-linux32.tar.bz2\nfebd770a616641ca8419c381c7fb224e515b892551d0db49a1231397ed38859d  pypy3.9-v7.3.14-linux64.tar.bz2\n4f8f2464a743f855b8fc8bda7ce7994a674616db3b5c2c1955cd08502fa782ca  pypy3.9-v7.3.14-macos_arm64.tar.bz2\n0e2fea9b2dadb82b7acf05f21c0144f7bb1cfaaa39c693ab1eba4aef5ed52680  pypy3.9-v7.3.14-macos_x86_64.tar.bz2\nba2451e9081db5bc724a05530a7f98817231de83ff6fdf15bad21a4e9b6dfeae  pypy3.9-v7.3.14-s390x.tar.bz2\n560fe6161e159557e1fe612aaadf9b293eefded1da372e70b8e3b23bba598366  pypy3.9-v7.3.14-src.tar.bz2\n16336170410dd13eb39fbacb412b640c9e3ab4dcdd3e2a8f3ba7978edae1dc2d  pypy3.9-v7.3.14-src.zip\n9b3d8496f2a4729fdf20d9f835299902048950baad3a42019b67da75ca5b38b7  pypy3.9-v7.3.14-win64.zip\npypy2.7-v7.3.14 sha256:\n98468f4cc704a2821401afdd001ebddd367e594e05a70c7767fb86f1364fb21a  pypy2.7-v7.3.14-aarch64.tar.bz2\nb12b4b587da55c8f212ae854e31d29258451e069c65aca596e577644e520bc8b  pypy2.7-v7.3.14-linux32.tar.bz2\n5938c3c6cddb2e8eb5e435cd3bf61d15134b94a9ac026e26a533bdda6c28a4a0  pypy2.7-v7.3.14-linux64.tar.bz2\na428e18fcf1470b032fb1f4d75795aeed9216b4314a4c8a3e4d7e13f10f8607e  pypy2.7-v7.3.14-macos_arm64.tar.bz2\n8af24683621937e65c518fbca1eb34e17ffc741c2ac917e4ca20694348157d78  pypy2.7-v7.3.14-macos_x86_64.tar.bz2\n5abc6a0f55a89c08def13b5f410b8e7bd706fe1b472f31db01ecbc4d0a49e8dc  pypy2.7-v7.3.14-s390x.tar.bz2\ne096fe67ce2d8d4d5e7dceb84fe1ca854498f00766d31b27d32c8d8833131373  pypy2.7-v7.3.14-src.tar.bz2\n680df6e172c5e5778fe3f7bd0a1f8902148f5de9decc5ec9252e72e94eb49bff  pypy2.7-v7.3.14-src.zip\na4c6d35e5ae68dfb773ec34b7d8f1503c8fbfcad817e6147babd6cfd3c8eb071  pypy2.7-v7.3.14-win64.zip\npypy3.10-v7.3.13 sha256:\nac476f01c9653358404f2e4b52f62307b2f64ccdb8c96dadcbfe355824d81a63  pypy3.10-v7.3.13-aarch64.tar.bz2\nbfba57eb1f859dd0ad0d6fe841bb12e1256f1f023c7fbca083b536cccbc1233b  pypy3.10-v7.3.13-linux32.tar.bz2\n54936eeafd9350a5ea0375b036272a260871b9bca82e1b0bb3201deea9f5a442  pypy3.10-v7.3.13-linux64.tar.bz2\nefb3007b7aace0af6e3b30d381088a5bbc175973a6627b6b0d624a2ca2dc63ce  pypy3.10-v7.3.13-macos_arm64.tar.bz2\n2c6238b9ece7b94ffdfd1d9b50619edef4b169a5c78adcdb691fce6709cd6610  pypy3.10-v7.3.13-macos_x86_64.tar.bz2\n3c813c7efa6a026b281313b299c186c585155fc164c7538e65d41efdabff87c9  pypy3.10-v7.3.13-s390x.tar.bz2\n4ac1733c19d014d3193c804e7f40ffccbf6924bcaaee1b6089b82b9bf9353a6d  pypy3.10-v7.3.13-src.tar.bz2\n828fc66eca1c097e44bc910c78ab773a98747268c7ce264da97022e5aca358dc  pypy3.10-v7.3.13-src.zip\n5b99422fb8978b2f4bbf97961bca49963a82dc47c2fa51b7d23c493db3a2e0f0  pypy3.10-v7.3.13-win64.zip\npypy3.9-v7.3.13 sha256:\n317d7876c5825a086f854253648b967a432b993ce87695d2895d3ad6ed0d2716  pypy3.9-v7.3.13-aarch64.tar.bz2\nac695238b4a3635ac6b482e74e04e2ea78b31acca0decd5de601dfd2f4ebf35a  pypy3.9-v7.3.13-linux32.tar.bz2\n323b05a9f607e932cda1995cbe77a96e4ea35994631aa6d734c8035e8479b74e  pypy3.9-v7.3.13-linux64.tar.bz2\na07b17a790a1952b551e69d47d77a5546ad5e666ed1bd90b9ad60baaca6aa51e  pypy3.9-v7.3.13-macos_arm64.tar.bz2\n180802aa0122d4a05ec480bf3130c78591ba88fdde25d8e65a92d4a798b318a3  pypy3.9-v7.3.13-macos_x86_64.tar.bz2\n213c88f652a99c4dc4e8e00b4b5b58f381c7f7e9ea1a9b65801fc0eb1e50df0a  pypy3.9-v7.3.13-s390x.tar.bz2\nbc6147268105e7cb3bd57b401e6d97f66aa4ede269104b2712a7cdd9f02f68cd  pypy3.9-v7.3.13-src.tar.bz2\n5036ba37fb07116754f3eab2df6d41f405f947ffbf8d99d62bf743dc1d2c195f  pypy3.9-v7.3.13-src.zip\n85745a2055c4a8cefac9b6d3f7f305b1edaaf62468c8f640b4511d9dd21d091c  pypy3.9-v7.3.13-win64.zip\npypy2.7-v7.3.13 sha256:\nf1e20f833cc86a097c1f1318069fc17d01c3988678c1438fe27ed567fcb5cfd0  pypy2.7-v7.3.13-aarch64.tar.bz2\nb727d2e759a740f45bab1e333029d001c4384b52949bcbb4bd2ad7912eae8dad  pypy2.7-v7.3.13-linux32.tar.bz2\ne41ceb5dc6c4d3a9311ed5f88edfeedbf3e8abbd1ed3c4f2e151a90a5cf4e1d7  pypy2.7-v7.3.13-linux64.tar.bz2\n5b86cf0750abc188a0355380d10c7bab1dec51b610cde23ce78f30a9ef296618  pypy2.7-v7.3.13-macos_arm64.tar.bz2\n50769df0091e8fa51c9e4356e0cb204e6f6aa54f502ec5a6e55aef03d0ac5675  pypy2.7-v7.3.13-macos_x86_64.tar.bz2\nfbb2f3d92831c02b094f17e9609b95a6202d4bdcddae437e380ab14388d4556e  pypy2.7-v7.3.13-s390x.tar.bz2\n976984bc6ca5ec9d37ae4e219b020cbed2751d1a02267033f59ed700ba8cec40  pypy2.7-v7.3.13-src.tar.bz2\n34976f32358349b535081d5b5d48759d6f112a31352dc11c15dcfea44bb041d8  pypy2.7-v7.3.13-src.zip\n0dc9c18f91f2aee97b95eaec2244e3b22e0183095f359c410d0090c54413dadc  pypy2.7-v7.3.13-win64.zip\npypy3.10-v7.3.12 sha256:\n26208b5a134d9860a08f74cce60960005758e82dc5f0e3566a48ed863a1f16a1  pypy3.10-v7.3.12-aarch64.tar.bz2\n811667825ae58ada4b7c3d8bc1b5055b9f9d6a377e51aedfbe0727966603f60e  pypy3.10-v7.3.12-linux32.tar.bz2\n6c577993160b6f5ee8cab73cd1a807affcefafe2f7441c87bd926c10505e8731  pypy3.10-v7.3.12-linux64.tar.bz2\n45671b1e9437f95ccd790af10dbeb57733cca1ed9661463b727d3c4f5caa7ba0  pypy3.10-v7.3.12-macos_arm64.tar.bz2\ndbc15d8570560d5f79366883c24bc42231a92855ac19a0f28cb0adeb11242666  pypy3.10-v7.3.12-macos_x86_64.tar.bz2\n043c13a585479428b463ab69575a088db74aadc16798d6e677d97f563585fee3  pypy3.10-v7.3.12-s390x.tar.bz2\n86e4e4eacc36046c6182f43018796537fe33a60e1d2a2cc6b8e7f91a5dcb3e42  pypy3.10-v7.3.12-src.tar.bz2\n191c275e3f6f2785da783cc7e951cc53cdf9df3b42d4533cd121c526e0b79991  pypy3.10-v7.3.12-src.zip\n8c3b1d34fb99100e230e94560410a38d450dc844effbee9ea183518e4aff595c  pypy3.10-v7.3.12-win64.zip\npypy3.9-v7.3.12 sha256:\ne9327fb9edaf2ad91935d5b8563ec5ff24193bddb175c1acaaf772c025af1824  pypy3.9-v7.3.12-aarch64.tar.bz2\naa04370d38f451683ccc817d76c2b3e0f471dbb879e0bd618d9affbdc9cd37a4  pypy3.9-v7.3.12-linux32.tar.bz2\n84c89b966fab2b58f451a482ee30ca7fec3350435bd0b9614615c61dc6da2390  pypy3.9-v7.3.12-linux64.tar.bz2\n0e8a1a3468b9790c734ac698f5b00cc03fc16899ccc6ce876465fac0b83980e3  pypy3.9-v7.3.12-macos_arm64.tar.bz2\n64f008ffa070c407e5ef46c8256b2e014de7196ea5d858385861254e7959f4eb  pypy3.9-v7.3.12-macos_x86_64.tar.bz2\n20d84658a6899bdd2ca35b00ead33a2f56cff2c40dce1af630466d27952f6d4f  pypy3.9-v7.3.12-s390x.tar.bz2\ne7a2046c7e6c25fc386abbb5132e92a7cc2491e3935699a946cb5dcbb342c2aa  pypy3.9-v7.3.12-src.tar.bz2\nc65e4082b6da1660041ccb23823e1cbd7759377c391f050e7c1ccad2220f08c0  pypy3.9-v7.3.12-src.zip\n0996054207b401aeacace1aa11bad82cfcb463838a1603c5f263626c47bbe0e6  pypy3.9-v7.3.12-win64.zip\npypy2.7-v7.3.12 sha256:\ne04dcb6286a7b4724ec3f0e50d3cc1ba8583301dd1658c06d7f37599e4201c59  pypy2.7-v7.3.12-aarch64.tar.bz2\nabf3ae477bd0e526ac6dcefe0bfa845e1535aa053342c0d641219bfcde4b9b56  pypy2.7-v7.3.12-linux32.tar.bz2\n1a61a2574b79466f606010f2999a2b995bd96cd085f91a78ebdd3d5c2c40e81d  pypy2.7-v7.3.12-linux64.tar.bz2\n6b747aa076ae8597e49603c5dec4ca5935a1a0a132d7404a559be96a260d9bf7  pypy2.7-v7.3.12-macos_arm64.tar.bz2\n6e89ffdd15537ce4ffce3145b65ee57c2e9c952892bd95b934012d2f009f503b  pypy2.7-v7.3.12-macos_x86_64.tar.bz2\n80c0154d8b0949f9dc6a227c322abbc9590c8ae4c9f11c13bf4022aa38b82064  pypy2.7-v7.3.12-s390x.tar.bz2\ndd61d88da274c2ce2cec77667d4a3df9a652bcc50e26f90991d4dd0af66bccf4  pypy2.7-v7.3.12-src.tar.bz2\n99cfea9862299cb043914167f4ddc69171c3f38462b6e1ab170df0aab423ca0f  pypy2.7-v7.3.12-src.zip\n84cd3b98812d47a1ddb36f3417cc96b3dbdfa32c2b4e16438f205e1253f7ccea  pypy2.7-v7.3.12-win64.zip\npypy3.10-v7.3.12rc2 sha256:\na6dc89b8100f423d5f8f5f579db3691e0ec5f14c2d92d529d70054263e202810  pypy3.10-v7.3.12rc2-aarch64.tar.bz2\n5607812d1fc9ec62956555a88b75f9178fadba090759f7c0941341b9d761e6ef  pypy3.10-v7.3.12rc2-linux32.tar.bz2\n6be46911c20152de7d317cf8b2b7c83933a18a9d4193c41e0b70810381fc8d09  pypy3.10-v7.3.12rc2-linux64.tar.bz2\n7c353cce25d76482e6b03e298891e7a5433b1c825391bc9f14b93abdd365276b  pypy3.10-v7.3.12rc2-macos_arm64.tar.bz2\n098e408004813c126f09989588d586428982278c2a79a5f216f55b29db2f05de  pypy3.10-v7.3.12rc2-macos_x86_64.tar.bz2\n2a842af10a5b1f3be97866af21a7108951c45af7b0ffb757a8e1e1ffd2c76718  pypy3.10-v7.3.12rc2-s390x.tar.bz2\nd8c51b7bb88dd1343195d088c95b4b53c704ae2c7a517ba8d8f8c728bf150683  pypy3.10-v7.3.12rc2-src.tar.bz2\ncc695d4e48bc29867e171071524d97cd4cd903ec965ee0748c3dde2b012ae36a  pypy3.10-v7.3.12rc2-src.zip\ncd3b1b409d41ea694a2d22f15afcab12305c058b8fa2a197c49e96b1c5fb776c  pypy3.10-v7.3.12rc2-win64.zip\npypy3.9-v7.3.12rc2 sha256:\n0e50aafa4e92413573cff9d579613175e5cdc128bda91a47154c9909b47c2f4c  pypy3.9-v7.3.12rc2-aarch64.tar.bz2\n37335affc962acd79fcd1f08cce19c3d2a3d2d2f6e9ba73d6c804160fd42b471  pypy3.9-v7.3.12rc2-linux32.tar.bz2\n79a3d32a21534d784f2ac4934d157354aba4871b72c39ac7908e9d853c16c3ad  pypy3.9-v7.3.12rc2-linux64.tar.bz2\n4b4adfb435c3677bf7c518413c2c53282789ceadd747bec19ed42ce0eb7192ed  pypy3.9-v7.3.12rc2-macos_arm64.tar.bz2\n3b29d34919f53136a2272363d819eb4e678368a01d5a182feae04a78a505d15d  pypy3.9-v7.3.12rc2-macos_x86_64.tar.bz2\n9d760b96db54f8d51c47c78397d70dbf61e1144de5afe6840deb3b9a7c265381  pypy3.9-v7.3.12rc2-s390x.tar.bz2\n4835d2f3814c92851f7930398f397cd0e938de165329c019d86561d9482c9daf  pypy3.9-v7.3.12rc2-src.tar.bz2\n453d84e4104216c23a466fc58f58231c051eafabf258c1c907b41ffe9955219b  pypy3.9-v7.3.12rc2-src.zip\n559fa00f89eab23c87ac2132ef30fb456631f4ff4bb8009d60900be57594dbea  pypy3.9-v7.3.12rc2-win64.zip\npypy2.7-v7.3.12rc2 sha256:\n561c6496251fbdf36ecfeaa08bc2dc89f24ef3044dde6d9f297efc798726e49d  pypy2.7-v7.3.12rc2-aarch64.tar.bz2\na66cfb8fd8a88a60bcefca14364c7e87f2932f978b81187572064e1df16c0285  pypy2.7-v7.3.12rc2-linux32.tar.bz2\n03d68b7d43751807cc4e7743a3977f2359cc4b6f0acaad00057b1b4158efe51a  pypy2.7-v7.3.12rc2-linux64.tar.bz2\n0cd0fc59894325ab30585fc2bee1d244b2b788d04e3aec46dafb0e2b3b232657  pypy2.7-v7.3.12rc2-macos_arm64.tar.bz2\n75587e171ea77ccbdcc9e0f062c9bd55bc374083ac106eeb788321dc5f031aa6  pypy2.7-v7.3.12rc2-macos_x86_64.tar.bz2\n5968a009c19bf723eda722e9ff1b95986a1b5c79247269532f99e0b25819089a  pypy2.7-v7.3.12rc2-s390x.tar.bz2\n6c69d4260554ef677d9dfb3b81a1dbd6f4d7302ef0170d1c66616865a711317f  pypy2.7-v7.3.12rc2-src.tar.bz2\na4cbe00a2bef9181929b4577c535f327021ee6af596ac0ad8d577e2a67b44a5f  pypy2.7-v7.3.12rc2-src.zip\n2bcab9251209b44eb0f7059f91c070d1de19abcfc42397e437ebe3be2faaaf5d  pypy2.7-v7.3.12rc2-win64.zip\npypy3.10-v7.3.12rc1 sha256:\n3e92ba4977c1937913c5a4cb04ee31fa809cb44d12eefcfcd5b7ef64fa2d1a45  pypy3.10-v7.3.12rc1-aarch64.tar.bz2\n889f887eada150cdbf3bfce5bb209fae90a64ad99590047c1123452431d43659  pypy3.10-v7.3.12rc1-linux32.tar.bz2\ncbc86894e22bd06f5d99dbd274dcfe0c2cacfb213f6522e06153010f40423dcc  pypy3.10-v7.3.12rc1-linux64.tar.bz2\n9e135570058c227515f742b0600c1a209f154a72247ba52073c0026f6bdc5022  pypy3.10-v7.3.12rc1-macos_arm64.tar.bz2\n3f423b794962e0ddbf95a1f40591f008e7b62a603206f65a239b25953308fbf6  pypy3.10-v7.3.12rc1-macos_x86_64.tar.bz2\n94d25c8777eff222e4cdb7419db7e49ad1b789e88fb6d59ab930e474180c74c1  pypy3.10-v7.3.12rc1-s390x.tar.bz2\n8952f17d401babd69f9bd4f7a417c19f07e1ed7bd078721eadf90f55914793e4  pypy3.10-v7.3.12rc1-src.tar.bz2\nc11b44ab9396bc6ce2a1ff5be514c27b1b327f79da6ba2cad635ea90e590ab5c  pypy3.10-v7.3.12rc1-src.zip\n2a2c285909f67984691f7861637a633c06cb30e59374744de08c0dbfbd89a151  pypy3.10-v7.3.12rc1-win64.zip\npypy3.9-v7.3.12rc1 sha256:\n4be87ceb5d522e8f0619a06660a7b68252add41b60ab4957d8f899d4893f6a15  pypy3.9-v7.3.12rc1-aarch64.tar.bz2\n0219d3353eda1526828d4b48e773d045469c0b0dafd95b0bfae72b4ef258bd02  pypy3.9-v7.3.12rc1-linux32.tar.bz2\n298ab60c5e1d56924767a4c2fcb5b3c66561c2128ca385c207193b2b3c61a5f9  pypy3.9-v7.3.12rc1-linux64.tar.bz2\n759b5d4de479b67c01df168c482f00cfdc75475f8401bfecd4f6bd7f0be2df23  pypy3.9-v7.3.12rc1-macos_arm64.tar.bz2\n5d3286920bba60af7bf8a4047b879a04302d2d0e7038965bef26f2dabd235b88  pypy3.9-v7.3.12rc1-macos_x86_64.tar.bz2\n77a27d2cde6e101b94acbc663c3c530568ed509fcdb0eaec149a195410c6efba  pypy3.9-v7.3.12rc1-s390x.tar.bz2\n7ef838e96bdd6e672868e705eb04cfbe67a5e4495e7bf374e6fc0d68fa285f7f  pypy3.9-v7.3.12rc1-src.tar.bz2\n4bf7eeb2263051838e38ff483f734994c0e1cfd2b818eddbe9e30ae8d9f6fd83  pypy3.9-v7.3.12rc1-src.zip\na78186a26590d87c48a81902a0118f6c3c70f4ef895f3ceb2fcc714a338832a7  pypy3.9-v7.3.12rc1-win64.zip\npypy2.7-v7.3.12rc1 sha256:\n79a87e1e7b3e6bd77117bedb2efe45c0de3cf9e055f688fc7a038969d058de1f  pypy2.7-v7.3.12rc1-aarch64.tar.bz2\n0aef12d0a4fe998125c3e6758325905c7b7fc9b348915c4241af89953e04fdc0  pypy2.7-v7.3.12rc1-linux32.tar.bz2\neb7f8be5f120edc29211c2ccaff4be219dcfb82030db3f667ce2c88e859217f1  pypy2.7-v7.3.12rc1-linux64.tar.bz2\n0552074ff977ea860b1989e298dd27d54f5d59e180b9b605922c0ba8becfcf6e  pypy2.7-v7.3.12rc1-macos_arm64.tar.bz2\n6dc763c8d25b00c4931e1989e09a429065b41eccf1d39cf85eb09b35846615b4  pypy2.7-v7.3.12rc1-macos_x86_64.tar.bz2\nb2a498c7d10150ad416b27be30b7ec38a61b208eecf2d58eadb6ce822e9d5ca3  pypy2.7-v7.3.12rc1-s390x.tar.bz2\n23c1ecf2b28aae2aa676a1b2eb2bdbf7db18d8718489db6d8501fb9a4b232f49  pypy2.7-v7.3.12rc1-src.tar.bz2\n60cf43bae08c87dfdd3e70be54604c6ca559c14ecf53181dc162c3befd5f8df0  pypy2.7-v7.3.12rc1-src.zip\n5f0786c0c797700458ff0cb9cfe750dd5b81a7ca3175d9ffcb55b5418b707e9c  pypy2.7-v7.3.12rc1-win64.zip\npypy3.9-v7.3.11 sha256:\n09175dc652ed895d98e9ad63d216812bf3ee7e398d900a9bf9eb2906ba8302b9  pypy3.9-v7.3.11-aarch64.tar.bz2\n0099d72c2897b229057bff7e2c343624aeabdc60d6fb43ca882bff082f1ffa48  pypy3.9-v7.3.11-linux32.tar.bz2\nd506172ca11071274175d74e9c581c3166432d0179b036470e3b9e8d20eae581  pypy3.9-v7.3.11-linux64.tar.bz2\n91ad7500f1a39531dbefa0b345a3dcff927ff9971654e8d2e9ef7c5ae311f57e  pypy3.9-v7.3.11-macos_arm64.tar.bz2\nd33f40b207099872585afd71873575ca6ea638a27d823bc621238c5ae82542ed  pypy3.9-v7.3.11-macos_x86_64.tar.bz2\ne1f30f2ddbe3f446ddacd79677b958d56c07463b20171fb2abf8f9a3178b79fc  pypy3.9-v7.3.11-s390x.tar.bz2\nb0f3166fb2a5aadfd5ceb9db5cdd5f7929a0eccca02b4a26c0dae0492f7ca8ea  pypy3.9-v7.3.11-src.tar.bz2\n3d2f473590b1390478e281a2e0d209b5df7cc9f26c33e73baecf7bd0f62bc848  pypy3.9-v7.3.11-src.zip\n57faad132d42d3e7a6406fcffafffe0b4f390cf0e2966abb8090d073c6edf405  pypy3.9-v7.3.11-win64.zip\npypy3.8-v7.3.11 sha256:\n9a2fa0b8d92b7830aa31774a9a76129b0ff81afbd22cd5c41fbdd9119e859f55  pypy3.8-v7.3.11-aarch64.tar.bz2\na79b31fce8f5bc1f9940b6777134189a1d3d18bda4b1c830384cda90077c9176  pypy3.8-v7.3.11-linux32.tar.bz2\n470330e58ac105c094041aa07bb05676b06292bc61409e26f5c5593ebb2292d9  pypy3.8-v7.3.11-linux64.tar.bz2\n78cdc79ff964c4bfd13eb45a7d43a011cbe8d8b513323d204891f703fdc4fa1a  pypy3.8-v7.3.11-macos_arm64.tar.bz2\n194ca0b4d91ae409a9cb1a59eb7572d7affa8a451ea3daf26539aa515443433a  pypy3.8-v7.3.11-macos_x86_64.tar.bz2\neab7734d86d96549866f1cba67f4f9c73c989f6a802248beebc504080d4c3fcd  pypy3.8-v7.3.11-s390x.tar.bz2\n4d6769bfca73734e8666fd70503b7ceb06a6e259110e617331bb3899ca4e6058  pypy3.8-v7.3.11-src.tar.bz2\n3e635c7d4d5ded1c5f41f7a9f277a0ee3dfd21a545516fb68e90240dca66fa07  pypy3.8-v7.3.11-src.zip\n0f46fb6df32941ea016f77cfd7e9b426d5ac25a2af2453414df66103941c8435  pypy3.8-v7.3.11-win64.zip\npypy2.7-v7.3.11 sha256:\nea924da1defe9325ef760e288b04f984614e405580f5321eb6a5c8f539bd415a  pypy2.7-v7.3.11-aarch64.tar.bz2\n30fd245fab7068c96a75b9ff1323ac55174c64fc8c4751cceb4b7a9bedc1851e  pypy2.7-v7.3.11-linux32.tar.bz2\nba8ed958a905c0735a4cfff2875c25089954dc020e087d982b0ffa5b9da316cd  pypy2.7-v7.3.11-linux64.tar.bz2\ncc5696ab4f93cd3481c1e4990b5dedd7ba60ac0602fa1890d368889a6c5bf771  pypy2.7-v7.3.11-macos_arm64.tar.bz2\n56deee9c22640f5686c35b9d64fdb1ce3abd044583e4078f0b171ca2fd2a198e  pypy2.7-v7.3.11-macos_x86_64.tar.bz2\n8fe9481c473178e53266983678684a70fe0c42bafc95f1807bf3ef28770316d4  pypy2.7-v7.3.11-s390x.tar.bz2\n1117afb66831da4ea6f39d8d2084787a74689fd0229de0be301f9ed9b255093c  pypy2.7-v7.3.11-src.tar.bz2\n6df2ddd9a925eac5294ae5a5f8916baefbc4bc3298d7cdada18fc1fa71aa0670  pypy2.7-v7.3.11-src.zip\n106942702de0df148e39fa44a33e76b8a362341e1460d4e5e61b3ff0e64e5514  pypy2.7-v7.3.11-win64.zip\npypy3.9-v7.3.10 sha256:\n657a04fd9a5a992a2f116a9e7e9132ea0c578721f59139c9fb2083775f71e514  pypy3.9-v7.3.10-aarch64.tar.bz2\nb6db59613b9a1c0c1ab87bc103f52ee95193423882dc8a848b68850b8ba59cc5  pypy3.9-v7.3.10-linux32.tar.bz2\n95cf99406179460d63ddbfe1ec870f889d05f7767ce81cef14b88a3a9e127266  pypy3.9-v7.3.10-linux64.tar.bz2\ne2a6bec7408e6497c7de8165aa4a1b15e2416aec4a72f2578f793fb06859ccba  pypy3.9-v7.3.10-macos_arm64.tar.bz2\nf90c8619b41e68ec9ffd7d5e913fe02e60843da43d3735b1c1bc75bcfe638d97  pypy3.9-v7.3.10-macos_x86_64.tar.bz2\nca6525a540cf0c682d1592ae35d3fbc97559a97260e4b789255cc76dde7a14f0  pypy3.9-v7.3.10-s390x.tar.bz2\n3738d32575ed2513e3e66878e4e4c6c208caed267570f3f9f814748830002967  pypy3.9-v7.3.10-src.tar.bz2\ne3e2c41db0a5590d31233fd2909feeb83b1e7f997a473d74a11ad87ba4bbdc30  pypy3.9-v7.3.10-src.zip\n07e18b7b24c74af9730dfaab16e24b22ef94ea9a4b64cbb2c0d80610a381192a  pypy3.9-v7.3.10-win64.zip\n\n2775f1eca62dd1eab0af09f8e4b1640b5c86f18a766ed46ff9aa7dc8aa916c13  pypy3.9-v7.3.10rc3-aarch64.tar.bz2\n68b2f1b986217475fc98bc0e5a98b4bb0c602ec1d603abbeef9ada89c9ff7048  pypy3.9-v7.3.10rc3-linux32.tar.bz2\n1cf9db691cadbf870c9af4a6af7ab89cbf24fef0469d63fd0d857656ee4adee6  pypy3.9-v7.3.10rc3-linux64.tar.bz2\nb585ab42f95aa7f0e713c6c22aba030e5d49d78ba79e8d005e754384d33cfaa4  pypy3.9-v7.3.10rc3-macos_arm64.tar.bz2\n73550941c02349c5d1051331f590962da9a0eff52e793295c1a3bd2a72dc461e  pypy3.9-v7.3.10rc3-macos_x86_64.tar.bz2\nabb736466180c3cc68ff5cd0d9b07cfabebc26989eb7fc5e9a9512e1bbe234c2  pypy3.9-v7.3.10rc3-s390x.tar.bz2\na313e85a073f3a4d9c592e142e69c856b40afd29473665d7f41fe07d50ecbad2  pypy3.9-v7.3.10rc3-src.tar.bz2\n6f5ead6ccdf7544eb5a7e33e352a361bfd19f6bfcd91f9e121843b4e2ae9c590  pypy3.9-v7.3.10rc3-src.zip\nf5ae260d8557d7380d595c93ccd2b7bbaff718d8dd82051034444479a89e1c37  pypy3.9-v7.3.10rc3-win64.zip\npypy3.8-v7.3.10 sha256:\ne4caa1a545f22cfee87d5b9aa6f8852347f223643ad7d2562e0b2a2f4663ad98  pypy3.8-v7.3.10-aarch64.tar.bz2\nb70ed7fdc73a74ebdc04f07439f7bad1a849aaca95e26b4a74049d0e483f071c  pypy3.8-v7.3.10-linux32.tar.bz2\nceef6496fd4ab1c99e3ec22ce657b8f10f8bb77a32427fadfb5e1dd943806011  pypy3.8-v7.3.10-linux64.tar.bz2\n6cb1429371e4854b718148a509d80143f801e3abfc72fef58d88aeeee1e98f9e  pypy3.8-v7.3.10-macos_arm64.tar.bz2\n399eb1ce4c65f62f6a096b7c273536601b7695e3c0dc0457393a659b95b7615b  pypy3.8-v7.3.10-macos_x86_64.tar.bz2\nc294f8e815158388628fe77ac5b8ad6cd93c8db1359091fa02d41cf6da4d61a1  pypy3.8-v7.3.10-s390x.tar.bz2\n218a1e062f17aba89f61bc398e8498f13c048b9fcf294343f5d9d56c3ac9b882  pypy3.8-v7.3.10-src.tar.bz2\n0e4dd55729a2bf8c9bf963c769004b287ef57576ddb402e71e387847a7c31c0a  pypy3.8-v7.3.10-src.zip\n362dd624d95bd64743190ea2539b97452ecb3d53ea92ceb2fbe9f48dc60e6b8f  pypy3.8-v7.3.10-win64.zip\n\nd7feab3fd0e670dc66277ad710d2a26dd5ec3def68cb4fdf2697e570b74ab62e  pypy3.8-v7.3.10rc3-aarch64.tar.bz2\n4a33b7e08033527e9f8c6dc2a3d6a8d0163c381b9e75813cfe1a7865caf335ae  pypy3.8-v7.3.10rc3-linux32.tar.bz2\n7ab218ab7f05a156ad3ea3b498e6da94dd7e7e77dfe03ee77e5827af755a6207  pypy3.8-v7.3.10rc3-linux64.tar.bz2\nd77a5f94690e8e74d3ae57d1f65ef657c670614559447a196da001de943e1fa5  pypy3.8-v7.3.10rc3-macos_arm64.tar.bz2\nfa15127affd9dbc6d447cf48a99fe4795423132070b84b802d2dc8cbecd9607e  pypy3.8-v7.3.10rc3-macos_x86_64.tar.bz2\n8d3e07840be537b6b879add1b34a082dde156f7c2a8c5d75be60e9192393533d  pypy3.8-v7.3.10rc3-s390x.tar.bz2\n5284dfba00f4ffcdf29b732cf7f2e63f29d1f33295f826a2caefb1f782cedaef  pypy3.8-v7.3.10rc3-src.tar.bz2\nd8a2992734463e8db5ca4209c5ce7f9fcc2965f9fbd975cb04a4e173b6d2411b  pypy3.8-v7.3.10rc3-src.zip\nfab16618e7adf8c268c7f48032f51d6d4985734d672d18712fe8b557fe9c9abe  pypy3.8-v7.3.10rc3-win64.zip\npypy2.7-v7.3.10 sha256:\n274342f0e75e99d60ba7a0cfb0e13792e7664163e01450d2f7f2f7825603a0ae  pypy2.7-v7.3.10-aarch64.tar.bz2\n0b17132f62d2a0c3c4572c57eb53820f25611afad71f3d6a310202942baed6e1  pypy2.7-v7.3.10-linux32.tar.bz2\n461fb6df524208af9e94ffb16989f628b585bdb4b9e97d81e668899fc3a064a3  pypy2.7-v7.3.10-linux64.tar.bz2\n14b178f005603e3df6db7574b77b9c65ae79feda1a629214cafcb4eee7da679d  pypy2.7-v7.3.10-macos_arm64.tar.bz2\n188551185ee945d5e42a3a619205d02ac31db77bdd5d98b6c11469e125c3bdb5  pypy2.7-v7.3.10-macos_x86_64.tar.bz2\n0fac1ec1e05c70941f758be05d40ce7ffe6a42c0416e70b55d40a7523e3e70ae  pypy2.7-v7.3.10-s390x.tar.bz2\n35e2cf4519cb51c4d5ffb4493ee24f0c7f42b4b04944903ca4b33981a04a3bc5  pypy2.7-v7.3.10-src.tar.bz2\nece8975f49b192cc6e3169301a3c3ef71822cc7b52e70d7d8b506f54f917e14e  pypy2.7-v7.3.10-src.zip\n2915b5201a5f71546951bc41efd80f40b2ed709511bc526219a70f3ae37b918e  pypy2.7-v7.3.10-win64.zip\n\n85f0b2f0bffe9a9a0fe17382c25f595be7c7ca9a4d070eaf98cb4258bdc8f703  pypy2.7-v7.3.10rc3-aarch64.tar.bz2\n38f0fe020ac7880ae4e843d2cacdfcceecd0d7dca5fd2769f13b60a1e6bf8e86  pypy2.7-v7.3.10rc3-linux32.tar.bz2\ne6d7330c16f503e1c21dacb22c525974f1d81fea86ef32e0d21239d9d372b4d5  pypy2.7-v7.3.10rc3-linux64.tar.bz2\n5f62122884e87b263ce3f416513e1f380276fc327570cff07daac864907b1d1e  pypy2.7-v7.3.10rc3-macos_arm64.tar.bz2\n6de0c73285378cae79ee92566e38296e91382cd5df0322224d006dd2e2429489  pypy2.7-v7.3.10rc3-macos_x86_64.tar.bz2\n0c350a480a928c9ed0fca0a531f333946269c32f9673c9d461772c48eccc5380  pypy2.7-v7.3.10rc3-s390x.tar.bz2\n2514df50aeb2dafd8fd13b299dd3a1a30986e5e396a7ea253410d3126b7ad245  pypy2.7-v7.3.10rc3-src.tar.bz2\ndbd30ad54104ffb9ada8717cec068958b15c4ad9a22e37b192acdd1495e9ec44  pypy2.7-v7.3.10rc3-src.zip\nf95114991fbe1bc6aa87466a62efbba6d6e4e1a8c95b5efd43a402ece0371357  pypy2.7-v7.3.10rc3-win64.zip\npypy3.9-v7.3.9 sha256:\n2e1ae193d98bc51439642a7618d521ea019f45b8fb226940f7e334c548d2b4b9  pypy3.9-v7.3.9-aarch64.tar.bz2\n0de4b9501cf28524cdedcff5052deee9ea4630176a512bdc408edfa30914bae7  pypy3.9-v7.3.9-linux32.tar.bz2\n46818cb3d74b96b34787548343d266e2562b531ddbaf330383ba930ff1930ed5  pypy3.9-v7.3.9-linux64.tar.bz2\n59c8852168b2b1ba1f0211ff043c678760380d2f9faf2f95042a8878554dbc25  pypy3.9-v7.3.9-osx64.tar.bz2\n774dca83bcb4403fb99b3d155e7bd572ef8c52b9fe87a657109f64e75ad71732  pypy3.9-v7.3.9-s390x.tar.bz2\n2abaa1e9fe1ec0e233c9fbc377a0c8e9a0634080a8f4f30eb6898301f6618c12  pypy3.9-v7.3.9-src.tar.bz2\n1c67e33882052ab53e464e398898abefd6df7ff7127bf754be88bb17938759f2  pypy3.9-v7.3.9-src.zip\nbe48ab42f95c402543a7042c999c9433b17e55477c847612c8733a583ca6dff5  pypy3.9-v7.3.9-win64.zip\npypy3.8-v7.3.9 sha256:\n5e124455e207425e80731dff317f0432fa0aba1f025845ffca813770e2447e32  pypy3.8-v7.3.9-aarch64.tar.bz2\n4b261516c6c59078ab0c8bd7207327a1b97057b4ec1714ed5e79a026f9efd492  pypy3.8-v7.3.9-linux32.tar.bz2\n08be25ec82fc5d23b78563eda144923517daba481a90af0ace7a047c9c9a3c34  pypy3.8-v7.3.9-linux64.tar.bz2\n91a5c2c1facd5a4931a8682b7d792f7cf4f2ba25cd2e7e44e982139a6d5e4840  pypy3.8-v7.3.9-osx64.tar.bz2\nc6177a0016c9145c7b99fddb5d74cc2e518ccdb216a6deb51ef6a377510cc930  pypy3.8-v7.3.9-s390x.tar.bz2\n5b5d9d9256f12a129af8384e2f581bdfab3bc0fbbe3a0a480d9c1d2e95490eb1  pypy3.8-v7.3.9-src.tar.bz2\nd4f716f324ebbd7ec3c0e0e309c2d7dd76846f693f50b7796820acf346147401  pypy3.8-v7.3.9-src.zip\n05022baaa55db2b60880f2422312d9e4025e1267303ac57f33e8253559d0be88  pypy3.8-v7.3.9-win64.zip\npypy3.7-v7.3.9 sha256:\ndfc62f2c453fb851d10a1879c6e75c31ffebbf2a44d181bb06fcac4750d023fc  pypy3.7-v7.3.9-aarch64.tar.bz2\n3398cece0167b81baa219c9cd54a549443d8c0a6b553ec8ec13236281e0d86cd  pypy3.7-v7.3.9-linux32.tar.bz2\nc58195124d807ecc527499ee19bc511ed753f4f2e418203ca51bc7e3b124d5d1  pypy3.7-v7.3.9-linux64.tar.bz2\n12d92f578a200d50959e55074b20f29f93c538943e9a6e6522df1a1cc9cef542  pypy3.7-v7.3.9-osx64.tar.bz2\nfcab3b9e110379948217cf592229542f53c33bfe881006f95ce30ac815a6df48  pypy3.7-v7.3.9-s390x.tar.bz2\n70426163b194ee46009986eea6d9426098a3ffb552d9cdbd3dfaa64a47373f49  pypy3.7-v7.3.9-src.tar.bz2\n3643392817cfd0826f70be3d026c2f119904b2bfb40c39c32bad84f5a6aa02f5  pypy3.7-v7.3.9-src.zip\n8acb184b48fb3c854de0662e4d23a66b90e73b1ab73a86695022c12c745d8b00  pypy3.7-v7.3.9-win64.zip\npypy2.7-v7.3.9 sha256:\naff4e4dbab53448f662cd01acb2251571d60f836d2f48382a7d8da54ca5b3442  pypy2.7-v7.3.9-aarch64.tar.bz2\nbbf4e7343d43c8217099a9bffeed6a1781f4b5a3e186ed1a0befca65e647aeb9  pypy2.7-v7.3.9-linux32.tar.bz2\n172a928b0096a7e00b7d58f523f57300c35c3de7f822491e2a7bc845375c23f8  pypy2.7-v7.3.9-linux64.tar.bz2\n77314f5a6b2cc35d24e6f952bef89f5da612b90e4127a8034aed708d9ae483c4  pypy2.7-v7.3.9-osx64.tar.bz2\n62481dd3c6472393ca05eb3a0880c96e4f5921747157607dbaa772a7369cab77  pypy2.7-v7.3.9-s390x.tar.bz2\n39b0972956f6548ce5828019dbae12503c32d6cbe91a2becf88d3e42cc52197b  pypy2.7-v7.3.9-src.tar.bz2\n3400e6b03cfcecd0a2f90271e4dd44e5fe862c7bf82a43535114ad57b57af555  pypy2.7-v7.3.9-src.zip\nca7b0f4c576995b388cfb4c796e3f6f20b037e5314571bf267daa068a3a2af31  pypy2.7-v7.3.9-win64.zip\npypy3.9-v7.3.8 sha256:\n89d7ee12a8c416e83fae80af82482531fc6502321e75e5b7a0cc01d756ee5f0e  pypy3.9-v7.3.8-aarch64.tar.bz2\nb7282bc4484bceae5bc4cc04e05ee4faf51cb624c8fc7a69d92e5fdf0d0c96aa  pypy3.9-v7.3.8-aarch64-portable.tar.bz2\na0d18e4e73cc655eb02354759178b8fb161d3e53b64297d05e2fff91f7cf862d  pypy3.9-v7.3.8-linux32.tar.bz2\n129a055032bba700cd1d0acacab3659cf6b7180e25b1b2f730e792f06d5b3010  pypy3.9-v7.3.8-linux64.tar.bz2\n95bd88ac8d6372cd5b7b5393de7b7d5c615a0c6e42fdb1eb67f2d2d510965aee  pypy3.9-v7.3.8-osx64.tar.bz2\n37b596bfe76707ead38ffb565629697e9b6fa24e722acc3c632b41ec624f5d95  pypy3.9-v7.3.8-s390x.tar.bz2\n546b7fc3789728869d5ada7b6a95ce9d03047e8489b92ada84613c900e431ee9  pypy3.9-v7.3.8-src.tar.bz2\nc5cece54ce0444943ae43fe672b13b21b3915d1e71ac730589de8204ec6f417a  pypy3.9-v7.3.8-src.zip\nc1b2e4cde2dcd1208d41ef7b7df8e5c90564a521e7a5db431673da335a1ba697  pypy3.9-v7.3.8-win64.zip\n\n81c58e0c0eb0f76801d0ac8cb528dd8a0b1e4138a4062e3e64e71beeadeccb79  pypy3.9-v7.3.8rc2-linux32.tar.bz2\n22ec1af269d68f7288a48f49ca58cb55fb9cb78f6ae58341cd13484838327751  pypy3.9-v7.3.8rc2-linux64.tar.bz2\nb49e569944f712f257e7557e61e21b36b388c9af09ce8a09085e93a51a8e3f95  pypy3.9-v7.3.8rc2-osx64.tar.bz2\n47824c665d7992dafbe8f00749f72b606bc3478c80adaaea340100f349e7b207  pypy3.9-v7.3.8rc2-s390x.tar.bz2\n53d47b101a6ff31b07b79429b0cf62e06efb29c3147799ab5aaac270ff17581b  pypy3.9-v7.3.8rc2-src.tar.bz2\nc84e8094ecca6f90930d527e2c2ca6c37d1da6009ba16d8eef4d02d02a5b05b5  pypy3.9-v7.3.8rc2-src.zip\nb118fd06197e1218917fa9577874d6bc31a7488f057d5000377c63ee6cd0beca  pypy3.9-v7.3.8rc2-win64.zip\n\n89dd0399a89a04b58c22e9b773747258807996bd5071dbf996a85bf8af432393  pypy3.9-v7.3.8rc1-linux32.tar.bz2\nf3f90203afcf7ee359e8c8a871bfaa06d96f926781fd94fb81f471dcd32f7332  pypy3.9-v7.3.8rc1-linux64.tar.bz2\n9a5d7217d8173bbdf1c7351b34651fee0596b0bcfe6fe4becae150d4a5469487  pypy3.9-v7.3.8rc1-osx64.tar.bz2\n4651d804341046be824af0ca35b7ebbbb6d5cdcef0d4a373891398dba182d010  pypy3.9-v7.3.8rc1-src.tar.bz2\nc4db62a854c2cc994d46fac0105a8e3bd4273093b9844c1f7cb69118fae6df72  pypy3.9-v7.3.8rc1-src.zip\nad214e4a44c893dc503e7e0b6f6bdfa7523db80b9d4890523f8ee96339d05fc9  pypy3.9-v7.3.8rc1-win64.zip\npypy3.8-v7.3.8 sha256:\nfe41df391f87239925e573e195e631a9d03d37f471eb1479790ee13ca47a28af  pypy3.8-v7.3.8-aarch64.tar.bz2\n0210536e9f1841ba283c13b04783394050837bb3e6f4091c9f1bd9c7f2b94b55  pypy3.8-v7.3.8-aarch64-portable.tar.bz2\nbea4b275decd492af6462157d293dd6fcf08a949859f8aec0959537b40afd032  pypy3.8-v7.3.8-linux32.tar.bz2\n089f8e3e357d6130815964ddd3507c13bd53e4976ccf0a89b5c36a9a6775a188  pypy3.8-v7.3.8-linux64.tar.bz2\nde1b283ff112d76395c0162a1cf11528e192bdc230ee3f1b237f7694c7518dee  pypy3.8-v7.3.8-osx64.tar.bz2\nad53d373d6e275a41ca64da7d88afb6a17e48e7bfb2a6fff92daafdc06da6b90  pypy3.8-v7.3.8-s390x.tar.bz2\nf1a378b264cdbfb0e03d77dfc4d105d02f91d542bd7c9c957d1f8083a9808f1f  pypy3.8-v7.3.8-src.tar.bz2\n7abf870044c95b31c8e1a0a32e887485b56f3c0a3151401446b113a0a65111b4  pypy3.8-v7.3.8-src.zip\n0894c468e7de758c509a602a28ef0ba4fbf197ccdf946c7853a7283d9bb2a345  pypy3.8-v7.3.8-win64.zip\n\n475883e59f6d2a90d273142da27f999a227d510f51b7cdec3f53ceaf832b6b4b  pypy3.8-v7.3.8rc2-linux32.tar.bz2\n141abedd8f0f46f61d9f05243c4fe32a88c6d9f2219cd3cd6a1312f56d4bd5eb  pypy3.8-v7.3.8rc2-linux64.tar.bz2\n3bd390bfa30f4225cc379d592c822b9bb2dea9530451904fa215b8649d614375  pypy3.8-v7.3.8rc2-osx64.tar.bz2\n735751d124140cb75c24848199230fe41110761fcb830ba2a253baa5846ec86f  pypy3.8-v7.3.8rc2-s390x.tar.bz2\n0ae9515b964865d5946bb48c41e1248cac00ba6f145f10ff230163f4a3c47c91  pypy3.8-v7.3.8rc2-src.tar.bz2\n973ec5dab8b1243b71d25acca4d6db3d1545e62e0984a5d43d407052e4767662  pypy3.8-v7.3.8rc2-src.zip\n089cbb1491eaf921bf905dc79936a95a90b0b5a06ebde3e26d1d2e98bdd2dcdd  pypy3.8-v7.3.8rc2-win64.zip\n\n56b62c57df91b4a04036535a94814da3c682ac5208d4a565f230fbc657d949e3  pypy3.8-v7.3.8rc1-linux32.tar.bz2\nfac68364acdebed2a11f6d5a62fc10e7c44985bfe9baafdb991f65e25a375998  pypy3.8-v7.3.8rc1-linux64.tar.bz2\ned62e2f5e25bda752463e2acd881de5876ccd383ce3589630b880de204d8ad75  pypy3.8-v7.3.8rc1-osx64.tar.bz2\n70aa9380fe19a3694d38aab92d46b96427dd8a98952a4d4637043739a485be4f  pypy3.8-v7.3.8rc1-src.tar.bz2\n9abb90bc11c5ba53aa7f8c23ab95eba864bb253082d23aa8552d23b322ecef85  pypy3.8-v7.3.8rc1-src.zip\n6a4d2405adc13c68140a48492178829a11ff8d3a22a27b9730166486be2688d0  pypy3.8-v7.3.8rc1-win64.zip\npypy3.7-v7.3.8 sha256:\n4fb2f8281f3aaca72e6fe62ecc5fc054fcc79cd061ca3e0eea730f7d82d610d4  pypy3.7-v7.3.8-aarch64.tar.bz2\n639c76f128a856747aee23a34276fa101a7a157ea81e76394fbaf80b97dcf2f2  pypy3.7-v7.3.8-aarch64-portable.tar.bz2\n38429ec6ea1aca391821ee4fbda7358ae86de4600146643f2af2fe2c085af839  pypy3.7-v7.3.8-linux32.tar.bz2\n409085db79a6d90bfcf4f576dca1538498e65937acfbe03bd4909bdc262ff378  pypy3.7-v7.3.8-linux64.tar.bz2\n76b8eef5b059a7e478f525615482d2a6e9feb83375e3f63c16381d80521a693f  pypy3.7-v7.3.8-osx64.tar.bz2\n5c2cd3f7cf04cb96f6bcc6b02e271f5d7275867763978e66651b8d1605ef3141  pypy3.7-v7.3.8-s390x.tar.bz2\n35752be62b148fa6f7fb69e58e1f993c7cc319bea54928eb03ed2e75b8248d5f  pypy3.7-v7.3.8-src.tar.bz2\n089fd12039ef92256fc218fc45652a93bbef1f5291181d07a4b55dad3f6987b9  pypy3.7-v7.3.8-src.zip\n96df67492bc8d62b2e71dddf5f6c58965a26cac9799c5f4081401af0494b3bcc  pypy3.7-v7.3.8-win64.zip\n\na85189cdbf717928a4c5c90f05ccf48668e38291d2ac438e644d06aa6fa1fb7e  pypy3.7-v7.3.8rc2-linux32.tar.bz2\nb8fe346d90561f34db1f23b0213ce247c148b7922d3b9acbfb7fdb1824c708b0  pypy3.7-v7.3.8rc2-linux64.tar.bz2\n480ad018194096736c47a2735ad453bbc0bd60117e7326508a723befe9543c28  pypy3.7-v7.3.8rc2-osx64.tar.bz2\nebc8d34d5b4c546cb2bdb22a848def94b07d23cc6833fd54b76226eb658126a2  pypy3.7-v7.3.8rc2-s390x.tar.bz2\n2d3059daaaaae35ffd70387b37e9bfe91224a24951be20e5edfbe836300fbdb3  pypy3.7-v7.3.8rc2-src.tar.bz2\n25df8cfc7510470c525e35d4a465499d0284ea4a895b08a1f75de3fb3a1698b3  pypy3.7-v7.3.8rc2-src.zip\n3fe66039537920d141cd5fca018e9778e283613dd791dab41122223224585db0  pypy3.7-v7.3.8rc2-win64.zip\n\n6db124cda7eb9ee54dbdaf8e5edc052bc32bd59c1a535faf34b175e3e5cd855d  pypy3.7-v7.3.8rc1-linux32.tar.bz2\n9f239262bcf31609b758a70dcf3c8aba4bfa9d1639285afba707414639ee5871  pypy3.7-v7.3.8rc1-linux64.tar.bz2\ned208dac960650f52c69cfc38d17af5e978acd1ad6f09de6aaac1603dea32ffa  pypy3.7-v7.3.8rc1-osx64.tar.bz2\n9c2ec87b0c827f9d37ce7c11a9b7b4c1cc9a2182b7f86a1bb36ee209dffda49d  pypy3.7-v7.3.8rc1-src.tar.bz2\n4cc32f99e4dbda8a20f1b9e0e95cdba59963a173e00a02baa574e4d00739b58f  pypy3.7-v7.3.8rc1-src.zip\n6eb5a637534dbcaa496208061ad19faf5f4413c941a450e091e22ef49e3af9ec  pypy3.7-v7.3.8rc1-win64.zip\npypy2.7-v7.3.8 sha256:\nca1f8d3146c83002ee97615906b0930e821297dcce3063b5b28933a0690ef298  pypy2.7-v7.3.8-aarch64.tar.bz2\nb5edfc995d83feea8b4c8aeffccb89753b4b182f076126550bd07cc35faa6208  pypy2.7-v7.3.8-aarch64-portable.tar.bz2\n7c84f173bbcd73d0eb10909259d11b5cc253d4c6ea4492e6da8f2532df9b3da5  pypy2.7-v7.3.8-linux32.tar.bz2\n1f2e84fb539ffce233c34769d2f11647955f894be091e85419e05f48011e8940  pypy2.7-v7.3.8-linux64.tar.bz2\ne5c1ff39ad9916ea23e3deb8012fe42367b6b19284cf13b1a1ea2b2f53a43add  pypy2.7-v7.3.8-osx64.tar.bz2\nb4ae4e708ba84602d976ad6ae391ef2eef4b1896d831b8f2b2ec69927dd92014  pypy2.7-v7.3.8-s390x.tar.bz2\n0cdad270c62d3ccc53cc87eeb069a6dc46acaf95521b584624bcd6697d94fa1c  pypy2.7-v7.3.8-src.tar.bz2\n13f70c6a0d4e5a59eb368c11d6b581ae09aa9715f96f84b890c5c9fa24cdaa93  pypy2.7-v7.3.8-src.zip\n806a29a6c5550b1e669d8870683d3379138d3d43eb1e07bdf26d65a0691265f2  pypy2.7-v7.3.8-win64.zip\n\n3e9744307a60740191341df2b4feb42ca08452eff354156322b760e1aac3ef54  pypy2.7-v7.3.8rc2-linux32.tar.bz2\na13ceb4a881a8da75475feea3d55dc337b7e2c6cf58e1e33924fa17012ace4e5  pypy2.7-v7.3.8rc2-linux64.tar.bz2\n6413048a6ab1ec5d7702a08f482443be0604a6f2019f32024a35e27c42ed7210  pypy2.7-v7.3.8rc2-osx64.tar.bz2\nb015012ac2f72a3971d4b4691df2a6f2dc478f2abb2252dec79ad2b4c66c18ed  pypy2.7-v7.3.8rc2-s390x.tar.bz2\n8b08ace5f402fe7b8b18416082534d2463409b6891ffa426a6989448c5d95064  pypy2.7-v7.3.8rc2-src.tar.bz2\nb507dac295d94972c62c1faf2206db6333993df60864d0c23be0206d8560e278  pypy2.7-v7.3.8rc2-src.zip\n270d289a6b32a83db1e0b1078801b2f36fce6d12e238346a2b8354bf31a64e1e  pypy2.7-v7.3.8rc2-win64.zip\n\n5ab938f2b0cff62be3869076f1fb99c859ef2df165ed33d329e2de4d32aaafef  pypy2.7-v7.3.8rc1-linux32.tar.bz2\n124de0f3d327e39e0344b70d71298315714fe0b1115db80b463dda06bd618c58  pypy2.7-v7.3.8rc1-linux64.tar.bz2\n183a9c0aa5c9ced4ce071ddedf6ae203a752574f06e96722077eb5708f583405  pypy2.7-v7.3.8rc1-osx64.tar.bz2\n96c9f5a85759cc92000064d3b32ce89748870b35a48e631f713be3f29bf64f3c  pypy2.7-v7.3.8rc1-src.tar.bz2\na11e32d93da35a5ab7bf0a6cd37abce4f1697ef22c0bb46957f2360526c20c7b  pypy2.7-v7.3.8rc1-src.zip\ne3b2e88b5785538ac3f7bccf3122e400b7d42f3871201fbfb2110b9eb93473be  pypy2.7-v7.3.8rc1-win64.zip\npypy3.8-v7.3.7 sha256:\ncbd44e0a9146b3c03a9d14b265774a848f387ed846316c3e984847e278d0efd3  pypy3.8-v7.3.7-aarch64.tar.bz2\ndfb9d005f0fc917edc60fd618143e4934c412f9168b55166f5519ba0a3b1a835  pypy3.8-v7.3.7-linux32.tar.bz2\n5dee37c7c3cb8b160028fbde3a5901c68043dfa545a16794502b897d4bc40d7e  pypy3.8-v7.3.7-linux64.tar.bz2\n1f044fe7bbdd443b7913ecf554683dab6dade5dcd7f47d4e6d01f4bb4cf84836  pypy3.8-v7.3.7-osx64.tar.bz2\nae7d6a76490b317a74b87788d596610c7ffd0ae2d3ffa2433d5bb5300f6b4b77  pypy3.8-v7.3.7-s390x.tar.bz2\n21ae339f4f5016d6ca7300305f3e3b554373835cb3c39a9041fe30e6811c80c6  pypy3.8-v7.3.7-src.tar.bz2\naa9aa0a800d06048d301fbafa7892ff8978e2d63b23cc23a147f2fd1fd288baf  pypy3.8-v7.3.7-src.zip\n8ceb03d2f7b73c6ce0758290bc42ba366a45c46e033eda36f1779d957a905735  pypy3.8-v7.3.7-win64.zip\npypy3.7-v7.3.7 sha256:\na1a84882525dd574c4b051b66e9b7ef0e132392acc2f729420d7825f96835216  pypy3.7-v7.3.7-aarch64.tar.bz2\n0ab9e2e8ae1ac463bb811b9d3ba24d138f41f7378c17ca9e2d8dee51bf151d19  pypy3.7-v7.3.7-linux32.tar.bz2\n8332f923755441fedfe4767a84601c94f4d6f8475384406cb5f259ad8d0b2002  pypy3.7-v7.3.7-linux64.tar.bz2\nedc9df7d0f7c56f7ee05b24117bdb6c03aa65e768471e210c05ccdbbfd11a866  pypy3.7-v7.3.7-osx64.tar.bz2\n7f91efc65a69e727519cc885ca6351f4bfdd6b90580dced2fdcc9ae1bf10013b  pypy3.7-v7.3.7-s390x.tar.bz2\n2ed02ac9e710859c41bc82deafb08619792bb9a27eeaa1676c741ededd214dd7  pypy3.7-v7.3.7-src.tar.bz2\n240ecf56c50b190cc7b728b07fc535be4b3d70a65406d0d8440edc02df4cce17  pypy3.7-v7.3.7-src.zip\n53505dc0b57590290efd7656117ee5384bcd036f7f7c4f0bc3f5cd10299037d1  pypy3.7-v7.3.7-win64.zip\npypy3.8-v7.3.6 sha256:\n704d5303096e8a3173e73435f3bb204e31a8bf02ed5ba617a4a0f1e7491edf50  pypy3.8-v7.3.6-aarch64.tar.bz2\ne857a04a76285f0ef5bae84f6f5e9943ca415d499204c531b1c33fe8f015b48d  pypy3.8-v7.3.6-linux32.tar.bz2\n8579ea990e95d2b7e101ef47fd9ebf25a9500d5086e8f708c43f9bae83306ece  pypy3.8-v7.3.6-linux64.tar.bz2\n8195e52a20cf2a4f42c2d7e4969fbf44fe349c1f80f758e20525dd0f8c134bec  pypy3.8-v7.3.6-osx64.tar.bz2\na36208d5e950ec4b630b33d0aede8ca3da383d973fc5ca387082c7e5bad8d245  pypy3.8-v7.3.6-s390x.tar.bz2\nf234c56eb0d4ab0afb196232fb38cd1ca8e19b1c65cf7b65eb691695499be259  pypy3.8-v7.3.6-src.tar.bz2\n055caaab4171e29915aaad602c9a49fa46e2b50a3f56c650772e31467c541858  pypy3.8-v7.3.6-src.zip\n1b216fd75f8f0a48633cc21dce7d6f25ba65016142df758842e1df661269b458  pypy3.8-v7.3.6-win64.zip\n\n\n59c299e9657334d651e2154c77490a743cb507f4f39344f934b2975ca91b4b2f  pypy3.8-v7.3.6rc3-aarch64.tar.bz2\n6cd36eb9857d6f7022099300c70666eb706f1e06b404234ea929a341fee40b68  pypy3.8-v7.3.6rc3-linux32.tar.bz2\nacdbc39ade2ef2cf2b4bcf0eb387ec0ef0d257175751d32e9d730886405439d0  pypy3.8-v7.3.6rc3-linux64.tar.bz2\n18fdba4a6c54c7df6fe2521858046ba865261c0e89557c4b53ef37eb7e562806  pypy3.8-v7.3.6rc3-osx64.tar.bz2\n128ede0f5565b626431755d58eb632362c748508e53777d32184eba5da8fdb6d  pypy3.8-v7.3.6rc3-s390x.tar.bz2\n0cb9c517a96850c4fba0494ee10b35e87861d71d8b1387e0588c316fa21230ee  pypy3.8-v7.3.6rc3-src.tar.bz2\n54704168785a6b22580d46a4a39f5a2c3f81e5d9f0c8e5ba906ac01603d42cbf  pypy3.8-v7.3.6rc3-src.zip\n1bd65ab6c82a696f2dcecd9b37679b474eadd149d96aab30438642236a1f7136  pypy3.8-v7.3.6rc3-win64.zip\n\n8ec2b28c6f1558a6abd0ce0a6fb504253b43b013a750c08c1e74470631afc1dd  pypy3.8-v7.3.6rc2-aarch64.tar.bz2\n008e9a9336108821f0080011aafe54a71e42ffffb7223d5183e610f689a0f8aa  pypy3.8-v7.3.6rc2-linux32.tar.bz2\nb1069fc7b08c2a230630f55f155c3ea016038471490ff0be020f850c5a8ec0cc  pypy3.8-v7.3.6rc2-linux64.tar.bz2\n4298d6b1a8333746c43dd313eb6ccd64f11b3dde795921d07f02c8e32d1ac44b  pypy3.8-v7.3.6rc2-osx64.tar.bz2\n9f3f7bb2842e626a85c8b314a3af959f98dc4a57fc0169c98b566b6fe645ea39  pypy3.8-v7.3.6rc2-s390x.tar.bz2\na9c3835e37e84a7667e3e548a176986a77663612d30594c7c4877ce0e712c6c9  pypy3.8-v7.3.6rc2-src.tar.bz2\ncae1f0a13b0da3b9db87141e662c3db73564f8fa4e4f1dab2d838341bf8bacc1  pypy3.8-v7.3.6rc2-src.zip\n6415bfd8afb6cef9cd7666de60f58d7fbbabae92042a9c1f3ce5e8ffe9ba4a26  pypy3.8-v7.3.6rc2-win64.zip\n\n18308f227c02ecb84ad21ed4a51bba8472acafe20386caef7ada0058d2d5a243  pypy3.8-v7.3.6rc1-aarch64.tar.bz2\n9b16a894477cbdb1275ab253d7bc71e8d64ad7d12dd61c835242fdac2cdf6cc7  pypy3.8-v7.3.6rc1-linux32.tar.bz2\n2abcd2a21f17216613c941a6bf6e26b395b089b9aa8f227af9e1b55c86d6d732  pypy3.8-v7.3.6rc1-linux64.tar.bz2\nd3aebc5c862e223606e3a79c245a748da7b9aa7d0206a2400e6c7d906676ef34  pypy3.8-v7.3.6rc1-osx64.tar.bz2\ne5013c21d21ca0eb16bc2e12c4093ec3095150b606830fb10f0c588629412b37  pypy3.8-v7.3.6rc1-s390x.tar.bz2\n999747cb4eacbc23c14e9f71d42c784c35cf45b52a7de9113c6db0811300e526  pypy3.8-v7.3.6rc1-src.tar.bz2\n3c9010fb3d1074c1ac350f0dbc8b215c53b2ab8ca3440d9ca4e903800e2ef1ce  pypy3.8-v7.3.6rc1-src.zip\ncef32837d4ab2cd9fbb6173472b633c6996f6a7915d89c66f87f0f0c69edcda2  pypy3.8-v7.3.6rc1-win64.zip\npypy3.7-v7.3.6 sha256:\nd446b6987eeaa03d706603863e83d6b99df69232cf1e06d3ee5706add6a84cd6  pypy3.7-v7.3.6-aarch64.tar.bz2\n459e77c845b31fa9367f7b1b1122155f0ba7888b1d4ce4455c35d2111eeeb275  pypy3.7-v7.3.6-linux32.tar.bz2\nc41d07063b1d002a91ad2a0763b4baaca2b306ec635889c2e4826e706cc7f9ca  pypy3.7-v7.3.6-linux64.tar.bz2\n26f0c5c2a5f4a2ce35281d2fa760aa10715300dd110387eac43699a78ed32365  pypy3.7-v7.3.6-osx64.tar.bz2\n3659bf96a177a53426ffc38d3619c6ee307e600c80e924edc9cee604680c141d  pypy3.7-v7.3.6-s390x.tar.bz2\n9252ccaa130094205b3c7f0a2cad5adc0d9dfba31658ff3172f788dec1fdb348  pypy3.7-v7.3.6-src.tar.bz2\nc2385436004d7d8d8978650efff1c22512ed9f9808c83ddfd68fe8fe812eb879  pypy3.7-v7.3.6-src.zip\n341e69a369da5a1f4f69dbbd47e7dff5e745439b203e28c7afcf98308a24b003  pypy3.7-v7.3.6-win64.zip\n\n742fc6fa7bdc377e8a8c976f57ef643a9068a0427a5ffbb50f8ba32aa6986392  pypy3.7-v7.3.6rc3-aarch64.tar.bz2\nb5382404935dd09b8a7ac160b593729151c9c907e6df029e3a7f312c53b5038a  pypy3.7-v7.3.6rc3-linux32.tar.bz2\n33db78a3c9c9f78eaaf7f52c9c174b1e4c795e5d3294e8364002470a3ced0986  pypy3.7-v7.3.6rc3-linux64.tar.bz2\n3218ef597290ec2983c692a01a6fe9ba5ebf05b8e95fed5e8431b750ec588544  pypy3.7-v7.3.6rc3-osx64.tar.bz2\n4f555251083f633bf044a1bc68d6c50629a374d90f1bee66e245cfac0fdd86f5  pypy3.7-v7.3.6rc3-s390x.tar.bz2\nf0f047f046bec43e433ee08db460c267518eb5b7df1f4d4d6bc3fd735c06a3bc  pypy3.7-v7.3.6rc3-src.tar.bz2\na27d35e75c2486029502590ee862e02af2a3453fa685b42916d618cdbc250fd0  pypy3.7-v7.3.6rc3-src.zip\n67c2e0676b04bbb3bbcf13f5c1f6c97a420b576e362c4948bed0fcbbf64419ee  pypy3.7-v7.3.6rc3-win64.zip\n\n7c5877b27ece045af7603436d64c8589eadc920045341bb16c9a773b924b1dfc  pypy3.7-v7.3.6rc2-aarch64.tar.bz2\n1afe2650a79ea2f234576986e599d504c1f4ab7928a50e3360cdac3b900c04b3  pypy3.7-v7.3.6rc2-linux32.tar.bz2\nd590359ea1a674b51ea13c2a79d883db38b21c43494c986f90af1f34053111a6  pypy3.7-v7.3.6rc2-linux64.tar.bz2\nbd9a96b9c5c542ef36e1e01f0e1987140d54f7bf04f0434bf3a3b9efe166c912  pypy3.7-v7.3.6rc2-osx64.tar.bz2\n22cab4d077f39dc2ff74ebb0d4505e5e3a5b88f2b909643181f57d7b810391da  pypy3.7-v7.3.6rc2-s390x.tar.bz2\n064e4f9fa408bacb67829782d95e2206b20319ae5b15e85993c76532350f57e8  pypy3.7-v7.3.6rc2-src.tar.bz2\n4071597a7450fb0d886005c82c52ed7773e9b0c2015bc93968850071d3195f6d  pypy3.7-v7.3.6rc2-src.zip\n6c6ac71a616882a53648d49e3b20dd1991c08e39a422e650cd58e2f12eecf19c  pypy3.7-v7.3.6rc2-win64.zip\n\n7cfb96afb7aa7478516c1747da77616edf92b46fda56570bcc3117bed46364c1  pypy3.7-v7.3.6rc1-aarch64.tar.bz2\n8079707602a24ab1b61f8982c8ef858f2780e60c08e02354c377d428326f57dd  pypy3.7-v7.3.6rc1-linux32.tar.bz2\nc40b7859933e14ca398e4eba0f70f9dbd521def5279acb4fc7c897d41ac0ac60  pypy3.7-v7.3.6rc1-linux64.tar.bz2\n8d9fde2810f84564902cb37d2d8f7294e5c3ea1fd664ab186864c71edb517d83  pypy3.7-v7.3.6rc1-osx64.tar.bz2\n8c4db2df86239c3e1fa5fb8a4efa5f5ec1f4d55f48ea92a01bd73bdce7fdf9bb  pypy3.7-v7.3.6rc1-s390x.tar.bz2\n25b980da5a5ca89a67e3752dfb1bb6ee3cd0804b7961d0a12e2f9180afe5bd07  pypy3.7-v7.3.6rc1-src.tar.bz2\nc2d21937db476d9c2d86f1e8622998278599f0cadda43a6335c6c7ada5403fec  pypy3.7-v7.3.6rc1-src.zip\na8d8a861dbff630f902d167da202b654e700b802b1c77643723cd246cef0b2ff  pypy3.7-v7.3.6rc1-win64.zip\npypy2.7-v7.3.6 sha256:\n90e9aafb310314938f54678d4d6d7db1163b57c9343e640b447112f74d7f9151  pypy2.7-v7.3.6-aarch64.tar.bz2\n7a1145f3a278ffab4da0e2d4c4bd024ab8d67106a502e4bb7f6d67337e7af2b7  pypy2.7-v7.3.6-linux32.tar.bz2\n82127f43fae6ce75d47d6c4539f8c1ea372e9c2dbfa40fae8b58351d522793a4  pypy2.7-v7.3.6-linux64.tar.bz2\n9a97de82037d4be1949ec0c35a4d638ba635e8b34948549ae2fa08abd2cbaa8c  pypy2.7-v7.3.6-osx64.tar.bz2\nbb29ecbe1f4a05045f0804b3e741267fc2db742249747b36cdbbd18866c15f04  pypy2.7-v7.3.6-s390x.tar.bz2\n0114473c8c57169cdcab1a69c60ad7fef7089731fdbe6f46af55060b29be41e4  pypy2.7-v7.3.6-src.tar.bz2\ncd88f99eccce3b9921a3c7fa452b25d7b60d87ff580bb03237bb1cd0fe2dd031  pypy2.7-v7.3.6-src.zip\nfcc8f6b3b472a77eaa754951f288fe234b4953bfba845888dd839b9b862cb891  pypy2.7-v7.3.6-win64.zip\n\n\ne92e4ba12a62f053e70799e463c7fcb2663b9fa270a16764250385024180cde4  pypy2.7-v7.3.6rc3-aarch64.tar.bz2\n918cf465e1339adcc66d9829b711e30d6a78d764ce74d79407ce35222f24e569  pypy2.7-v7.3.6rc3-linux32.tar.bz2\n21d9ed5a80aee8c320321b32eb3ca0bc89d630646a7371ee560c15296e68e4aa  pypy2.7-v7.3.6rc3-linux64.tar.bz2\ndcb0f049626b47d0bef1ff4f6d19c43b92f7c99a2cf2032afcbf3456b0e00425  pypy2.7-v7.3.6rc3-osx64.tar.bz2\n648e6e02e31d0ee17428f90da7fc938c2b6d0a8bd790ca73887c94a1016013d7  pypy2.7-v7.3.6rc3-s390x.tar.bz2\n0b868fe3b6c5a1a498b558395876a5d9cd3f0add649d5c281542db31a086c16b  pypy2.7-v7.3.6rc3-src.tar.bz2\neec6ec44cb9e4da0a29118fe98d4c289374af617e5279a77f6759a9713b68d2d  pypy2.7-v7.3.6rc3-src.zip\n47f9003c5909271c3ee4ce81de3703e2f17e20d7eba7d7328e8dc29407107b3d  pypy2.7-v7.3.6rc3-win64.zip\n\n9de5474ae55d31b02b9d43be26d7b3ea70e24e6e8a24bdc1d2ee396e191f315d  pypy2.7-v7.3.6rc2-aarch64.tar.bz2\n85a57d385a0e6072dfcf979654160fecb3f7d3d7a43352a28dff2c9dd63c7b01  pypy2.7-v7.3.6rc2-linux32.tar.bz2\n5e5800b1dcc705476bdc1bb6a195e857390d3fafc6406ba27513bff461cfadf7  pypy2.7-v7.3.6rc2-linux64.tar.bz2\nc6cb5bc6107bdbbf18a18db5b143a9d0476c6578f2d35792c49274d14f6f55ab  pypy2.7-v7.3.6rc2-osx64.tar.bz2\na490ab50a846c5587d525aba6ec6cbaeca758e9c6c6941ea0a1738bb78d32b22  pypy2.7-v7.3.6rc2-s390x.tar.bz2\n1e3870ba5ca5567e4808893ca3361e79f1ba02424059e4459936810ff304ba63  pypy2.7-v7.3.6rc2-src.tar.bz2\n38d18c15a64950822a404e98b9fba8aac671671e4d51553a60923de5992a6ddd  pypy2.7-v7.3.6rc2-src.zip\n965f3581e53de1d55f150d78aa9d90b7717a243be494b78d9b88b30ab4a1a8be  pypy2.7-v7.3.6rc2-win64.zip\n\nb2957fc3a3fe3957529fdb3e0e85965d46f4b7c09e4101237869f34ddfe5f0d4  pypy2.7-v7.3.6rc1-aarch64.tar.bz2\n37b9c8d41b5ba85b8ab9defd86da98b842f975d72c473bf92c3c1143a9c293cf  pypy2.7-v7.3.6rc1-linux32.tar.bz2\nb83967849db84c6e7b7c80b2135788da9c235a89a689729fd044b58d1d92c12f  pypy2.7-v7.3.6rc1-linux64.tar.bz2\n63a57129987f54ee692129b53fdf13d635cb6097dc0a1c8cd77f255fc95edda4  pypy2.7-v7.3.6rc1-osx64.tar.bz2\n187e9de4fc4d7edc332275031a40f0de8dc882050b14d5e9b588808c51efedf9  pypy2.7-v7.3.6rc1-s390x.tar.bz2\nbe979c8742181d5646ee1b78eac467612cf61484713ae6862e2b3475b4325b98  pypy2.7-v7.3.6rc1-src.tar.bz2\nc746176c507128e8e5aca14e5a0eaa101955b7cc860ceeba8b20f4f011da4061  pypy2.7-v7.3.6rc1-src.zip\nc515b46bccf1b56fd2f7761a9e3984aa6d56843e848eae67a28fd58fb158a5a9  pypy2.7-v7.3.6rc1-win64.zip\npypy3.7-v7.3.5 sha256:\n85d83093b3ef5b863f641bc4073d057cc98bb821e16aa9361a5ff4898e70e8ee  pypy3.7-v7.3.5-aarch64.tar.bz2\n3dd8b565203d372829e53945c599296fa961895130342ea13791b17c84ed06c4  pypy3.7-v7.3.5-linux32.tar.bz2\n9000db3e87b54638e55177e68cbeb30a30fe5d17b6be48a9eb43d65b3ebcfc26  pypy3.7-v7.3.5-linux64.tar.bz2\nb3a7d3099ad83de7c267bb79ae609d5ce73b01800578ffd91ba7e221b13f80db  pypy3.7-v7.3.5-osx64.tar.bz2\ndffdf5d73613be2c6809dc1a3cf3ee6ac2f3af015180910247ff24270b532ed5  pypy3.7-v7.3.5-s390x.tar.bz2\nd920fe409a9ecad9d074aa8568ca5f3ed3581be66f66e5d8988b7ec66e6d99a2  pypy3.7-v7.3.5-src.tar.bz2\n61bb9740eaac5dd93577e6b76e8bb1a998daa1df5314bc3b192e6803552e12ea  pypy3.7-v7.3.5-src.zip\n072bd22427178dc4e65d961f50281bd2f56e11c4e4d9f16311c703f69f46ae24  pypy3.7-v7.3.5-win64.zip\n\ndbf579f7eb5c527d37ecd43da88cbad02920881b608eb7486d70b4fa31bfc146  pypy3.7-v7.3.5rc3-aarch64.tar.bz2\nd2daf8b1966497d09be703b939bd0020394e0738095243396b3d5f87cef0d815  pypy3.7-v7.3.5rc3-linux32.tar.bz2\n1f9712fa86a50b1de00eb776f3e99033c2a7911dceaa8bc9daf77aa3d2a95842  pypy3.7-v7.3.5rc3-linux64.tar.bz2\nff1d1ce25f60d9474a950ccc90c5c4af376cba2b8af83b4e30cf33de97611c7e  pypy3.7-v7.3.5rc3-osx64.tar.bz2\n8e1c4035ba05161083105f452dfcd463c657085405444afc0acf26ceedb1e8a3  pypy3.7-v7.3.5rc3-s390x.tar.bz2\n9f7215f77106a6df0c201b6025dffdc605cd0731d60ee85a81343a51e64edc76  pypy3.7-v7.3.5rc3-src.tar.bz2\n21cae47ec47bead5d0c5e7a902a1bec85cab1eb30bf7190bd140309c20602110  pypy3.7-v7.3.5rc3-src.zip\n8e40ddc6e4360602597bed44f3ae227d20f8eaa0adfb6a728d10805f76456b74  pypy3.7-v7.3.5rc3-win64.zip\n\n\nc01e59167a26976e764f7b230f6febe0af59982911cd727c551191aed0a843c4  pypy3.7-v7.3.5rc2-aarch64.tar.bz2\n7f8e55f34bf9422576a501c22ae8b82d5d6ffcbf40251a9daf53b5d8d96c2f43  pypy3.7-v7.3.5rc2-linux32.tar.bz2\n93f9ccf44ec92145cf2fe17ac98a07f0adc08866b001c7f023b64a3729ed9710  pypy3.7-v7.3.5rc2-linux64.tar.bz2\n4902ac65329447f2451d2b2b264a12fb95d97a4bb734c75410d2b5abc6e6de52  pypy3.7-v7.3.5rc2-osx64.tar.bz2\nf0d4bbbe4000c836c17168cc709b233b6184039aad69bc9929c415a92bc462a9  pypy3.7-v7.3.5rc2-s390x.tar.bz2\nb1ac30e5e7cd8d04c4472b5c4a71a414d6b0cf08a2026fd1bfc84994598abfda  pypy3.7-v7.3.5rc2-src.tar.bz2\nc6c004550444c2f8749d7e34bcdfe404333b5f4bdf08af7745e28371c8358050  pypy3.7-v7.3.5rc2-src.zip\nea41d9e5cb94c7b9e7df2652b74fcc1018ce3e786c9636791b70e46d90e7e8ac  pypy3.7-v7.3.5rc2-win64.zip\n\n8dcd20e35e26bf92ce08fc8c97350acb4c773e19a78a89d3b4f28a8be63006d3  pypy3.7-v7.3.5rc1-aarch64.tar.bz2\n04573fd71618d5c26b0828dd306fa02e9eece8a33a020081e55b60d9a6bc6240  pypy3.7-v7.3.5rc1-linux32.tar.bz2\n97c1142f7ac99af03b2c56eb379af6e9ed4eef7d0d37675f4ca5ec33c841d62f  pypy3.7-v7.3.5rc1-linux64.tar.bz2\nf4893667f0b978deb891b0b7d91a1117e25299f19c65b31281c40e87dea523d3  pypy3.7-v7.3.5rc1-osx64.tar.bz2\n2880cfa6349aebc5c28aff5df06cabb8c8733dc7090f7f36410eb9ff3def37bc  pypy3.7-v7.3.5rc1-s390x.tar.bz2\nddccb7e8b24523f3f0e31e6c34b3a61c260b895ac9c7567f560f8ceda675fef8  pypy3.7-v7.3.5rc1-src.tar.bz2\nf39baa99eb0cb4d1505cd43676f86c54cae142f88b9b875542520b8596368ba7  pypy3.7-v7.3.5rc1-src.zip\nab8c5e6bf756f6dda2eba5c2e8d65d8d5de9b3a2c54f2f7a3dfb4f111e40ba0d  pypy3.7-v7.3.5rc1-win64.zip\npypy2.7-7.3.5 sha256:\n8dc2c753f8a94eca1a304d7736c99b439c09274f492eaa3446770c6c32ed010e  pypy2.7-v7.3.5-aarch64.tar.bz2\n35bb5cb1dcca8e05dc58ba0a4b4d54f8b4787f24dfc93f7562f049190e4f0d94  pypy2.7-v7.3.5-linux32.tar.bz2\n4858b347801fba3249ad90af015b3aaec9d57f54d038a58d806a1bd3217d5150  pypy2.7-v7.3.5-linux64.tar.bz2\n8b10442ef31c3b28048816f858adde6d6858a190d9367001a49648e669cbebb6  pypy2.7-v7.3.5-osx64.tar.bz2\nb91aaa5819ba8af90799eed8eaaba87ceca1fd4dbcbcdb2defc6d313d663b5dd  pypy2.7-v7.3.5-s390x.tar.bz2\nc0444fd9873058c1c0d99e13a934e92285cb05992c9968bf523c32bf9bec0a9d  pypy2.7-v7.3.5-src.tar.bz2\nc67214acee357d383bb2716269663406611e17cee580026d6d7baa7891afa85b  pypy2.7-v7.3.5-src.zip\n0b90eded11ba89a526c4288f17fff7e75000914ac071bd6d67912748ae89d761  pypy2.7-v7.3.5-win64.zip\n\n0f83212202d51835dcedfdfe607fe157d1111a368f7f28738792417acd987c37  pypy2.7-v7.3.5rc3-aarch64.tar.bz2\n6dc2fec9894121cc75500c84509c869648e6fa95c8e8084c81bf17191d80ba8c  pypy2.7-v7.3.5rc3-linux32.tar.bz2\n8a918307a51a02ae222e71e2973a4d0dc520a3bae2d510a6571aaf53cf7cead7  pypy2.7-v7.3.5rc3-linux64.tar.bz2\n9376ba404009ce435e7b04a3c194f783b841464031607081081429f079797faa  pypy2.7-v7.3.5rc3-osx64.tar.bz2\nc95f5d5cef6181fe08f54824872c94f27177feb5d156fa6dae279a5b8228b13c  pypy2.7-v7.3.5rc3-s390x.tar.bz2\nb643dd908e6d07d703f388798e0355e3378a8157833680cbea55c3cf3e4256e2  pypy2.7-v7.3.5rc3-src.tar.bz2\nbaeafa81e445a5b6c8da8ec92c8587a11104f7e125478d669d9eaa45492b7b90  pypy2.7-v7.3.5rc3-src.zip\n21b21873124572043749bb5b19cc33a14ffbf6d8ea5e538006689cc4e3af3d5a  pypy2.7-v7.3.5rc3-win64.zip\n\n8250c8db8f227aec3d85f8866f8ad78d925ed338a5622f64c22d6a7fb0963b5a  pypy2.7-v7.3.5rc2-aarch64.tar.bz2\n978ed1e445809adbaa0ca593abd445384c28d72344bf67184b5cee5e0f76fc3c  pypy2.7-v7.3.5rc2-linux32.tar.bz2\na933976a2adc840d07be9ed4ac1dc1b1986fd68f875c4258ed214a2ce9f5f659  pypy2.7-v7.3.5rc2-linux64.tar.bz2\ncbdfe3f9e49cb96b5b182b19ce257a086dbb7204ba01c178db13b4e6272a3260  pypy2.7-v7.3.5rc2-osx64.tar.bz2\nda2bf8e5e8f03f10ffd8c7e970e20ff702a91fc44a6bd0de51f1a79401804e79  pypy2.7-v7.3.5rc2-s390x.tar.bz2\nb47ce66e8d716b22e7b78f1ec0e2d212a27afd355adcb94e00b6d76ffa9a513f  pypy2.7-v7.3.5rc2-src.tar.bz2\nb031352443dff2202fcc0ee131887a232214363af1d87ba35886dc683b18eb85  pypy2.7-v7.3.5rc2-src.zip\n47a355033a4c61e679f5ed34274a320adda8df2c27ed313bda0841dc8e11a354  pypy2.7-v7.3.5rc2-win64.zip\n\n4431bc2193f76b97add9726420c6d6ab14b46178e9cfeade5f596016b66b6549  pypy2.7-v7.3.5rc1-aarch64.tar.bz2\nb0d2432bf50bfeeb00e91e048db6df1bba40ca54b0d19d9f61db0f3a4e6e2bf5  pypy2.7-v7.3.5rc1-linux32.tar.bz2\n5a81b1e5733351a1e27e8072f474c60d24ab987dc1355873861b69961da425f5  pypy2.7-v7.3.5rc1-linux64.tar.bz2\nd2e3077b6c0a84e07af5e4c5eb9c883e54bf649ef982dd5310b3e8e68dfffc0e  pypy2.7-v7.3.5rc1-osx64.tar.bz2\n5d6a52bbed77855303dadf10a44c1f5e07920ad28948ecf6f13c57eed0c95f8b  pypy2.7-v7.3.5rc1-s390x.tar.bz2\n45639e3b398f1dbac54f35e2aebc4770432519dd8838e0190708f1dcfa945356  pypy2.7-v7.3.5rc1-src.tar.bz2\n67329cae37163b4838bb5768dd04ebc75ce1bbb0a62b74da404587f7344d80fc  pypy2.7-v7.3.5rc1-src.zip\n6d36595d6cf6f61c33c0e36ae47d9f84abe1ab99cee6cb910a2517d4d3db6cb0  pypy2.7-v7.3.5rc1-win64.zip\npypy3.7-7.3.4 sha256:\na4148fa73b74a091e004e1f378b278c0b8830984cbcb91e10fa31fd915c43efe  pypy3.7-v7.3.4-aarch64.tar.bz2\n04de1a2e80530f3d74abcf133ec046a0fb12d81956bc043dee8ab4799f3b77eb  pypy3.7-v7.3.4-linux32.tar.bz2\n09d7298b44a38648a87995ec06e1e093761644e50f547c8bb0b2d7f4fe433548  pypy3.7-v7.3.4-linux64.tar.bz2\n8a4f0e6c7e3845820202bf7f46b48e36886ceb820ff0767963fd74091c4f5d13  pypy3.7-v7.3.4-osx64.tar.bz2\n7d6fb180c359a66a158ef6e81eeca88fbabbb62656a1700f425a70db18de2a0f  pypy3.7-v7.3.4-s390x.tar.bz2\n74d3c1e79f3fc7d384ffb32d3d2a95c2d5f61b81091eccce12ac76030d96ad08  pypy3.7-v7.3.4-src.tar.bz2\n80d4da3aaeb8b4cc5e4e4ea747f2e468e9f448da549aa7ada4d59c24380cda43  pypy3.7-v7.3.4-src.zip\n0ff4e4653f1ff0653f105680eb101c64c857fa8f828a54a61b02f65c94b5d262  pypy3.7-v7.3.4-win64.zip\n\n647e34857d181e7560205eb877915b787836237929c7bd52860de626d5e85e9d  pypy3.7-v7.3.4rc2-aarch64.tar.bz2\ncfc661034347d79ba907078b4e3acea4f09d0de0eaf474c5bde173666319780c  pypy3.7-v7.3.4rc2-linux32.tar.bz2\ndcf1fa6dd5da4076f040ed4302a22c8da3838335e64cd118c29d69eb7d443d6b  pypy3.7-v7.3.4rc2-linux64.tar.bz2\nc9ecc213cdc3169ef230d85e49d9d073ffc1ba0a36bc1d8483f724e31b9d9d12  pypy3.7-v7.3.4rc2-osx64.tar.bz2\nfcc5c02382f67c7ee6f267b459131519b6a72e60ae370d6e398d54c0e07080f9  pypy3.7-v7.3.4rc2-s390x.tar.bz2\nf1257d4d8a3d84e84ff85c83f4f5bc2e126727d7595c536ccbe1a03a280c0df6  pypy3.7-v7.3.4rc2-src.tar.bz2\ndfab9881e2c42ae61115aa6ed77389f835094fd783dc08cf4dee1ebfdd4c1d47  pypy3.7-v7.3.4rc2-src.zip\nb62b7aad962a8c42895a13b08d68b32254934d6d1b1f5f1f02f762cbe111b035  pypy3.7-v7.3.4rc2-win64.zip\n\n958a562528d24fdb33b9fd12f2076f4b546dc218e0793324558560823234adb1  pypy3.7-v7.3.4rc1-aarch64.tar.bz2\nd05299744ac8c6f12bb3587541ce106f3a93d9ed64b0529c46e79b56efd27b24  pypy3.7-v7.3.4rc1-linux32.tar.bz2\nbb7ee16bdf7c1bbbca45d1228502a5c276be33e27e849525aa5a61c0eaec5b4a  pypy3.7-v7.3.4rc1-linux64.tar.bz2\n6d3aea12b744413c874e33ff456f6591049e12dc1a356d975dc0e29a047a151e  pypy3.7-v7.3.4rc1-osx64.tar.bz2\n8deb01eb54b95e480d2ee03ee9148ba0c1684b410165c198e9f68a015656246e  pypy3.7-v7.3.4rc1-src.tar.bz2\nbf247839954a4518327d5cbc9ab1a1b4296982c2fe78671d59a58373239e675e  pypy3.7-v7.3.4rc1-src.zip\n0819de5a5212bddef0f615f7ced03dfd9f5d4ee115ec3564119d45b6b447843f  pypy3.7-v7.3.4rc1-win64.zip\npypy2.7-7.3.4 sha256:\n9e741162ce486b14fbcf5aa377796d26b0529a9352fb602ee8b66c005f8420d1  pypy2.7-v7.3.4-aarch64.tar.bz2\n653cc3f0612399e494021027f4463d62639dffa4345736a16d0704f3f8a61d5f  pypy2.7-v7.3.4-linux32.tar.bz2\nd3f7b0625e770d9be62201765d7d2316febc463372fba9c93a12969d26ae03dd  pypy2.7-v7.3.4-linux64.tar.bz2\nee7bf42ce843596521e02c763408a5164d18f23c9617f1b8e032ce0675686582  pypy2.7-v7.3.4-osx64.tar.bz2\nf19b70ca5bd918d1349444be775bc2194c8165b0140e6e8b87c3ee101765a5ba  pypy2.7-v7.3.4-s390x.tar.bz2\nff9b928237767efe08ccfba79dae489519b3c768fb6e3af52d39c2a8a1c21ca4  pypy2.7-v7.3.4-src.tar.bz2\ne0811ecc272fee58e01b95c4c12f23b115a3e64075a1b50dcefe8faaa6cca869  pypy2.7-v7.3.4-src.zip\n1080012d7a3cea65182528259b51d52b1f61a3717377c2d9ba11ef36e06162d5  pypy2.7-v7.3.4-win64.zip\n\nf0a11bd48a01b27595e659c3a1b7fb936ac6e0a21574f1fc2f57fd032830342a  pypy2.7-v7.3.4rc2-aarch64.tar.bz2\n81dd5ac16b11f6f9ba0ff2536306dd85997a6cad86aa4e7971e7805264d61716  pypy2.7-v7.3.4rc2-linux32.tar.bz2\n077acdb14e797878341fc6f50d87a2f0c9b7d25215c6b2f73541bacb7730f64d  pypy2.7-v7.3.4rc2-linux64.tar.bz2\n6a220785a962c56db26dd56245aacb7cb6658879ecaad9ada04d26df56da172c  pypy2.7-v7.3.4rc2-osx64.tar.bz2\na3201493550457f932ddf743118635a7e8ff6b5c5fd69d0b8596dfeabcc5bffd  pypy2.7-v7.3.4rc2-s390x.tar.bz2\n1965dfc3de6fdae83bd954fed206111a020898708d8754705fb1312473be35bf  pypy2.7-v7.3.4rc2-src.tar.bz2\n1072727a4a948b16ccebb165015e43716ffc586f5249356c97c454b24aacb2dd  pypy2.7-v7.3.4rc2-src.zip\ne20f206ba8751d2c17ad80c66b7f4bd63c2f500cbfa9e8a3906cd7d77955e00f  pypy2.7-v7.3.4rc2-win64.zip\n\nee4894169260d3e4c55e06232c96d690e41d13e9f82f1512edcf6b8d960b695d  pypy2.7-v7.3.4rc1-aarch64.tar.bz2\nfd736003d5a7f5f2744269d67dc9a96005a5a2ceac8987007bd27ab57681c0f2  pypy2.7-v7.3.4rc1-linux32.tar.bz2\nec1cd67c28416c359dbe1caddf7ae7a0be10e3fbe6435150d39d4b7492469852  pypy2.7-v7.3.4rc1-linux64.tar.bz2\ncce4e360b31010e415e397ce8982535db482e36c0f13934eaa6d9e1e30eb2bc3  pypy2.7-v7.3.4rc1-osx64.tar.bz2\n84930e433a81f16dcf81b678c12167ef951cd74534ee1ee8e6b0b27b0a128e1d  pypy2.7-v7.3.4rc1-src.tar.bz2\n7bdc1e5431a7429bd2ec2853c86a68f09069f080b9765a87084904f52adab789  pypy2.7-v7.3.4rc1-src.zip\n02befc534dbcc2da6ad4c7e60735d977dc8b4f6901630eb599d1684cb86a58c7  pypy2.7-v7.3.4rc1-win64.zip\npypy3.7-7.3.3 sha256:\nee4aa041558b58de6063dd6df93b3def221c4ca4c900d6a9db5b1b52135703a8  pypy3.7-v7.3.3-aarch64.tar.bz2\n7d81b8e9fcd07c067cfe2f519ab770ec62928ee8787f952cadf2d2786246efc8  pypy3.7-v7.3.3-linux32.tar.bz2\n37e2804c4661c86c857d709d28c7de716b000d31e89766599fdf5a98928b7096  pypy3.7-v7.3.3-linux64.tar.bz2\nd72b27d5bb60813273f14f07378a08822186a66e216c5d1a768ad295b582438d  pypy3.7-v7.3.3-osx64.tar.bz2\n92000d90b9a37f2e9cb7885f2a872adfa9e48e74bf7f84a8b8185c8181f0502d  pypy3.7-v7.3.3-s390x.tar.bz2\nf6c96401f76331e474cca2d14437eb3b2f68a0f27220a6dcbc537445fe9d5b78  pypy3.7-v7.3.3-src.tar.bz2\n9e4756903b14c5f971989a2f5a4de6ee19b21a59f2a798b3ad2ad0e71b2582a5  pypy3.7-v7.3.3-src.zip\na282ce40aa4f853e877a5dbb38f0a586a29e563ae9ba82fd50c7e5dc465fb649  pypy3.7-v7.3.3-win32.zip\n\n54a1697d39f136c3e3961afbd58a049e10a5ed10e6d230e6729d696c226d5185  pypy3.7-v7.3.3rc2-aarch64.tar.bz2\n796c0b57b28850f9a212593f30baf7c241c0ed3fe857048d2ea50b3e13b9773b  pypy3.7-v7.3.3rc2-linux32.tar.bz2\nbe427afe0434ac42b4da997c841250c499286c57f1c1e9a764d49787bbeeda38  pypy3.7-v7.3.3rc2-linux64.tar.bz2\ne670772077ea400c8f276f8bea301a0c3fa0f037f7e174ae08b34d46e43ce433  pypy3.7-v7.3.3rc2-osx64.tar.bz2\nb230bfd935d6a4ecfaf890c91431b56cb53325ad988899542b178610f94d5970  pypy3.7-v7.3.3rc2-s390x.tar.bz2\nc4a7f8c8a00073de1f987562bed486c372005e021505d3847562966541e0ea6f  pypy3.7-v7.3.3rc2-src.tar.bz2\n26ba0babe260fbc9264c15070b129593ca871c7658a661eacf4c5e27507542f7  pypy3.7-v7.3.3rc2-src.zip\n53959607ea55de6ec5cf15227c195e3356d56629e91279ce26744cb3e392a863  pypy3.7-v7.3.3rc2-win32.zip\n\n45357c23a05bc4e4828c0c0964142a7c45f0bcc6653cae67837ff00a02ececb2  pypy3.7-v7.3.3rc1-aarch64.tar.bz2\n22c04f6984c986895999c73d845e57957d86ab788137e482b60f83aa4983e278  pypy3.7-v7.3.3rc1-linux32.tar.bz2\n2069912448749295537c2b381957c5e07dec103fc9a3322f2ce8a57b3fa6e60c  pypy3.7-v7.3.3rc1-linux64.tar.bz2\n9fbbf9cfb9ca699e00ea08aaec6248625541998c251033aa3e6d8c592c0a6ff9  pypy3.7-v7.3.3rc1-osx64.tar.bz2\nf502ed792c9da1531a413cd8a7c4c8158c649d7820cb4a910a5852866579c365  pypy3.7-v7.3.3rc1-s390x.tar.bz2\n6780d79e205768a5b2c1d6ecc9e1c4a8c05811cc6b130ed728ba1a53088e0406  pypy3.7-v7.3.3rc1-src.tar.bz2\nedaed54347b69d2a3037e427c60eb88050226cf082d26fff594221cbedab9cd8  pypy3.7-v7.3.3rc1-src.zip\n3c82f4569293dcff5085f0c61af1ba2671217256c58b6e6092629a406eee4fc5  pypy3.7-v7.3.3rc1-win32.zip\npypy3.6-7.3.3 sha256:\nbc82cf7f0182b942a2cfad4a0d167f364bfbf18f434e100a2fe62bc88547ac9b  pypy3.6-v7.3.3-aarch64.tar.bz2\nf183c61e66fd2c536a65695bd7ff770748c2884c235a589b9c6ac63690770c69  pypy3.6-v7.3.3-linux32.tar.bz2\n4fb85fdd516482cab727bb9473b066ff8fb672940dedf7ccc32bf92957d29e0a  pypy3.6-v7.3.3-linux64.tar.bz2\n84126fcb957f260de221244222152c981643144df1d817329781f555daa52e35  pypy3.6-v7.3.3-osx64.tar.bz2\n0de9c33ff3500c6e7fd273d0a6d341bc839b0298f697c4d6fe141f2b54c5c3e2  pypy3.6-v7.3.3-s390x.tar.bz2\na23d21ca0de0f613732af4b4abb0b0db1cc56134b5bf0e33614eca87ab8805af  pypy3.6-v7.3.3-src.tar.bz2\ndf534213c27c6ecc8e7d4f2a6950305301711ea3e132ec7a836959146761c9d8  pypy3.6-v7.3.3-src.zip\nb935253877b703d29b1b11f79e66944f1f88adb8a76f871abf765d4de9d25f8a  pypy3.6-v7.3.3-win32.zip\n\n58a35d069bc887c09f8106aec1c0da18241f887dc227bd9e31bd2819496b8256  pypy3.6-v7.3.3rc2-aarch64.tar.bz2\ne171477f56ada45ce64df6f91ad4961c13b674d268b8b16850d1bae5eda43393  pypy3.6-v7.3.3rc2-linux32.tar.bz2\ndf2f421c3782e09ca304f00afd79d7ac24224c3346b41ddae9ab919f4b243538  pypy3.6-v7.3.3rc2-linux64.tar.bz2\n1b2715c8bdf97bbe2135a13562aaeab3408c1459d714412a0b0c607309c5c48b  pypy3.6-v7.3.3rc2-osx64.tar.bz2\nd1eaa8ea52f8ce7b02ddc08cff56a64405cfdc7f657edd9bfbb8788484ab9c01  pypy3.6-v7.3.3rc2-s390x.tar.bz2\n3c91a1e911eee1baf9093dcb66899bd06a9ddc095ee60c51c2bca1626497148f  pypy3.6-v7.3.3rc2-src.tar.bz2\ne9e5dc879afcddc7ffea09500a092fe00c9070d8fd5008ef0342e0b77c9f9161  pypy3.6-v7.3.3rc2-src.zip\n7bfdc3544216003b96e76f133073084f2918c5cd29642211735c8507142d107a  pypy3.6-v7.3.3rc2-win32.zip\n\n9e65dff7a5bc34d32ea88b9436a9f9629542dd3eb8f948f49ecce40112530199  pypy3.6-v7.3.3rc1-aarch64.tar.bz2\n13a67079e78eaa01dcc2a8aa986a50944bc4bf42469c3c39e3ecb0f0cee31439  pypy3.6-v7.3.3rc1-linux32.tar.bz2\n17fb6dff3a5fd9d9e791ce1cd8ae9076e5f47b8b463b7575e4403f01656b0735  pypy3.6-v7.3.3rc1-linux64.tar.bz2\n2f62a9c9876d83a2bf04d8e5e1373aa7e0dcd1e523a58216e60f20329a536b9b  pypy3.6-v7.3.3rc1-osx64.tar.bz2\na652572f3c783c4c9cfae477a6a64584f2df39e4df75773131ab512e486d61f3  pypy3.6-v7.3.3rc1-s390x.tar.bz2\nbd5e6d6ba3bd9bc1a233c2dd77b518fd1d337a37670fe0e23edf837852254ee7  pypy3.6-v7.3.3rc1-src.tar.bz2\ne26c8c95e2d131507a08c3e8b8010e6dd366e8e9bf6e77db6844bc5145be1932  pypy3.6-v7.3.3rc1-src.zip\n773ffcabddc3bdc626318f24f0ba256153eca517775425b618c1c7b8b10f1680  pypy3.6-v7.3.3rc1-win32.zip\npypy2.7-7.3.3 sha256:\n23b145b7cfbaeefb6ee76fc8216c83b652ab1daffac490558718edbbd60082d8  pypy2.7-v7.3.3-aarch64.tar.bz2\nbfbc81874b137837a8ba8c517b97de29f5a336f7ec500c52f2bfdbd3580d1703  pypy2.7-v7.3.3-linux32.tar.bz2\nf412b602ccd6912ddee0e7523e0e38f4b2c7a144449c2cad078cffbdb66fd7b1  pypy2.7-v7.3.3-linux64.tar.bz2\nf34dc4f5ded1f6bcea05841aa9781b9307329e3ab755607917148568824ae0b0  pypy2.7-v7.3.3-osx64.tar.bz2\n8254a7fb98ea66c33324a403d06ccb052d616a4176ce0130591693ceeb011cf7  pypy2.7-v7.3.3-s390x.tar.bz2\nf63488051ba877fd65840bf8d53822a9c6423d947839023b8720139f4b6e2336  pypy2.7-v7.3.3-src.tar.bz2\n5ce67ea6afb0cf1a3e20bbd4bbd375e375f572d5325524f9c7760edf8521f029  pypy2.7-v7.3.3-src.zip\nb3e660dae8d25d8278fd6a0db77e76a16ac9a8c1dca22e7e103d39ed696dc69e  pypy2.7-v7.3.3-win32.zip\n\n4f2eee1d8ae2571d6fde76141237cf7717324dd6b6a1aa50036c42266d92cbce  pypy2.7-v7.3.3rc2-aarch64.tar.bz2\n79c741bd28f293820382f4ecd81414a327745956fa402a5dcfe38900e7520214  pypy2.7-v7.3.3rc2-linux32.tar.bz2\nb227698c4797170b7fdb427a56632fa7733695dd3b31fd404ce4c0939505f918  pypy2.7-v7.3.3rc2-linux64.tar.bz2\n451fca86c965e498ce2ada9474c36d316a627bd6aeeeb808b952a447c938c936  pypy2.7-v7.3.3rc2-osx64.tar.bz2\n83147a40ecc2ab39679129f7898756febd09422ee63a0074fb7f844964c189d8  pypy2.7-v7.3.3rc2-s390x.tar.bz2\n1d60d7f9662278ba59f34cd20c0332993c0bb117009309bc06bd3cb651318c36  pypy2.7-v7.3.3rc2-src.tar.bz2\n4810fb6761eccf6f3e6a14f7a8e4010548e551928fef27fb9482b0c7e3e501d5  pypy2.7-v7.3.3rc2-src.zip\n72a43db2c5bd639023adad2a5c9fd7d4db639c5269dcfeb19ef5b0576771ea9b  pypy2.7-v7.3.3rc2-win32.zip\n\n061be51e14fc5f16ce38a61b3873239a0a74b02af51be5930b52941bbb3e6eb2  pypy2.7-v7.3.3rc1-aarch64.tar.bz2\n395113ae0a9d1e352e5aef22b1d9e272b029b186d5e1c7e204dd6df044647fc1  pypy2.7-v7.3.3rc1-linux32.tar.bz2\n1e160ff884fdcdc3388b3c88a00ee54d0b11e7b3c94c4787a217eeea76da63e3  pypy2.7-v7.3.3rc1-linux64.tar.bz2\n761b6e9485dd218e63d231f351f908e74c6cc6bb38cc3b61992b92a0e5384f02  pypy2.7-v7.3.3rc1-osx64.tar.bz2\n72d62a3d0bfcb1693f44d5bc3601d528188838df9fbb885e3e18770f81f97e5a  pypy2.7-v7.3.3rc1-s390x.tar.bz2\n39fa3f6f0921785c4b44ab2e47777d64480737c710672f09913b2306a1430281  pypy2.7-v7.3.3rc1-src.tar.bz2\n6b5b466e74505e59985ff9583587a417a200ab2d41829b8c72c74daef4c0d44c  pypy2.7-v7.3.3rc1-src.zip\n403bce17882ca7f305fedd9f604f5657364e4ef76086064bbed0a31dfbf47155  pypy2.7-v7.3.3rc1-win32.zip\npypy3.6-7.3.2 sha256:\n164d6a0503c83dd328e1a6bf7fcb2b2e977c1d27c6fcc491a7174fd37bc32a12  pypy3.6-v7.3.2-aarch64.tar.bz2\n6fa871dedf5e60372231362d2ccb0f28f623d42267cabb49be11a3e10bee2726  pypy3.6-v7.3.2-linux32.tar.bz2\nd7a91f179076aaa28115ffc0a81e46c6a787785b2bc995c926fe3b02f0e9ad83  pypy3.6-v7.3.2-linux64.tar.bz2\nfd457bfeaf54aa69417b6aa4817df40e702dc8aaaf7e83ba005d391a1bddfa96  pypy3.6-v7.3.2-osx64.tar.bz2\n16afbaa245c016c054d9300c19433efcc76c50664ff2c86d913ff76ed0a729dc  pypy3.6-v7.3.2-s390x.tar.bz2\nfd6175fed63ff9fccd7886068078853078948d98afae9bd4f5554c6f7873c10d  pypy3.6-v7.3.2-src.tar.bz2\nedcbcd3598a91de3115f86550d1bc76ac46fc0a3e86a1e951769a993f6fbcbf0  pypy3.6-v7.3.2-src.zip\n13a39d46340afed20f11de24e9068968386e4bb7c8bd168662711916e2bf1da6  pypy3.6-v7.3.2-win32.zip\n\n62e525c6c71c8264c8476e2c4afe11d2aa07b71f9bcf6d694fc4aae27bfcbb66  pypy3.6-v7.3.2rc2-aarch64.tar.bz2\ne9de7036c663f08f06f760340c5d165d8bdecad159abd14d0d93d1bde714ed38  pypy3.6-v7.3.2rc2-linux32.tar.bz2\ne3ac3cf1560f8aee41e542bd999214cbbe0645a4786e4d8a5dc3d58b219429f3  pypy3.6-v7.3.2rc2-linux64.tar.bz2\n7995b74b190f619feb3f393620f63dd0f7cae9e8e298c0616bd184090c356c90  pypy3.6-v7.3.2rc2-osx64.tar.bz2\n9c09100e3302221dbe9776bb3f99e870a8404a2f6afd7a056fa3b7116f5ab013  pypy3.6-v7.3.2rc2-s390x.tar.bz2\nb7d4b3cf3ba7e7749421b1eb857be32d8e5fede124cb2a1d1e1bc606a437b4c5  pypy3.6-v7.3.2rc2-src.tar.bz2\nf5c4f219a974c69b949221082b789a455a67f9f6a37c173cb48a6246ab57f05c  pypy3.6-v7.3.2rc2-src.zip\n0555340fdd2e2fcbf114d1f2b57d798269dfccddf1b6419dbe3ce937927b0504  pypy3.6-v7.3.2rc2-win32.zip\n\n1c69cca7292e3c3ffcb7a09f5cdeb51d45e24dc75510b2c9bb410b8ffc57a579  pypy3.6-v7.3.2rc1-aarch64.tar.bz2\nd5738cffc11b364b5f0bf4883c2e1fd46431822f3bd126c7d8c83e9b5f0e6543  pypy3.6-v7.3.2rc1-linux32.tar.bz2\n41cab069841cfc713cc2d0526034f04fcbd741d67d70212926a3ff90754a39f5  pypy3.6-v7.3.2rc1-linux64.tar.bz2\nafabd1ea5a7da31df547c1d4b7028caef1dfaad0ba7e9dda81da2884dfe3062c  pypy3.6-v7.3.2rc1-osx64.tar.bz2\n9202fa080d821cca5fe788acfdee3020449e3c36df720ede89ef7389ad6d4a37  pypy3.6-v7.3.2rc1-src.tar.bz2\n8dc4d906720208d590133d580bc7976f7aca1fedf49c3dec1eba1fccb39e0bdc  pypy3.6-v7.3.2rc1-src.zip\n29d47b72cf417d12b23161d898dae38f48e48788733623ffb09807e913fbeb44  pypy3.6-v7.3.2rc1-win32.zip\npypy3.7-7.3.2 sha256:\nc5c35a37917f759c19e2a6b3df3b4d56298faa2fae83c143469bcbda42ca5dd2  pypy3.7-v7.3.2-aarch64.tar.bz2\n34c7e1c7bd06e437ad43cc90a20f9444be1f0a264d0955e32098294c30274784  pypy3.7-v7.3.2-linux32.tar.bz2\na285ddcbc909d68c648585fae4f33b0ba24961bb4e8fafe5874cf725d6e83df6  pypy3.7-v7.3.2-linux64.tar.bz2\n337dd4d9e529d2f221e0beb092236c18430e0564ab835c6bba425a1daf7c9958  pypy3.7-v7.3.2-osx64.tar.bz2\nd4ce71ebba148bf83c24fc963e8282c9b7f0c81fcf6b612301b8efe6bd7658d1  pypy3.7-v7.3.2-s390x.tar.bz2\n9274186eb0c28716a8c6134803b1df857bc3f496e25e50e605c4d95201c8817d  pypy3.7-v7.3.2-src.tar.bz2\n23363123c607058dac29995cf281c4609a8d8d278841a8f05ea8559bdb1678a8  pypy3.7-v7.3.2-src.zip\ne3c589be07760bc3042981c379b7fd1603e832a4db426075f09e090473846a96  pypy3.7-v7.3.2-win32.zip\n\n78fe46fa8706e325bd0bdb81d6f0865b7dae0ffb22a77c533a24fa960e885b1b  pypy3.7-v7.3.2rc2-aarch64.tar.bz2\n2ed3489e1ea42b1807e79ba46a2dfb2c763bdd4d15efac0fd8ba9cf05ab436bb  pypy3.7-v7.3.2rc2-linux32.tar.bz2\n6c67701914b7885e67d282c1286e9109fc79e73ab65b5c164492fb024b8deb7f  pypy3.7-v7.3.2rc2-linux64.tar.bz2\n28b48a691276a806bcf0009df5e367d90159b9b4a4161ad9857454999e6915ec  pypy3.7-v7.3.2rc2-osx64.tar.bz2\n544023b22670be740970bfc8d67a102dfa045cb229e40271a4197a9e8d3bc5da  pypy3.7-v7.3.2rc2-s390x.tar.bz2\n9a3f29338340ab5e006300b68369745bd16f99943a7d48d8440c5a0ad67a5c68  pypy3.7-v7.3.2rc2-src.tar.bz2\n73a6c2241d0a5ce7741a15f8cfd205a6f1eb10310799d912c069d6be58907ba7  pypy3.7-v7.3.2rc2-src.zip\n9a44c694f9c642a7a127241466f72ca58f303d3e148bf5488e34a162c7d7a55b  pypy3.7-v7.3.2rc2-win32.zip\n\na7e2376f5e64256aa2e3cf3d403b4c48753c9c2588c57e0fc6bddebefacb3a9d  pypy3.7-v7.3.2rc1-aarch64.tar.bz2\ne2b2fa3f83f4a3cc138eb88c3bbf4fde395faec6bc04cd72721623865a366d96  pypy3.7-v7.3.2rc1-linux32.tar.bz2\n8173935a5d1cae7238cb27e35bf881ab0ed0d8bd978d3cf6c80311ed596324ba  pypy3.7-v7.3.2rc1-linux64.tar.bz2\ne730cf9e5be8566544a478bf2da4bc4ab84428ac4f4a7bb8e001ea4516a3f3be  pypy3.7-v7.3.2rc1-osx64.tar.bz2\n209c2136654ea116c316c6d5305659e8e33d49b9f9f61eee36c06330bb3214ba  pypy3.7-v7.3.2rc1-src.tar.bz2\n419020e81793030cb6d011e7c0b75183163a7586a31ae88a6a52689e9c45926e  pypy3.7-v7.3.2rc1-src.zip\na6fc9d568c05504759e945e70b94fc55f5e99748eb01da4fb5192231238fa1d7  pypy3.7-v7.3.2rc1-win32.zip\npypy2.7-7.3.2 sha256:\nfce1f06f20ab8bcacb9ac1c33572d6425033de53c3a93fbd5391189cc3e106cb  pypy2.7-v7.3.2-aarch64.tar.bz2\n78f30ac17abe3cc077fc2456ef55adb51b052c5126011b2a32bacc858acaca7d  pypy2.7-v7.3.2-linux32.tar.bz2\n8d4f08116a97153a0f739de8981874d544b564cbc87dd064cca33f36c29da13b  pypy2.7-v7.3.2-linux64.tar.bz2\n10ca57050793923aea3808b9c8669cf53b7342c90c091244e9660bf797d397c7  pypy2.7-v7.3.2-osx64.tar.bz2\n042d5e99f660de098de979c4b27f7f8c1332d904db379bb2bf2c3402729749bb  pypy2.7-v7.3.2-s390x.tar.bz2\n8189480d8350ad6364d05c2b39fd7d832644d4b1cd018f785126389df45928d1  pypy2.7-v7.3.2-src.tar.bz2\nd891c55f4e657b5e3fe609cee02b2288790abb5554a544ca047f088310d129c4  pypy2.7-v7.3.2-src.zip\n0fd62265e0421a02432f10a294a712a5e784a8e061375e6d8ea5fd619be1be62  pypy2.7-v7.3.2-win32.zip\n\nfa76bfc65200eeb3b32253e674a9339a417aef23f5a5c54e0c519bbbfefcdc7e  pypy2.7-v7.3.2rc2-aarch64.tar.bz2\n40ff311202eca98ef3d6edeac4171470135087a8de34296f486c17ec376ebe51  pypy2.7-v7.3.2rc2-linux32.tar.bz2\n379d458c1a9d38c2b3a6a32bd805786fc584739548a697a4ef7b683bcfdfda3e  pypy2.7-v7.3.2rc2-linux64.tar.bz2\n3d515a233c83cbc833bcdd0b75354b20dc79b9f6ca892a5db9cadaea36c6bb5b  pypy2.7-v7.3.2rc2-osx64.tar.bz2\n41344e1e4d27d774780e9cace6e70c5025b510c82de708ea55b64d21ed0c2f40  pypy2.7-v7.3.2rc2-s390x.tar.bz2\n144bfc9607e6319ba950de9a4d1587020e3f1311cc25a79d1711de78c5992f4f  pypy2.7-v7.3.2rc2-src.tar.bz2\nf9de3fe464ca11dfcdd6816b64051f03bdba7c66755b17ddd4f071c4d08cc0fb  pypy2.7-v7.3.2rc2-src.zip\n01a9b5b266fde443698cb01c7bac843cc0ed8747f47f1e8930666a4303bf83b2  pypy2.7-v7.3.2rc2-win32.zip\n\n925543a3161153d9b15df49000e96ce2625bf4371619667b5f37616b699acc21  pypy2.7-v7.3.2rc1-linux32.tar.bz2\n6216e1bbac3b86bfd38d16f0685c34c8c9c7aaf908ebd00388844ec295b89c17  pypy2.7-v7.3.2rc1-linux64.tar.bz2\na6fcdb44f12379eb1a547750322bd4c154b6e0c5ee30f9de2d9e2b86b2f2f319  pypy2.7-v7.3.2rc1-osx64.tar.bz2\n9f58b5bacab010d945d9c31e8b7a2539034858f4cdf048f016d8d04430688cc6  pypy2.7-v7.3.2rc1-src.tar.bz2\n0c86b52f6ad09dce1275427c18a216a0cbb5cf0db89eba2389e97ae81416eef7  pypy2.7-v7.3.2rc1-src.zip\nbbb737f4ce714af0e7797fc951f5231b26ee10f8bca3d969c5b732982f952957  pypy2.7-v7.3.2rc1-win32.zip\npypy2.7-7.3.1 sha256:\n094f23ab262e666d8740bf27459a6b1215a628dad9b6c2a88f1ed5c793fab267  pypy2.7-v7.3.1-aarch64.tar.bz2\ncd155d06cd0956d9de4a16e8a6bdf0722cb45b5bc4bbf805825d393ebd6690ad  pypy2.7-v7.3.1-linux32.tar.bz2\nbe74886547df7bf7094096a11fc0a48496779d0d1b71901797b0c816f92caca3  pypy2.7-v7.3.1-linux64.tar.bz2\ndfd4651243441d2f8f1c348e9ecc09848642d0c31bb323aa8ac320e5b9f232f0  pypy2.7-v7.3.1-osx64.tar.bz2\n1b65e085118e44ac57d38a9ba79516c68bf1fdcd65c81c66b5b5ffff06b4463b  pypy2.7-v7.3.1-ppc64.tar.bz2\nd81c7177e25bd8b1c99081e32362a29ee467ccd310b17a11161f4a9b96222b20  pypy2.7-v7.3.1-ppc64le.tar.bz2\n71ad5132a6fd32af0b538c17ebd1e0bfe5f5dfa74b129bce242bd28357bf35fc  pypy2.7-v7.3.1-s390x.tar.bz2\nfa3771514c8a354969be9bd3b26d65a489c30e28f91d350e4ad2f4081a9c9321  pypy2.7-v7.3.1-src.tar.bz2\n71d764c94f467f9dd75b6af086e2b69e0d520bf6227bcb39055c24c799c135be  pypy2.7-v7.3.1-src.zip\ne3c0dfb385d9825dd7723f26576d55d43ed92f1178f2399ab39e9fa11621a47b  pypy2.7-v7.3.1-win32.zip\npypy3.6-7.3.1 sha256:\n0069bc3c1570b935f1687f5e128cf050cd7229309e48fad2a2bf2140d43ffcee  pypy3.6-v7.3.1-aarch64.tar.bz2\n2e7a818c67f3ac0708e4d8cdf1961f30cf9586b3f3ca2f215d93437c5ea4567b  pypy3.6-v7.3.1-linux32.tar.bz2\nf67cf1664a336a3e939b58b3cabfe47d893356bdc01f2e17bc912aaa6605db12  pypy3.6-v7.3.1-linux64.tar.bz2\nd9c1778cd1ba37e129b495ea0f35ccdd9b68f5cd9d33ef0ce24e955c16d8840b  pypy3.6-v7.3.1-osx64.tar.bz2\nee02b3e65f0ca49dc09850b57835c2b65d1234f26f7991027ca6d65fadbaa4d9  pypy3.6-v7.3.1-ppc64.tar.bz2\n089fd806629ebf79cb0cb4b0c303d8665f360903b79f0df9214b58dbc42e8231  pypy3.6-v7.3.1-ppc64le.tar.bz2\n147592888e25678c1ae1c2929dc7420b3a0990117fdb25f235cb22476b4e4b5a  pypy3.6-v7.3.1-s390x.tar.bz2\n0c2cc3229da36c6984baee128c8ff8bb4516d69df1d73275dc4622bf249afa83  pypy3.6-v7.3.1-src.tar.bz2\n91e7ba30519f2c4c1833280acfb660b48392ef57c5ed0fa4e8af78587a7b8f20  pypy3.6-v7.3.1-src.zip\n752fbe8c4abee6468e5ce22af82818f821daded36faa65f3d69423f9c217007a  pypy3.6-v7.3.1-win32.zip\npypy2.7-7.3.0 sha256:\na3dd8d5e2a656849fa344dce4679d854a19bc4a096a0cf62b46a1be127a5d56c  pypy2.7-v7.3.0-aarch64.tar.bz2\neac1308b7d523003a5f6d20f58406d52ab14611bcec750122ae513a5a35110db  pypy2.7-v7.3.0-linux32.tar.bz2\nf4950a54378ac637da2a6defa52d6ffed96af12fcd5d74e1182fb834883c9826  pypy2.7-v7.3.0-linux64.tar.bz2\nca7b056b243a6221ad04fa7fc8696e36a2fb858396999dcaa31dbbae53c54474  pypy2.7-v7.3.0-osx64.tar.bz2\n82e62869812aa2953a4f83e96c813cbc52973dfa5e42605e72b6610ac13f2481  pypy2.7-v7.3.0-ppc64.tar.bz2\n592a6db77270b922ffa13cbeced9eabbc36c532ded9fc145f6a19073d3e78499  pypy2.7-v7.3.0-ppc64le.tar.bz2\nd254b82a00021339762198e41ba7f72316010d0f9bd4dcd7b0755185da9c005e  pypy2.7-v7.3.0-s390x.tar.bz2\nb0b25c7f8938ab0fedd8dedf26b9e73c490913b002b484c1b2f19d5844a518de  pypy2.7-v7.3.0-src.tar.bz2\n42dc84a277e7a5e635fe39bbd745f06135902c229a257123332b7555800d915b  pypy2.7-v7.3.0-src.zip\na9e3c5c983edba0313a41d3c1ab55b080816c4129e67a6c272c53b9dbcdd97ec  pypy2.7-v7.3.0-win32.zip\npypy3.6-7.3.0 sha256:\nb900241bca7152254c107a632767f49edede99ca6360b9a064141267b47ef598  pypy3.6-v7.3.0-aarch64.tar.bz2\n7045b295d38ba0b5ee65bd3f078ca249fcf1de73fedeaab2d6ad78de2eab0f0e  pypy3.6-v7.3.0-linux32.tar.bz2\nd3d549e8f43de820ac3385b698b83fa59b4d7dd6cf3fe34c115f731e26ad8856  pypy3.6-v7.3.0-linux64.tar.bz2\n87b2545dad75fe3027b4b2108aceb9fdadcdd24e61ae312ac48b449fdd452bf3  pypy3.6-v7.3.0-osx64.tar.bz2\ne2587e8da2abb12a86bf75941ce739124d2a1156367a9a3d729ac31d0841c300  pypy3.6-v7.3.0-ppc64.tar.bz2\nd6f3b701313df69483b43ebdd21b9652ae5e808b2eea5fbffe3b74b82d2e7433  pypy3.6-v7.3.0-ppc64le.tar.bz2\n0fe2f7bbf42ea88b40954d7de773a43179a44f40656f2f58201524be70699544  pypy3.6-v7.3.0-s390x.tar.bz2\n48d12c15fbcbcf4a32882a883195e1f922997cde78e7a16d4342b9b521eefcfa  pypy3.6-v7.3.0-src.tar.bz2\n8ae9efd0a2aadb19e892bbd07eca8ef51536296a3ef93964149aceba511e79ca  pypy3.6-v7.3.0-src.zip\n30e6870c4f3d8ef91890a6556a98080758000ba7c207cccdd86a8f5d358998c1  pypy3.6-v7.3.0-win32.zip\npypy2.7-7.2.0 sha256:\n57b0be053c6a5f069e23b843f38863cf7920f5eef7bc89f2e086e5c3a28a2ba9  pypy2.7-v7.2.0-aarch64.tar.bz2\n76d666e5aee54b519d6ec1af4ef0cbdc85f7f9276dd554e97deb026adfd0c936  pypy2.7-v7.2.0-linux32.tar.bz2\n05acf28e6a243026ecad933b9361d8f74b41f00818071b76b38c4694cc4c9599  pypy2.7-v7.2.0-linux64.tar.bz2\n36aa2f2440e762333569118dd0b3d5371d575c40966effa194d116c5453ddb52  pypy2.7-v7.2.0-osx64.tar.bz2\nfb51150a4ce94b0ca8587899ba69c41fc58a6b35c5340ea6926376ecb9cfcac4  pypy2.7-v7.2.0-ppc64.tar.bz2\n5c4224525657c29b815cb2c6b3f9bc5a267368cc6adf0fedb235a6052929f65f  pypy2.7-v7.2.0-ppc64le.tar.bz2\nbb7ae585ecb4d904c890e28a2c5b6bd379f57cc3d9e38ff45597ff54fa935eaa  pypy2.7-v7.2.0-s390x.tar.bz2\n55cb7757784fbe3952102447f65b27d80e6c885a464a7af1a9ce264492439dcc  pypy2.7-v7.2.0-src.tar.bz2\n897038550614d558f9f6718409b107e27903ef2b2b57ec250939d1b1ebdf0aba  pypy2.7-v7.2.0-src.zip\n956eeaaaac053e5d0917e77a3d2ad1933ab5561eb3e6e71235780b5aa5fd2bb7  pypy2.7-v7.2.0-win32.zip\npypy2.7-7.1.1 sha256:\n41ca390a76ca0d47b8353a0d6a20d5aab5fad8b0bb647b960d8c33e873d18ef5  pypy2.7-v7.1.1-linux32.tar.bz2\n73b09ef0860eb9ad7997af3030b22909806a273d90786d78420926df53279d66  pypy2.7-v7.1.1-linux64.tar.bz2\n31a17294dec96c2191885c776b4ee02112957dc874f7ba03e570537a77b78c35  pypy2.7-v7.1.1-osx64.tar.bz2\n1ef94c3a9c67c2335cee0b21753036b4696ed588b9d54b7b8036a6ae47f7001d  pypy2.7-v7.1.1-s390x.tar.bz2\n5f06bede6d71dce8dfbfe797aab26c8e35cb990e16b826914652dc093ad74451  pypy2.7-v7.1.1-src.tar.bz2\nd9b07a2954ad6dbde94feffd848311e2b5169563d33e3e9f17969579b01a4158  pypy2.7-v7.1.1-src.zip\n9c59226311f216a181e70ee7b5aa4d9665a15d00f24ae02acec9af7d96355f63  pypy2.7-v7.1.1-win32.zip\npypy2.7-7.1.0 sha256:\n44ec91e8cb01caab289d8763c203f3aaf288d14325a6c42692bd1ac4e870d758  pypy2.7-v7.1.0-linux32.tar.bz2\nfef176a29a2ef068c00c8098e59dab935ca6e956f089672b3f7351da95a034f5  pypy2.7-v7.1.0-linux64.tar.bz2\n8be43685ce718b0768387450fc6dc395d60809b778b6146c353ef67826022153  pypy2.7-v7.1.0-osx64.tar.bz2\nb065f55741bcb37863f1eca30ce91c9d79159371a6994100930cdc2ede3237bc  pypy2.7-v7.1.0-s390x.tar.bz2\nb051a71ea5b4fa27d0a744b28e6054661adfce8904dcc82500716b5edff5ce4b  pypy2.7-v7.1.0-src.tar.bz2\ne60ce30f9947844da43daaa7658adc0c05330681305225954114772f42df06ec  pypy2.7-v7.1.0-src.zip\n76658c9ad679d562b8b6a09d006caa666406337b9834ff56db16980c5e549f20  pypy2.7-v7.1.0-win32.zip\npypy3.6-7.2.0 sha256:\nf82dc9dc6c692417ee9727f23beae75364a5757ebdc657a2a1d0010ac3ad17ab  pypy3.6-v7.2.0-aarch64.tar.bz2\n45e99de197cb3e974cfc8d45e0076ad2066852e61e56b3eafd1237efafd2c43e  pypy3.6-v7.2.0-linux32.tar.bz2\naa128e555ad0fe5c4c15104ae0903052bd232b6e3a73f5fe023d27b8fd0d6089  pypy3.6-v7.2.0-linux64.tar.bz2\n836abb0ec303b90a684533711ed3b8269d3e8c64805b595e410920abdea678ac  pypy3.6-v7.2.0-osx64.tar.bz2\n14021d196e393b3a6d2395ab94ceec347753715e37223efe4c50b7c141b351a2  pypy3.6-v7.2.0-ppc64.tar.bz2\n6aef73a3b68e9a6c062cadd83d3db16790960cf97401ca6f2aad2195e9b05c35  pypy3.6-v7.2.0-ppc64le.tar.bz2\na11da8118064db102d159e9221319c428b298c4a87f26166fd6ae94be8d6ae0d  pypy3.6-v7.2.0-s390x.tar.bz2\n0d7c707df5041f1593fe82f29c40056c21e4d6cb66554bbd66769bd80bcbfafc  pypy3.6-v7.2.0-src.tar.bz2\n405ac35695dd374d5ea192cb44cb47231f9a65812cc7b6549df33df12ffe54db  pypy3.6-v7.2.0-src.zip\nc926f622bec24a8b348591d631717ace83b3a6c3c2dac02b157b622b97d1fc9c  pypy3.6-v7.2.0-win32.zip\npypy3.6-7.1.1 sha256:\ncb11ef4b0df569c28390b1ee93029159e1b90bfbad98df6abd629d5203b2abd9  pypy3.6-v7.1.1-linux32.tar.bz2\n8014f63b1a34b155548852c7bf73aab2d41ebddf2c8fb603dc9dd8509be93db0  pypy3.6-v7.1.1-linux64.tar.bz2\na5c2f2bfa2b4a4d29e8a67baab95699b169054066df218a14f171bb84a6df0c0  pypy3.6-v7.1.1-osx64.tar.bz2\n4a91bf2d9a142b6dbf82b5301cb510535ae9a54e1645546b2e0735a7b5ed85ba  pypy3.6-v7.1.1-s390x.tar.bz2\n6a3ef876e3691a54f4cff045028ec3be94ab9beb2e99f051b83175302c1899a8  pypy3.6-v7.1.1-src.tar.bz2\n4a3ebeb767740f2dc0b886d02797d21d7d69f154cf951bb991c19bd485e6cae1  pypy3.6-v7.1.1-src.zip\n8b513b254de5f31890f5956569de9aec3a0a91d7aba72fc89d66901f4a8ccf49  pypy3.6-v7.1.1-win32.zip\npypy 3.6-v7.1.0 sha256:\n031bfac61210a6e161bace0691b854dc15d01b0e624dc0588c544ee5e1621a83  pypy3.6-v7.1.0-linux32.tar.bz2\n270dd06633cf03337e6f815d7235e790e90dabba6f4b6345c9745121006925fc  pypy3.6-v7.1.0-linux64.tar.bz2\nd46e005ba095cb4a7006079ffbf4fe63c18cf5e9d8ce9ce8383efc1a4863ab5b  pypy3.6-v7.1.0-osx64.tar.bz2\n243cd0cc188a94c1f064f402ae72b8ba4303eb3137eac53c53826472b8005098  pypy3.6-v7.1.0-s390x.tar.bz2\nfaa81f469bb2a7cbd22c64f22d4b4ddc5a1f7c798d43b7919b629b932f9b1c6f  pypy3.6-v7.1.0-src.tar.bz2\n4858e7e8a0007bc3b381bd392208b28d30889a4e5a88a3c28e3d9dc4f25b654e  pypy3.6-v7.1.0-src.zip\n77a0576a3d518210467f0df2d0d9a1892c664566dc02f25d974c2dbc6b4749e7  pypy3.6-v7.1.0-win32.zip",
      "tags": "",
      "url": "https://www.pypy.org/checksums.html"
    },
    {
      "title": "Some Ways that PyPy uses Graphviz",
      "text": "Some way that PyPy uses Graphviz\nSomebody wrote this super cool thread on Twitter about using Graphviz to make\nsoftware visualize its internal state:\n\ud83e\uddf5 Make yours and everybody else's lives slightly less terrible by having all your programs print out their internal stuff as pictures; \u2728 a thread \u2728 pic.twitter.com/NjQ42bXN2E\u2014 Kate (@thingskatedid) April 24, 2021 PyPy is using this approach a lot too and I collected a few screenshots of that\ntechnique on Twitter and I thought it would make a nice blog post too!\nThe most important view early in the project, and the way that our Graphviz\nvisualizations got started was that we implemented a way to look at the control\nflow graphs of our RPython functions after type inference. They are in static\nsingle information form (SSI), a variant of SSA form. Hovering over the\nvariables shows the inferred types in the footer:\n\nThere's another view that shows the inferred call graph of the program:\n\nA related viewer shows the inferred class hierarchy (in this case the exception\nhierarchy) and you can focus on a single class, which will show you its base\nclasses and all the methods and instance attributes that were found:\n\n\nWe also have a view to show us the traces that are produced by the tracing JIT\ntests. this viewer doesn't really scale to the big traces that the full Python\ninterpreter produces, but it's really useful during testing:\n\nThen there are more traditional tree views, eg here is a parse tree for a small\npiece of Python source code:\n\nParsing-related we have visualized the DFAs of the parser in the past,\nthough the code is unfortunately lost.\nAll these visualizations are made by walking the relevant data structures and\nproducing a Graphviz input file using a bit of string manipulation, which is\nquite easy to do. Knowing a bit of Graphviz is a really useful skill, it's\nsuper easy to make throwaway visualizations.\nFor example here is a one-off thing I did when debugging our JSON parser to\nshow the properties of the objects used in a huge example json file:\n\nOn top of graphviz, we have a custom tool called the dotviewer, which is\nwritten in Python and uses Pygame to give you a zoomable, pannable, searchable\nway to look at huge Graphviz graphs. All the images in this post are\nscreenshots of that tool. In its simplest form it takes any .dot files as\ninput.\nHere's a small video dotviewer, moving around and searching in the json graph.\nBy writing a bit of extra Python code the dotviewer can also be extended to add\nhyperlinks in the graphs to navigate to different views (for example, we did\nthat for the callgraphs above).\nAll in all this is a really powerful approach to understand the behaviour of\nsome of code, or when debugging complicated problems and we have gotten a\nhuge amount of mileage out of this over the years. It can be seen as an instance\nof moldable development (\"a way of programming through which you construct\ncustom tools for each problem\"). And it's really easy to get into! The Graphviz\nlanguage is quite a simple text-based language that can be applied to a huge\namount of different visualization situations.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/04/ways-pypy-graphviz.html"
    },
    {
      "title": "PyPy v7.3.4: release of python 2.7 and 3.7",
      "text": "PyPy v7.3.4: release of python 2.7 and 3.7\nThe PyPy team is proud to release the version 7.3.4 of PyPy, which includes\ntwo different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18+ (the + is for\nbackported security updates)\nPyPy3.7,  which is an interpreter supporting the syntax and the features of\nPython 3.7, including the stdlib for CPython 3.7.10. We no longer refer to\nthis as beta-quality as the last incompatibilities with CPython (in the\nre module) have been fixed.\n\n\nWe are no longer releasing a Python3.6 version, as we focus on updating to\nPython 3.8. We have begun streaming the advances towards this goal on Saturday\nevenings European time on https://www.twitch.tv/pypyproject. If Python3.6 is\nimportant to you, please reach out as we could offer sponsored longer term\nsupport.\nThe two interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the other 7.3\nreleases. Highlights of the release include binary Windows 64 support,\nfaster numerical instance fields, and a preliminary HPy backend.\nA new contributor (Ondrej Baranovi\u010d - thanks!) took us up on the challenge to get\nwindows 64-bit support.  The work has been merged and for the first time we\nare releasing a 64-bit Windows binary package.\nThe release contains the biggest change to PyPy's implementation of the\ninstances of user-defined classes in many years. The optimization was\nmotivated by the report of performance problems running a numerical particle\nemulation. We implemented an optimization that stores int and float\ninstance fields in an unboxed way, as long as these fields are type-stable\n(meaning that the same field always stores the same type, using the principle\nof type freezing). This gives significant performance improvements on\nnumerical pure-Python code, and other code where instances store many integers\nor floating point numbers.\nThere were also a number of optimizations for methods around strings and bytes,\nfollowing user reported performance problems. If you are unhappy with PyPy's\nperformance on some code of yours, please report an issue!\nA major new feature is prelminary support for the Universal mode of HPy: a\nnew way of writing c-extension modules to totally encapsulate PyObject*.\nThe goal, as laid out in the HPy documentation and recent HPy blog post,\nis to enable a migration path\nfor c-extension authors who wish their code to be performant on alternative\ninterpreters like GraalPython (written on top of the Java virtual machine),\nRustPython, and PyPy. Thanks to Oracle and IBM for sponsoring work on HPy.\nSupport for the vmprof statistical profiler has been extended to ARM64 via a\nbuilt-in backend.\nSeveral issues exposed in the 7.3.3 release were fixed. Many of them came from the\ngreat work ongoing to ship PyPy-compatible binary packages in conda-forge.\nA big shout out to them for taking this on.\nDevelopment of PyPy takes place on https://foss.heptapod.net/pypy/pypy.\nWe have seen an increase in the number of drive-by contributors who are able to\nuse gitlab + mercurial to create merge requests.\nThe CFFI backend has been updated to version 1.14.5 and the cppyy backend\nto 1.14.2. We recommend using CFFI rather than C-extensions to interact with C,\nand using cppyy for performant wrapping of C++ code for Python.\nAs always, we strongly recommend updating to the latest versions. Many fixes\nare the direct result of end-user bug reports, so please continue reporting\nissues as they crop up.\nYou can find links to download the v7.3.4 releases here:\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work. If PyPy is helping you out, we would love to hear about\nit and encourage submissions to our renovated blog site via a pull request\nto https://github.com/pypy/pypy.org\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython's JIT even better. Since the\nprevious release, we have accepted contributions from 10 new contributors,\nthanks for pitching in, and welcome to the project!\nIf you are a python library maintainer and use C-extensions, please consider\nmaking a cffi / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.7, and\nsoon 3.8. It's fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32/64 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n64-bit ARM machines running Linux.\n\n\nPyPy does support ARM 32 bit processors, but does not release binaries.\n\n\nWhat else is new?\nFor more information about the 7.3.4 release, see the full changelog.\nPlease update, and continue to help us make PyPy better.\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2021/04/pypy-v734-release-of-python-27-and-37.html"
    },
    {
      "title": "New HPy blog",
      "text": "Regular readers of this blog\nalready know\nabout HPy, a project which aims to develop a new C\nAPI for Python to make it easier/faster to support C extensions on alternative\nPython implementations, including PyPy.\nThe HPy team just published the\nfirst post of HPy new\nblog, so if you are interested in its development, make sure to check it out!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/03/new-hpy-blog.html"
    },
    {
      "title": "PyPy's blog has moved",
      "text": "For many years, PyPy has been publishing blog posts at\nhttps://morepypy.blogspot.com. From now on,\nthe posts will be here, at https://pypy.org/blog. The\nRSS feed is https://pypy.org/rss.xml. The original\ncontent has been migrated to the newer site, including comments.\n\n\nAmong the motivations for the move were:\nOne site to rule them all\nAdding the blog posts here seems like a natural extension of the web site\nrather than outsourcing it to a third-party. Since the site is generated using\nthe static site generator nikola from the github repo\nhttps://github.com/pypy/pypy.org, we also\nhave good source control for the content.\nCI previews, and github\nThose of you who follow PyPy may note something new in the URL for the repo:\nuntil now PyPy has been using mercurial as hosted\non https://foss.heptapod.net.  While\nheptapod (a community driven effort to bring mercurial\nsupport to GitLab\u2122) does provide a GitLab CI runner for the open source\noffering, on github it is easier to integrate netlify\nfor previews. Hopefully the move to the more popular github platform will\nencourage new contributors to publish their success stories around using PyPy\nand the RPython toolchain.\nComments\nComments to blog posts are generated via the utterances\njavascript plugin. The comments appear as issues in the repo.\nWhen viewing the site, a query is made to fetch the comments to the issue with\nthat name. To comment, users must authorize the utterances app to post on their\nbehalf using the GitHub\nOAuth flow.\nAlternatively, users can comment on the GitHub issue directly. The interaction\nwith github for authentication and moderation seems more natural than the\nmanual moderation required on blogspot.\nPlease prove to us that the move is worth it\nHelp us with guest blog posts, and PRs to improve the styling of the site. One\nalready open issue is that the\nnavbar needlessly uses javascript, help to keep the responsive style in pure\nCSS is welcome. The theme could also use tweaking.\nBut more importantly, we want to hear from you.  Guest blog posts about\nPyPy are welcome. Just follow the directions in the repo's README to create a\nPR with your favorite PyPy story.\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2021/03/pypys-blog-has-moved.html"
    },
    {
      "title": "Mac meets Arm64",
      "text": "Looking for sponsorship\n\nApple now ships Macs which are running on an arm64 variant machine with the\nlatest version of MacOS, Big Sur M1.  We are getting requests for PyPy to\nsupport this new architecture.  Here is our position on this topic (or at least\nmine, Armin Rigo's), and how you can help.\n\nPorting PyPy is harder than just re-running the compiler, because PyPy contains\na few big architecture-dependent \"details\", like the JIT compiler and the\nforeign function interfaces (CFFI and ctypes).\n\nFixing the JIT compiler should not be too much work: we already support arm64,\njust the Linux one.  But Apple made various details different (like the calling\nconventions).  A few other parts need to be fixed too, notably CFFI and ctypes,\nagain because of the calling conventions.\n\nFixing that would be a reasonable amount of work.  I would do it myself for a\nsmall amount of money.  However, the story doesn't finish here.  Obviously, the\nstart of the story would be to get ssh access to a Big Sur M1 machine.  (If at\nthis point you're thinking \"sure, I can give you ssh access for three months\",\nthen please read on.)  The next part of the story is that we need a machine\navailable long term.  It can be either a machine provided and maintained by a\nthird party, or alternatively a pot of money big enough to support the\nacquision of a machine and ongoing work of one of us.\n\nIf we go with the provided-machine solution:  What we need isn't a lot of\nresources.  Our CI requires maybe 10 GB of disk space, and a few hours of CPU\nper run.  It should fit into 8 GB of RAM.  We normally do a run every night but\nwe can certainly lower the frequency a bit if that would help.  However, we'd\nideally like some kind of assurance that you are invested into maintaining the\nmachine for the next 3-5 years (I guess, see below).  We had far too many\nmachines that disappeared after a few months.\n\nIf we go with the money-supported solution: it's likely that after 3-5 years\nthe whole Mac base will have switched to arm64, we'll drop x86-64 support for\nMac, and we'll be back to the situation of the past where there was only one\nkind of Mac machine to care about.  In the meantime, we are looking at 3-5\nyears of lightweight extra maintenance.  We have someone that has said he would\ndo it, but not for free.\n\nIf either of these two solutions occurs, we'll still have, I quote, \"probably\nsome changes in distutils-type stuff to make python happy\", and then some\npackaging/deployment changes to support the  \"universal2\" architecture, i.e.\nincluding both versions inside a single executable (which will not be just an\nextra switch to clang, because the two versions need a different JIT backend\nand so must be translated separately).\n\nSo, now all the factors are on the table.  We won't do the minimal \"just the\nJIT compiler fixes\" if we don't have a plan that goes farther.  Either we get\nsufficient money, and maybe support, and then we can do it quickly; or PyPy\nwill just remain not natively available on M1 hardware for the next 3-5 years.\nWe are looking forward to supporting M1, and view resources contributed by\nthe community as a vote of confidence in assuring the future of PyPy on this\nhardware.  Contact us: pypy-dev@python.org, or our private mailing\nlist pypy-z@python.org.\n\nThanks for reading!\n\nArmin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2020/12/mac-meets-arm64-940822335619099039.html"
    },
    {
      "title": "PyPy 7.3.3 triple release: python 3.7, 3.6, and 2.7",
      "text": "The PyPy team is proud to release the version 7.3.3 of PyPy, which includes\nthree different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.18 (updated from the\nprevious version)PyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.12 (updated from the\nprevious version).PyPy3.7 beta: which is our second release of an interpreter supporting the\nsyntax and the features of Python 3.7, including the stdlib for CPython\n3.7.9. We call this beta quality software, there may be issues about\ncompatibility with new and changed features in CPython 3.7.\nPlease let us know what is broken or missing. We have not implemented the\ndocumented changes in the re module, and a few other pieces are also\nmissing. For more information, see the PyPy 3.7 wiki page\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the 7.3\nreleases, but read on to find out what is new.\nSeveral issues found in the 7.3.2 release were fixed. Many of them came from the\ngreat work by conda-forge to ship PyPy binary packages.  A big shout out\nto them for taking this on.\nDevelopment of PyPy has moved to https://foss.heptapod.net/pypy/pypy.\nThis was covered more extensively in this blog post. We have seen an\nincrease in the number of drive-by contributors who are able to use gitlab +\nmercurial to create merge requests.\nThe CFFI backend has been updated to version 1.14.3. We recommend using CFFI\nrather than c-extensions to interact with C, and using cppyy for performant\nwrapping of C++ code for Python.\nA new contributor took us up on the challenge to get windows 64-bit support.\nThe work is proceeding on the win64 branch, more help in coding or\nsponsorship is welcome. In anticipation of merging this large change, we fixed\nmany test failures on windows.\nAs always, this release fixed several issues and bugs.  We strongly recommend\nupdating. Many of the fixes are the direct result of end-user bug reports, so\nplease continue reporting issues as they crop up.\nYou can find links to download the v7.3.3 releases here:\n\nhttps://pypy.org/download.html\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 2 new contributors,\nthanks for pitching in.\nIf you are a python library maintainer and use c-extensions, please consider\nmaking a cffi / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a Python interpreter, a drop-in replacement for CPython 2.7, 3.6, and\n3.7. It\u2019s fast (PyPy and CPython 3.7.4 performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)big- and little-endian variants of PPC64 running Linux,s390x running Linux64-bit ARM machines running Linux.\n\nPyPy does support ARM 32 bit processors, but does not release binaries.\u00a0\nWhat else is new?\nFor more information about the 7.3.3 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2020/11/pypy-733-triple-release-python-37-36-3446596804408262749.html"
    },
    {
      "title": "Download (advanced)",
      "text": "Contents\n\n\"JIT Compiler\" version\nLinux binaries and common distributions\nPyPy-STM 2.5.1\nOther versions\nInstalling\nInstalling more modules\nBuilding from source\nPackaging\nChecksums\n\n\nWe provide pre-compiled binaries for many platforms and OSes:\n\nthe Python2.7 compatible release \u2014 PyPy2.7 v7.3.20\nthe Python3.11 compatible release \u2014 PyPy3.11 v7.3.20\n\n\nNote\nOur nightly binary builds have the most recent bugfixes and performance\nimprovements, though they can be less stable than the official releases. See\nthis link for older versions.\n\n\n\n\nPyPy latest\n\n\n\n\n\n\n\nOS\nPyPy3.11\nPyPy2.7\nNotes\n\n\n\nLinux x86 64 bit\nDownload\nDownload\ncompatible with CentOS7 and later.\n\nWindows 64 bit\nDownload\nDownload\ncompatible with any windows 64-bit\nyou might need the VC runtime library installer vcredist.x64.exe\n\nMacOS arm64\nDownload\nDownload\nMacOS >= 11. Not signed, for signed packages use Homebrew.\n\nMacOS x86_64\nDownload\nDownload\nMacOS >= 10.15, not for Mojave and below. Not signed, for signed\npackages use Homebrew.\n\nLinux ARM64\nDownload\nDownload\ncompatible with CentOS7 and later.\n\n\n\n\n\nOther Platforms\n\n\n\n\n\n\n\nOS\nPyPy3.11\nPyPy2.7\nNotes\n\n\n\nLinux x86 32 bit\nDownload\nDownload\ncompatible with CentOS7 and later\n\n\n\n\n\"JIT Compiler\" version\nThe binaries above include a Just-in-Time compiler. On x86-32, they only work on\nCPUs that have the SSE2 instruction set (most of them do, nowadays).. They also\ncontain stackless extensions, like greenlets.\n\n\nLinux binaries and common distributions\nSince version 7.3, the linux x86 binaries ship with versions\nof OpenSSL, SQLite3, libffi, expat, and TCL/TK binary libraries linked in. This\nmake the binaries \"portable\" so that they should run on any current glibc-based\nlinux platform. The ideas were adopted from the portable-pypy package.\nThis solution to the portability problem means that the versions of the\npackaged libraries are frozen to the version shipped, so updating your system\nlibraries will not affect this installation of PyPy. Also see the note about\nSSL certificates below.\nThere are other solutions:\n\ndownload PyPy from your release vendor (usually an outdated\nversion): Ubuntu (PPA), Debian, Homebrew, MacPorts,\nFedora, Gentoo and Arch are known to package PyPy, with various\ndegrees of being up-to-date. FreshPorts packages for FreeBSD.\nrecompile the CFFI-based TCL/TK, OpenSSL, or sqlite3 modules, using system\nlibraries and the scripts in pypy/lib_pypy/pypy_tools. This solution will\nnot solve compatibility issues with libffi, since that is baked into PyPy.\nor translate your own PyPy.\n\n\n\nNote\nSSL Certificates\nWhile the linux binaries ship an OpenSSL library, they do not ship a\ncertificate store for SSL certificates. If you wish to use SSL module,\nyou will need a valid certificate store. You can use the certifi package\nand set SSL_CERT_FILE to certifi.where() or install your platform\ncertificates which should be discovered by the _ssl module.\n\n\nPrevious version can be downloaded from here, or directly from the buildbot's\nmirror.\nIf your CPU is really, really old, it may be a x86-32 without SSE2.\nThere is untested support for manually translating PyPy's JIT without\nSSE2 (--jit-backend=x86-without-sse2) but note that your machine\nis probably low-spec enough that running CPython on it is a better\nidea in the first place.\n\n\nPyPy-STM 2.5.1\nThis is a special version of PyPy!  See the Software Transactional\nMemory (STM) documentation.\n\nPyPy-STM Linux x86-64 binary (64bit, tar.bz2 built on Ubuntu 12.04 - 16.04)\n\n\n\nOther versions\nThe other versions of PyPy are:\n\nTry the most up-to-date nightly binary builds , if the official\nrelease is too old for what you want to do.\nReverse debugger: This version enables debugging your Python\nprograms by going forward and backward in time.  See the RevDB\ndocumentation.\n\n\nOld-style sandboxing: A special safe version.\nThis is NOT the version announced in-development during 2019!\nRead the docs about sandboxing.\nThis version is not supported and not actively maintained.  You\nwill likely have to fix some issues yourself, or checkout an old\nversion, or otherwise play around on your own.  We provide this\ndocumentation only for historical reasons.  Please do not use in\nproduction.  For reference, there are some very old, unmaintained\nbinaries for Linux (32bit, 64bit).\n\n\n\nInstalling\nAll binary versions are packaged in a tar.bz2 or zip file.  When\nuncompressed, they run in-place.  You can uncompress them\neither somewhere in your home directory or, say, in /opt.\nIf you want, put a symlink from somewhere like\n/usr/local/bin/pypy to /path/to/pypy_expanded/bin/pypy.  Do\nnot move or copy the executable pypy outside the tree --- put\na symlink to it, otherwise it will not find its libraries.\n\n\nInstalling more modules\nThe typical pip workflow for packages with binary extensions\nrequires that the package maintainers provide a wheel for PyPy, which is\nsometimes too much work for the overburdened maintainers. For more information\nsee the installation documentation_\nIf you use your distribution's PyPy package we recommend you install packages\ninto a virtualenv. If you try to build a module and the build process complains\nabout \"missing Python.h\", you may need to install the pypy-dev package.\n\n\nBuilding from source\n(see more build instructions)\n\nGet the source code.  The preferred way is to checkout the current\ntrunk using git.  The trunk usually works and is of course\nmore up-to-date:\ngit clone https://github.com/pypy/pypy\n\nThe trunk contains PyPy 2.  For PyPy 3, switch to the correct branch:\n# switch to the branch that implements Python 3.10\ngit checkout branches/py3.10\n\nAlternatively, get one of the following smaller packages for the source at\nthe same revision as the above binaries:\n\npypy3.11-v7.3.20-src.tar.bz2 (sources, PyPy 3.10 only)\npypy2.7-v7.3.20-src.tar.bz2 (sources, PyPy 2.7 only)\n\n\nMake sure you installed the dependencies.  See the list here.\n\nEnter the goal directory:\ncd pypy/pypy/goal\n\n\nRun the rpython script.  Here are the common combinations\nof options (works also with python instead of pypy;\nrequires CPython 2.7 or PyPy 2, even to build PyPy 3):\n# get the JIT version\npypy ../../rpython/bin/rpython -Ojit targetpypystandalone\n# get the no-jit version\npypy ../../rpython/bin/rpython -O2 targetpypystandalone\n# get the sandbox version\npypy ../../rpython/bin/rpython -O2 --sandbox targetpypystandalone\n\n\nEnjoy Mandelbrot :-)  It takes on the order of half an hour to\nfinish the translation, and about 3GB of RAM on a 32-bit system\nand about 5GB on 64-bit systems.  (Do not start a translation on a\nmachine with insufficient RAM!  It will just swap forever.  See\nnotes below in that case.)\nIf you want to install this PyPy as root, please read the next section,\nPackaging.\n\nNotes:\n\nIt is recommended to use PyPy to do translations, instead of using CPython,\nbecause it is twice as fast.  You should just start by downloading an\nofficial release of PyPy (with the JIT).  If you really have to use CPython\nthen note that we are talking about CPython 2.7 here, not CPython 3.x.\n(Older versions like 2.6 are out.)\nOn some 32-bit systems, the address space limit of 2 or 3 GB of RAM\ncan be an issue.  More generally you may be just a little bit low of\nRAM.  First note that 2 GB is really not enough nowadays; on Windows\nyou first need to refer to the Windows build instructions.  More\nprecisely, translation on 32-bit takes at this point 2.7 GB if PyPy is\nused and 2.9 GB if CPython is used.  There are two workarounds:\n1. use PyPy, not CPython.  If you don't have any PyPy so far, not even\nan older version, then you need to build one first, with some parts\nremoved.  So, first translate with:\ncpython2 rpython -Ojit targetpypystandalone \\\n--withoutmod-micronumpy --withoutmod-cpyext\n\nthen copy pypy-c and libpypy_c.so somewhere else, and finally\ncall it with ...pypy-c ../../rpython/bin/rpython -Ojit.\n2. if even using PyPy instead of CPython is not enough, try to tweak\nsome internal parameters.  Example (slower but saves around 400MB):\nPYPY_DONT_RUN_SUBPROCESS=1 PYPY_GC_MAX_DELTA=200MB \\\npypy --jit loop_longevity=300 ../../rpython/bin/rpython \\\n-Ojit --source\n# then read the next point about --source\n\n\nYou can run translations with --source, which only builds the C\nsource files (and prints at the end where).  Then you can cd there\nand execute make.  This is another way to reduce memory usage.\nNote that afterwards, you have to run manually pypy-c\n.../pypy/tool/build_cffi_imports.py if you want to be able to import\nthe cffi-based modules.\nLike other JITs, PyPy doesn't work out of the box on some Linux\ndistributions that trade full POSIX compliance for extra security\nfeatures.  E.g. with PAX, you have to run PyPy with paxctl -cm.\nThis also applies to translation (unless you use CPython to run the\ntranslation and you specify --source).\n\n\n\nPackaging\nOnce PyPy is translated from source, a binary package similar to those\nprovided in the section Default (with a JIT Compiler) above can be\ncreated with the package.py script:\ncd ./pypy/pypy/tool/release/\npython package.py --help  # for information\npython package.py --archive-name pypy-my-own-package-name\n\nIt is recommended to use package.py because custom scripts will\ninvariably become out-of-date.  If you want to write custom scripts\nanyway, note an easy-to-miss point: some modules are written with CFFI,\nand require some compilation.  If you install PyPy as root without\npre-compiling them, normal users will get errors like\nImportError: no module named _gdbm_cffi.  Installers need to run pypy\n_gdbm_build.py in the lib_pypy directory during the installation process\n(plus others; see the exact list in package.py).  Users seeing a broken\ninstallation of PyPy can fix it after-the-fact, by running pypy\n/path/to/lib_pypy/_gdbm_build.py.  This command produces a file\ncalled _gdbm_cffi.pypy-41.so locally, which is a C extension\nmodule for PyPy.  You can move it at any place where modules are\nnormally found: e.g. in your project's main directory, or in a\ndirectory that you add to the env var PYTHONPATH.\n\n\nChecksums\nChecksums for the downloads are here",
      "tags": "",
      "url": "https://www.pypy.org/download_advanced.html"
    },
    {
      "title": "PyPy 7.3.2 triple release: python 2.7, 3.6, and 3.7",
      "text": "The PyPy team is proud to release version 7.3.2 of PyPy, which includes\nthree different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.13PyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.9.PyPy3.7 alpha: which is our first release of an interpreter supporting the\nsyntax and the features of Python 3.7, including the stdlib for CPython\n3.7.9. We call this an alpha release since it is our first. It is based off PyPy 3.6 so\nissues should be around compatibility and not stability. Please try it out\nand let us know what is broken or missing. We have not implemented some of the\ndocumented changes in the re module, and other pieces are also\nmissing. For more information, see the PyPy 3.7 wiki page\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, all APIs are compatible with the 7.3.0 (Dec\n2019) and 7.3.1 (April 2020) releases, but read on to find out what is new.\nConda Forge now supports PyPy as a python interpreter. The support is quite\ncomplete for linux and macOS. This is the result of a lot of\nhard work and good will on the part of the Conda Forge team.  A big shout out\nto them for taking this on.\nDevelopment of PyPy has transitioning to https://foss.heptapod.net/pypy/pypy.\nThis move was covered more extensively in this blog post. We have seen an\nincrease in the number of drive-by contributors who are able to use gitlab +\nmercurial to create merge requests.\nThe CFFI backend has been updated to version 1.14.2. We recommend using CFFI\nrather than c-extensions to interact with C, and using cppyy for performant\nwrapping of C++ code for Python.\nNumPy has begun shipping wheels on PyPI for PyPy, currently for linux 64-bit\nonly.  Wheels for PyPy windows will be available from the next NumPy release. Thanks to NumPy for their support.\nA new contributor took us up on the challenge to get windows 64-bit support.\nThe work is proceeding on the win64 branch, more help in coding or\nsponsorship is welcome.\nAs always, this release fixed several issues and bugs.  We strongly recommend\nupdating. Many of the fixes are the direct result of end-user bug reports, so\nplease continue reporting issues as they crop up.You can find links to download the v7.3.2 releases here:\n\nhttps://pypy.org/download.html\nWe would like to thank our donors for the continued support of the PyPy\nproject. Please help support us at Open Collective. If PyPy is not yet good enough for your needs, we are available for\ndirect consulting work.\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 8 new contributors,\nthanks for pitching in.\nIf you are a python library maintainer and use c-extensions, please consider\nmaking a cffi / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6, and 3.7. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThis PyPy release supports:\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)big- and little-endian variants of PPC64 running Linux,s390x running Linux64-bit ARM machines running Linux.\n\nPyPy does support ARM 32 bit processors, but does not release binaries.\n\n\n\n\nWhat else is new?\nFor more information about the 7.3.2 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2020/09/pypy-732-triple-release-python-27-36-3980901335490872787.html"
    },
    {
      "title": "PyPy is on Open Collective",
      "text": "Hi all,\n\nPyPy is now a member of Open Collective, a fiscal host.  We have been thinking about switching to this organization for a couple of years; we like it for various reasons, like the budget transparency and the lightweight touch.  We can now officially announce our membership!\n\nWith this, we are now again free to use PyPy for all financial issues, like receiving funds professionally, paying parts of sprint budgets as we like, and so on.  We will shortly be reintroducing buttons that link to Open Collective from the PyPy web site.\n\nAlthough the old donation buttons were removed last year, we believe that there are still a few people that send regularly money to the SFC, the not-for-profit charity we were affiliated with.  If you do, please stop doing it now (and, if you like to do so, please set up an equivalent donation to PyPy on Open Collective).\n\nAnd by the way, sorry for all of you who were getting mixed feelings from the previous blog post (co-written with the SFC).  PyPy is committed to continue being Open Source just like before.  This was never in question.  What these two blog posts mean is only that we switched to a different organization for our internal finances.\n\nWe're looking forward to how this new relationship will go!\n\nArmin Rigo, for the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2020/08/pypy-is-on-open-collective-5673322428814364737.html"
    },
    {
      "title": "A new chapter for PyPy",
      "text": "PyPy winds down its membership in the Software Freedom Conservancy\n\nConservancy and PyPy's great work together\n\nPyPy joined Conservancy in\nthe second half of 2010, shortly after the release of\nPyPy 1.2, the first version to contain a fully functional JIT. In 2013, PyPy\nstarted supporting ARM, bringing its just-in-time speediness to many more devices and began working toward supporting NumPy to help\nscientists crunch their numbers faster. Together, PyPy and Conservancy ran successful fundraising drives and facilitated payment\nand oversight for contractors and code sprints.\n\nConservancy supported PyPy's impressive growth as it expanded support for\ndifferent hardware platforms, greatly improved the performance of C extensions,\nand added support for Python 3 as the language itself evolved.\n\nThe road ahead\n  \nConservancy provides a fiscal and organizational home for projects that find the\nfreedoms and guardrails that come along with a charitable home advantageous for\ntheir community goals. While this framework was a great fit for the early PyPy\ncommunity, times change and all good things must come to an end.\n\nPyPy will remain a free and open source project, but the community's structure\nand organizational underpinnings will be changing and the PyPy community will be\nexploring options outside of the charitable realm for its next phase of growth\n(\"charitable\" in the legal sense -- PyPy will remain a community project).\n\nDuring the last year PyPy and Conservancy have worked together to properly\nutilise the generous donations made by stalwart PyPy enthusiats over the years\nand to wrap up PyPy's remaining charitable obligations. PyPy is grateful for\nthe Conservancy's help in shepherding the project toward its next chapter.\n\nThank yousFrom Conservancy: \"We are happy that Conservancy was able to help PyPy bring important software\nfor the public good during a critical time in its history. We wish the\ncommunity well and look forward to seeing it develop and succeed in new ways.\" \u2014 Karen Sandler, Conservancy's Executive DirectorFrom PyPy:\"PyPy would like to thank Conservancy for their decade long support in\nbuilding the community and wishes Conservancy continued success in their\njourney promoting, improving, developing and defending free and open source\nsofware.\" \u2014 Simon Cross & Carl Friedrich Bolz-Tereick, on behalf of PyPy.\n\n\nAbout\n\nPyPy is a multi-layer python interpreter with a built-in JIT compiler that runs\nPython quickly across different computing environments.\nSoftware Freedom Conservancy (Conservancy) is a charity that provides a home\nto over forty free and open source software projects.",
      "tags": "pypy",
      "url": "https://www.pypy.org/posts/2020/08/a-new-chapter-for-pypy-8388322709667328389.html"
    },
    {
      "title": "PyPy 7.3.1 released",
      "text": "The PyPy team is proud to release the version 7.3.1 of PyPy, which includes\ntwo different interpreters:\n\n\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.13\nPyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.9.\n\n\n\nThe interpreters are based on much the same codebase, thus the multiple\nrelease. This is a micro release, no APIs have changed since the 7.3.0 release\nin December, but read on to find out what is new.\n\n\nConda Forge now supports PyPy as a Python interpreter. The support right now\nis being built out. After this release, many more c-extension-based\npackages can be successfully built and uploaded. This is the result of a lot of\nhard work and good will on the part of the Conda Forge team.  A big shout out\nto them for taking this on.\n\n\nWe have worked with the Python packaging group to support tooling around\nbuilding third party packages for Python, so this release updates the pip and\nsetuptools installed when executing pypy -mensurepip to pip>=20. This\ncompletes the work done to update the PEP 425 python tag from pp373 to\nmean \u201cPyPy 7.3 running python3\u201d to pp36 meaning \u201cPyPy running Python\n3.6\u201d (the format is recommended in the PEP). The tag itself was\nchanged in 7.3.0, but older pip versions build their own tag without querying\nPyPy. This means that wheels built for the previous tag format will not be\ndiscovered by pip from this version, so library authors should update their\nPyPy-specific wheels on PyPI.\n\n\nDevelopment of PyPy is transitioning to https://foss.heptapod.net/pypy/pypy.\nThis move was covered more extensively in the blog post from last month.\n\n\nThe CFFI backend has been updated to version 14.0. We recommend using CFFI\nrather than c-extensions to interact with C, and using cppyy for performant\nwrapping of C++ code for Python. The cppyy backend has been enabled\nexperimentally for win32, try it out and let use know how it works.\n\n\nEnabling cppyy requires a more modern C compiler, so win32 is now built\nwith MSVC160 (Visual Studio 2019). This is true for PyPy 3.6 as well as for 2.7.\n\n\nWe have improved warmup time by up to 20%, performance of io.StringIO to\nmatch if not be faster than CPython, and improved JIT code generation for\ngenerators (and generator expressions in particular) when passing them to\nfunctions like sum, map, and map that consume them. Performance of closures has also be improved in certain situations.\n\n\nAs always, this release fixed several issues and bugs raised by the growing\ncommunity of PyPy users.  We strongly recommend updating. Many of the fixes are\nthe direct result of end-user bug reports, so please continue reporting issues\nas they crop up.\n\nYou can find links to download the v7.3.1 releases here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\n\n\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 13 new contributors,\nthanks for pitching in.\n\n\nIf you are a Python library maintainer and use c-extensions, please consider\nmaking a cffi / cppyy version of your library that would be performant on PyPy.\nIn any case both cibuildwheel and the multibuild system support\nbuilding wheels for PyPy wheels.\n\n\n\n\n\u00a0\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6, and soon 3.7. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\n\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\n\nThis PyPy release supports:\n\n\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n64-bit ARM machines running Linux.\n\n\n\n\n\n\nWhat else is new?\nFor more information about the 7.3.1 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team\n\n\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2020/04/pypy-731-released-6266451647387657480.html"
    },
    {
      "title": "Leysin 2020 Sprint Report",
      "text": "At the end of February ten of us gathered in Leysin, Switzerland to work on\na variety of topics including HPy, PyPy Python 3.7 support and the PyPy\nmigration to Heptapod.\n\n\n\n\nWe had a fun and productive week. The snow was beautiful. There was skiing\nand lunch at the top of Berneuse, cooking together, some late nights at\nthe pub next door, some even later nights coding, and of course the\nobligatory cheese fondue outing.\n\nThere were a few of us participating in a PyPy sprint for the first time\nand a few familiar faces who had attended many sprints. Many different\nprojects were represented including PyPy, HPy, GraalPython,\nHeptapod, and rust-cpython. The atmosphere was relaxed and welcoming, so if\nyou're thinking of attending the next one -- please do!\n\nTopics worked on:\n\n\nHPy\nHPy is a new project to design and implement a better API for extending\nPython in C. If you're unfamiliar with it you can read more about it at\nHPy.\n\nA lot of attention was devoted to the Big HPy Design Discussion which\ntook up two full mornings. So much was decided that this will likely\nget its own detailed write-up, but bigger topics included:\n\nthe HPy GetAttr, SetAttr, GetItem and SetItem methods,\nHPy_FromVoidP and HPy_AsVoidP for passing HPy handles to C functions\nthat pass void* pointers to callbacks,\navoiding having va_args as part of the ABI,\nexception handling,\nsupport for creating custom types.\n\nQuite a few things got worked on too:\n\nimplemented support for writing methods that take keyword arguments with\nHPy_METH_KEYWORDS,\nimplemented HPy_GetAttr, HPy_SetAttr, HPy_GetItem, and HPy_SetItem,\nstarted implementing support for adding custom types,\nstarted implementing dumping JSON objects in ultrajson-hpy,\nrefactored the PyPy GIL to improve the interaction between HPy and\nPyPy's cpyext,\nexperimented with adding HPy support to rust-cpython.\n\nAnd there was some discussion of the next steps of the HPy initiative\nincluding writing documentation, setting up websites and funding, and\npossibly organising another HPy gathering later in the year.\n\n\nPyPy\n\nGeorges gave a presentation on the Heptapod topic and branch workflows\nand showed everyone how to use hg-evolve.\nWork was done on improving the PyPy CI buildbot post the move to\nheptapod, including a light-weight pre-merge CI and restricting\nwhen the full CI is run to only branch commits.\nA lot of work was done improving the -D tests. \n\n\n\nMiscellaneous\n\nArmin demoed VRSketch and NaN Industries in VR, including an implementation\nof the Game of Life within NaN Industries!\nSkiing!\n\n\n\nAftermath\nImmediately after the sprint large parts of Europe and the world were\nhit by the COVID-19 epidemic. It was good to spend time together before\ntravelling ceased to be a sensible idea and many gatherings were cancelled.\n\nKeep safe out there everyone.\n\nThe HPy & PyPy Team & Friends\n\nIn joke for those who attended the sprint: Please don't replace this blog post\nwith its Swedish translation (or indeed a translation to any other language :).",
      "tags": "cpyext,CPython,GraalPython,Heptapod,hpy,pypy,pypy3",
      "url": "https://www.pypy.org/posts/2020/03/leysin-2020-sprint-report-764567777353955897.html"
    },
    {
      "title": "PyPy and CFFI have moved to Heptapod",
      "text": "It has been a very busy month, not so much because of deep changes in the JIT of PyPy but more around the development, deployment, and packaging of the project.\n\n\n\u00a0\n\nHosting\nThe biggest news is that we have moved the center of our development off Bitbucket and to the new https://foss.heptapod.net/pypy. This is a friendly fork of Gitlab called heptapod that understands Mercurial and is hosted by Clever Cloud. When Atlassian decided to close down Mercurial hosting on bitbucket.org, PyPy debated what to do. Our development model is based on long-lived branches, and we want to keep the ability to immediately see which branch each commit came from. Mercurial has this, git does not (see our FAQ). Octobus, whose business is Mercurial, developed a way to use Mercurial with Gitlab called heptapod. The product is still under development, but quite usable (i.e., it doesn't get in the way). Octobus partnered with Clever Cloud hosting to offer community FOSS projects hosted on Bitbucket who wish to remain with Mercurial a new home. PyPy took them up on the offer, and migrated its repos to https://foss.heptapod.net/pypy. We were very happy with how smooth it was to import the repos to heptapod/GitLab, and are learning the small differences between Bitbucket and GitLab. All the pull requests, issues, and commits kept the same ids, but work is still being done to attribute the issues, pull requests, and comments to the correct users. So from now on, when you want to contribute to PyPy, you do so at the new home.\n\nCFFI, which previously was also hosted on Bitbucket, has joined the PyPy group at https://foss.heptapod.net/pypy/cffi.\n\n\n\u00a0\n\nWebsite\nSecondly, thanks to work by https://baroquesoftware.com/ in leading a redesign and updating the logo, the https://www.pypy.org website has undergone a facelift. It should now be easier to use on small-screen devices. Thanks also to the PSF for hosting the site.\n\n\n\u00a0\n\nPackaging\nAlso, building PyPy from source takes a fair amount of time. While we provide downloads in the form of tarballs or zipfiles, and some platforms such as debian and Homebrew provide packages, traditionally the downloads have only worked on a specific flavor of operating system. A few years ago squeaky-pl started providing portable builds. We have adopted that build system for our linux offerings, so the nightly downloads and release downloads should now work on any glibc platform that has not gone EndOfLife. So there goes another excuse not to use PyPy. And the \"but does it run scipy\" excuse also no longer holds, although \"does it speed up scipy\" still has the wrong answer. For that we are working on HPy, and will be sprinting soon.\nThe latest versions of pip, wheel, and setuptools, together with the manylinux2010 standard for linux wheels and tools such as multibuild or cibuildwheels (well, from the next version) make it easier for library developers to build binary wheels for PyPy. If you are having problems getting going with this, please reach out.\n\n\n\n\u00a0\n\nGive it a try\nThanks to all the folks who provide the infrastructure PyPy depends on. We hope the new look will encourage more involvement and engagement. Help prove us right!\n\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2020/02/pypy-and-cffi-have-moved-to-heptapod-5791595152472747032.html"
    },
    {
      "title": "Leysin Winter sprint 2020: Feb 29 - March 8th",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the fourteenth\ntime.  This is a fully public sprint: newcomers and topics other than\nthose proposed below are welcome.\n\n\n\n\nGoals and topics of the sprint\nThe list of topics is open.\u00a0 For reference, we would like to work at least partially on the following topics:\n\nHPy \nPython 3.7 support (buildbot status)\n\nAs usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off (for ski or anything else).\n\n\nTimes and accomodation\nThe sprint will occur for one week starting on Saturday, the 29th of February, to Sunday, the 8th of March 2020\u00a0(dates were pushed back one day!)\u00a0 It will occur in Les Airelles, a different bed-and-breakfast place from the traditional one in Leysin.\u00a0 It is a nice old house at the top of the village.\n\nWe have a 4- or 5-people room as well as up to three double-rooms.\u00a0 Please register early!\u00a0 These rooms are not booked for the sprint in advance, and might be already taken if you end up announcing yourself late.\u00a0 We have a big room for up to 7 people with nice view, which might be split in two or three sub-rooms; plus possibly separately-booked double rooms if needed. (But it is of course always possible to book at a different place in Leysin.)\n\nFor more information, see our repository or write to me directly at armin.rigo@gmail.com.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2020/01/leysin-winter-sprint-2020-feb-28-march-6349761524797409012.html"
    },
    {
      "title": "Python compatibility",
      "text": "The goal of this page is to point out some of the differences between running\npython with PyPy and with CPython\n\nTL;DR\nPure python code works, but there are a few differences with object lifetime\nmanagement. Modules that use the CPython C API will probably work, but will\nnot achieve a speedup via the JIT. We encourage library authors to use CFFI\nand HPy instead.\n\n\nRefcounting, __del__, and resource use\nThe main difference in pure-python code that is not going to be fixed is that\nPyPy does\nnot support refcounting semantics for \"automatically\" releasing state when\nan object's __del__ is called. The following code won't fill the\nfile immediately, but only after a certain period of time, when the GC\ndoes a collection and flushes the output, since the file is only closed when\nthe __del__ method is called:\nopen(\"filename\", \"w\").write(\"stuff\")\n\nThe proper fix is\nwith open(\"filename\", \"w\") as f:\n    f.write(\"stuff\")\n\nThe same problem---not closing your files---can also show up if your\nprogram opens a large number of files without closing them explicitly.\nIn that case, you can easily hit the system limit on the number of file\ndescriptors that are allowed to be opened at the same time.\nPyPy can be run with the command-line option -X track-resources (as in,\npypy -X track-resources myprogram.py). This produces a ResourceWarning\nwhen the GC closes a non-closed file or socket.  The traceback for the place\nwhere the file or socket was allocated is given as well, which aids finding\nplaces where close() is missing.\nSimilarly, remember that you must close() a non-exhausted\ngenerator in order to have its pending finally or with\nclauses executed immediately:\ndef mygen():\n    with foo:\n        yield 42\n\nfor x in mygen():\n    if x == 42:\n        break    # foo.__exit__ is not run immediately!\n\n# fixed version:\ngen = mygen()\ntry:\n    for x in gen:\n        if x == 42:\n            break\nfinally:\n    gen.close()\n\nMore generally, __del__() methods are not executed as predictively\nas on CPython: they run \"some time later\" in PyPy (or not at all if\nthe program finishes running in the meantime).  See more details\nhere.\n\n\nWhy is memory usage so high?\nNote that PyPy returns unused memory to the operating system only after\na madvise() system call (at least Linux, OS X, BSD) or on Windows.  It is\nimportant to realize that you may not see this in top.  The unused\npages are marked with MADV_FREE, which tells the system \"if you\nneed more memory at some point, grab this page\".  As long as memory is\nplentiful, the RES column in top might remains high.  (Exceptions to\nthis rule are systems with no MADV_FREE, where we use\nMADV_DONTNEED, which forcefully lowers the RES.  This includes\nLinux <= 4.4.)\n\n\nMore info\nA more complete list of known differences is available at our dev site.",
      "tags": "",
      "url": "https://www.pypy.org/compat.html"
    },
    {
      "title": "Contact",
      "text": "irc: #pypy on irc.libera.chat\nmailing list: pypy-dev at python.org\nfor security related issues, non-public funding enquiries etc. please contact pypy-z@python.org\nthe issue tracker (registration required to open new issues or to comment)\nmore on our dev site.\ncode on https://github.com/pypy/pypy\nReach out to our consultants for specific projects",
      "tags": "",
      "url": "https://www.pypy.org/contact.html"
    },
    {
      "title": "Download and Install",
      "text": "We provide pre-compiled binaries for many platforms and OSes.\n\nNote\nOur nightly binary builds have the most recent bugfixes and performance\nimprovements, though they can be less stable than the official releases. See\nthese links for other versions or more information including other\nplatforms.\n\n\n\nPyPy latest\n\n\n\n\n\n\n\nOS\nPyPy3.11\nPyPy2.7\nNotes\n\n\n\nLinux x86 64 bit\nDownload\nDownload\ncompatible with CentOS7 and later.\n\nWindows 64 bit\nDownload\nDownload\ncompatible with any windows 64-bit\nyou might need the VC runtime library installer vcredist.x64.exe\n\nMacOS arm64\nDownload\nDownload\nMacOS >= 11. Not signed, for signed packages use Homebrew.\n\nMacOS x86_64\nDownload\nDownload\nMacOS >= 10.15, not for Mojave and below. Not signed, for signed\npackages use Homebrew.\n\nLinux ARM64\nDownload\nDownload\ncompatible with CentOS7 and later.\n\n\n\n\n\nNote\nSSL Certificates\nWhile the linux binaries ship an OpenSSL library, they do not ship a\ncertificate store for SSL certificates. If you wish to use SSL module,\nyou will need a valid certificate store. You can use the certifi package\nand set SSL_CERT_FILE to certifi.where() or install your platform\ncertificates which should be discovered by the _ssl module.\n\n\n\nSource\n\n3.11 Source (tar.bz2); 3.11 Source (zip).\n2.7 Source (tar.bz2); 2.7 Source (zip).\n\n\n\nMore information\nVisit the more information page for other platforms, information about\nrunning PyPy, STM, instructions on building from source and more.\n\n\nChecksums\nChecksums for the downloads are here",
      "tags": "",
      "url": "https://www.pypy.org/download.html"
    },
    {
      "title": "PyPy - Features",
      "text": "PyPy is a replacement for CPython.  It is built using the RPython\nlanguage that was co-developed with it.  The main reason to use it\ninstead of CPython is speed: it runs generally faster (see next section).\nPyPy implements Python 2.7.18 and 3.11.11.\nIt supports all of the core language. It supports most of\nthe commonly used Python standard library modules. For known differences with\nCPython, see our compatibility page.\nThe following CPU architectures are supported and maintained:\n\nx86 (IA-32) and x86_64\nARM platforms (ARMv6 or ARMv7, with VFPv3, and Apple Silicon arm64)\nAArch64\nRISCV\nPowerPC 64bit both little and big endian\nSystem Z (s390x)\n\nPyPy's x86 version runs on several operating systems, such as Linux\n(32/64 bits), MacOS (64 bits), Windows (32 bits), OpenBSD, FreeBSD.\nNon-x86 versions are supported on Linux, and ARM64 is supported on MacOS.\nIf you are interested in helping, see our howtohelp page.\n\nThe main features of PyPy:\n\nSpeed\nOur main executable comes with a Just-in-Time compiler.  It is\nreally fast in running most benchmarks\u2014including very large and\ncomplicated Python applications, not just 10-liners.\nThere are two cases that you should be aware where PyPy will not be\nable to speed up your code:\n\nShort-running processes: if it doesn't run for at least a few seconds,\nthen the JIT compiler won't have enough time to warm up.\nIf all the time is spent in run-time libraries (i.e. in C functions),\nand not actually running Python code, the JIT compiler will not help.\n\nSo the case where PyPy works best is when executing long-running\nprograms where a significant fraction of the time is spent executing\nPython code.  This is the case covered by the majority of our\nbenchmarks, but not all of them --- the goal of PyPy is to get speed\nbut still support (ideally) any Python program.\n\n\nMemory usage\nMemory-hungry Python programs (several hundreds of MBs or more) might\nend up taking less space than they do in CPython.  It is not always\nthe case, though, as it depends on a lot of details.  Also note that\nthe baseline is higher than CPython's.\n\n\nStackless\nSupport for Stackless and greenlets are now integrated in the normal\nPyPy.  More detailed information is available here.\n\n\nOther features\nPyPy has many secondary features and semi-independent\nprojects.  We will mention here:\n\nOther languages:  we also implemented other languages that makes\nuse of our RPython toolchain: Prolog (almost complete), as\nwell as Smalltalk, JavaScript, Io, Scheme and Gameboy.\nThere is also a Ruby implementation called Topaz and a PHP implementation\ncalled HippyVM.\n\nEmulators: PyPy really shines as a platform to model hardware. The\nPydrofoil emulator for RISC-V and ARM64 ISA models written in Sail is\nvery performant, usually an order of magnitude faster than other emulators\ngenerated by Sail\n\n\n\nSandboxing\nPyPy's sandboxing is a working prototype for the idea of running untrusted\nuser programs. Unlike other sandboxing approaches for Python, PyPy's does not\ntry to limit language features considered \"unsafe\". Instead we replace all\ncalls to external libraries (C or platform) with a stub that communicates\nwith an external process handling the policy.\n\nNote\nPlease be aware that it is a prototype only.  It needs work to become\nmore complete, and you are welcome to help.  In particular, almost none\nof the extension modules work (not even time ), and pypy_interact\nis merely a demo.  Also, a more complete system would include a way\nto do the same as pypy_interact from other languages than Python,\nto embed a sandboxed interpreter inside programs written in other\nlanguages.\n\nTo run the sandboxed process, you need to get the full sources and\nbuild pypy-sandbox from it (see Building from source).  These\ninstructions give you a pypy-c that you should rename to\npypy-sandbox to avoid future confusion.  Then run:\ncd pypy/sandbox\npypy_interact.py path/to/pypy-sandbox\n# don't confuse it with pypy/goal/pyinteractive.py!\n\nYou get a fully sandboxed interpreter, in its own filesystem hierarchy\n(try os.listdir('/')).  For example, you would run an untrusted\nscript as follows:\nmkdir virtualtmp\ncp untrusted.py virtualtmp/\npypy_interact.py --tmp=virtualtmp pypy-sandbox /tmp/untrusted.py\n\nNote that the path /tmp/untrusted.py is a path inside the sandboxed\nfilesystem.  You don't have to put untrusted.py in the real /tmp\ndirectory at all.\nTo read more about its features, try pypy_interact.py --help or go to\nour documentation site.",
      "tags": "",
      "url": "https://www.pypy.org/features.html"
    },
    {
      "title": "PyPy",
      "text": "A fast, compliant alternative implementation of Python\n\n    \n    Download PyPy\nWhat is PyPy ?\nDocumentation (external link)\n\n\n\nOn average, PyPy is about 3 times faster than CPython 3.11. We currently support python 3.11 and 2.7.\n\n\n\nPyPy (with JIT) benchmark times normalized to CPython. Smaller is\nbetter. Based on the geometric average of all benchmarks\n\n\n\"... we are avid fans of PyPy and\ncommensurately thankful for the great work by the PyPy team over the\nyears. PyPy has enabled us to use Python for a larger part of our\ntoolset than CPython alone would have supported, and its smooth\nintegration with C/C++ through CFFI has helped us attain a better\ntradeoff between performance and programmer productivity in our\nprojects\"\n-- Vilhj\u00e1lmur \u00deorsteinsson, founder and CEO of Mi\u00f0eind, Feb 2022\nAdvantages and distinct Features\n\nSpeed: thanks to its Just-in-Time compiler, Python programs\noften run faster on PyPy.  (What is a JIT compiler?)\nMemory usage: memory-hungry Python programs (several hundreds of\nMBs or more) might end up taking less space than they do in CPython.\nCompatibility: PyPy is highly compatible with existing python code.\nIt supports cffi, cppyy, and can run popular python libraries like\ntwisted, and django. It can also run NumPy, Scikit-learn and more via a\nc-extension compatibility layer.\nStackless: PyPy comes by default with support for stackless mode,\nproviding micro-threads for massive concurrency.\nAs well as other features.",
      "tags": "",
      "url": "https://www.pypy.org/"
    },
    {
      "title": "The PyPy Team (from 2008)",
      "text": "Armin Rigo\n\nArmin Rigo is a former researcher at the Heinrich-Heine Universitat\nD\u00fcsseldorf (Germany).  He studied Mathematics at the University\nof Lausanne (Switzerland), obtained his Ph.D. in Logic and Set\nTheory at the Free University of Brussels (Belgium) in 2002, and\nworked at the University of Southampton (UK) until 2005.  He is\nthe author of Psyco, the first just-in-time compiler for Python.\nHe is one of the founders and lead developers of the PyPy project\nwhich began in 2003.  He has taken part in all areas, from the Python\nlanguage definition to the RPython translation framework,\nincluding the garbage collector and the tracing just-in-time\ncompiler.\n\n\nMaciej Fija\u0142kowski\n\nMaciej is a freelancer working mostly on PyPy for the past several years.\nHe's a core developer since 2006, working on all kinds of parts in\nthe entire codebase including JIT, GC and assembler backends.\nMaciej has been going to many conferences, advertising PyPy to a broader\naudience for the past several years, including a keynote at Pycon 2010.\nHe's also the main maintainer of\njitviewer, a tool for analyzing performance of your python programs under\nPyPy.\n\n\nCarl Friedrich Bolz\n\nCarl Friedrich is a core developer since 2005, currently doing his PhD at the\nHeinrich-Heine Universit\u00e4t D\u00fcsseldorf (Germany). He has worked on most aspects\nof PyPy, from the core interpreter to the GC to the JIT. He has published\nseveral papers about the inner workings of PyPy, presenting them at various\nscientific conferences. Carl Friedrich is also interested in other dynamic\nlanguage implementation and was the original author of the Prolog\nimplementation.\nCarl Friedrich likes science fiction novels and sometimes plays the bassoon.\n\n\nAntonio Cuni\n\nAntonio Cuni loves skiing, mountains and programming languages.  He studied\nComputer Science at the University of Genova (Italy), and then at the same\nuniversity he obtained his Ph.D. in Computer Science in 2010, with a\ndissertation about the PyPy CLI JIT backend.  He has been a core PyPy\ndeveloper since 2006, working in various areas including the \"object oriented\nbackends\" for the CLI and JVM, the RPython translation framework, the Python\ninterpreter and the JIT compiler generator.  Apart from PyPy, he is the author of\nother popular tools such as pdb++.\n\n\nBenjamin Peterson\nBoth a PyPy and CPython core developer, Benjamin knows way too much about the\nnooks and cranies of the Python language. He is driven by a fascination with\ninterpreters and compilers of all shapes and sizes. Around the PyPy project, he\ntries to be generally useful and has taken on major projects including rewriting\nPyPy's Python compiler and porting PyPy to Python 2.7.\n\n\nAlex Gaynor\n\nAlex is software engineer living in Washington, DC. He's been a PyPy developer\nsince 2010, and has worked on many parts of the codebase, including the JIT\ncompiler's optimizers, the RPython translation toolchain, and the Python\ninterpreter. In addition to his work on PyPy, Alex is also the creator of\nTopaz, a Ruby VM built on RPython and a core developer of Django (a Python web\nframework) and CPython, as well as a retired member of the board of directors\nof the Python Software Foundation.\n\n\nH\u00e5kan Ard\u00f6\n\nH\u00e5kan Ard\u00f6 received his master of science degree in electrical\nengineering from Lund University in 2002. He specialized in\nVLSI-design and Image Processing. He worked as a software\nengineer at Axis Communications 2002-2003 before doing his\nPhD at the Centre for Mathematical Sciences of Lund University\n2003-2009 in the Mathematical Imaging Group. His thesis work consisted\nof designing image processing algorithms for traffic surveillance,\naiming for a system that automatically measures the safety of an\nintersection or road segment. He is currently working part-time as a\npostdoc at the Centre for Mathematical Sciences of Lund University\ncontinuing this work and part-time as CTO with a spinoff company\nCognimatics. His contributions to PyPy started 2010 and consists of\nthe array module as well as work on the JIT compiler's trace optimizers.\n\n\nHolger Krekel\n\nHolger Krekel is a founder of the PyPy project and has participated in\nPyPy core development for several years as well as maintained much of\nits infrastructure.  He also is the author of the popular py.test and\ntox testing tools as well as execnet, a library for easily deploying\ndifferent interacting Python interpreters side by side.  He helped\nmanage multiple PyPy funding contracts through his company merlinux and was a\nPyPy representative within the Software Freedom Conservancy (SFC).  He\nholds a summa cum laude degree in computer science with a thesis about\nartificial intelligence applied to the game of Go.  As of 2011 he is on\nanother sabbatical-ish leave, caring for his newborn son, travelling\nand pondering what comes next.  Other than that he continues to care\nfor testing and some PyPy co-ordination bits behind the scene.\n\n\nSamuele Pedroni\nSamuele Pedroni got involved with PyPy almost at its inception in the\nspring of 2003. One of the design contributors to PyPy, his help has\nranged from infrastructure and processes, through building out\nRPython... optimizing the Python interpreter, to compressing resume\ndata in the last incarnation of the JIT compiler. Tempted away into the\napplication side of the software equation, these days he contributes\nsome words and wisdom to PyPy's paper writing.\n\n\nMany more people\nPyPy is and has always been an effort of many volunteers. Consult the LICENSE\nfile for details.",
      "tags": "",
      "url": "https://www.pypy.org/people.html"
    },
    {
      "title": "Performance",
      "text": "Contents\n\nProfiling: vmprof\nOptimization strategy\nMicro-tuning tips\n\n\nThis document collects strategies, tactics and tricks for making your\ncode run faster under PyPy.  Many of these are also useful hints for\nstock Python and other languages.  For contrast, we also describe some\nCPython (stock Python) optimizations that are not needed in PyPy.\n\n\nProfiling: vmprof\nAs a general rule, when considering performance issues, follow these\nthree points: first measure them (it is counter-productive to fight\nimaginary performance issues); then profile your code (it is useless\nto optimize the wrong parts).  Only optimize then.\nPyPy 2.6 introduced vmprof, a very-low-overhead statistical profiler.\nThe standard, non-statistical cProfile is also supported, and can be\nenabled without turning off the JIT.  We do recommend vmprof anyway\nbecause turning on cProfile can distort the result (sometimes massively,\nthough hopefully this should not be too common).\n\n\n\nOptimization strategy\nThese suggestions apply to all computer languages.  They're here as\nreminders of things to try before any Python or PyPy-specific tweaking.\n\nBuild a regression-test suite\nBefore you start tuning, build a regression-test suite for your code.\nThis front-loads a significant amount of work, but it means you can\ntry lots of optimizations without worrying so much about introducing\nfunctional bugs.\n\n\nMeasure, don't guess\nHuman beings are bad at guessing or intuiting where the hotspots in code are.\nMeasure, don't guess; use a profiler to pin down the 20% of the\ncode where the code is spending 80% of its time, then speed-tune that.\nMeasuring will save you a lot of effort wasted on tuning parts of the code\nthat aren't actually bottlenecks.\nAs you tune, re-profile frequently  so you can see how the hottest spots\nare shifting around.\n\n\nI/O-bound is different from compute-bound\nBe aware of the difference between code that is compute-bound (slow\nbecause it's doing a huge number of instructions) and code that is I/O\nbound (slow because of disk or network delays).\nExpect to get most of your gains from optimizing compute-bound code.\nIt's usually (though not always) a sign that you're near the end of\nworthwhile tuning when profiling shows that the bulk of the\napplication's time is spent on network and disk I/O.\n\n\nTune your algorithms first\nGenerally, when your code is doing things that are O(n**2) or larger\nin the size of your data set, the cost of those operations is going\nto swamp any small gains you can pick up with the tricks we describe\nhere.\nTune your algorithms first.  It's time to think about applying our\nlist of micro-tuning tips  after you think you've optimized out\nintrinsically expensive operations.\nThat said, be prepared for the possibility that you will discover\nbetter-hidden algorithmic problems as you micro-tune.  Likely\nyou will go through this cycle more than once.\n\n\nFocus on tight loops\nIt's extremely common for high time costs to lurk within some\ninnocuous-looking code inside a tight loop - especially in code\nthat does something like a searching/matching/lookup operation\nor any kind of graph traversal.\nProbably the most common kind of performance-killer in compute-bound\ncode is an O(n**2) operation that is disguised by being some sort of\nO(n) lookup or match inside an O(n) loop.\nAnother common time-sink is relatively expensive common-setup\noperations that are performed inside tight loops but could be moved\nto before they start.  (For a representative case of this, see the\nmicro-tuning tip on regexp compilation.)\n\n\nSmaller is faster\nModern computers have multiple levels of memory caching, some directly\non the processor chip.  Causing a cache miss at any level incurs a\nperformance penalty proportional to random-access time for the next\noutward (and much slower) layer of cache.\nAccordingly, smaller is faster.  Programs or routines with a small\nenough working set to fit inside a fast cache will be as fast as\nthat cache is. To make your code fast, reduce the length of the\nseries of Python or JIT-compiler opcodes it generates by making\nit simpler.\nThe tradeoff here is that algorithmic tuning often trades time for\nspace - that is, it increases the size of an algorithm's working set\nby including pre-computations or tables or reverse maps in order to\navoid O(n**2) operations.\nIt's impossible to predict in advance where the sweet spot in that\ntradeoff will be.  You have to try different things and measure -\nwhich takes us right back to \"Measure, don't guess\".  And another\nfunction of your regression test suite can be as a speed benchmark.\n\n\n\n\nMicro-tuning tips\nThese are in no particular order.\n\nKeep it simple\nSimple is better than complex. The PyPy JIT is not very smart; the\nsimpler your code is the better it will run. Here again, though, you face\na tradeoff: you may need to pay with more algorithmic complexity in order\nto avoid brute-force operations that are O(n**2) or worse.\nWrite plain-vanilla code in plain-vanilla ways. The PyPy JIT has many\nproductions that optimize a common usage pattern against an uncommon\nusage pattern.\n\n\nGlobal variables\nIn CPython, global variables and functions (including package imports)\nare much more expensive to reference than locals; avoid them.  (This\nis also good modularity practice).\nThe cost of CPython global references is high enough that, for example, if you\nhave code in a frequently-visited inner loop that uses int() a lot, it\nmay be worthwhile to create a local copy of the reference with \"int =\nint\" in an enclosing block.\nHowever, this in not true in JITted PyPy code. The \"int = int\" hack\nwon't buy you performance, it's just an extra copy.  The modularity\nreason for avoiding globals are still valid.\n\n\nRegular expressions\nRegular-expression compilation is expensive.  If the regexp pattern in\na search, match, or replace operation is static (doesn't mutate at\nruntime) refactor so it's only done once.\nIf the regexp compilation is in a class method, consider doing it as\nthe initializer of a regexp-valued static (shared) class member and\nusing that class member in your operation.\nIf the regexp compilation is in a free function, consider moving it\nto module level and referencing the resulting regexp object\n(but see the warning above about global variables).\n\n\nOld- vs. new-style classes\nNew-style classes allow faster attribute access and take up less core\nper instance than old-style classes.  Much of this advantage may be\nlost, however, if attribute names are not constant. For example: x.a\n= y or even setattr(x, 'a', y) will be much faster than a dynamic\nversion: setattr(x, 'a' + some_variable, y).\nClasses that inherit from both new- and old-style classes are\nextremely slow; avoid at all costs.\nIn PyPy, isinstance() called against an old-style class was very slow\nuntil 2.0.\n\n\nString concatenation is expensive\nIn CPython, you may want to replace:\ns = head + body + maybe + tail\n\nwith the admittedly less readable:\ns = \"%(head)s%(body)s%(maybe)s%(tail)s\" % locals()\n\nor even:\ns = \"{head}{body}{maybe}{tail}\".format(**locals())\n\nBoth of the latter forms avoid multiple-allocation overhead.\nBut PyPy's JIT makes the overhead of intermediate concatenations\ngo away in linear code that keeps the number of concatenations\nsmall, bound and constant.  (And locals() is rather slow\nwith PyPy's JIT.)\nOn the other hand, in code like this with a string-valued foo() function:\nfor x in mylist:\n    s += foo(x)\n\nthe JIT cannot optimize out intermediate copies.  This code is\nactually quadratic in the total size of the mylist strings due to\nrepeated string copies of ever-larger prefix segments.  (Such code\nis always fine for bytearrays, because in this case += is an\nin-place operation.)\nThis:\nparts = []\nfor x in mylist:\n    parts.append(foo(x))\ns = \"\".join(parts)\n\ncan be much faster because all the string concatenation in the last\nline creates exactly one new string object with one C-level copy\nsequence (and list operations are relatively cheap).\n\n\nFrame introspection and tracing are slow\nCertain function calls can disable PyPy's speed options over\nstretches of surrounding code called \"JIT scopes\".\nA JIT like PyPy's works based on the assumption that the only thing\nworth optimizing are loops that are executed often. Whenever the\ninterpreter enters a loop in the interpreted program, the JIT records\nwhat the interpreter does, creating a trace. This trace is optimized,\ncompiled to machine code and executed when the loop is hit with the\nconditions observed during tracing.  This trace is one kind of JIT scope.\nAnother kind of JIT scope that matters is a function, considered as\na unit for inlining.\nNote that a JIT scope is a run-time phenomenon, not a compile-time\none.  It's not confined by source-code module boundaries.  A library-\nor foreign-module call in a frequently-called loop or inlined function\nwill be part of its JIT scope.\nlocals(), globals(), sys._getframe(), sys.exc_info(), and sys.settrace\nwork in PyPy, but they incur a performance penalty that can be huge by\ndisabling the JIT over the enclosing JIT scope.\n(Thanks Eric S. Raymond for the text above)\n\nInsider's point of view\nThis section describes performance issues from the point of view of\ninsiders of the project; it should be particularly interesting if you\nplan to contribute in that area.\nOne of the goals of the PyPy project is to provide a fast and compliant\npython interpreter. Some of the ways we achieve this are by providing a\nhigh-performance garbage collector (GC) and a high-performance\nJust-in-Time compiler (JIT).  Results of comparing PyPy and CPython can\nbe found on the speed website. Those benchmarks are not a random\ncollection: they are a combination of real-world Python programs ---\nbenchmarks originally included with the (now dead) Unladen Swallow\nproject --- and benchmarks for which we found PyPy to be slow (and improved).\nConsult the descriptions of each for details.\nThe JIT, however, is not a magic bullet. There are several characteristics\nthat might surprise people who are not used to JITs in\ngeneral or to the PyPy JIT in particular.  The JIT is generally good at\nspeeding up straight-forward Python code that spends a lot of time in the\nbytecode dispatch loop, i.e., running actual Python code --- as opposed\nto running things that only are invoked by Python code.  Good\nexamples include numeric calculations or any kind of heavily\nobject-oriented program.  Bad examples include doing computations with\nlarge longs --- which is performed by unoptimizable support code.  When the\nJIT cannot help, PyPy is generally slower than CPython.\nMore specifically, the JIT is known not to work on:\n\nTests: The ideal unit tests execute each piece of tested code\nonce.  This leaves no time for the JIT to warm up.\nReally short-running scripts: A rule of thumb is if something runs below\n0.2s the JIT has no chance, but it depends a lot on the program in question.\nIn general, make sure you warm up your program before running benchmarks, if\nyou're measuring something long-running like a server.  The time required\nto warm up the JIT varies; give it at least a couple of seconds.  (PyPy's\nJIT takes an especially long time to warm up.)\nLong-running runtime functions: These are the functions provided\nby the runtime of PyPy that do a significant amount of work.\nPyPy's runtime is generally not as optimized as CPython's and we expect those\nfunctions to take somewhere between the same time as CPython to twice as long.\nThis includes, for example, computing with longs, or sorting large lists.\nA counterexample is regular expressions: although they take time, they\ncome with their own JIT.\n\nUnrelated things that we know PyPy to be slow at (note that we're probably\nworking on it):\n\nCPython C extension modules: Any C extension module recompiled\nwith PyPy takes a very large hit in performance.  PyPy supports C\nextension modules solely to provide basic functionality.\nIf the extension module is for speedup purposes only, then it\nmakes no sense to use it with PyPy at the moment.  Instead, remove it\nand use a native Python implementation, which also allows opportunities\nfor JIT optimization.  If the extension module is\nboth performance-critical and an interface to some C library, then it\nmight be worthwhile to consider rewriting it as a pure Python version\nthat uses CFFI for the interface.\nMissing RPython modules: A few modules of the standard library\n(like csv and cPickle) are written in C in CPython, but written\nnatively in pure Python in PyPy.  Sometimes the JIT is able to do a\ngood job on them, and sometimes not.  In most cases (like csv and\ncPickle), we're slower than CPython, with the notable exception of\njson and heapq.\nAbuse of itertools: The itertools module is often \"abused\" in the\nsense that it is used for the wrong purposes.  From our point of view,\nitertools is great if you have iterations over millions of items, but\nnot for most other cases.  It gives you 3 lines in functional style\nthat replace 10 lines of Python loops (longer but arguably much easier\nto read).  The pure Python version is generally not slower even on\nCPython, and on PyPy it allows the JIT to work much better --- simple\nPython code is fast.  The same argument also applies to filter(),\nreduce(), and to some extend map() (although the simple case\nis JITted), and to all usages of the operator module we can think\nof.\nCtypes: Ctypes is slower than on CPython.  Consider CFFI or HPy\ninstead which have special paths inside the JIT.\n\nWe generally consider things that are slower on PyPy than CPython to be bugs\nof PyPy.  If you find some issue that is not documented here,\nplease report it to our bug tracker for investigation.",
      "tags": "",
      "url": "https://www.pypy.org/performance.html"
    },
    {
      "title": "PyPy 7.3.0 released",
      "text": "The PyPy team is proud to release the version 7.3.0 of PyPy, which includes\ntwo different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.13\nPyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.9.\n\n\n\n\n\nThe interpreters are based on much the same codebase, thus the double\nrelease.\n\n\nWe have worked with the python packaging group to support tooling around\nbuilding third party packages for python, so this release changes the ABI tag\nfor PyPy.\n\n\nBased on the great work done in portable-pypy, the linux downloads we\nprovide are now built on top of the manylinux2010 CentOS6 docker image.\nThe tarballs include the needed shared objects to run on any platform that\nsupports manylinux2010 wheels, which should include all supported versions of\ndebian- and RedHat-based distributions (including Ubuntu, CentOS, and Fedora).\n\n\nThe CFFI backend has been updated to version 1.13.1. We recommend using CFFI\nrather than c-extensions to interact with C.\n\nThe built-in cppyy module was upgraded to 1.10.6, which\nprovides, among others, better template resolution, stricter enum handling,\nanonymous struct/unions, cmake fragments for distribution, optimizations for\nPODs, and faster wrapper calls. We reccomend using cppyy for performant\nwrapping of C++ code for Python.\n\n\nThe vendored pyrepl package for interaction inside the REPL was updated.\n\n\nSupport for codepage encoding and decoding was added for Windows.\n\n\nAs always, this release fixed several issues and bugs raised by the growing\ncommunity of PyPy users.  We strongly recommend updating. Many of the fixes are\nthe direct result of end-user bug reports, so please continue reporting issues\nas they crop up.\n\nYou can download the v7.3 releases here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\n\n\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular packages to run\non pypy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 3 new contributors,\nthanks for pitching in.\n\nIf you are a python library maintainer and use c-extensions, please consider making a cffi / cppyy version of your library that would be performant on PyPy. If you are stuck with using the C-API, you can use docker images with PyPy built in or the multibuild system to build wheels.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\nThis PyPy release supports:\n\nx86 machines on most common operating systems\n(Linux 32/64 bit, Mac OS X 64-bit, Windows 32-bit, OpenBSD, FreeBSD)\n\n\n\n\nbig- and little-endian variants of PPC64 running Linux \n\n\n\n\ns390x running Linux\n\n\n\n\n64-bit ARM machines running Linux\n\nUnfortunately at the moment of writing our ARM buildbots are out of service,\nso for now we are not releasing any binary for the ARM architecture (32-bit), although PyPy does support ARM 32-bit processors.\n\n\n\nWhat else is new?\nPyPy 7.2 was released in October, 2019.\nThere are many incremental improvements to RPython and PyPy, For more information about the 7.3.0 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/12/pypy-730-released-3614026620096963655.html"
    },
    {
      "title": "HPy kick-off sprint report",
      "text": "Recently Antonio, Armin and Ronan had a small internal sprint in the beautiful\ncity of Gda\u0144sk to kick-off the development of HPy. Here is a brief report of\nwhat was accomplished during the sprint.\n\nWhat is HPy?\nThe TL;DR answer is \"a better way to write C extensions for Python\".\nThe idea of HPy was born during EuroPython 2019 in Basel, where there was an\ninformal meeting which included core developers of PyPy, CPython (Victor\nStinner and Mark Shannon) and Cython (Stefan Behnel). The ideas were later also\ndiscussed with Tim Felgentreff of GraalPython, to make sure they would also be\napplicable to this very different implementation, Windel Bouwman of RustPython\nis following the project as well.\nAll of us agreed that the current design of the CPython C API is problematic\nfor various reasons and, in particular, because it is too tied to the current\ninternal design of CPython.  The end result is that:\n\n\nalternative implementations of Python (such as PyPy, but not only) have a\nhard time loading and executing existing C extensions;\nCPython itself is unable to change some of its internal implementation\ndetails without breaking the world. For example, as of today it would be\nimpossible to switch from using reference counting to using a real GC,\nwhich in turns make it hard for example to remove the GIL, as gilectomy\nattempted.\n\n\nHPy tries to address these issues by following two major design guidelines:\n\nobjects are referenced and passed around using opaque handles, which are\nsimilar to e.g., file descriptors in spirit. Multiple, different handles\ncan point to the same underlying object, handles can be duplicated and\neach handle must be released independently of any other duplicate.\nThe internal data structures and C-level layout of objects are not\nvisible nor accessible using the API, so each implementation if free to\nuse what fits best.\n\nThe other major design goal of HPy is to allow incremental transition and\nporting, so existing modules can migrate their codebase one method at a time.\nMoreover, Cython is considering to optionally generate HPy code, so extension\nmodule written in Cython would be able to benefit from HPy automatically.\nMore details can be found in the README of the official HPy repository.\n\n\nTarget ABI\nWhen compiling an HPy extension you can choose one of two different target ABIs:\n\n\nHPy/CPython ABI: in this case, hpy.h contains a set of macros and\nstatic inline functions. At compilation time this translates the HPy API\ninto the standard C-API. The compiled module will have no performance\npenalty, and it will have a \"standard\" filename like\nfoo.cpython-37m-x86_64-linux-gnu.so.\nUniversal HPy ABI: as the name implies, extension modules compiled\nthis way are \"universal\" and can be loaded unmodified by multiple Python\ninterpreters and versions.  Moreover, it will be possible to dynamically\nenable a special debug mode which will make it easy to find e.g., open\nhandles or memory leaks, without having to recompile the extension.\n\n\nUniversal modules can also be loaded on CPython, thanks to the\nhpy_universal module which is under development. An extra layer of\nindirection enables loading extensions compiled with the universal ABI. Users\nof hpy_universal will face a small performance penalty compared to the ones\nusing the HPy/CPython ABI.\nThis setup gives several benefits:\n\n\nExtension developers can use the extra debug features given by the\nUniversal ABI with no need to use a special debug version of Python.\nProjects which need the maximum level of performance can compile their\nextension for each relevant version of CPython, as they are doing now.\nProjects for which runtime speed is less important will have the choice of\ndistributing a single binary which will work on any version and\nimplementation of Python.\n\n\n\n\nA simple example\nThe HPy repo contains a proof of concept module. Here is a simplified\nversion which illustrates what a HPy module looks like:\n\n#include \"hpy.h\"\n\nHPy_DEF_METH_VARARGS(add_ints)\nstatic HPy add_ints_impl(HPyContext ctx, HPy self, HPy *args, HPy_ssize_t nargs)\n{\n    long a, b;\n    if (!HPyArg_Parse(ctx, args, nargs, \"ll\", &a, &b))\n        return HPy_NULL;\n    return HPyLong_FromLong(ctx, a+b);\n}\n\n\nstatic HPyMethodDef PofMethods[] = {\n    {\"add_ints\", add_ints, HPy_METH_VARARGS, \"\"},\n    {NULL, NULL, 0, NULL}\n};\n\nstatic HPyModuleDef moduledef = {\n    HPyModuleDef_HEAD_INIT,\n    .m_name = \"pof\",\n    .m_doc = \"HPy Proof of Concept\",\n    .m_size = -1,\n    .m_methods = PofMethods\n};\n\n\nHPy_MODINIT(pof)\nstatic HPy init_pof_impl(HPyContext ctx)\n{\n    HPy m;\n    m = HPyModule_Create(ctx, &moduledef);\n    if (HPy_IsNull(m))\n        return HPy_NULL;\n    return m;\n}\n\nPeople who are familiar with the current C-API will surely notice many\nsimilarities. The biggest differences are:\n\n\nInstead of PyObject *, objects have the type HPy, which as\nexplained above represents a handle.\nYou need to explicitly pass an HPyContext around: the intent is\nprimary to be future-proof and make it easier to implement things like\nsub- interpreters.\nHPy_METH_VARARGS is implemented differently than CPython's\nMETH_VARARGS: in particular, these methods receive an array of HPy\nand its length, instead of a fully constructed tuple: passing a tuple\nmakes sense on CPython where you have it anyway, but it might be an\nunnecessary burden for alternate implementations.  Note that this is\nsimilar to the new METH_FASTCALL which was introduced in CPython.\nHPy relies a lot on C macros, which most of the time are needed to support\nthe HPy/CPython ABI compilation mode. For example, HPy_DEF_METH_VARARGS\nexpands into a trampoline which has the correct C signature that CPython\nexpects (i.e., PyObject (*)(PyObject *self, *PyObject *args)) and\nwhich calls add_ints_impl.\n\n\n\n\nSprint report and current status\nAfter this long preamble, here is a rough list of what we accomplished during\nthe week-long sprint and the days immediatly after.\nOn the HPy side, we kicked-off the code in the repo: at the moment of writing\nthe layout of the directories is a bit messy because we moved things around\nseveral times, but we identified several main sections:\n\n\nA specification of the API which serves both as documentation and as an\ninput for parts of the projects which are automatically\ngenerated. Currently, this lives in public_api.h.\n\nA set of header files which can be used to compile extension modules:\ndepending on whether the flag -DHPY_UNIVERSAL_ABI is passed to the\ncompiler, the extension can target the HPy/CPython ABI or the HPy\nUniversal ABI\n\nA CPython extension module called hpy_universal which makes it\npossible to import universal modules on CPython\n\nA set of tests which are independent of the implementation and are meant\nto be an \"executable specification\" of the semantics.  Currently, these\ntests are run against three different implementations of the HPy API:\n\n\nthe headers which implements the \"HPy/CPython ABI\"\nthe hpy_universal module for CPython\nthe hpy_universal module for PyPy (these tests are run in the PyPy repo)\n\n\n\n\n\nMoreover, we started a PyPy branch in which to implement the\nhpy_univeral module: at the moment of writing PyPy can pass all the HPy\ntests apart the ones which allow conversion to and from PyObject *.\nAmong the other things, this means that it is already possible to load the\nvery same binary module in both CPython and PyPy, which is impressive on its\nown :).\nFinally, we wanted a real-life use case to show how to port a module to HPy\nand to do benchmarks.  After some searching, we choose ultrajson, for the\nfollowing reasons:\n\n\nit is a real-world extension module which was written with performance in\nmind\nwhen parsing a JSON file it does a lot of calls to the Python API to\nconstruct the various parts of the result message\nit uses only a small subset of the Python API\n\n\nThis repo contains the HPy port of ultrajson. This commit shows an example\nof what the porting looks like.\nujson_hpy is also a very good example of incremental migration: so far\nonly ujson.loads is implemented using the HPy API, while ujson.dumps\nis still implemented using the old C-API, and both can coexist nicely in the\nsame compiled module.\n\n\nBenchmarks\nOnce we have a fully working ujson_hpy module, we can finally run\nbenchmarks!  We tested several different versions of the module:\n\n\nujson: this is the vanilla implementation of ultrajson using the\nC-API. On PyPy this is executed by the infamous cpyext compatibility\nlayer, so we expect it to be much slower than on CPython\nujson_hpy: our HPy port compiled to target the HPy/CPython ABI. We\nexpect it to be as fast as ujson\nujson_hpy_universal: same as above but compiled to target the\nUniversal HPy ABI. We expect it to be slightly slower than ujson on\nCPython, and much faster on PyPy.\n\n\nFinally, we also ran the benchmark using the builtin json module. This is\nnot really relevant to HPy, but it might still be an interesting as a\nreference data point.\nThe benchmark is very simple and consists of parsing a big JSON file 100\ntimes. Here is the average time per iteration (in milliseconds) using the\nvarious versions of the module, CPython 3.7 and the latest version of the hpy\nPyPy branch:\n\n\n\n\n\n\n\n\u00a0\nCPython\nPyPy\n\nujson\n154.32\n633.97\n\nujson_hpy\n152.19\n\u00a0\n\nujson_hpy_universal\n168.78\n207.68\n\njson\n224.59\n135.43\n\n\n\nAs expected, the benchmark proves that when targeting the HPy/CPython ABI, HPy\ndoesn't impose any performance penalty on CPython. The universal version is\n~10% slower on CPython, but gives an impressive 3x speedup on PyPy! It it\nworth noting that the PyPy hpy module is not fully optimized yet, and we\nexpect to be able to reach the same performance as CPython for this particular\nexample (or even more, thanks to our better GC).\nAll in all, not a bad result for two weeks of intense hacking :)\nIt is also worth noting than PyPy's builtin json module does really\nwell in this benchmark, thanks to the recent optimizations that were described\nin an earlier blog post.\n\n\nConclusion and future directions\nWe think we can be very satisfied about what we have got so far. The\ndevelopment of HPy is quite new, but these early results seem to indicate that\nwe are on the right track to bring Python extensions into the future.\nAt the moment, we can anticipate some of the next steps in the development of\nHPy:\n\n\nThink about a proper API design: what we have done so far has\nbeen a \"dumb\" translation of the API we needed to run ujson. However,\none of the declared goal of HPy is to improve the design of the API. There\nwill be a trade-off between the desire of having a clean, fresh new API\nand the need to be not too different than the old one, to make porting\neasier.  Finding the sweet spot will not be easy!\nImplement the \"debug\" mode, which will help developers to find\nbugs such as leaking handles or using invalid handles.\nInstruct Cython to emit HPy code on request.\nEventually, we will also want to try to port parts of numpy to HPy to\nfinally solve the long-standing problem of sub-optimal numpy\nperformance in PyPy.\n\n\nStay tuned!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/12/hpy-kick-off-sprint-report-1840829336092490938.html"
    },
    {
      "title": "PyPy v7.2 released",
      "text": "The PyPy team is proud to release the version 7.2.0 of PyPy, which includes\ntwo different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7 including the stdlib for CPython 2.7.13\n\n\nPyPy3.6: which is an interpreter supporting the syntax and the features of\nPython 3.6, including the stdlib for CPython 3.6.9.\n\n\n\n\n\nThe interpreters are based on much the same codebase, thus the double\nrelease.\n\n\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nWe strongly recommend updating. Many of the fixes are the direct result of\nend-user bug reports, so please continue reporting issues as they crop up.\n\n\nYou can download the v7.2 releases here:\n\n\n\nhttps://pypy.org/download.html\n\nWith the support of Arm Holdings Ltd. and Crossbar.io, this release supports\nthe 64-bit aarch64 ARM architecture. More about the work and the\nperformance data around this welcome development can be found in the blog\npost.\n\n\nThis release removes the \u201cbeta\u201d tag from PyPy3.6. While there may still be some\nsmall corner-case incompatibilities (around the exact error messages in\nexceptions and the handling of faulty codec errorhandlers) we are happy with\nthe quality of the 3.6 series and are looking forward to working on a Python\n3.7 interpreter.\n\n\nWe updated our benchmark runner at https://speed.pypy.org to a more modern\nmachine and updated the baseline python to CPython 2.7.11. Thanks to Baroque\nSoftware for maintaining the benchmark runner.\n\n\nThe CFFI-based _ssl module was backported to PyPy2.7 and updated to use\ncryptography version 2.7. Additionally, the _hashlib, and crypt (or\n_crypt on Python3) modules were converted to CFFI. This has two\nconsequences: end users and packagers can more easily update these libraries\nfor their platform by executing (cd lib_pypy; ../bin/pypy _*_build.py).\nMore significantly, since PyPy itself links to fewer system shared objects\n(DLLs), on platforms with a single runtime namespace like linux, different CFFI\nand c-extension modules can load different versions of the same shared object\ninto PyPy without collision (issue 2617).\n\n\nUntil downstream providers begin to distribute c-extension builds with PyPy, we\nhave made packages for some common packages available as wheels.\n\n\nThe CFFI backend has been updated to version 1.13.0. We recommend using CFFI\nrather than c-extensions to interact with C, and cppyy for interacting with\nC++ code.\n\n\nThanks to Anvil, we revived the PyPy Sandbox, (soon to be released) which allows total control\nover a Python interpreter\u2019s interactions with the external world.\n\n\nWe implemented a new JSON decoder that is much faster, uses less memory, and\nuses a JIT-friendly specialized dictionary. More about that in the recent blog post\n\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\n\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non PyPy, or general help with making RPython\u2019s JIT even better. Since the\nprevious release, we have accepted contributions from 27 new contributors,\nso thanks for pitching in.\n\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\n\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\n\nThis PyPy release supports:\n\nx86 machines on most common operating systems\n(Linux 32/64 bit, Mac OS X 64-bit, Windows 32-bit, OpenBSD, FreeBSD)\n\n\n\n\nbig- and little-endian variants of PPC64 running Linux \n\n\n\n\ns390x running Linux\n\n\n\n\n64-bit ARM machines running Linux\n\n\n\n\n\nUnfortunately at the moment of writing our ARM buildbots are out of service,\nso for now we are not releasing any binary for the ARM architecture (32-bit), although PyPy does support ARM 32-bit processors.\n\n\n\nWhat else is new?\nPyPy 7.1 was released in March, 2019.\nThere are many incremental improvements to RPython and PyPy, For more information about the 7.2.0 release, see the full changelog.\n\nPlease update, and continue to help us make PyPy better.\n\nCheers,\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/10/pypy-v72-released-1090406556726313495.html"
    },
    {
      "title": "PyPy's new JSON parser",
      "text": "Introduction\nIn the last year or two I have worked on and off on making PyPy's\nJSON faster, particularly when parsing large\nJSON files. In this post I am going to document those techniques and\nmeasure their performance impact. Note that I am quite a lot more\nconstrained in what optimizations I can apply here, compared to some of\nthe much more advanced approaches like\nMison,\nSparser or\nSimdJSON because I don't want to\nchange the json.loads API that Python programs expect, and because I\ndon't want to only support CPUs with wide SIMD extensions. With a more\nexpressive API, more optimizations would be possible.\nThere are a number of problems of working with huge JSON files:\ndeserialization takes a long time on the one hand, and the resulting\ndata structures often take a lot of memory (usually they can be many\ntimes bigger than the size of the file they originated from). Of course\nthese problems are related, because allocating and initializing a big\ndata structure takes longer than a smaller data structure. Therefore I\nalways tried to attack both of these problems at the same time.\nOne common theme of the techniques I am describing is that of optimizing\nthe parser for how JSON files are typically used, not how they could\ntheoretically be used. This is a similar approach to the way dynamic\nlanguages are optimized more generally: most JITs will optimize for\ntypical patterns of usage, at the cost of less common usage patterns,\nwhich might even become slower as a result of the optimizations.\n\nMaps\nThe first technique I investigated is to use maps in the JSON parser.\nMaps, also called hidden classes or shapes, are a fairly common way to\n(generally, not just in the context of JSON parsing) optimize instances\nof\nclasses\nin dynamic language VMs. Maps exploit the fact that while it is in\ntheory possible to add arbitrary fields to an instance, in practice most\ninstances of a class are going to have the same set of fields (or one of\na small number of different sets). Since JSON dictionaries or objects\noften come from serialized instances of some kind, this property often\nholds in JSON files as well: dictionaries often have the same fields in\nthe same order, within a JSON file.\nThis property can be exploited in two ways: on the one hand, it can be\nused to again store the deserialized dictionaries in a more memory\nefficient way by not using a hashmap in most cases, but instead\nsplitting the dictionary into a shared description of the set of keys\n(the map) and an array of storage with the values. This makes the\ndeserialized dictionaries smaller if the same set of keys is repeated a\nlot. This is completely transparent to the Python programmer, the\ndictionary will look completely normal to the Python program but its\ninternal representation is different.\nOne downside of using maps is that sometimes files will contain many\ndictionaries that have unique key sets. Since maps themselves are quite\nlarge data structures and since dictionaries that use maps contain an\nextra level of indirection we want to fall back to using normal hashmaps\nto represent the dictionaries where that is the case. To prevent this we\nperform some statistics at runtime, how often every map (i.e. set of\nkeys) is used in the file. For uncommonly used maps, the map is\ndiscarded and the dictionaries that used the map converted into using a\nregular hashmap.\n\nUsing Maps to Speed up Parsing\nAnother benefit of using maps to store deserialized dictionaries is that\nwe can use them to speed up the parsing process itself. To see how this\nworks, we need to understand maps a bit better. All the maps produced as\na side-effect of parsing JSON form a tree. The tree root is a map that\ndescribes the object without any attributes. From every tree node we\nhave a number of edges going to other nodes, each edge for a specific\nnew attribute added:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n\n  \n\n\n\n\n  \n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n\n\n\n  \n\n\n\n  \n  \n  \n  \n\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\nThis map tree is the result of parsing a file that has dictionaries with\nthe keys a, b, c many times, the keys a, b, f less often, and also some\nobjects with the keys x, y.\nWhen parsing a dictionary we traverse this tree from the root, according\nto the keys that we see in the input file. While doing this, we\npotentially add new nodes, if we get key combinations that we have never\nseen before. The set of keys of a dictionary parsed so far are\nrepresented by the current tree node, while we can store the values into\nan array. We can use the tree of nodes to speed up parsing. A lot of the\nnodes only have one child, because after reading the first few keys of\nan object, the remaining ones are often uniquely determined in a given\nfile. If we have only one child map node, we can speculatively parse the\nnext key by doing a memcmp between the key that the map tree says is\nlikely to come next and the characters that follow the ',' that started\nthe next entry in the dictionary. If the memcmp returns true this\nmeans that the speculation paid off, and we can transition to the new map\nthat the edge points to, and parse the corresponding value. If not, we\nfall back to general code that parses the string, handles escaping rules\netc. This trick was explained to me by some V8 engineers, the same trick\nis supposedly used as part of the V8 JSON parser.\nThis scheme doesn't immediately work for map tree nodes that have more\nthan one child. However, since we keep statistics anyway about how often\neach map is used as the map of a parsed dictionary, we can speculate\nthat the most common map transition is taken more often than the others\nin the future, and use that as the speculated next node.\nSo for the example transition tree shown in the figure above the key\nspeculation would succeed for objects with keys a, b, c. For objects\nwith keys a, b, f the speculation would succeed for the first two\nkeys, but not for the third key f. For objects with the keys\nx, y the speculation would fail for the first key x but succeed\nfor the second key y.\nFor real-world datasets these transition trees can become a lot more\ncomplicated, for example here is a visualization of a part of the\ntransition tree generated for parsing a New York Times dataset:\n\n\n\n\nCaching Strings\nA rather obvious observation we can use to improve performance of the\nparser is the fact that string values repeat a lot in most JSON files.\nFor strings that are used as dictionary keys this is pretty obvious.\nHowever it happens also for strings that are used as values in\ndictionaries (or are stored in lists). We can use this fact to\nintern/memoize strings and save memory. This is an approach that many\nJSON parsers use, including\nCPython's.\nTo do this, I keep a dictionary of strings that we have seen so far\nduring parsing and look up new strings that are deserialized. If we have\nseen the string before, we can re-use the deserialized previous string.\nRight now I only consider utf-8 strings for caching that do not contain\nany escapes (whether stuff like \\\", \\n or escaped unicode chars).\nThis simple approach works extremely well for dictionary keys, but needs\na number of improvements to be a win in general. The first observation\nis that computing the hash to look up the string in the dictionary of\nstrings we've seen so far is basically free. We can compute the hash\nwhile scanning the input for the end of the string we are currently\ndeserializing. Computing the hash while scanning doesn't increase the\ntime spent scanning much. This is not a new idea, I am sure many other\nparsers do the same thing (but CPython doesn't seem to).\nAnother improvement follows from the observation that inserting every\nsingle deserialized non-key string into a hashmap is too expensive.\nInstead, we insert strings into the cache more conservatively, by\nkeeping a small ring buffer of hashes of recently deserialized strings.\nThe hash is looked for in the ring buffer, and only if the hash is\npresent we insert the string into the memoization hashmap. This has the\neffect of only inserting strings into the memoization hashmap that\nre-occur a second time not too far into the file. This seems to give a\ngood trade-off between still re-using a lot of strings but keeping the\ntime spent updating and the size of the memoization hashmap low.\nAnother twist is that in a lot of situations caching strings is not\nuseful at all, because it will almost never succeed. Examples of this\nare UUIDs (which are unique), or the content of a tweet in a JSON file\nwith many tweets (which is usually unique). However, in the same file it\nmight be useful to cache e.g. the user name of the Twitter user, because\nmany tweets from the same person could be in such a file. Therefore the\nusefulness of the string cache depends on which fields of objects we are\ndeserializing the value off. Therefore we keep statistics per map field\nand disable string memoization per individual field if the cache hit\nrate falls below a certain threshold. This gives the best of both\nworlds: in the cases where string values repeat a lot in certain fields\nwe use the cache to save time and memory. But for those fields that\nmostly contain unique strings we don't waste time looking up and adding\nstrings in the memoization table. Strings outside of dictionaries are\nquite rare anyway, so we just always try to use the cache for them.\nThe following pseudocode sketches the code to deserialize a string in\nthe input at a given position. The function also takes a map, which is\nthe point in the map tree that we are currently deserializing a field\noff (if we are deserializing a string in another context, some kind of\ndummy map can be used there).\n\ndef deserialize_string(pos, input, map):\n    # input is the input string, pos is the position of the starting \" of\n    # the string\n\n    # find end of string, check whether it contains escape codes,\n    # compute hash, all at the same time\n    end, escapes, hash = find_end_of_string(pos + 1, input)\n    if end == -1:\n        raise ParseError\n    if escapes:\n        # need to be much more careful with escaping\n        return deserialize_string_escapes(pos, input)\n    \n    # should we cache at all?\n    if map.cache_disabled():\n        return input[pos + 1:end]\n\n    # if string is in cache, return it\n    if hash in cache:\n        map.cache_hit += 1\n        return cache[hash]\n\n    result = input[pos + 1:end]\n    map.cache_miss += 1\n\n    # if hash is in the ring buffer of recently seen hashes,\n    # add the string to the cache\n    if hash in ring_buffer:\n        cache[hash] = result\n    else:\n        ring_buffer.write(hash)\n    return result\n\n\n\n\nEvaluation\nTo find out how much the various techniques help, I implemented a number\nof JSON parsers in PyPy with different combinations of the techniques\nenabled. I compared the numbers with the JSON parser of CPython 3.7.3\n(simplejson), with ujson, with the JSON parser of Node 12.11.1 (V8) and with\nRapidJSON (in DOM mode).\nI collected a number of medium-to-large JSON files to try the JSON\nparsers on:\n\nCensys: A subset of the Censys port and\nprotocol scan data for websites in the Alexa top million domains\nGharchive: Github activity from\nJanuary 15-23, 2015 from Github Archive\nReddit: Reddit\ncomments from May 2009\nRosie: The nested matches produced using the Rosie pattern\nlanguage all.things pattern on a log\nfile\nNytimes: Metadata of a collection of New York Times articles\nTpch: The TPC-H database benchmark's deals table as a JSON file\nTwitter: A JSON export of the @pypyproject Twitter account data\nWikidata: A file storing a subset of the Wikidata fact dump from Nov\n11, 2014\nYelp: A file of yelp\nbusinesses\n\nHere are the file sizes of the benchmarks:\n\n  \n\n      Benchmark\n      File Size [MiB]\n    \n\n  \n\n      Censys\n      898.45\n    \n\n      Gharchive\n      276.34\n    \n\n      NYTimes\n      12.98\n    \n\n      Reddit\n      931.65\n    \n\n      Rosie\n      388.88\n    \n\n      TPCH\n      173.86\n    \n\n      Wikidata\n      119.75\n    \n\n      Yelp\n      167.61\n    \n\n\nI measured the times of each benchmark with a number of variations\nof the improved PyPy algorithms:\n\nPyPyBaseline: The PyPy JSON parser as it was before my work with JSON\nparsing started (PyPy version 5.8)\nPyPyKeyStringCaching: Memoizing the key strings of dictionaries, but\nnot the other strings in a json file, and not using maps to represent\ndictionaries (this is the JSON parser that PyPy has been shipping since\nversion 5.9, in the benchmarks I used 7.1).\nPyPyMapNoCache: Like PyPyKeyStringCaching, but using maps to\nrepresent dictionaries. This includes speculatively parsing the next\nkey using memcmp, but does not use string caching of non-key strings.\nPyPyFull: Like PyPyMapNoCache but uses a string cache for all\nstrings, not just keys. This is equivalent to what will be released soon as part of PyPy 7.2\n\nIn addition to wall clock time of parsing, I also measured the increase\nin memory use of each implementation after the input string has been\ndeserialized, i.e. the size of the in-memory representation of every\nJSON file.\n\n\nContributions of Individual Optimizations\nLet's first look at the contributions of the individual optimizations to the\noverall performance and memory usage.\n\n\n\n\nAll the benchmarks were run 30 times in new processes, all the numbers are\nnormalized to PyPyFull.\nThe biggest individual improvement to both parsing time and memory used comes\nfrom caching just the keys in parsed dictionaries. This is the optimization in\nPyPy's JSON parser that has been implemented for a while already. To understand\nwhy this optimization is so useful, let's look at some numbers about each\nbenchmark, namely the number of total keys across all dictionaries in each\nfile, as well as the number of unique keys. As we can see, for all benchmarks\nthe number of unique keys is significantly smaller than the number of keys in\ntotal.\n\n  \n\n      Benchmark\n      Number of keys\n      Number of unique keys\n    \n\n  \n\n      Censys\n      14\u2009404\u2009234\n      163\n    \n\n      Gharchive\n      6\u2009637\u2009881\n      169\n    \n\n      NYTimes\n      417\u2009337\n      60\n    \n\n      Reddit\n      25\u2009226\u2009397\n      21\n    \n\n      Rosie\n      28\u2009500\u2009101\n      5\n    \n\n      TPCH\n      6\u2009700\u2009000\n      45\n    \n\n      Wikidata\n      6\u2009235\u2009088\n      1\u2009602\n    \n\n      Yelp\n      5\u2009133\u2009914\n      61\n    \n\n\nThe next big jump in deserialization time and memory comes from introducing\nmaps to represent deserialized dictionaries. With PyPyMapNoCache\ndeserialization time goes down because it's much cheaper to walk the tree\nof maps and store all deserialized objects into an array of values than to\nbuild hashmaps with the same keys again and again. Memory use goes down\nfor the same reason: it takes a lot less memory to store the shared\nstructure of each set of keys in the map, as opposed to repeating it again\nand again in every hashmap.\nWe can look at some numbers about every benchmark again. The table shows how\nmany map-based dictionaries are deserialized for every benchmark, and how many\nhashmap-backed dictionaries. We see that the number of hashmap-backed\ndictionaries is often zero, or at most a small percentage of all dictionaries\nin each benchmark. Yelp has the biggest number of hashmap-backed dictionaries.\nThe reason for this is that the input file contains hashmaps that store\ncombinations of various features of Yelp businesses, and a lot of these\ncombinations are totally unique to a business. Therefore the heuristics\ndetermine that it's better to store these using hashmaps.\n\n  \n    \n      Benchmark\n      Map Dicts\n      Regular Dicts\n      % Regular Dicts\n    \n  \n  \n    \n      Censys\n      4\u2009049\u2009235\n      1\u2009042\n      0.03\n    \n    \n      Gharchive\n      955\u2009301\n      0\n      0.00\n    \n    \n      NYTimes\n      80\u2009393\n      0\n      0.00\n    \n    \n      Reddit\n      1\u2009201\u2009257\n      0\n      0.00\n    \n    \n      Rosie\n      6\u2009248\u2009966\n      0\n      0.00\n    \n    \n      TPCH\n      1\u2009000\u2009000\n      0\n      0.00\n    \n    \n      Wikidata\n      1\u2009923\u2009460\n      46\u2009905\n      2.38\n    \n    \n      Yelp\n      443\u2009140\n      52\u2009051\n      10.51\n    \n  \n\n\nWe can also look at numbers about how often the memcmp-based speculative\nparsing of the next key of a given map succeeds. Looking at statistics\nabout each benchmark, we can see that the speculation of what key we\nexpect next pays off in a significant percentage of cases, between 63% for\nWikidata where the dictionary structures are quite irregular, and 99% for\nReddit, where all the dictionaries have the same set of keys.\n\n  \n\n      Benchmark\n      Number of Keys\n      Map Transitions\n      % Successful Speculation\n    \n\n  \n\n      Censys\n      14\u2009404\u2009234\n      14\u2009403\u2009243\n      65.79\n    \n\n      Gharchive\n      6\u2009637\u2009881\n      6\u2009637\u2009881\n      86.71\n    \n\n      NYTimes\n      417\u2009337\n      417\u2009337\n      79.85\n    \n\n      Reddit\n      25\u2009226\u2009397\n      25\u2009226\u2009397\n      100.00\n    \n\n      Rosie\n      28\u2009500\u2009101\n      28\u2009500\u2009101\n      90.37\n    \n\n      TPCH\n      6\u2009700\u2009000\n      6\u2009700\u2009000\n      86.57\n    \n\n      Wikidata\n      6\u2009235\u2009088\n      5\u2009267\u2009744\n      63.68\n    \n\n      Yelp\n      5\u2009133\u2009914\n      4\u2009593\u2009980\n      90.43\n    \n\n      geomean\n      \n      \n      82.04\n    \n\n\nGeneral string caching is the most unclear optimization. On the one hand its\nimpact on memory usage is quite substantial, leading to a 20% reduction for\nGharchive and Reddit, up to a 2\u00d7 improvement for Yelp. On the other hand, the\neffect on performance is less clear, since it even leads to a slowdown in\nGharchive and Reddit, and generally only a small improvement. Choosing the\nright heuristic for when to disable the cache also has somewhat unclear effects\nand is definitely a topic worthy of further investigation.\n\nComparison against other JSON Decoders\nTo get a more general feeling of the performance and memory usage of the\nimproved PyPy parser, we compare it against CPython's built-in json\nparser, ujson for CPython, Node's (V8) JSON parser and RapidJSON. For\nbetter context for the memory usage I also show the file size of the input\nfiles.\nThese benchmarks are not really an apples-to-apple comparison. All of the\nimplementations use different in-memory representations of strings in\nthe deserialized data-structure (Node uses two bytes per character in\na string, in CPython it\ndepends but 4 bytes on my\nmachine), PyPyBaseline uses four bytes, PyPy and RapidJSON use utf-8). But\nit's still interesting to get some ballpark numbers. The results are as\nfollows:\n\n\n\n\nAs we can see, PyPyFull handily beats CPython and ujson, with a geometric\nmean of the improvement of about 2.5\u00d7. The memory improvement can be even\nmore extreme, with an improvement of over 4\u00d7 against CPython/ujson in some\ncases (CPython gives better memory sizes, because its parser caches the\nkeys of dictionaries as well). Node is often more than 50% slower, whereas\nRapidJSON beats us easily, by a factor of 2\u00d7 on average.\n\nConclusions\nWhile the speedup I managed to achieve over the course of this project is\nnice and I am certainly happy to beat both CPython and Node, I am\nultimately still annoyed that RapidJSON manages to maintain such a clear\nlead over PyPyFull, and would like to get closer to it. One problem that\nPyPy suffers compared to RapidJSON is the overhead of garbage collection.\nDeserializing large JSON files is pretty much the worst case for the\ngenerational GC that PyPy uses, since none of the deserialized objects die\nyoung (and the GC expects that most objects do). That means that a lot of\nthe deserialization time of PyPy is wasted allocating the resulting\nobjects in the nursery, and then copying them into the old generation.\nSomehow, this should be done in better ways, but all my attempts to not\nhave to do the copy did not seem to help much. So maybe more improvements\nare possible, if I can come up with more ideas.\nOn the memory side of things, Node/V8 is beating PyPy clearly which might\nindicate more general problems in how we represent Python objects in\nmemory. On the other hand, I think it's cool that we are competitive with\nRapidJSON in terms of memory and often within 2\u00d7 of the file size.\nAn effect that I didn't consider at all in this blog post is the fact that\naccessing the deserialized objects with constants strings is also faster\nthan with regular dictionaries, due to them being represented with maps.\nMore benchmarking work to do in the future!\nIf you have your own programs that run on PyPy and use the json parser\na lot, please measure them on the new code and let me know whether you see\nany difference!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/10/pypys-new-json-parser-492911724084305501.html"
    },
    {
      "title": "A second life for the Sandbox",
      "text": "Hi all,\n\nAnvil is a UK-based company sponsoring one month of work to revive PyPy's\n\"sandbox\" mode and upgrade it to PyPy3.  Thanks to them, sandboxing will be\ngiven a second life!\n\nThe sandboxed PyPy is a special version of PyPy that runs\nfully isolated.  It gives a safe way to execute arbitrary Python\nprograms (whole programs, not small bits of code inside your larger Python\nprogram).  Such scripts can be fully untrusted, and they can try to do\nanything\u2014there are no syntax-based restrictions, for example\u2014but whatever\nthey do, any communication with the external world is not actually done but\ndelegated to the parent process.  This is similar but much more flexible than\nLinux's Seccomp approach, and it is more lightweight than setting up a full\nvirtual machine.  It also works without operating system support.\n\nHowever, during the course of the years the sandbox mode of PyPy has been\nmostly unmaintained and unsupported by the core developers, mostly because of\na lack of interest by users and because it took too much effort to maintain\nit.\n\nNow we have found that we have an actual user, Anvil.  As far as I can tell\nthey are still using a very old version of PyPy, the last one that supported\nsandboxing. This is where this contract comes from: the goal is to modernize sandboxing and port it to PyPy3.\n\nPart of my motivation for accepting this work is that I may have found a way to\ntweak the protocol on the pipe between the sandboxed PyPy and the parent\ncontroller process.  This should make the sandboxed PyPy more resilient against\nfuture developments and easier to maintain; at most, in the future some tweaks will be needed in the\ncontroller process but hopefully not deep inside the guts of the sandboxed\nPyPy.  Among the advantages, such a more robust solution should mean that we\ncan actually get a working sandboxed PyPy\u2014or sandboxed PyPy3 or sandboxed\nversion of any other interpreter written in RPython\u2014with just an extra\nargument when calling rpython to translate this interpreter.  If everything\nworks as planned, sandboxing may be given a second life.\n\nArmin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/08/a-second-life-for-sandbox-6848726729476245390.html"
    },
    {
      "title": "PyPy JIT for Aarch64",
      "text": "Hello everyone.\nWe are pleased to announce the availability of the new PyPy for AArch64. This\nport brings PyPy's high-performance just-in-time compiler to the AArch64\nplatform, also known as 64-bit ARM. With the addition of AArch64, PyPy now\nsupports a total of 6 architectures: x86 (32 & 64bit), ARM (32 & 64bit), PPC64,\nand s390x. The AArch64 work was funded by ARM Holdings Ltd. and Crossbar.io.\nPyPy has a good record of boosting the performance of Python programs on the\nexisting platforms. To show how well the new PyPy port performs, we compare the\nperformance of PyPy against CPython on a set of benchmarks. As a point of\ncomparison, we include the results of PyPy on x86_64.\nNote, however, that the results presented here were measured on a Graviton A1\nmachine from AWS, which comes with a very serious word of warning: Graviton A1's\nare virtual machines, and, as such, they are not suitable for benchmarking. If\nsomeone has access to a beefy enough (16G) ARM64 server and is willing to give\nus access to it, we are happy to redo the benchmarks on a real machine. One\nmajor concern is that while a virtual CPU is 1-to-1 with a real CPU, it is not\nclear to us how CPU caches are shared across virtual CPUs. Also, note that by no\nmeans is this benchmark suite representative enough to average the results. Read\nthe numbers individually per benchmark.\nThe following graph shows the speedups on AArch64 of PyPy (hg id 2417f925ce94) compared to\nCPython (2.7.15), as well as the speedups on a x86_64 Linux laptop\ncomparing the most recent release, PyPy 7.1.1, to CPython 2.7.16.\n\n\n\nIn the majority of benchmarks, the speedups achieved on AArch64 match those\nachieved on the x86_64 laptop. Over CPython, PyPy on AArch64 achieves speedups\nbetween 0.6x to 44.9x. These speedups are comparable to x86_64, where the\nnumbers are between 0.6x and 58.9x.\nThe next graph compares between the speedups achieved on AArch64 to the speedups\nachieved on x86_64, i.e., how great the speedup is on AArch64 vs. the same\nbenchmark on x86_64. This comparison should give a rough idea about the\nquality of the generated code for the new platform.\n\n\n\nNote that we see a large variance: There are generally three groups of\nbenchmarks - those that run at more or less the same speed, those that\nrun at 2x the speed, and those that run at 0.5x the speed of x86_64.\nThe variance and disparity are likely related to a variety of issues, mostly due\nto differences in architecture. What is however interesting is that, compared\nto measurements performed on older ARM boards, the branch predictor on the\nGraviton A1 machine appears to have improved. As a result, the speedups achieved\nby PyPy over CPython are smaller than on older ARM boards: sufficiently branchy\ncode, like CPython itself, simply runs a lot faster. Hence, the advantage\nof the non-branchy code generated by PyPy's just-in-time compiler is smaller.\nOne takeaway here is that many possible improvements for PyPy have yet to be\nimplemented. This is true for both of the above platforms, but probably more so\nfor AArch64, which comes with a large number of CPU registers. The PyPy backend\nwas written with x86 (the 32-bit variant) in mind, which has a really low number\nof registers. We think that we can improve in the area of emitting more modern\nmachine code, which may have a higher impact on AArch64 than on x86_64. There is\nalso a number of missing features in the AArch64 backend. These features are\ncurrently implemented as expensive function calls instead of inlined native\ninstructions, something we intend to improve.\nBest,\nMaciej Fijalkowski, Armin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/07/pypy-jit-for-aarch64-7161523403247118006.html"
    },
    {
      "title": "PyPy 7.1.1 Bug Fix Release",
      "text": "The PyPy team is proud to release a bug-fix release version 7.1.1 of PyPy, which\nincludes two different interpreters:\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.\nPyPy3.6-beta: the second official release of PyPy to support 3.6\nfeatures.\n\n\n\n\n\nThe interpreters are based on much the same codebase, thus the double\nrelease.\n\n\nThis bugfix fixes bugs related to large lists, dictionaries, and sets, some corner cases with unicode, and PEP 3118 memory views of ctype structures. It also fixes a few issues related to the ARM 32-bit backend. For the complete list see the changelog.\n\nYou can download the v7.1.1 releases here:\n\n\n\nhttps://pypy.org/download.html\n\n\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nWe strongly recommend updating.\n\nThe PyPy3.6 release is rapidly maturing, but is still considered beta-quality.\n\nThe PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/04/pypy-711-bug-fix-release-6539023630991217367.html"
    },
    {
      "title": "An RPython JIT for LPegs",
      "text": "The following is a guest post by Stefan Troost, he describes the work he did in his bachelor thesis:\n\nIn this project we have used the RPython infrastructure to generate an RPython\nJIT for a\nless-typical use-case: string pattern matching. The work in this project is\nbased on Parsing Expression Grammars and\nLPeg, an implementation of PEGs\ndesigned to be used in Lua. In this post I will showcase some of the work that\nwent into this project, explain PEGs in general and LPeg in particular, and\nshow some benchmarking results.\nParsing Expression Grammars\nParsing Expression Grammas (PEGs) are a type of formal grammar similar to\ncontext-free grammars, with the main difference being that they are unambiguous.\nThis is achieved by redefining the ambiguous choice operator of CFGs (usually\nnoted as |) as an ordered choice operator. In practice this means that if a\nrule in a PEG presents a choice, a PEG parser should prioritize the leftmost\nchoice. Practical uses include parsing and pattern-searching. In comparison to\nregular expressions PEGs stand out as being able to be parsed in linear time,\nbeing strictly more powerful than REs, as well as being arguably more readable.\nLPeg\nLPeg is an implementation of PEGs written in C to be used in the Lua\nprogramming language. A crucial detail of this implementation is that it parses\nhigh level function calls, translating them to bytecode, and interpreting that\nbytecode. Therefore, we are able to improve that implementation by replacing\nLPegs C-interpreter with an RPython JIT. I use a modified version of LPeg to\nparse PEGs and pass the generated Intermediate Representation, the LPeg\nbytecode, to my VM.\nThe LPeg Library\nThe LPeg Interpreter executes bytecodes created by parsing a string of commands\nusing the LPeg library. Our JIT supports a subset of the LPeg library, with\nsome of the more advanced or obscure features being left out. Note that this\nsubset is still powerful enough to do things like parse JSON.\n\n\n\nOperator\nDescription\n\n\n\n\nlpeg.P(string)\nMatches string literally\n\n\nlpeg.P(n)\nMatches exactly n characters\n\n\nlpeg.P(-n)\nMatches at most n characters\n\n\nlpeg.S(string)\nMatches any character in string (Set)\n\n\nlpeg.R(\u201cxy\u201d)\nMatches any character between x and y (Range)\n\n\npatternn\nMatches at least n repetitions of pattern\n\n\npattern-n\nMatches at most n repetitions of pattern\n\n\npattern1 * pattern2\nMatches pattern1 followed by pattern2\n\n\npattern1 + pattern2\nMatches pattern1 or pattern2 (ordered choice)\n\n\npattern1 - pattern2\nMatches pattern1 if pattern2 does not match\n\n\n-pattern\nEquivalent to (\"\" - pattern)\n\n\n\nAs a simple example, the pattern lpeg.P\"ab\"+lpeg.P\"cd\" would match either the\nstring ab or the string cd.\nTo extract semantic information from a pattern, captures are needed. These are\nthe following operations supported for capture creation.\n\n\n\nOperation\nWhat it produces\n\n\n\n\nlpeg.C(pattern)\nthe match for pattern plus all captures made by pattern\n\n\nlpeg.Cp()\nthe current position (matches the empty string)\n\n\n\n(tables taken from the LPeg documentation)\nThese patterns are translated into bytecode by LPeg, at which point we are able\nto pass them into our own VM.\nThe VM\nThe state of the VM at any point is defined by the following variables:\n\nPC: program counter indicating the current instruction\nfail: an indicator that some match failed and the VM must backtrack\nindex: counter indicating the current character of the input string\nstackentries: stack of return addresses and choice points\ncaptures: stack of capture objects\n\nThe execution of bytecode manipulates the values of these variables in order to\nproduce some output. How that works and what that output looks like will be\nexplained now.\nThe Bytecode\nFor simplicity\u2019s sake I will not go over every individual bytecode, but instead\nchoose some that exemplify the core concepts of the bytecode set.\ngeneric character matching bytecodes\n\n\nany: Checks if there\u2019s any characters left in the inputstring. If it succeeds\nit advances the index and PC by 1, if not the bytecode fails.\n\n\nchar c: Checks if there is another bytecode in the input and if that\ncharacter is equal to c. Otherwise the bytecode fails.\n\n\nset c1-c2: Checks if there is another bytecode in the input and if that\ncharacter is between (including) c1 and c2. Otherwise the bytecode fails.\n\n\nThese bytecodes are the easiest to understand with very little impact on the\nVM. What it means for a bytecode to fail will be explained when\nwe get to control flow bytecodes.\nTo get back to the example, the first half of the pattern lpeg.P\"ab\" could be\ncompiled to the following bytecodes:\nchar a\nchar b\n\ncontrol flow bytecodes\n\n\njmp n: Sets PC to n, effectively jumping to the n\u2019th bytecode. Has no defined\nfailure case.\n\n\ntestchar c n: This is a lookahead bytecode. If the current character is equal\nto c it advances the PC but not the index. Otherwise it jumps to n.\n\n\ncall n: Puts a return address (the current PC + 1) on the stackentries stack\nand sets the PC to n. Has no defined failure case.\n\n\nret: Opposite of call. Removes the top value of the stackentries stack (if\nthe string of bytecodes is valid this will always be a return address) and\nsets the PC to the removed value. Has no defined failure case.\n\n\nchoice n: Puts a choice point on the stackentries stack. Has no defined\nfailure case.\n\n\ncommit n: Removes the top value of the stackentries stack (if the string of\nbytecodes is valid this will always be a choice point) and jumps to n. Has no\ndefined failure case.\n\n\nUsing testchar we can implement the full pattern lpeg.P\"ab\"+lpeg.P\"cd\" with\nbytecode as follows:\ntestchar a -> L1\nany\nchar b\nend\nany\nL1: char c\nchar d\nend\n\nThe any bytecode is needed because testchar does not consume a character\nfrom the input.\nFailure Handling, Backtracking and Choice Points\nA choice point consist of the VM\u2019s current index and capturestack as well as a\nPC. This is not the VM\u2019s PC at the time of creating the\nchoicepoint, but rather the PC where we should continue trying to find\nmatches when a failure occurs later.\nNow that we have talked about choice points, we can talk about how the VM\nbehaves in the fail state. If the VM is in the fail state, it removed entries\nfrom the stackentries stack until it finds a choice point. Then it backtracks\nby restoring the VM to the state defined by the choice point. If no choice\npoint is found this way, no match was found in the string and the VM halts.\nUsing choice points we could implement the example lpeg.P\"ab\" + lpeg.P\"cd\" in\nbytecodes in a different way (LPEG uses the simpler way shown above, but for\nmore complex patterns it can\u2019t use the lookahead solution using testchar):\nchoice L1\nchar a\nchar b\ncommit\nend\nL1: char c\nchar d\nend\n\nCaptures\nSome patterns require the VM to produce more output than just \u201cthe pattern\nmatched\u201d or \u201cthe pattern did not match\u201d. Imagine searching a document for an\nIPv4 address and all your program responded was \u201cI found one\u201d. In order to\nrecieve additional information about our inputstring, captures are used.\nThe capture object\nIn my VM, two types of capture objects are supported, one of them being the\nposition capture. It consists of a single index referencing the point in the\ninputstring where the object was created.\nThe other type of capture object is called simplecapture. It consists of an\nindex and a size value, which are used to reference a substring of the\ninputstring. In addition, simplecaptures have a variable status indicating they\nare either open or full. If a simplecapture object is open, that means that its\nsize is not yet determined, since the pattern we are capturing is of variable\nlength.\nCapture objects are created using the following bytecodes:\n\n\nFullcapture Position: Pushes a positioncapture object with the current index\nvalue to the capture stack.\n\n\nFullcapture Simple n: Pushes a simplecapture object with current index value\nand size=n to the capture stack.\n\n\nOpencapture Simple: Pushes an open simplecapture object with current index\nvalue and undetermined size to the capture stack.\n\n\nclosecapture: Sets the top element of the capturestack to full and sets its\nsize value using the difference between the current index and the index of\nthe capture object.\n\n\nThe RPython Implementation\nThese, and many more bytecodes were implemented in an RPython-interpreter.\nBy adding jit hints, we were able to generate an efficient JIT.\nWe will now take a closer look at some implementations of bytecodes.\n...\n        elif instruction.name == \"any\":\n            if index >= len(inputstring):\n                fail = True\n            else:\n                pc += 1\n                index += 1\n\n...\n\nThe code for the any-bytecode is relatively straight-forward. It either\nadvances the pc and index or sets the VM into the fail state,\ndepending on whether the end of the inputstring has been reached or not.\n...\n        if instruction.name == \"char\":\n            if index >= len(inputstring):\n                fail = True\n            elif instruction.character == inputstring[index]:\n                pc += 1\n                index += 1\n            else:\n                fail = True\n...\n\nThe char-bytecode also looks as one would expect. If the VM\u2019s string index is\nout of range or the character comparison fails, the VM is put into the\nfail state, otherwise the pc and index are advanced by 1. As you can see, the\ncharacter we\u2019re comparing the current inputstring to is stored in the\ninstruction object (note that this code-example has been simplified for\nclarity, since the actual implementation includes a jit-optimization that\nallows the VM to execute multiple successive char-bytecodes at once).\n...\n        elif instruction.name == \"jmp\":\n            pc = instruction.goto\n...\n\nThe jmp-bytecode comes with a goto value which is a pc that we want\nexecution to continue at.\n...\n        elif instruction.name == \"choice\":\n            pc += 1\n            choice_points = choice_points.push_choice_point(\n                instruction.goto, index, captures)\n...\n\nAs we can see here, the choice-bytecode puts a choice point onto the stack that\nmay be backtracked to if the VM is in the fail-state. This choice point\nconsists of a pc to jump to which is determined by the bytecode.\nBut it also includes the current index and captures values at the time the choice\npoint was created. An ongoing topic of jit optimization is which data structure\nis best suited to store choice points and return addresses. Besides naive\nimplementations of stacks and single-linked lists, more case-specific\nstructures are also being tested for performance.\nBenchmarking Result\nIn order to find out how much it helps to JIT LPeg patterns we ran a small\nnumber of benchmarks. We used an otherwise idle Intel Core i5-2430M CPU with\n3072 KiB of cache and 8 GiB of RAM, running with 2.40GHz. The machine was\nrunning Ubuntu 14.04 LTS, Lua 5.2.3 and we used GNU grep 2.16 as a point of\ncomparison for one of the benchmarks. The benchmarks were run 100 times in\na new process each. We measured the full runtime of the called process,\nincluding starting the process.\nNow we will take a look at some plots generated by measuring the runtime of\ndifferent iterations of my JIT compared to lua and using bootstrapping to\ngenerate a sampling distribution of mean values. The plots contain a few different\nvariants of pypeg, only the one called \"fullops\" is important for this blog post, however.\n\n\n\nThis is the plot for a search pattern that searches a text file for valid URLs.\nAs we can see, if the input file is as small as 100 kb, the benefits of JIT\noptimizations do not outweigh the time required to generate the\nmachine code. As a result, all of our attempts perform significantly slower\nthan LPeg.\n\n\n\nThis is the plot for the same search pattern on a larger input file. As we can\nsee, for input files as small as 500 kb our VM already outperforms LPeg\u2019s. An\nongoing goal of continued development is to get this lower boundary as small as\npossible.\n\n\n\nThe benefits of a JIT compared to an Interpreter become more and more relevant\nfor larger input files. Searching a file as large as 5 MB makes this fairly\nobvious and is exactly the behavior we expect.\n\n\n\nThis time we are looking at a different more complicated pattern, one that parses JSON used on a\n50 kb input file. As expected, LPeg outperforms us, however, something\nunexpected happens as we increase the filesize.\n\n\n\nSince LPeg has a defined maximum depth of 400 for the choicepoints and\nreturnaddresses Stack, LPeg by default refuses to parse files as small as\n100kb. This raises the question if LPeg was intended to be used for parsing.\nUntil a way to increase LPeg\u2019s maximum stack depth is found, no comparisons to\nLPeg can be performed at this scale. This has been a low priority in the past\nbut may be addressed in the future.\nTo conclude, we see that at sufficiently high filesizes, our JIT outperforms\nthe native LPeg-interpreter. This lower boundary is currently as low as 100kb\nin filesize.\nConclusion\nWriting a JIT for PEG\u2019s has proven itself to be a challenge worth pursuing, as\nthe expected benefits of a JIT compared to an Interpreter have been achieved.\nFuture goals include getting LPeg to be able to use parsing patterns on larger\nfiles, further increasing the performance of our JIT and comparing it to other\nwell-known programs serving a similar purpose, like grep.\nThe prototype implementation that I described in this post can be found\non Github\n(it's a bit of a hack in some places, though).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/04/an-rpython-jit-for-lpegs-4779548053359386284.html"
    },
    {
      "title": "PyPy v7.1 released; now uses utf-8 internally for unicode strings",
      "text": "The PyPy team is proud to release version 7.1.0 of PyPy, which includes\ntwo different interpreters:\n\n\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7\nPyPy3.6-beta: this is the second official release of PyPy to support 3.6\nfeatures, although it is still considered beta quality.\n\n\n\nThe interpreters are based on much the same codebase, thus the double\nrelease.\n\n\nThis release, coming fast on the heels of 7.0 in February, finally merges the\ninternal refactoring of unicode representation as UTF-8. Removing the\nconversions from strings to unicode internally lead to a nice speed bump. We merged the utf-8 changes to the py3.5 branch (Python3.5.3) but will concentrate on 3.6 going forward.\n\n\nWe also improved the ability to use the buffer protocol with ctype structures\nand arrays.\n\n\nThe CFFI backend has been updated to version 1.12.2. We recommend using CFFI\nrather than c-extensions to interact with C, and cppyy for interacting with\nC++ code.\n\u00a0You can download the v7.1 releases here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\n\n\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython\u2019s JIT even better.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.6. It\u2019s fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\n\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\nThis PyPy release supports:\n\u00a0\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux\n\u00a0ARM32 although we do not supply downloadable binaries at this time\ns390x running Linux\n\n\n\nWhat else is new?\nPyPy 7.0 was released in February, 2019.\nThere are many incremental improvements to RPython and PyPy, for more information see the changelog.\n\nPlease update, and continue to help us make PyPy better.\n\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/03/pypy-v71-released-now-uses-utf-8-451324088028792912.html"
    },
    {
      "title": "PyPy v7.0.0: triple release of 2.7, 3.5 and 3.6-alpha",
      "text": "The PyPy team is proud to release the version 7.0.0 of PyPy, which includes\nthree different interpreters:\n\n\nPyPy2.7, which is an interpreter supporting the syntax and the features of\nPython 2.7\nPyPy3.5, which supports Python 3.5\nPyPy3.6-alpha: this is the first official release of PyPy to support 3.6\nfeatures, although it is still considered alpha quality.\n\n\nAll the interpreters are based on much the same codebase, thus the triple\nrelease.\nUntil we can work with downstream providers to distribute builds with PyPy, we\nhave made packages for some common packages available as wheels.\nThe GC hooks , which can be used to gain more insights into its\nperformance, has been improved and it is now possible to manually manage the\nGC by using a combination of gc.disable and gc.collect_step. See the\nGC blog post.\nWe updated the cffi module included in PyPy to version 1.12, and the\ncppyy backend to 1.4. Please use these to wrap your C and C++ code,\nrespectively, for a JIT friendly experience.\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nWe strongly recommend updating.\nThe PyPy3.6 release and the Windows PyPy3.5 release are still not production\nquality so your mileage may vary. There are open issues with incomplete\ncompatibility and c-extension support.\nThe utf8 branch that changes internal representation of unicode to utf8 did not\nmake it into the release, so there is still more goodness coming.\nYou can download the v7.0 releases here:\n\nhttps://pypy.org/download.html\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython's JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7, 3.5 and 3.6. It's fast (PyPy and CPython 2.7.x performance\ncomparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThe PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\nUnfortunately at the moment of writing our ARM buildbots are out of service,\nso for now we are not releasing any binary for the ARM architecture.\n\n\nWhat else is new?\nPyPy 6.0 was released in April, 2018.\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\nPlease update, and continue to help us make PyPy better.\n\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2019/02/pypy-v700-triple-release-of-27-35-and-606875333356156076.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Report 2019",
      "text": "Hello everyone!\nWe are happy to report a successful and well attended sprint that is wrapping up\nin D\u00fcsseldorf, Germany. In the last week we had eighteen people sprinting\nat the Heinrich-Heine-Universit\u00e4t D\u00fcsseldorf on various topics.\nTotally serious work going on here constantly.\nA big\nchunk of the sprint was dedicated to various discussions, since we did not\nmanage to gather the core developers in one room in quite a while.\nDiscussion topics included:\n\nFunding and general sustainability of open source.\nCatching up with CPython 3.7/3.8 \u2013 we are planning to release 3.6 some time\nin the next few months and we will continue working on 3.7/3.8.\nWhat to do with VMprof\nHow can we support Cython inside PyPy in a way that will be understood\nby the JIT, hence fast.\nThe future of supporting the numeric stack on pypy \u2013 we have made significant\nprogress in the past few years and most of the numeric stack works out of the box,\nbut deployment and performance remain problems. Improving on those problems\nremains a very important focus for PyPy as a project.\nUsing the presence of a CPython developer (\u0141ukasz Langa) and a Graal Python developer\n(Tim Felgentreff) we discussed ways to collaborate in order to improve Python\necosystem across implementations.\nPierre-Yves David and Georges Racinet from octobus gave us an exciting demo\non Heptapod, which adds mercurial support to gitlab.\nMaciej and Armin gave demos of their current (non-PyPy-related) project VRSketch.\n\n\n\nVisiting the Landschaftspark Duisburg Nord on the break day\n\nSome highlights of the coding tasks worked on:\n\nAarch64 (ARM64) JIT backend work has been started, we are able to run the first\ntest! Tobias Oberstein from Crossbar GmbH and Rodolph Perfetta from ARM joined the\nsprint to help kickstart the project.\nThe long running math-improvements branch that was started by Stian Andreassen got merged\nafter bugfixes done by Alexander Schremmer. It should improve operations on large integers.\nThe arcane art of necromancy was used to revive long dormant regalloc branch started\nand nearly finished by Carl Friedrich Bolz-Tereick. The branch got merged and gives\nsome modest speedups across the board.\nAndrew Lawrence worked on MSI installer for PyPy on windows.\n\u0141ukasz worked on improving failing tests on the PyPy 3.6 branch. He knows very obscure\ndetails of CPython (e.g. how pickling works), hence we managed to progress very quickly.\nMatti Picus set up a new benchmarking server for PyPy 3 branches.\nThe Utf8 branch, which changes the internal representation of unicode might be finally\nmerged at some point very soon. We discussed and improved upon the last few\nblockers. It gives significant speedups in a lot of cases handling strings.\nZlib was missing couple methods, which were added by Ronan Lamy and Julian Berman.\nManuel Jacob fixed RevDB failures.\nAntonio Cuni and Matti Picus worked on 7.0 release which should happen in a few days.\n\nNow we are all quite exhausted, and are looking forward to catching up on sleep.\nBest regards,\nMaciej Fija\u0142kowski, Carl Friedrich Bolz-Tereick and the whole PyPy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2019/02/dusseldorf-sprint-report-2019-6107623654916313905.html"
    },
    {
      "title": "PyPy for low-latency systems",
      "text": "PyPy for low-latency systems\nRecently I have merged the gc-disable branch, introducing a couple of features\nwhich are useful when you need to respond to certain events with the lowest\npossible latency.  This work has been kindly sponsored by Gambit Research\n(which, by the way, is a very cool and geeky place where to work, in case you\nare interested).  Note also that this is a very specialized use case, so these\nfeatures might not be useful for the average PyPy user, unless you have the\nsame problems as described here.\n\nThe PyPy VM manages memory using a generational, moving Garbage Collector.\nPeriodically, the GC scans the whole heap to find unreachable objects and\nfrees the corresponding memory.  Although at a first look this strategy might\nsound expensive, in practice the total cost of memory management is far less\nthan e.g. on CPython, which is based on reference counting.  While maybe\ncounter-intuitive, the main advantage of a non-refcount strategy is\nthat allocation is very fast (especially compared to malloc-based allocators),\nand deallocation of objects which die young is basically for free. More\ninformation about the PyPy GC is available here.\n\nAs we said, the total cost of memory managment is less on PyPy than on\nCPython, and it's one of the reasons why PyPy is so fast.  However, one big\ndisadvantage is that while on CPython the cost of memory management is spread\nall over the execution of the program, on PyPy it is concentrated into GC\nruns, causing observable pauses which interrupt the execution of the user\nprogram.\nTo avoid excessively long pauses, the PyPy GC has been using an incremental\nstrategy since 2013. The GC runs as a series of \"steps\", letting the user\nprogram to progress between each step.\n\nThe following chart shows the behavior of a real-world, long-running process:\n\n\n\n\nThe orange line shows the total memory used by the program, which\nincreases linearly while the program progresses. Every ~5 minutes, the GC\nkicks in and the memory usage drops from ~5.2GB to ~2.8GB (this ratio is controlled\nby the PYPY_GC_MAJOR_COLLECT env variable).\nThe purple line shows aggregated data about the GC timing: the whole\ncollection takes ~1400 individual steps over the course of ~1 minute: each\npoint represent the maximum time a single step took during the past 10\nseconds. Most steps take ~10-20 ms, although we see a horrible peak of ~100 ms\ntowards the end. We have not investigated yet what it is caused by, but we\nsuspect it is related to the deallocation of raw objects.\n\nThese multi-millesecond pauses are a problem for systems where it is important\nto respond to certain events with a latency which is both low and consistent.\nIf the GC kicks in at the wrong time, it might causes unacceptable pauses during\nthe collection cycle.\n\nLet's look again at our real-world example. This is a system which\ncontinuously monitors an external stream; when a certain event occurs, we want\nto take an action. The following chart shows the maximum time it takes to\ncomplete one of such actions, aggregated every minute:\n\n\n\n\nYou can clearly see that the baseline response time is around ~20-30\nms. However, we can also see periodic spikes around ~50-100 ms, with peaks up\nto ~350-450 ms! After a bit of investigation, we concluded that most (although\nnot all) of the spikes were caused by the GC kicking in at the wrong time.\n\nThe work I did in the gc-disable branch aims to fix this problem by\nintroducing two new features to the gc module:\n\n\ngc.disable(), which previously only inhibited the execution of\nfinalizers without actually touching the GC, now disables the GC major\ncollections. After a call to it, you will see the memory usage grow\nindefinitely.\ngc.collect_step() is a new function which you can use to manually\nexecute a single incremental GC collection step.\n\n\nIt is worth to specify that gc.disable() disables only the major\ncollections, while minor collections still runs.  Moreover, thanks to the\nJIT's virtuals, many objects with a short and predictable lifetime are not\nallocated at all. The end result is that most objects with short lifetime are\nstill collected as usual, so the impact of gc.disable() on memory growth\nis not as bad as it could sound.\n\nCombining these two functions, it is possible to take control of the GC to\nmake sure it runs only when it is acceptable to do so.  For an example of\nusage, you can look at the implementation of a custom GC inside pypytools.\nThe peculiarity is that it also defines a \"with nogc():\" context manager\nwhich you can use to mark performance-critical sections where the GC is not\nallowed to run.\n\nThe following chart compares the behavior of the default PyPy GC and the new\ncustom GC, after a careful placing of nogc() sections:\n\n\n\n\nThe yellow line is the same as before, while the purple line shows the new\nsystem: almost all spikes have gone, and the baseline performance is about 10%\nbetter. There is still one spike towards the end, but after some investigation\nwe concluded that it was not caused by the GC.\n\nNote that this does not mean that the whole program became magically\nfaster: we simply moved the GC pauses in some other place which is not\nshown in the graph: in this specific use case this technique was useful\nbecause it allowed us to shift the GC work in places where pauses are more\nacceptable.\n\nAll in all, a pretty big success, I think.  These functionalities are already\navailable in the nightly builds of PyPy, and will be included in the next\nrelease: take this as a New Year present :)\n\nAntonio Cuni and the PyPy team",
      "tags": "gc,sponsors",
      "url": "https://www.pypy.org/posts/2019/01/pypy-for-low-latency-systems-613165393301401965.html"
    },
    {
      "title": "PyPy Winter Sprint Feb 4-9 in D\u00fcsseldorf",
      "text": "PyPy Sprint February 4th-9th 2019 in D\u00fcsseldorf\n\n\nThe next PyPy sprint will be held in the Computer Science department of Heinrich-Heine Universit\u00e4t D\u00fcsseldorf from the 4th to the 9st of February 2019 (nine years after the last sprint there). This is a fully public sprint, everyone is welcome to join us.\n\nTopics and goals\n\n\n\nimprove Python 3.6 support\ndiscuss benchmarking situation\nprogress on utf-8 branches\ncpyext performance and completeness\npackaging: are we ready to upload to PyPI?\n\nissue 2617\u00a0 - we expose too many functions from lib-pypy.so\nmanylinux2010 - will it solve our build issues?\nformulate an ABI name and upgrade policy\n\n\n\nmemoryview(ctypes.Structure) does not create the correct format string\ndiscussing the state and future of PyPy and the wider Python ecosystem\n\n\n\n\nLocation\n\nThe sprint will take place in seminar room 25.12.02.55 of the computer science department.\u00a0 It is in the building 25.12 of the university campus, second floor. Travel instructions\n\n\nExact times\n\nWork days: starting February 4th (10:00), ending February 9th (~afternoon). The break day will probably be Thursday.\n\nRegistration\n\n\nPlease register by Mercurial::\nhttps://bitbucket.org/pypy/extradoc/\n\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/ddorf2019/people.txt\n\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\n\n\n\nLooking forward to seeing everyone there!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/12/pypy-winter-sprint-feb-4-9-in-dusseldorf-7199110498451574074.html"
    },
    {
      "title": "Funding for 64-bit Armv8-a support in PyPy",
      "text": "Hello everyone\n\nAt PyPy we are trying to support a relatively wide range of platforms. We have PyPy working on OS X, Windows and various flavors of linux (and unofficially various flavors of BSD) on the software side, with hardware side having x86, x86_64, PPC, 32-bit Arm (v7) and even zarch. This is harder than for other projects, since PyPy emits assembler on the fly from the just in time compiler and it requires significant amount of work to port it to a new platform.\n\nWe are pleased to inform that Arm Limited, together with Crossbar.io GmbH, are sponsoring the development of 64-bit Armv8-a architecture support through Baroque Software OU, which would allow PyPy to run on a new variety of low-power, high-density servers with that architecture. We believe this will be beneficial for the funders, for the PyPy project as well as to the wider community.\n\nThe work will commence soon and will be done some time early next year with expected speedups either comparable to x86 speedups or, if our current experience with ARM holds, more significant than x86 speedups.\n\nBest,\nMaciej Fijalkowski and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/11/hello-everyone-at-pypy-we-are-trying-to-5336557946798583063.html"
    },
    {
      "title": "Guest Post: Implementing a Calculator REPL in RPython",
      "text": "This is a tutorial style post that walks through using the RPython translation\ntoolchain to create a REPL that executes basic math expressions. \n\nWe will do that by scanning the user's input into tokens, compiling those \ntokens into bytecode and running that bytecode in our own virtual machine. Don't\nworry if that sounds horribly complicated, we are going to explain it step by\nstep. \n\nThis post is a bit of a diversion while on my journey to create a compliant \nlox implementation\nusing the RPython translation toolchain. The \nmajority of this work is a direct RPython translation of the low level C \nguide from Bob Nystrom (@munificentbob) in the\nexcellent book craftinginterpreters.com\nspecifically the chapters 14 \u2013 17.\n\nThe road ahead\n\nAs this post is rather long I'll break it into a few major sections. In each section we will\nhave something that translates with RPython, and at the end it all comes together. \n\n\nREPL\n\nVirtual Machine\n\nScanning the source\n\nCompiling Expressions\n\nEnd to end\n\n\nA REPL\n\nSo if you're a Python programmer you might be thinking this is pretty trivial right?\n\nI mean if we ignore input errors, injection attacks etc couldn't we just do something\nlike this:\n\n\"\"\"\nA pure python REPL that can parse simple math expressions\n\"\"\"\nwhile True:\n    print(eval(raw_input(\"> \")))\n\n\nWell it does appear to do the trick:\n\n$ python2 section-1-repl/main.py\n> 3 + 4 * ((1.0/(2 * 3 * 4)) + (1.0/(4 * 5 * 6)) - (1.0/(6 * 7 * 8)))\n3.1880952381\n\n\nSo can we just ask RPython to translate this into a binary that runs magically\nfaster?\n\nLet's see what happens. We need to add two functions for RPython to\nget its bearings (entry_point and target) and call the file targetXXX:\n\ntargetrepl1.py\n\ndef repl():\n    while True:\n        print eval(raw_input('> '))\n\n\ndef entry_point(argv):\n    repl()\n    return 0\n\n\ndef target(driver, *args):\n    return entry_point, None\n\n\nWhich at translation time gives us this admonishment that accurately tells us\nwe are trying to call a Python built-in raw_input that is unfortunately not \nvalid RPython.\n\n$ rpython ./section-1-repl/targetrepl1.py\n...SNIP...\n[translation:ERROR] AnnotatorError: \n\nobject with a __call__ is not RPython: <built-in function raw_input>\nProcessing block:\n block@18 is a <class 'rpython.flowspace.flowcontext.SpamBlock'> \n in (target1:2)repl \n containing the following operations: \n       v0 = simple_call((builtin_function raw_input), ('> ')) \n       v1 = simple_call((builtin_function eval), v0) \n       v2 = str(v1) \n       v3 = simple_call((function rpython_print_item), v2) \n       v4 = simple_call((function rpython_print_newline)) \n\n\nOk so we can't use raw_input or eval but that doesn't faze us. Let's get \nthe input from a stdin stream and just print it out (no evaluation).\n\ntargetrepl2.py\n\nfrom rpython.rlib import rfile\n\nLINE_BUFFER_LENGTH = 1024\n\n\ndef repl(stdin):\n    while True:\n        print \"> \",\n        line = stdin.readline(LINE_BUFFER_LENGTH)\n        print line\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n    try:\n        repl(stdin)\n    except:\n        return 0\n\n\ndef target(driver, *args):\n    return entry_point, None\n\n\nTranslate targetrepl2.py \u2013 we can add an optimization level if we\nare so inclined:\n\n$ rpython --opt=2 section-1-repl/targetrepl2.py\n...SNIP...\n[Timer] Timings:\n[Timer] annotate                       ---  1.2 s\n[Timer] rtype_lltype                   ---  0.9 s\n[Timer] backendopt_lltype              ---  0.6 s\n[Timer] stackcheckinsertion_lltype     ---  0.0 s\n[Timer] database_c                     --- 15.0 s\n[Timer] source_c                       ---  1.6 s\n[Timer] compile_c                      ---  1.9 s\n[Timer] =========================================\n[Timer] Total:                         --- 21.2 s\n\n\nNo errors!? Let's try it out:\n\n$ ./target2-c \n1 + 2\n>  1 + 2\n\nC\n\n\nAhh our first success \u2013 let's quickly deal with the flushing fail by using the \nstdout stream directly as well. Let's print out the input in quotes:\n\nfrom rpython.rlib import rfile\n\nLINE_BUFFER_LENGTH = 1024\n\n\ndef repl(stdin, stdout):\n    while True:\n        stdout.write(\"> \")\n        line = stdin.readline(LINE_BUFFER_LENGTH)\n        print '\"%s\"' % line.strip()\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n    try:\n        repl(stdin, stdout)\n    except:\n        pass\n    return 0\n\n\ndef target(driver, *args):\n    return entry_point, None\n\n\nTranslation works, and the test run too:\n\n$ ./target3-c \n> hello this seems better\n\"hello this seems better\"\n> C\n\n\nSo we are in a good place with taking user input and printing output... What about\nthe whole math evaluation thing we were promised? For that we are can probably leave\nour RPython REPL behind for a while and connect it up at the end.\n\nA virtual machine\n\nA virtual machine is the execution engine of our basic math interpreter. It will be very simple,\nonly able to do simple tasks like addition. I won't go into any depth to describe why we want\na virtual machine, but it is worth noting that many languages including Java and Python make \nthis decision to compile to an intermediate bytecode representation and then execute that with\na virtual machine. Alternatives are compiling directly to native machine code like (earlier versions of) the V8\nJavaScript engine, or at the other end of the spectrum executing an abstract syntax tree \u2013 \nwhich is what the Truffle approach to building VMs is based on. \n\nWe are going to keep things very simple. We will have a stack where we can push and pop values,\nwe will only support floats, and our VM will only implement a few very basic operations.\n\nOpCodes\n\nIn fact our entire instruction set is:\n\nOP_CONSTANT\nOP_RETURN\nOP_NEGATE\nOP_ADD\nOP_SUBTRACT\nOP_MULTIPLY\nOP_DIVIDE\n\n\nSince we are targeting RPython we can't use the nice enum module from the Python standard\nlibrary, so instead we just define a simple class with class attributes.\n\nWe should start to get organized, so we will create a new file \nopcodes.py and add this:\n\nclass OpCode:\n    OP_CONSTANT = 0\n    OP_RETURN = 1\n    OP_NEGATE = 2\n    OP_ADD = 3\n    OP_SUBTRACT = 4\n    OP_MULTIPLY = 5\n    OP_DIVIDE = 6\n\n\nChunks\n\nTo start with we need to get some infrastructure in place before we write the VM engine.\n\nFollowing craftinginterpreters.com\nwe start with a Chunk object which will represent our bytecode. In RPython we have access \nto Python-esq lists so our code object will just be a list of OpCode values \u2013 which are \njust integers. A list of ints, couldn't get much simpler.\n\nsection-2-vm/chunk.py\n\nclass Chunk:\n    code = None\n\n    def __init__(self):\n        self.code = []\n\n    def write_chunk(self, byte):\n        self.code.append(byte)\n\n    def disassemble(self, name):\n        print \"== %s ==\\n\" % name\n        i = 0\n        while i < len(self.code):\n            i = disassemble_instruction(self, i)\n\n\nFrom here on I'll only present minimal snippets of code instead of the whole lot, but \nI'll link to the repository with the complete example code. For example the \nvarious debugging including disassemble_instruction isn't particularly interesting\nto include verbatim. See the github repo for full details\n\nWe need to check that we can create a chunk and disassemble it. The quickest way to do this\nis to use Python during development and debugging then every so often try to translate it.\n\nGetting the disassemble part through the RPython translator was a hurdle for me as I\nquickly found that many str methods such as format are not supported, and only very basic\n% based formatting is supported. I ended up creating helper functions for string manipulation\nsuch as:\n\ndef leftpad_string(string, width, char=\" \"):\n    l = len(string)\n    if l > width:\n        return string\n    return char * (width - l) + string\n\n\nLet's write a new entry_point that creates and disassembles a chunk of bytecode. We can\nset the target output name to vm1 at the same time:\n\ntargetvm1.py\n\ndef entry_point(argv):\n    bytecode = Chunk()\n    bytecode.write_chunk(OpCode.OP_ADD)\n    bytecode.write_chunk(OpCode.OP_RETURN)\n    bytecode.disassemble(\"hello world\")\n    return 0\n\ndef target(driver, *args):\n    driver.exe_name = \"vm1\"\n    return entry_point, None\n\n\nRunning this isn't going to be terribly interesting, but it is always nice to\nknow that it is doing what you expect:\n\n$ ./vm1 \n== hello world ==\n\n0000 OP_ADD       \n0001 OP_RETURN    \n\n\nChunks of data\n\nRef: https://www.craftinginterpreters.com/chunks-of-bytecode.html#constants\n\nSo our bytecode is missing a very crucial element \u2013 the values to operate on!\n\nAs with the bytecode we can store these constant values as part of the chunk\ndirectly in a list. Each chunk will therefore have a constant data component,\nand a code component. \n\nEdit the chunk.py file and add the new instance attribute constants as an\nempty list, and a new method add_constant.\n\n    def add_constant(self, value):\n        self.constants.append(value)\n        return len(self.constants) - 1\n\n\nNow to use this new capability we can modify our example chunk\nto write in some constants before the OP_ADD:\n\n    bytecode = Chunk()\n    constant = bytecode.add_constant(1.0)\n    bytecode.write_chunk(OpCode.OP_CONSTANT)\n    bytecode.write_chunk(constant)\n\n    constant = bytecode.add_constant(2.0)\n    bytecode.write_chunk(OpCode.OP_CONSTANT)\n    bytecode.write_chunk(constant)\n\n    bytecode.write_chunk(OpCode.OP_ADD)\n    bytecode.write_chunk(OpCode.OP_RETURN)\n\n    bytecode.disassemble(\"adding constants\")\n\n\nWhich still translates with RPython and when run gives us the following disassembled\nbytecode:\n\n== adding constants ==\n\n0000 OP_CONSTANT  (00)        '1'\n0002 OP_CONSTANT  (01)        '2'\n0004 OP_ADD       \n0005 OP_RETURN\n\n\nWe won't go down the route of serializing the bytecode to disk, but this bytecode chunk\n(including the constant data) could be saved and executed on our VM later \u2013 like a Java\n.class file. Instead we will pass the bytecode directly to our VM after we've created\nit during the compilation process. \n\nEmulation\n\nSo those four instructions of bytecode combined with the constant value mapping\n00 -> 1.0 and 01 -> 2.0 describes individual steps for our virtual machine\nto execute. One major point in favor of defining our own bytecode is we can \ndesign it to be really simple to execute \u2013 this makes the VM really easy to implement.\n\nAs I mentioned earlier this virtual machine will have a stack, so let's begin with that.\nNow the stack is going to be a busy little beast \u2013 as our VM takes instructions like \nOP_ADD it will pop off the top two values from the stack, and push the result of adding \nthem together back onto the stack. Although dynamically resizing Python lists \nare marvelous, they can be a little slow. RPython can take advantage of a constant sized\nlist which doesn't make our code much more complicated.\n\nTo do this we will define a constant sized list and track the stack_top directly. Note\nhow we can give the RPython translator hints by adding assertions about the state that\nthe stack_top will be in.\n\nclass VM(object):\n    STACK_MAX_SIZE = 256\n    stack = None\n    stack_top = 0\n\n    def __init__(self):\n        self._reset_stack()\n\n    def _reset_stack(self):\n        self.stack = [0] * self.STACK_MAX_SIZE\n        self.stack_top = 0\n\n    def _stack_push(self, value):\n        assert self.stack_top < self.STACK_MAX_SIZE\n        self.stack[self.stack_top] = value\n        self.stack_top += 1\n\n    def _stack_pop(self):\n        assert self.stack_top >= 0\n        self.stack_top -= 1\n        return self.stack[self.stack_top]\n\n    def _print_stack(self):\n        print \"         \",\n        if self.stack_top <= 0:\n            print \"[]\",\n        else:\n            for i in range(self.stack_top):\n                print \"[ %s ]\" % self.stack[i],\n        print\n\n\nNow we get to the main event, the hot loop, the VM engine. Hope I haven't built it up to\nmuch, it is actually really simple! We loop until the instructions tell us to stop \n(OP_RETURN), and dispatch to other simple methods based on the instruction.\n\n    def _run(self):\n        while True:\n            instruction = self._read_byte()\n\n            if instruction == OpCode.OP_RETURN:\n                print \"%s\" % self._stack_pop()\n                return InterpretResultCode.INTERPRET_OK\n            elif instruction == OpCode.OP_CONSTANT:\n                constant = self._read_constant()\n                self._stack_push(constant)\n            elif instruction == OpCode.OP_ADD:\n                self._binary_op(self._stack_add)    \n\n\nNow the _read_byte method will have to keep track of which instruction we are up \nto. So add an instruction pointer (ip) to the VM with an initial value of 0.\nThen _read_byte is simply getting the next bytecode (int) from the chunk's code:\n\n    def _read_byte(self):\n        instruction = self.chunk.code[self.ip]\n        self.ip += 1\n        return instruction\n\n\n\n\nIf the instruction is OP_CONSTANT we take the constant's address from the next byte\nof the chunk's code, retrieve that constant value and add it to the VM's stack.\n\n    def _read_constant(self):\n        constant_index = self._read_byte()\n        return self.chunk.constants[constant_index]\n\n\nFinally our first arithmetic operation OP_ADD, what it has to achieve doesn't \nrequire much explanation: pop two values from the stack, add them together, push \nthe result. But since a few operations all have the same template we introduce a\nlayer of indirection \u2013 or abstraction \u2013 by introducing a reusable _binary_op \nhelper method.\n\n    @specialize.arg(1)\n    def _binary_op(self, operator):\n        op2 = self._stack_pop()\n        op1 = self._stack_pop()\n        result = operator(op1, op2)\n        self._stack_push(result)\n\n    @staticmethod\n    def _stack_add(op1, op2):\n        return op1 + op2\n\n\n\n\nNote we tell RPython to specialize _binary_op on the first argument. This causes\nRPython to make a copy of _binary_op for every value of the first argument passed,\nwhich means that each copy contains a call to a particular operator, which can then be\ninlined.\n\nTo be able to run our bytecode the only thing left to do is to pass in the chunk \nand call _run():\n\n    def interpret_chunk(self, chunk):\n        if self.debug_trace:\n            print \"== VM TRACE ==\"\n        self.chunk = chunk\n        self.ip = 0\n        try:\n            result = self._run()\n            return result\n        except:\n            return InterpretResultCode.INTERPRET_RUNTIME_ERROR\n\n\ntargetvm3.py connects the pieces:\n\ndef entry_point(argv):\n    bytecode = Chunk()\n    constant = bytecode.add_constant(1)\n    bytecode.write_chunk(OpCode.OP_CONSTANT)\n    bytecode.write_chunk(constant)\n    constant = bytecode.add_constant(2)\n    bytecode.write_chunk(OpCode.OP_CONSTANT)\n    bytecode.write_chunk(constant)\n    bytecode.write_chunk(OpCode.OP_ADD)\n    bytecode.write_chunk(OpCode.OP_RETURN)\n\n    vm = VM()\n    vm.interpret_chunk(bytecode)\n\n    return 0\n\n\nI've added some trace debugging so we can see what the VM and stack is doing.\n\nThe whole thing translates with RPython, and when run gives us:\n\n./vm3\n== VM TRACE ==\n          []\n0000 OP_CONSTANT  (00)        '1'\n          [ 1 ]\n0002 OP_CONSTANT  (01)        '2'\n          [ 1 ] [ 2 ]\n0004 OP_ADD       \n          [ 3 ]\n0005 OP_RETURN    \n3\n\n\nYes we just computed the result of 1+2. Pat yourself on the back. \n\nAt this point it is probably valid to check that the translated executable is actually\nfaster than running our program directly in Python. For this trivial example under \nPython2/pypy this targetvm3.py file runs in the 20ms \u2013 90ms region, and the \ncompiled vm3 runs in <5ms. Something useful must be happening during the translation.\n\nI won't go through the code adding support for our other instructions as they are\nvery similar and straightforward. Our VM is ready to execute our chunks of bytecode,\nbut we haven't yet worked out how to take the entered expression and turn that into\nthis simple bytecode. This is broken into two steps, scanning and compiling.\n\nScanning the source\n\nAll the source for this section can be found in \nsection-3-scanning.\n\nThe job of the scanner is to take the raw expression string and transform it into\na sequence of tokens. This scanning step will strip out whitespace and comments, \ncatch errors with invalid token and tokenize the string. For example the input \n\"( 1 + 2 ) would get tokenized into LEFT_PAREN, NUMBER(1), PLUS, NUMBER(2), RIGHT_PAREN.\n\nAs with our OpCodes we will just define a simple Python class to define an int\nfor each type of token:\n\nclass TokenTypes:\n    ERROR = 0\n    EOF = 1\n    LEFT_PAREN = 2\n    RIGHT_PAREN = 3\n    MINUS = 4\n    PLUS = 5\n    SLASH = 6\n    STAR = 7\n    NUMBER = 8\n\n\nA token has to keep some other information as well \u2013 keeping track of the location and \nlength of the token will be helpful for error reporting. The NUMBER token clearly needs \nsome data about the value it is representing: we could include a copy of the source lexeme \n(e.g. the string 2.0), or parse the value and store that, or \u2013 what we will do in this \nblog \u2013 use the location and length information as pointers into the original source \nstring. Every token type (except perhaps ERROR) will use this simple data structure: \n\nclass Token(object):\n\n    def __init__(self, start, length, token_type):\n        self.start = start\n        self.length = length\n        self.type = token_type\n\n\nOur soon to be created scanner will create these Token objects which refer back to \naddresses in some source. If the scanner sees the source \"( 1 + 2.0 )\" it would emit\nthe following tokens:\n\nToken(0, 1, TokenTypes.LEFT_PAREN)\nToken(2, 1, TokenTypes.NUMBER)\nToken(4, 1, TokenTypes.PLUS)\nToken(6, 3, TokenTypes.NUMBER)\nToken(10, 1, TokenTypes.RIGHT_PAREN)\n\n\nScanner\n\nLet's walk through the scanner implementation method\nby method. The scanner will take the source and pass through it once, creating tokens\nas it goes.\n\nclass Scanner(object):\n\n    def __init__(self, source):\n        self.source = source\n        self.start = 0\n        self.current = 0\n\n\nThe start and current variables are character indices in the source string that point to \nthe current substring being considered as a token. \n\nFor example in the string \"(51.05+2)\" while we are tokenizing the number 51.05\nwe will have start pointing at the 5, and advance current character by character\nuntil the character is no longer part of a number. Midway through scanning the number \nthe start and current values might point to 1 and 4 respectively:\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n\"(\"\n\"5\"\n\"1\"\n\".\"\n\"0\"\n\"5\"\n\"+\"\n\"2\"\n\")\"\n\n\n\n\u00a0\n\n\n\u00a0\n\n\n\n\n\n\n\n\nFrom current=4 the scanner peeks ahead and sees that the next character (5) is\na digit, so will continue to advance.\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n\"(\"\n\"5\"\n\"1\"\n\".\"\n\"0\"\n\"5\"\n\"+\"\n\"2\"\n\")\"\n\n\n\n\u00a0\n\n\n\n\u00a0\n\n\n\n\n\n\n\nWhen the scanner peeks ahead and sees the \"+\" it will create the number\ntoken and emit it. The method that carry's out this tokenizing is _number:\n\n    def _number(self):\n        while self._peek().isdigit():\n            self.advance()\n\n        # Look for decimal point\n        if self._peek() == '.' and self._peek_next().isdigit():\n            self.advance()\n            while self._peek().isdigit():\n                self.advance()\n\n        return self._make_token(TokenTypes.NUMBER)\n\n\nIt relies on a few helpers to look ahead at the upcoming characters:\n\n    def _peek(self):\n        if self._is_at_end():\n            return '\\0'\n        return self.source[self.current]\n\n    def _peek_next(self):\n        if self._is_at_end():\n            return '\\0'\n        return self.source[self.current+1]\n\n    def _is_at_end(self):\n        return len(self.source) == self.current\n\n\nIf the character at current is still part of the number we want to call advance\nto move on by one character.\n\n    def advance(self):\n        self.current += 1\n        return self.source[self.current - 1]\n\n\nOnce the isdigit() check fails in _number() we call _make_token() to emit the\ntoken with the NUMBER type.\n\n    def _make_token(self, token_type):\n        return Token(\n            start=self.start,\n            length=(self.current - self.start),\n            token_type=token_type\n        )\n\n\nNote again that the token is linked to an index address in the source, rather than \nincluding the string value.\n\nOur scanner is pull based, a token will be requested via scan_token. First we skip \npast whitespace and depending on the characters emit the correct token:\n\n    def scan_token(self):\n        # skip any whitespace\n        while True:\n            char = self._peek()\n            if char in ' \\r\\t\\n':\n                self.advance()\n            break\n\n        self.start = self.current\n\n        if self._is_at_end():\n            return self._make_token(TokenTypes.EOF)\n\n        char = self.advance()\n\n        if char.isdigit():\n            return self._number()\n\n        if char == '(':\n            return self._make_token(TokenTypes.LEFT_PAREN)\n        if char == ')':\n            return self._make_token(TokenTypes.RIGHT_PAREN)\n        if char == '-':\n            return self._make_token(TokenTypes.MINUS)\n        if char == '+':\n            return self._make_token(TokenTypes.PLUS)\n        if char == '/':\n            return self._make_token(TokenTypes.SLASH)\n        if char == '*':\n            return self._make_token(TokenTypes.STAR)\n\n        return ErrorToken(\"Unexpected character\", self.current)\n\n\n\n\nIf this was a real programming language we were scanning, this would be the point where we \nadd support for different types of literals and any language identifiers/reserved words.\n\nAt some point we will need to parse the literal value for our numbers, but we leave that\njob for some later component, for now we'll just add a get_token_string helper. To make\nsure that RPython is happy to index arbitrary slices of source we add range assertions:\n\n    def get_token_string(self, token):\n        if isinstance(token, ErrorToken):\n            return token.message\n        else:\n            end_loc = token.start + token.length\n            assert end_loc < len(self.source)\n            assert end_loc > 0\n            return self.source[token.start:end_loc]\n\n\nA simple entry point can be used to test our scanner with a hard coded \nsource string:\n\ntargetscanner1.py\n\nfrom scanner import Scanner, TokenTypes, TokenTypeToName\n\n\ndef entry_point(argv):\n\n    source = \"(   1   + 2.0 )\"\n\n    scanner = Scanner(source)\n    t = scanner.scan_token()\n    while t.type != TokenTypes.EOF and t.type != TokenTypes.ERROR:\n        print TokenTypeToName[t.type],\n        if t.type == TokenTypes.NUMBER:\n            print \"(%s)\" % scanner.get_token_string(t),\n        print\n        t = scanner.scan_token()\n    return 0\n\n\nRPython didn't complain, and lo it works:\n\n$ ./scanner1 \nLEFT_PAREN\nNUMBER (1)\nPLUS\nNUMBER (2.0)\nRIGHT_PAREN\n\n\nLet's connect our REPL to the scanner.\n\ntargetscanner2.py\n\nfrom rpython.rlib import rfile\nfrom scanner import Scanner, TokenTypes, TokenTypeToName\n\nLINE_BUFFER_LENGTH = 1024\n\n\ndef repl(stdin, stdout):\n    while True:\n        stdout.write(\"> \")\n        source = stdin.readline(LINE_BUFFER_LENGTH)\n\n        scanner = Scanner(source)\n        t = scanner.scan_token()\n        while t.type != TokenTypes.EOF and t.type != TokenTypes.ERROR:\n            print TokenTypeToName[t.type],\n            if t.type == TokenTypes.NUMBER:\n                print \"(%s)\" % scanner.get_token_string(t),\n            print\n            t = scanner.scan_token()\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n    try:\n        repl(stdin, stdout)\n    except:\n        pass\n    return 0\n\n\nWith our REPL hooked up we can now scan tokens from arbitrary input:\n\n$ ./scanner2\n> (3 *4) - -3\nLEFT_PAREN\nNUMBER (3)\nSTAR\nNUMBER (4)\nRIGHT_PAREN\nMINUS\nMINUS\nNUMBER (3)\n> C\n\n\nCompiling expressions\n\nReferences\n\n\nhttps://www.craftinginterpreters.com/compiling-expressions.html\n\nhttps://effbot.org/zone/simple-top-down-parsing.htm\n\n\nThe final piece is to turn this sequence of tokens into our low level \nbytecode instructions for the virtual machine to execute. Buckle up, \nwe are about to write us a compiler.\n\nOur compiler will take a single pass over the tokens using \nVaughan Pratt\u2019s \nparsing technique, and output a chunk of bytecode \u2013 if we do it\nright it will be compatible with our existing virtual machine.\n\nRemember the bytecode we defined above is really simple \u2013 by relying \non our stack we can transform a nested expression into a sequence of\nour bytecode operations.\n\nTo make this more concrete let's go through by hand translating an\nexpression into bytecode.\n\nOur source expression:\n\n(3 + 2) - (7 * 2)\n\n\nIf we were to make an abstract syntax tree we'd get something \nlike this:\n\n\n\nNow if we start at the first sub expression (3+2) we can clearly\nnote from the first open bracket that we must see a close bracket,\nand that the expression inside that bracket must be valid on its \nown. Not only that but regardless of the inside we know that the whole\nexpression still has to be valid. Let's focus on this first bracketed\nexpression, let our attention recurse into it so to speak.\n\nThis gives us a much easier problem \u2013 we just want to get our virtual\nmachine to compute 3 + 2. In this bytecode dialect we would load the \ntwo constants, and then add them with OP_ADD like so:  \n\nOP_CONSTANT  (00) '3.000000'\nOP_CONSTANT  (01) '2.000000'\nOP_ADD\n\n\nThe effect of our vm executing these three instructions is that sitting\npretty at the top of the stack is the result of the addition. Winning.\n\nJumping back out from our bracketed expression, our next token is MINUS,\nat this point we have a fair idea that it must be used in an infix position. \nIn fact whatever token followed the bracketed expression it must be a \nvalid infix operator, if not the expression is over or had a syntax error. \n\nAssuming the best from our user (naive), we handle MINUS the same way\nwe handled the first PLUS. We've already got the first operand on the\nstack, now we compile the right operand and then write out the bytecode\nfor OP_SUBTRACT.\n\nThe right operand is another simple three instructions:\n\nOP_CONSTANT  (02) '7.000000'\nOP_CONSTANT  (03) '2.000000'\nOP_MULTIPLY\n\n\nThen we finish our top level binary expression and write a OP_RETURN to\nreturn the value at the top of the stack as the execution's result. Our\nfinal hand compiled program is:\n\nOP_CONSTANT  (00) '3.000000'\nOP_CONSTANT  (01) '2.000000'\nOP_ADD\nOP_CONSTANT  (02) '7.000000'\nOP_CONSTANT  (03) '2.000000'\nOP_MULTIPLY\nOP_SUBTRACT\nOP_RETURN\n\n\nOk that wasn't so hard was it? Let's try make our code do that.\n\nWe define a parser object which will keep track of where we are, and\nwhether things have all gone horribly wrong:\n\nclass Parser(object):\n    def __init__(self):\n        self.had_error = False\n        self.panic_mode = False\n        self.current = None\n        self.previous = None\n\n\nThe compiler will also be a class, we'll need one of our Scanner instances\nto pull tokens from, and since the output is a bytecode Chunk let's go ahead\nand make one of those in our compiler initializer:\n\nclass Compiler(object):\n\n    def __init__(self, source):\n        self.parser = Parser()\n        self.scanner = Scanner(source)\n        self.chunk = Chunk()\n\n\nSince we have this (empty) chunk of bytecode we will make a helper method\nto add individual bytes. Every instruction will pass from our compiler into\nan executable program through this simple .\n\n    def emit_byte(self, byte):\n        self.current_chunk().write_chunk(byte)\n\n\nTo quote from Bob Nystrom on the Pratt parsing technique:\n\n\n  the implementation is a deceptively-simple handful of deeply intertwined code\n\n\nI don't actually think I can do justice to this section. Instead I suggest \nreading his treatment in \nPratt Parsers: Expression Parsing Made Easy\nwhich explains the magic behind the parsing component. Our only major difference is \ninstead of creating an AST we are going to directly emit bytecode for our VM.\n\nNow that I've absolved myself from taking responsibility in explaining this somewhat\ntricky concept, I'll discuss some of the code from \ncompiler.py, and walk through what happens \nfor a particular rule.\n\nI'll jump straight to the juicy bit the table of parse rules. We define a ParseRule\nfor each token, and each rule comprises:\n\n\nan optional handler for when the token is as a prefix (e.g. the minus in (-2)),\n\nan optional handler for whet the token is used infix (e.g. the slash in 2/47)\n\na precedence value (a number that determines what is of higher precedence)\n\n\nrules = [\n    ParseRule(None,              None,            Precedence.NONE),   # ERROR\n    ParseRule(None,              None,            Precedence.NONE),   # EOF\n    ParseRule(Compiler.grouping, None,            Precedence.CALL),   # LEFT_PAREN\n    ParseRule(None,              None,            Precedence.NONE),   # RIGHT_PAREN\n    ParseRule(Compiler.unary,    Compiler.binary, Precedence.TERM),   # MINUS\n    ParseRule(None,              Compiler.binary, Precedence.TERM),   # PLUS\n    ParseRule(None,              Compiler.binary, Precedence.FACTOR), # SLASH\n    ParseRule(None,              Compiler.binary, Precedence.FACTOR), # STAR\n    ParseRule(Compiler.number,   None,            Precedence.NONE),   # NUMBER\n]\n\n\nThese rules really are the magic of our compiler. When we get to a particular\ntoken such as MINUS we see if it is an infix operator and if so we've gone and\ngot its first operand ready. At all times we rely on the relative precedence; consuming \neverything with higher precedence than the operator we are currently evaluating.\n\nIn the expression:\n\n2 + 3 * 4\n\n\nThe * has higher precedence than the +, so 3 * 4 will be parsed together\nas the second operand to the first infix operator (the +) which follows\nthe BEDMAS \norder of operations I was taught at high school.\n\nTo encode these precedence values we make another Python object moonlighting\nas an enum:\n\nclass Precedence(object):\n    NONE = 0\n    DEFAULT = 1\n    TERM = 2        # + -\n    FACTOR = 3      # * /\n    UNARY = 4       # ! - +\n    CALL = 5        # ()\n    PRIMARY = 6\n\n\nWhat happens in our compiler when turning -2.0 into bytecode? Assume we've just \npulled the token MINUS from the scanner. Every expression has to start with some\ntype of prefix \u2013 whether that is:\n\n\na bracket group (, \n\na number 2, \n\nor a prefix unary operator -. \n\n\nKnowing that, our compiler assumes there is a prefix handler in the rule table \u2013 in\nthis case it points us at the unary handler.\n\n    def parse_precedence(self, precedence):\n        # parses any expression of a given precedence level or higher\n        self.advance()\n        prefix_rule = self._get_rule(self.parser.previous.type).prefix\n        prefix_rule(self)\n\n\n\n\nunary is called:\n\n    def unary(self):\n        op_type = self.parser.previous.type\n        # Compile the operand\n        self.parse_precedence(Precedence.UNARY)\n        # Emit the operator instruction\n        if op_type == TokenTypes.MINUS:\n            self.emit_byte(OpCode.OP_NEGATE)\n\n\nHere \u2013 before writing the OP_NEGATE opcode we recurse back into parse_precedence\nto ensure that whatever follows the MINUS token is compiled \u2013 provided it has \nhigher precedence than unary \u2013 e.g. a bracketed group. \nCrucially at run time this recursive call will ensure that the result is left \non top of our stack. Armed with this knowledge, the unary method just\nhas to emit a single byte with the OP_NEGATE opcode.\n\nTest compilation\n\nNow we can test our compiler by outputting disassembled bytecode\nof our user entered expressions. Create a new entry_point \ntargetcompiler:\n\nfrom rpython.rlib import rfile\nfrom compiler import Compiler\n\nLINE_BUFFER_LENGTH = 1024\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n\n    try:\n        while True:\n            stdout.write(\"> \")\n            source = stdin.readline(LINE_BUFFER_LENGTH)\n            compiler = Compiler(source, debugging=True)\n            compiler.compile()\n    except:\n        pass\n    return 0\n\n\nTranslate it and test it out:\n\n$ ./compiler1 \n> (2/4 + 1/2)\n== code ==\n\n0000 OP_CONSTANT  (00) '2.000000'\n0002 OP_CONSTANT  (01) '4.000000'\n0004 OP_DIVIDE    \n0005 OP_CONSTANT  (02) '1.000000'\n0007 OP_CONSTANT  (00) '2.000000'\n0009 OP_DIVIDE    \n0010 OP_ADD       \n0011 OP_RETURN\n\n\nNow if you've made it this far you'll be eager to finally connect everything\ntogether by executing this bytecode with the virtual machine.\n\nEnd to end\n\nAll the pieces slot together rather easily at this point, create a new \nfile targetcalc.py and define our \nentry point:\n\nfrom rpython.rlib import rfile\nfrom compiler import Compiler\nfrom vm import VM\n\nLINE_BUFFER_LENGTH = 4096\n\n\ndef entry_point(argv):\n    stdin, stdout, stderr = rfile.create_stdio()\n    vm = VM()\n    try:\n        while True:\n            stdout.write(\"> \")\n            source = stdin.readline(LINE_BUFFER_LENGTH)\n            if source:\n                compiler = Compiler(source, debugging=False)\n                compiler.compile()\n                vm.interpret_chunk(compiler.chunk)\n    except:\n        pass\n    return 0\n\n\ndef target(driver, *args):\n    driver.exe_name = \"calc\"\n    return entry_point, None\n\n\n\n\nLet's try catch it out with a double negative:\n\n$ ./calc \n> 2--3\n== VM TRACE ==\n          []\n0000 OP_CONSTANT  (00) '2.000000'\n          [ 2.000000 ]\n0002 OP_CONSTANT  (01) '3.000000'\n          [ 2.000000 ] [ 3.000000 ]\n0004 OP_NEGATE    \n          [ 2.000000 ] [ -3.000000 ]\n0005 OP_SUBTRACT  \n          [ 5.000000 ]\n0006 OP_RETURN    \n5.000000\n\n\nOk well let's evaluate the first 50 terms of the \nNilakantha Series:\n\n$ ./calc\n> 3 + 4 * ((1/(2 * 3 * 4)) + (1/(4 * 5 * 6)) - (1/(6 * 7 * 8)) + (1/(8 * 9 * 10)) - (1/(10 * 11 * 12)) + (1/(12 * 13 * 14)) - (1/(14 * 15 * 16)) + (1/(16 * 17 * 18)) - (1/(18 * 19 * 20)) + (1/(20 * 21 * 22)) - (1/(22 * 23 * 24)) + (1/(24 * 25 * 26)) - (1/(26 * 27 * 28)) + (1/(28 * 29 * 30)) - (1/(30 * 31 * 32)) + (1/(32 * 33 * 34)) - (1/(34 * 35 * 36)) + (1/(36 * 37 * 38)) - (1/(38 * 39 * 40)) + (1/(40 * 41 * 42)) - (1/(42 * 43 * 44)) + (1/(44 * 45 * 46)) - (1/(46 * 47 * 48)) + (1/(48 * 49 * 50)) - (1/(50 * 51 * 52)) + (1/(52 * 53 * 54)) - (1/(54 * 55 * 56)) + (1/(56 * 57 * 58)) - (1/(58 * 59 * 60)) + (1/(60 * 61 * 62)) - (1/(62 * 63 * 64)) + (1/(64 * 65 * 66)) - (1/(66 * 67 * 68)) + (1/(68 * 69 * 70)) - (1/(70 * 71 * 72)) + (1/(72 * 73 * 74)) - (1/(74 * 75 * 76)) + (1/(76 * 77 * 78)) - (1/(78 * 79 * 80)) + (1/(80 * 81 * 82)) - (1/(82 * 83 * 84)) + (1/(84 * 85 * 86)) - (1/(86 * 87 * 88)) + (1/(88 * 89 * 90)) - (1/(90 * 91 * 92)) + (1/(92 * 93 * 94)) - (1/(94 * 95 * 96)) + (1/(96 * 97 * 98)) - (1/(98 * 99 * 100)) + (1/(100 * 101 * 102)))\n\n== VM TRACE ==\n          []\n0000 OP_CONSTANT  (00) '3.000000'\n          [ 3.000000 ]\n0002 OP_CONSTANT  (01) '4.000000'\n...SNIP...\n0598 OP_CONSTANT  (101) '102.000000'\n          [ 3.000000 ] [ 4.000000 ] [ 0.047935 ] [ 1.000000 ] [ 10100.000000 ] [ 102.000000 ]\n0600 OP_MULTIPLY  \n          [ 3.000000 ] [ 4.000000 ] [ 0.047935 ] [ 1.000000 ] [ 1030200.000000 ]\n0601 OP_DIVIDE    \n          [ 3.000000 ] [ 4.000000 ] [ 0.047935 ] [ 0.000001 ]\n0602 OP_ADD       \n          [ 3.000000 ] [ 4.000000 ] [ 0.047936 ]\n0603 OP_MULTIPLY  \n          [ 3.000000 ] [ 0.191743 ]\n0604 OP_ADD       \n          [ 3.191743 ]\n0605 OP_RETURN    \n3.191743\n\n\nWe just executed 605 virtual machine instructions to compute pi to 1dp!\n\nThis brings us to the end of this tutorial. To recap we've walked through the whole \ncompilation process: from the user providing an expression string on the REPL, scanning\nthe source string into tokens, parsing the tokens while accounting for relative \nprecedence via a Pratt parser, generating bytecode, and finally executing the bytecode \non our own VM. RPython translated what we wrote into C and compiled it, meaning\nour resulting calc REPL is really fast.\n\n\n  \u201cThe world is a thing of utter inordinate complexity and richness and strangeness that is absolutely awesome.\u201d\n  \n  \u2015 Douglas Adams \n\n\nMany thanks to Bob Nystrom for writing the book that inspired this post, and thanks to \nCarl Friedrich and Matt Halverson for reviewing.\n\n\u2015 Brian (@thorneynzb)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/11/guest-post-implementing-calculator-repl-6271483514675006846.html"
    },
    {
      "title": "Inside cpyext: Why emulating CPython C API is so Hard",
      "text": "cpyext is PyPy's subsystem which provides a compatibility\nlayer to compile and run CPython C extensions inside PyPy.  Often people ask\nwhy a particular C extension doesn't work or is very slow on PyPy.\nUsually it is hard to answer without going into technical details. The goal of\nthis blog post is to explain some of these technical details, so that we can\nsimply link here instead of explaining again and again :).\nFrom a 10.000 foot view, cpyext is PyPy's version of \"Python.h\". Every time\nyou compile an extension which uses that header file, you are using cpyext.\nThis includes extension explicitly written in C (such as numpy) and\nextensions which are generated from other compilers/preprocessors\n(e.g. Cython).\nAt the time of writing, the current status is that most C extensions \"just\nwork\". Generally speaking, you can simply pip install them,\nprovided they use the public, official C API instead of poking at private\nimplementation details.  However, the performance of cpyext is generally\npoor. A Python program which makes heavy use of cpyext extensions\nis likely to be slower on PyPy than on CPython.\nNote: in this blog post we are talking about Python 2.7 because it is still\nthe default version of PyPy: however most of the implementation of cpyext is\nshared with PyPy3, so everything applies to that as well.\n\n\nC API Overview\nIn CPython, which is written in C, Python objects are represented as PyObject*,\ni.e. (mostly) opaque pointers to some common \"base struct\".\nCPython uses a very simple memory management scheme: when you create an\nobject, you allocate a block of memory of the appropriate size on the heap.\nDepending on the details, you might end up calling different allocators, but\nfor the sake of simplicity, you can think that this ends up being a call to\nmalloc(). The resulting block of memory is initialized and casted to to\nPyObject*: this address never changes during the object lifetime, and the\nC code can freely pass it around, store it inside containers, retrieve it\nlater, etc.\nMemory is managed using reference counting. When you create a new reference to\nan object, or you discard a reference you own, you have to increment or\ndecrement the reference counter accordingly. When the reference counter goes to\n0, it means that the object is no longer used and can safely be\ndestroyed. Again, we can simplify and say that this results in a call to\nfree(), which finally releases the memory which was allocated by malloc().\nGenerally speaking, the only way to operate on a PyObject* is to call the\nappropriate API functions. For example, to convert a given PyObject* to a C\ninteger, you can use PyInt_AsLong(); to add two objects together, you can\ncall PyNumber_Add().\nInternally, PyPy uses a similar approach. All Python objects are subclasses of\nthe RPython W_Root class, and they are operated by calling methods on the\nspace singleton, which represents the interpreter.\nAt first, it looks very easy to write a compatibility layer: just make\nPyObject* an alias for W_Root, and write simple RPython functions\n(which will be translated to C by the RPython compiler) which call the\nspace accordingly:\ndef PyInt_AsLong(space, o):\n    return space.int_w(o)\n\ndef PyNumber_Add(space, o1, o2):\n    return space.add(o1, o2)\n\nActually, the code above is not too far from the real\nimplementation. However, there are tons of gory details which make it much\nharder than it looks, and much slower unless you pay a lot of attention\nto performance.\n\n\nThe PyPy GC\nTo understand some of cpyext challenges, you need to have at least a rough\nidea of how the PyPy GC works.\nContrarily to the popular belief, the \"Garbage Collector\" is not only about\ncollecting garbage: instead, it is generally responsible for all memory\nmanagement, including allocation and deallocation.\nWhereas CPython uses a combination of malloc/free/refcounting to manage\nmemory, the PyPy GC uses a completely different approach. It is designed\nassuming that a dynamic language like Python behaves the following way:\n\n\nYou create, either directly or indirectly, lots of objects.\nMost of these objects are temporary and very short-lived. Think e.g. of\ndoing a + b + c: you need to allocate an object to hold the temporary\nresult of a + b, then it dies very quickly because you no longer need it\nwhen you do the final + c part.\nOnly small fraction of the objects survive and stay around for a while.\n\n\nSo, the strategy is: make allocation as fast as possible; make deallocation of\nshort-lived objects as fast as possible; find a way to handle the remaining\nsmall set of objects which actually survive long enough to be important.\nThis is done using a Generational GC: the basic idea is the following:\n\n\nWe have a nursery, where we allocate \"young objects\" very quickly.\nWhen the nursery is full, we start what we call a \"minor collection\".\nWe do a quick scan to determine the small set of objects which survived so\nfar\nWe move these objects out of the nursery, and we place them in the\narea of memory which contains the \"old objects\". Since the address of the\nobjects changes, we fix all the references to them accordingly.\n\n\n\n\nnow the nursery contains only objects which \"died young\". We can\ndiscard all of them very quickly, reset the nursery, and use the same area\nof memory to allocate new objects from now.\n\n\nIn practice, this scheme works very well and it is one of the reasons why PyPy\nis much faster than CPython.  However, careful readers have surely noticed\nthat this is a problem for cpyext. On one hand, we have PyPy objects which\ncan potentially move and change their underlying memory address; on the other\nhand, we need a way to represent them as fixed-address PyObject* when we\npass them to C extensions.  We surely need a way to handle that.\n\n\nPyObject* in PyPy\nAnother challenge is that sometimes, PyObject* structs are not completely\nopaque: there are parts of the public API which expose to the user specific\nfields of some concrete C struct. For example the definition of PyTypeObject\nwhich exposes many of the tp_* slots to the user.\nSince the low-level layout of PyPy W_Root objects is completely different\nthan the one used by CPython, we cannot simply pass RPython objects to C; we\nneed a way to handle the difference.\nSo, we have two issues so far: objects can move, and incompatible\nlow-level layouts. cpyext solves both by decoupling the RPython and the C\nrepresentations. We have two \"views\" of the same entity, depending on whether\nwe are in the PyPy world (the movable W_Root subclass) or in the C world\n(the non-movable PyObject*).\nPyObject* are created lazily, only when they are actually needed. The\nvast majority of PyPy objects are never passed to any C extension, so we don't\npay any penalty in that case. However, the first time we pass a W_Root to\nC, we allocate and initialize its PyObject* counterpart.\nThe same idea applies also to objects which are created in C, e.g. by calling\nPyObject_New(). At first, only the PyObject* exists and it is\nexclusively managed by reference counting. As soon as we pass it to the PyPy\nworld (e.g. as a return value of a function call), we create its W_Root\ncounterpart, which is managed by the GC as usual.\nHere we start to see why calling cpyext modules is more costly in PyPy than in\nCPython. We need to pay some penalty for all the conversions between\nW_Root and PyObject*.\nMoreover, the first time we pass a W_Root to C we also need to allocate\nthe memory for the PyObject* using a slowish \"CPython-style\" memory\nallocator. In practice, for all the objects which are passed to C we pay more\nor less the same costs as CPython, thus effectively \"undoing\" the speedup\nguaranteed by PyPy's Generational GC under normal circumstances.\n\n\nMaintaining the link between W_Root and PyObject*\nWe now need a way to convert between W_Root and PyObject* and\nvice-versa; also, we need to to ensure that the lifetime of the two entities\nare in sync. In particular:\n\n\nas long as the W_Root is kept alive by the GC, we want the\nPyObject* to live even if its refcount drops to 0;\nas long as the PyObject* has a refcount greater than 0, we want to\nmake sure that the GC does not collect the W_Root.\n\n\nThe PyObject* \u21e8 W_Root link is maintained by the special field\nob_pypy_link which is added to all PyObject*. On a 64 bit machine this\nmeans that all PyObject* have 8 bytes of overhead, but then the\nconversion is very quick, just reading the field.\nFor the other direction, we generally don't want to do the same: the\nassumption is that the vast majority of W_Root objects will never be\npassed to C, and adding an overhead of 8 bytes to all of them is a\nwaste. Instead, in the general case the link is maintained by using a\ndictionary, where W_Root are the keys and PyObject* the values.\nHowever, for a few selected W_Root subclasses we do maintain a\ndirect link using the special _cpy_ref field to improve performance. In\nparticular, we use it for W_TypeObject (which is big anyway, so a 8 bytes\noverhead is negligible) and W_NoneObject. None is passed around very\noften, so we want to ensure that the conversion to PyObject* is very\nfast. Moreover it's a singleton, so the 8 bytes overhead is negligible as\nwell.\nThis means that in theory, passing an arbitrary Python object to C is\npotentially costly, because it involves doing a dictionary lookup.  We assume\nthat this cost will eventually show up in the profiler: however, at the time\nof writing there are other parts of cpyext which are even more costly (as we\nwill show later), so the cost of the dict lookup is never evident in the\nprofiler.\n\n\nCrossing the border between RPython and C\nThere are two other things we need to care about whenever we cross the border\nbetween RPython and C, and vice-versa: exception handling and the GIL.\nIn the C API, exceptions are raised by calling PyErr_SetString() (or one of\nmany other functions which have a similar effect), which basically works by\ncreating an exception value and storing it in some global variable. The\nfunction then signals that an exception has occurred by returning an error value,\nusually NULL.\nOn the other hand, in the PyPy interpreter, exceptions are propagated by raising the\nRPython-level OperationError exception, which wraps the actual app-level\nexception values. To harmonize the two worlds, whenever we return from C to\nRPython, we need to check whether a C API exception was raised and if so turn it\ninto an OperationError.\nWe won't dig into details of how the GIL is handled in cpyext.\nFor the purpose of this post, it is enough to know that whenever we enter\nC land, we store the current thread id into a global variable which is\naccessible also from C; conversely, whenever we go back from RPython to C, we\nrestore this value to 0.\nSimilarly, we need to do the inverse operations whenever you need to cross the\nborder between C and RPython, e.g. by calling a Python callback from C code.\nAll this complexity is automatically handled by the RPython function\ngeneric_cpy_call. If you look at the code you see that it takes care of 4\nthings:\n\n\nHandling the GIL as explained above.\nHandling exceptions, if they are raised.\nConverting arguments from W_Root to PyObject*.\nConverting the return value from PyObject* to W_Root.\n\n\nSo, we can see that calling C from RPython introduce some overhead.\nCan we measure it?\nAssuming that the conversion between W_Root and PyObject* has a\nreasonable cost (as explained by the previous section), the overhead\nintroduced by a single border-cross is still acceptable, especially if the\ncallee is doing some non-negligible amount of work.\nHowever this is not always the case. There are basically three problems that\nmake (or used to make) cpyext super slow:\n\n\nPaying the border-crossing cost for trivial operations which are called\nvery often, such as Py_INCREF.\nCrossing the border back and forth many times, even if it's not strictly\nneeded.\nPaying an excessive cost for argument and return value conversions.\n\n\nThe next sections explain in more detail each of these problems.\n\n\nAvoiding unnecessary roundtrips\nPrior to the 2017 Cape Town Sprint, cpyext was horribly slow, and we were\nwell aware of it: the main reason was that we never really paid too much\nattention to performance. As explained in the blog post, emulating all the\nCPython quirks is basically a nightmare, so better to concentrate on\ncorrectness first.\nHowever, we didn't really know why it was so slow. We had theories and\nassumptions, usually pointing at the cost of conversions between W_Root\nand PyObject*, but we never actually measured it.\nSo, we decided to write a set of cpyext microbenchmarks to measure the\nperformance of various operations.  The result was somewhat surprising: the\ntheory suggests that when you do a cpyext C call, you should pay the\nborder-crossing costs only once, but what the profiler told us was that we\nwere paying the cost of generic_cpy_call several times more than what we expected.\nAfter a bit of investigation, we discovered this was ultimately caused by our\n\"correctness-first\" approach. For simplicity of development and testing, when\nwe started cpyext we wrote everything in RPython: thus, every single API call\nmade from C (like the omnipresent PyArg_ParseTuple(), PyInt_AsLong(), etc.)\nhad to cross back the C-to-RPython border. This was especially daunting for\nvery simple and frequent operations like Py_INCREF and Py_DECREF,\nwhich CPython implements as a single assembly instruction!\nAnother source of slow down was the implementation of PyTypeObject slots.\nAt the C level, these are function pointers which the interpreter calls to do\ncertain operations, e.g. tp_new to allocate a new instance of that type.\nAs usual, we have some magic to implement slots in RPython; in particular,\n_make_wrapper does the opposite of generic_cpy_call: it takes a\nRPython function and wraps it into a C function which can be safely called\nfrom C, handling the GIL, exceptions and argument conversions automatically.\nThis was very handy during the development of cpyext, but it might result in\nsome bad nonsense; consider what happens when you call the following C\nfunction:\nstatic PyObject* foo(PyObject* self, PyObject* args)\n{\n    PyObject* result = PyInt_FromLong(1234);\n    return result;\n}\n\n\nyou are in RPython and do a cpyext call to foo: RPython-to-C;\nfoo calls PyInt_FromLong(1234), which is implemented in RPython:\nC-to-RPython;\nthe implementation of PyInt_FromLong indirectly calls\nPyIntType.tp_new, which is a C function pointer: RPython-to-C;\nhowever, tp_new is just a wrapper around an RPython function, created\nby _make_wrapper: C-to-RPython;\nfinally, we create our RPython W_IntObject(1234); at some point\nduring the RPython-to-C crossing, its PyObject* equivalent is\ncreated;\nafter many layers of wrappers, we are again in foo: after we do\nreturn result, during the C-to-RPython step we convert it from\nPyObject* to W_IntObject(1234).\n\nPhew! After we realized this, it was not so surprising that cpyext was very\nslow :). And this was a simplified example, since we are not passing a\nPyObject* to the API call. When we do, we need to convert it back and\nforth at every step.  Actually, I am not even sure that what I described was\nthe exact sequence of steps which used to happen, but you get the general\nidea.\nThe solution is simple: rewrite as much as we can in C instead of RPython,\nto avoid unnecessary roundtrips. This was the topic of most of the Cape Town\nsprint and resulted in the cpyext-avoid-roundtrip branch, which was\neventually merged.\nOf course, it is not possible to move everything to C: there are still\noperations which need to be implemented in RPython. For example, think of\nPyList_Append: the logic to append an item to a list is complex and\ninvolves list strategies, so we cannot replicate it in C.  However, we\ndiscovered that a large subset of the C API can benefit from this.\nMoreover, the C API is huge. While we invented this new way of writing\ncpyext code, we still need to\nconvert many of the functions to the new paradigm.  Sometimes the rewrite is\nnot automatic\nor straighforward. cpyext is a delicate piece of software, so it happens often\nthat we make a mistake and end up staring at a segfault in gdb.\nHowever, the most important takeaway is that the performance improvements we got\nfrom this optimization are impressive, as we will detail later.\n\n\nConversion costs\nThe other potential big source of slowdown is the conversion of arguments\nbetween W_Root and PyObject*.\nAs explained earlier, the first time you pass a W_Root to C, you need to\nallocate its PyObject* counterpart. Suppose you have a foo function\ndefined in C, which takes a single int argument:\nfor i in range(N):\n    foo(i)\n\nTo run this code, you need to create a different PyObject* for each value\nof i: if implemented naively, it means calling N times malloc()\nand free(), which kills performance.\nCPython has the very same problem, which is solved by using a free list to\nallocate ints. So, what we did was to simply steal the code from CPython\nand do the exact same thing. This was also done in the\ncpyext-avoid-roundtrip branch, and the benchmarks show that it worked\nperfectly.\nEvery type which is converted often to PyObject* must have a very fast\nallocator. At the moment of writing, PyPy uses free lists only for ints and\ntuples: one of the next steps on our TODO list is certainly to use this\ntechnique with more types, like float.\nConversely, we also need to optimize the conversion from PyObject* to\nW_Root: this happens when an object is originally allocated in C and\nreturned to Python. Consider for example the following code:\nimport numpy as np\nmyarray = np.random.random(N)\nfor i in range(len(arr)):\n    myarray[i]\n\nAt every iteration, we get an item out of the array: the return type is a an\ninstance of numpy.float64 (a numpy scalar), i.e. a PyObject'*: this is\nsomething which is implemented by numpy entirely in C, so completely\nopaque to cpyext. We don't have any control on how it is allocated,\nmanaged, etc., and we can assume that allocation costs are the same as on\nCPython.\nAs soon as we return these PyObject* to Python, we need to allocate\ntheir W_Root equivalent. If you do it in a small loop like in the example\nabove, you end up allocating all these W_Root inside the nursery, which is\na good thing since allocation is super fast (see the section above about the\nPyPy GC).\nHowever, we also need to keep track of the W_Root to PyObject* link.\nCurrently, we do this by putting all of them in a dictionary, but it is very\ninefficient, especially because most of these objects die young and thus it\nis wasted work to do that for them.  Currently, this is one of the biggest\nunresolved problem in cpyext, and it is what causes the two microbenchmarks\nallocate_int and allocate_tuple to be very slow.\nWe are well aware of the problem, and we have a plan for how to fix it. The\nexplanation is too technical for the scope of this blog post as it requires a\ndeep knowledge of the GC internals to be understood, but the details are\nhere.\n\n\nC API quirks\nFinally, there is another source of slowdown which is beyond our control. Some\nparts of the CPython C API are badly designed and expose some of the\nimplementation details of CPython.\nThe major example is reference counting. The Py_INCREF / Py_DECREF API\nis designed in such a way which forces other implementation to emulate\nrefcounting even in presence of other GC management schemes, as explained\nabove.\nAnother example is borrowed references. There are API functions which do\nnot incref an object before returning it, e.g. PyList_GetItem().  This is\ndone for performance reasons because we can avoid a whole incref/decref pair,\nif the caller needs to handle the returned item only temporarily: the item is\nkept alive because it is in the list anyway.\nFor PyPy, this is a challenge: thanks to list strategies, lists are often\nrepresented in a compact way. For example, a list containing only integers is\nstored as a C array of long.  How to implement PyList_GetItem? We\ncannot simply create a PyObject* on the fly, because the caller will never\ndecref it and it will result in a memory leak.\nThe current solution is very inefficient. The first time we do a\nPyList_GetItem, we convert the whole list to a list of\nPyObject*. This is bad in two ways: the first is that we potentially pay a\nlot of unneeded conversion cost in case we will never access the other items\nof the list. The second is that by doing that we lose all the performance\nbenefit granted by the original list strategy, making it slower for the\nrest of the pure-python code which will manipulate the list later.\nPyList_GetItem is an example of a bad API because it assumes that the list\nis implemented as an array of PyObject*: after all, in order to return a\nborrowed reference, we need a reference to borrow, don't we?\nFortunately, (some) CPython developers are aware of these problems, and there\nis an ongoing project to design a better C API which aims to fix exactly\nthis kind of problem.\nNonetheless, in the meantime we still need to implement the current\nhalf-broken APIs. There is no easy solution for that, and it is likely that\nwe will always need to pay some performance penalty in order to implement them\ncorrectly.\nHowever, what we could potentially do is to provide alternative functions\nwhich do the same job but are more PyPy friendly: for example, we could think\nof implementing PyList_GetItemNonBorrowed or something like that: then, C\nextensions could choose to use it (possibly hidden inside some macro and\n#ifdef) if they want to be fast on PyPy.\n\n\nCurrent performance\nDuring the whole blog post we claimed cpyext is slow. How\nslow it is, exactly?\nWe decided to concentrate on microbenchmarks for now. It should be evident\nby now there are simply too many issues which can slow down a cpyext\nprogram, and microbenchmarks help us to concentrate on one (or few) at a\ntime.\nThe microbenchmarks measure very simple things, like calling functions and\nmethods with the various calling conventions (no arguments, one arguments,\nmultiple arguments); passing various types as arguments (to measure conversion\ncosts); allocating objects from C, and so on.\nHere are the results from the old PyPy 5.8 relative and normalized to CPython\n2.7, the lower the better:\n\n\n\n\n\n\n\n\n\n\n\nPyPy was horribly slow everywhere, ranging from 2.5x to 10x slower. It is\nparticularly interesting to compare simple.noargs, which measures the cost\nof calling an empty function with no arguments, and simple.onearg(i),\nwhich measures the cost calling an empty function passing an integer argument:\nthe latter is ~2x slower than the former, indicating that the conversion cost\nof integers is huge.\nPyPy 5.8 was the last release before the famous Cape Town sprint, when we\nstarted to look at cpyext performance seriously. Here are the performance data for\nPyPy 6.0, the latest release at the time of writing:\n\n\n\n\nThe results are amazing! PyPy is now massively faster than before, and for\nmost benchmarks it is even faster than CPython: yes, you read it correctly:\nPyPy is faster than CPython at doing CPython's job, even considering all the\nextra work it has to do to emulate the C API.  This happens thanks to the JIT,\nwhich produces speedups high enough to counterbalance the slowdown caused by\ncpyext.\nThere are two microbenchmarks which are still slower though: allocate_int\nand allocate_tuple, for the reasons explained in the section about\nConversion costs.\n\n\nNext steps\nDespite the spectacular results we got so far, cpyext is still slow enough to\nkill performance in most real-world code which uses C extensions extensively\n(e.g., the omnipresent numpy).\nOur current approach is something along these lines:\n\n\nrun a real-world small benchmark which exercises cpyext\nmeasure and find the major bottleneck\nwrite a corresponding microbenchmark\noptimize it\nrepeat\n\n\nOn one hand, this is a daunting task because the C API is huge and we need to\ntackle functions one by one.  On the other hand, not all the functions are\nequally important, and is is enough to optimize a relatively small subset to\nimprove many different use cases.\nWhere a year ago we announced we have a working answer to run c-extension in\nPyPy, we now have a clear picture of what are the performance bottlenecks, and\nwe have developed some technical solutions to fix them. It is \"only\" a matter\nof tackling them, one by one.  It is worth noting that most of the work was\ndone during two sprints, for a total 2-3 person-months of work.\nWe think this work is important for the Python ecosystem. PyPy has established\na baseline for performance in pure python code, providing an answer for the\n\"Python is slow\" detractors. The techniques used to make cpyext performant\nwill let PyPy become an alternative for people who mix C extensions with\nPython, which, it turns out, is just about everyone, in particular those using\nthe various scientific libraries. Today, many developers are forced to seek\nperformance by converting code from Python to a lower language. We feel there\nis no reason to do this, but in order to prove it we must be able to run both\ntheir python and their C extensions performantly, then we can begin to educate\nthem how to write JIT-friendly code in the first place.\nWe envision a future in which you can run arbitrary Python programs on PyPy,\nwith the JIT speeding up the pure Python parts and the C parts running as fast\nas today: the best of both worlds!",
      "tags": "cpyext,profiling,speed",
      "url": "https://www.pypy.org/posts/2018/09/inside-cpyext-why-emulating-cpython-c-8083064623681286567.html"
    },
    {
      "title": "The First 15 Years of PyPy \u2014 a Personal Retrospective",
      "text": "A few weeks ago I (=Carl Friedrich Bolz-Tereick) gave a keynote at ICOOOLPS in\nAmsterdam with the above title. I was very happy to have been given that\nopportunity, since a number of our papers have been published at ICOOOLPS,\nincluding the very first one I published when I'd just started my PhD. I decided\nto turn the talk manuscript into a (longish) blog post, to make it available to a wider audience.\nNote that this blog post describes my personal recollections and research, it is\nthus necessarily incomplete and coloured by my own experiences.\nPyPy has turned 15 years old this year, so I decided that that's a good reason\nto dig into and talk about the history of the project so far. I'm going to do\nthat using the lens of how performance developed over time, which is from\nsomething like 2000x slower than CPython, to roughly 7x faster. In this post\nI am going to present the history of the project, and also talk about some\nlessons that we learned.\nThe post does not make too many assumptions about any prior knowledge of what\nPyPy is, so if this is your first interaction with it, welcome! I have tried to\nsprinkle links to earlier blog posts and papers into the writing, in case you\nwant to dive deeper into some of the topics.\nAs a disclaimer, in this post I am going to mostly focus on ideas, and not\nexplain who had or implemented them. A huge amount of people contributed to the\ndesign, the implementation, the funding and the organization of PyPy over the\nyears, and it would be impossible to do them all justice.\n\nContents\n\n2003: Starting the Project\n2003: Implementing the Interpreter\nEarly organizational ideas\n2004-2007: EU-Funding\n2005: Bootstrapping PyPy\nRPython's Modularity Problems\n2006: The Meta-JIT\nThe First JIT Generator\nPromote\nVirtuals\nJIT Status 2007\n2007: RSqueak and other languages\n2008-2009: Four More JIT Generators\n2009: Meta-Tracing\nWhy did we Abandon Partial Evaluation?\n2009-2011: The PyJIT Eurostars Project\nTracing JIT improvements\n2010: speed.pypy.org\nContinuous Integration\n2010: Implementing Python Objects with Maps\n2011: Container Storage Strategies\nDeep Changes in the Runtime are Necessary\nJIT Status 2011\n2012-2017: Engineering and Incremental Progress\nCPyExt\nPython 3\nIncentives of OSS compared to Academia\nMeta-Tracing really works!\nAcknowledgements\n\n\n\n2003: Starting the Project\nOn the technical level PyPy is a Python interpreter written in Python, which is\nwhere the name comes from. It also has an automatically generated JIT compiler,\nbut I'm going to introduce that gradually over the rest of the blog post, so\nlet's not worry about it too much yet. On the social level PyPy is an\ninteresting mixture of a open source project, that sometimes had research done\nin it.\nThe project got started in late 2002 and early 2003. To set the stage, at that\npoint Python was a significantly less popular language than it is today. Python\n2.2 was the version at the time, Python didn't even have a bool type yet.\nIn fall 2002 the PyPy project was started by a number of Python programmers on a\nmailing list who said\nsomething like (I am exaggerating somewhat) \"Python is the greatest most\nwonderful most perfect language ever, we should use it for absolutely\neverything. Well, what aren't we using it for? The Python virtual machine itself\nis written in C, that's bad. Let's start a project to fix that.\"\nOriginally that project was called \"minimal python\", or \"ptn\", later gradually\nrenamed to PyPy. Here's the mailing list post to announce the project more\nformally:\nMinimal Python Discussion, Coding and Sprint\n--------------------------------------------\n\nWe announce a mailinglist dedicated to developing\na \"Minimal Python\" version.  Minimal means that\nwe want to have a very small C-core and as much\nas possible (re)implemented in python itself.  This\nincludes (parts of) the VM-Code.\nWhy would that kind of project be useful? Originally it wasn't necessarily meant\nto be useful as a real implementation at all, it was more meant as a kind of\nexecutable explanation of how Python works, free of the low level details of\nCPython. But pretty soon there were then also plans for how the virtual machine\n(VM) could be bootstrapped to be runnable without an existing Python\nimplementation, but I'll get to that further down.\n\n\n\n\n2003: Implementing the Interpreter\nIn early 2003 a group of Python people met in Hildesheim (Germany) for the first\nof many week long development sprints, organized by Holger Krekel. During that\nweek a group of people showed up and started working on the core interpreter.\nIn May 2003 a second sprint was organized by Laura Creighton and Jacob Hal\u00e9n in\nGothenburg (Sweden). And already at that sprint enough of the Python bytecodes\nand data structures were implemented to make it possible to run a program that\ncomputed how much money everybody had to pay for the food bills of the week. And\neverybody who's tried that for a large group of people knows that that\u2019s an\namazingly complex mathematical problem.\nIn the next two years, the project continued as a open source project with\nvarious contributors working on it in their free time, and meeting for the\noccasional sprint. In that time, the rest of the core interpreter and the core\ndata types were implemented.\nThere's not going to be any other code in this post, but to give a bit of a\nflavor of what the Python interpreter at that time looked like, here's the\nimplementation of the DUP_TOP bytecode after these first sprints. As you can\nsee, it's in Python, obviously, and it has high level constructs such as method\ncalls to do the stack manipulations:\ndef DUP_TOP(f):\n    w_1 = f.valuestack.top()\n    f.valuestack.push(w_1)\nHere's the early code for integer addition:\ndef int_int_add(space, w_int1, w_int2):\n    x = w_int1.intval\n    y = w_int2.intval\n    try:\n        z = x + y\n    except OverflowError:\n        raise FailedToImplement(space.w_OverflowError,\n                                space.wrap(\"integer addition\"))\n    return W_IntObject(space, z)\n(the current implementations look slightly but not fundamentally different.)\n\n\n\n\nEarly organizational ideas\nSome of the early organizational ideas of the project were as follows. Since the\nproject was started on a sprint and people really liked that style of working\nPyPy continued to be developed on various subsequent sprints.\nFrom early on there was a very heavy emphasis on testing. All the parts of the\ninterpreter that were implemented had a very careful set of unit tests to make\nsure that they worked correctly. From early on, there was a continuous\nintegration infrastructure, which grew over time (nowadays it is very natural\nfor people to have automated tests, and the concept of green/red builds: but\nembracing this workflow in the early 2000s was not really mainstream yet, and\nit is probably one of the reasons behind PyPy's success).\nAt the sprints there was also an emphasis on doing pair programming to make\nsure that everybody understood the codebase\nequally. There was also a heavy emphasis on writing good code and on regularly\ndoing refactorings to make sure that the codebase remained nice, clean and\nunderstandable. Those ideas followed from the early thoughts that PyPy would be\na sort of readable explanation of the language.\nThere was also a pretty fundamental design decision made at the time. That was\nthat the project should stay out of language design completely. Instead it would\nfollow CPython's lead and behave exactly like that implementation in all cases.\nThe project therefore committed to being almost quirk-to-quirk compatible and to\nimplement even the more obscure (and partially unnecessary) corner cases of\nCPython.\nAll of these principles continue pretty much still today (There are a few places\nwhere we had to deviate from being completely compatible, they are documented\nhere).\n\n\n\n\n2004-2007: EU-Funding\nWhile all this coding was going on it became clear pretty soon that the goals\nthat various participants had for the project would be very hard to achieve with\njust open source volunteers working on the project in their spare time.\nParticularly also the sprints became expensive given that those were just\nvolunteers doing this as a kind of weird hobby. Therefore a couple of people of\nthe project got together to apply for an EU grant in the framework programme 6\nto solve these money problems. In mid-2004 that application proved to be\nsuccessful.\u00a0And so the project got a grant of a 1.3 million Euro for\ntwo years to be able to employ some of the core developers and to make it\npossible for them work on the project full time. The EU grant went to seven\nsmall-to-medium companies and Uni D\u00fcsseldorf. The budget also contained money to\nfund sprints, both for the employed core devs as well as other open source\ncontributors.\n\nThe EU project started in December 2004 and that was a fairly heavy change in\npace for the project. Suddenly a lot of people were working full time on it, and\nthe pace and the pressure picked up quite a lot. Originally it had been a\nleisurely project people worked on for fun. But afterwards people discovered\nthat doing this kind of work full time becomes slightly less fun, particularly\nalso if you have to fulfill the ambitious technical goals that the EU proposal\ncontained. And the proposal indeed contained a bit everything to increase its\nchance of acceptance, such as aspect oriented programming, semantic web, logic\nprogramming, constraint programming, and so on. Unfortunately it\nturned out that those things then have to be implemented, which can be called\nthe first thing we learned: if you promise something to the EU, you'll have to\nactually go do it (After the funding ended, a lot of these features were\nactually removed from the project again, at a cleanup sprint).\n\n\n\n\n2005: Bootstrapping PyPy\nSo what were the actually useful things done as part of the EU project?\nOne of the most important goals that the EU project was meant to solve was the\nquestion of how to turn PyPy into an actually useful VM for Python. The\nbootstrapping plans were taken quite directly from Squeak, which is a Smalltalk\nVM written in a subset of Smalltalk called Slang, which can then be bootstrapped\nto C code. The plan for PyPy was to do something similar, to define a restricted\nsubset of Python called RPython, restricted in such a way that it should be\npossible to statically compile RPython programs to C code. Then the Python\ninterpreter should only use that subset, of course.\nThe main difference from the Squeak approach is that Slang, the subset of Squeak\nused there, is actually quite a low level language. In a way, you could almost\ndescribe it as C with Smalltalk syntax. RPython was really meant to be a\nmuch higher level language, much closer to Python, with full support for single\ninheritance classes, and most of Python's built-in data structures.\n\n\n(BTW, you don\u2019t have to understand any of the illustrations in this blog post,\nthey are taken from talks and project reports we did over the years so they are\nof archaeological interest only and I don\u2019t understand most of them myself.)\nFrom 2005 on, work on the RPython type inference engine and C backend started in\nearnest, which was sort of co-developed with the RPython language definition and\nthe PyPy Python interpreter. This is also roughly the time that I joined the\nproject as a volunteer.\nAnd at the second sprint I went to, in July 2005, two and a half years after the\nproject got started, we managed to bootstrap the PyPy interpreter to C for the\nfirst time. When we ran the compiled program, it of course immediately\nsegfaulted. The reason for that was that the C backend had turned characters\ninto signed chars in C, while the rest of the infrastructure assumed that they\nwere unsigned chars. After we fixed that, the second attempt worked and we\nmanaged to run an incredibly complex program, something like 6 * 7. That\nfirst bootstrapped version was really really slow, a couple of hundred times\nslower than CPython.\n\n\nThe bootstrapping process of RPython has a number of nice benefits, a big one\nbeing that a number of the properties of the generated virtual machine don't\nhave to expressed in the interpreter. The biggest example of this is garbage\ncollection. RPython is a garbage collected language, and the interpreter does\nnot have to care much about GC in most cases. When the C source code is\ngenerated, a GC is automatically inserted. This is a source of great\nflexibility. Over time we experimented with a number of different GC\napproaches, from reference counting to Boehm to our current incremental\ngenerational collector. As an aside, for a long time we were also working on\nother backends to the RPython language and hoped to be able to target Java and\n.NET as well. Eventually we abandoned this strand of work, however.\n\n\n\n\nRPython's Modularity Problems\nNow we come to the first thing I would say we learned in the project, which is\nthat the quality of tools we thought of as internal things still matters a lot.\nOne of the biggest technical mistakes we've made in the project was that we\ndesigned RPython without any kind of story for modularity. There is no concept\nof modules in the language or any other way to break up programs into smaller\ncomponents. We always thought that it would be ok for RPython to be a little bit\ncrappy. It was meant to be this sort of internal language with not too many\nexternal users. And of course that turned out to be completely wrong later.\nThat lack of modularity led to various problems that persist until today. The\nbiggest one is that there is no separate compilation for RPython programs at\nall! You always need to compile all the parts of your VM together, which leads\nto infamously bad compilation times.\nAlso by not considering the modularity question we were never forced to fix\nsome internal structuring issues of the RPython compiler itself.\nVarious layers of the compiler keep very badly defined and porous interfaces between\nthem. This was made possible by being able to work with all the program information in one heap,\nmaking the compiler less approachable and maintainable than it maybe could be.\nOf course this mistake just got more and more costly to fix over time,\nand so it means that so far nobody has actually done it.\nNot thinking more carefully about RPython's design, particularly its\nmodularity story, is in my opinion the biggest technical mistake the project\nmade.\n\n\n\n\n2006: The Meta-JIT\nAfter successfully bootstrapping the VM we did some fairly straightforward\noptimizations on the interpreter and the C backend and managed to reduce the\nslowdown versus CPython to something like 2-5 times slower. That's great! But of\ncourse not actually useful in practice. So where do we go from here?\nOne of the not so secret goals of Armin Rigo, one of the PyPy founders, was to\nuse PyPy together with some advanced partial evaluation magic sauce to\nsomehow automatically generate a JIT compiler from the interpreter. The goal was\nsomething like, \"you write your interpreter in RPython, add a few annotations\nand then we give you a JIT for free for the language that that interpreter\nimplements.\"\nWhere did the wish for that approach come from, why not just write a JIT for\nPython manually in the first place? Armin had actually done just that before he\nco-founded PyPy, in a project called Psyco. Psyco was an extension module for\nCPython that contained a method-based JIT compiler for Python code.\u00a0And Psyco\nproved to be an amazingly frustrating compiler to write. There were two main\nreasons for that. The first reason was that Python is actually quite a complex\nlanguage underneath its apparent simplicity. The second reason for the\nfrustration was that Python was and is very much an alive language, that gains\nnew features in the language core in every version. So every time a new Python\nversion came out, Armin had to do fundamental changes and rewrites to Psyco, and\nhe was getting pretty frustrated with it. So he hoped that that effort could be\ndiminished by not writing the JIT for PyPy by hand at all. Instead, the goal was\nto generate a method-based JIT from the interpreter automatically. By taking the\ninterpreter, and applying a kind of advanced transformation to it, that would\nturn it into a method-based JIT. And all that would still be translated into a\nC-based VM, of course.\n\nSlide from Psyco presentation at EuroPython 2002\n\n\n\n\nThe First JIT Generator\nFrom early 2006 on until the end of the EU project a lot of work went into\nwriting such a JIT generator. The idea was to base it on runtime partial\nevaluation. Partial evaluation is an old idea in computer science. It's supposed\nto be a way to automatically turn interpreters for a language into a compiler\nfor that same language. Since PyPy was trying to generate a JIT compiler, which\nis in any case necessary to get good performance for a dynamic language like\nPython, the partial evaluation was going to happen at runtime.\nThere are various ways to look at partial evaluation, but if you've never heard\nof it before, a simple way to view it is that it will compile a Python function\nby gluing\u00a0together the implementations of the bytecodes of that function and\noptimizing the result.\nThe main new ideas of PyPy's partial-evaluation based JIT generator as opposed\nto earlier partial-evaluation approaches are the ideas of \"promote\" and the idea\nof \"virtuals\". Both of these techniques had already been present (in a slightly\nless general form) in Psyco, and the goal was to keep using them in PyPy. Both\nof these techniques also still remain in use today in PyPy. I'm\ngoing on a slight technical diversion now, to give a high level explanation of\nwhat those ideas are for.\n\n\n\n\n\nPromote\nOne important ingredient of any JIT compiler is the ability to do runtime\nfeedback. Runtime feedback is most commonly used to know something about which\nconcrete types are used by a program in practice. Promote is basically a way to\neasily introduce runtime feedback into the JIT produced by the JIT generator.\nIt's an annotation the implementer of a language can use to express their wish\nthat specialization should happen at this point. This mechanism can be used to\nexpress all kinds of runtime feedback, moving values from the interpreter\ninto the compiler, whether they be types or other things.\n\n\n\n\nVirtuals\nVirtuals are a very aggressive form of partial escape analysis. A dynamic\nlanguage often puts a lot of pressure on the garbage collector, since most\nprimitive types (like integers, floats and strings) are boxed in the heap, and\nnew boxes are allocated all the time.\nWith the help of virtuals a very significant portion of all allocations in the\ngenerated machine code can be completely removed. Even if they can't be removed,\noften the allocation can be delayed or moved into an error path, or even\ninto a deoptimization path, and thus disappear from the generated machine code\ncompletely.\nThis optimization really is the super-power of PyPy's optimizer, since it\ndoesn't work only for primitive boxes but for any kind of object allocated on\nthe heap with a predictable lifetime.\nAs an aside, while this kind of partial escape analysis is sort of new for\nobject-oriented languages, it has actually existed in Prolog-based partial\nevaluation systems since the 80s, because it's just extremely natural there.\n\n\n\n\nJIT Status 2007\nSo, back to our history. We're now in 2007, at the end of the EU project (you\ncan find the EU-reports we wrote during the projects here). The EU project\nsuccessfully finished, we survived the final review with the EU. So, what's the\n2007 status of the JIT generator? It works kind of, it can be applied to PyPy. It\nproduces a VM with a JIT that will turn Python code into machine code at runtime\nand run it. However, that machine code is not particularly fast. Also, it tends\nto generate many megabytes of machine code even for small Python programs. While\nit's always faster than PyPy without JIT, it's only sometimes faster than\nCPython, and most of the time Psyco still beats it. On the one hand, this is\nstill an amazing achievement! It's arguably the biggest application of partial\nevaluation at this point in time! On the other hand, it was still quite\ndisappointing in practice, particularly since some of us had believed at the\ntime that it should have been possible to reach and then surpass the speed of\nPsyco with this approach.\n\n\n\n\n2007: RSqueak and other languages\nAfter the EU project ended we did all kinds of things. Like sleep for a month\nfor example, and have the cleanup sprint that I already mentioned. We also had a\nslightly unusual sprint in Bern, with members of the Software Composition\nGroup of Oscar Nierstrasz. As I wrote above, PyPy had been heavily influenced\nby Squeak Smalltalk, and that group is a heavy user of Squeak, so we wanted to\nsee how to collaborate with them. At the beginning of the sprint, we decided\ntogether that the goal of that week should be to try to write a Squeak virtual\nmachine in RPython, and at the end of the week we'd gotten surprisingly far with\nthat goal. Basically most of the bytecodes and the Smalltalk object system\nworked, we had written an image loader and could run some benchmarks (during the\nsprint we also regularly updated a blog, the success of which led us to start\nthe PyPy blog).\n\n\nThe development of the Squeak interpreter was very interesting for the project,\nbecause it was the first real step that moved RPython from being an\nimplementation detail of PyPy to be a more interesting project in its own right.\nBasically a language to write interpreters in, with the eventual promise to get\na JIT for that language almost for free. That Squeak implementation is now\ncalled RSqueak (\"Research Squeak\").\nI'll not go into more details about any of the other language implementations in\nRPython in this post, but over the years we've had a large variety of language\nof them done by various people and groups, most of them as research vehicles,\nbut also some as real language implementations. Some very cool research results\ncame out of these efforts, here's a slightly outdated list of some of them.\nThe use of RPython for other languages complicated the PyPy narrative a lot, and\nin a way we never managed to recover the simplicity of the original project\ndescription \"PyPy is Python in Python\". Because now it's something like \"we have\nthis somewhat strange language, a subset of Python, that's called RPython, and\nit's good to write interpreters in. And if you do that, we'll give you a JIT for\nalmost free. And also, we used that language to write a Python implementation,\ncalled PyPy.\". It just doesn't roll off the tongue as nicely.\n\n\n\n\n2008-2009: Four More JIT Generators\nBack to the JIT. After writing the first JIT generator as part of the EU\nproject, with somewhat mixed results, we actually wrote several more JIT\ngenerator prototypes with different architectures to try to solve some of the\nproblems of the first approach. To give an impression of these prototypes,\nhere\u2019s a list of them.\n\nThe second JIT generator we started working on in 2008 behaved exactly like\nthe first one, but had a meta-interpreter based architecture, to make it more\nflexible and easier to experiment with. The meta-interpreter was called\nthe \"rainbow interpreter\", and in general the JIT is an area where we went\nsomewhat overboard with borderline silly terminology, with notable\noccurrences of \"timeshifter\", \"blackhole interpreter\" etc.\nThe third JIT generator was an experiment based on the second one which\nchanged\ncompilation strategy. While the previous two had compiled many control flow\npaths of the currently compiled function eagerly, that third JIT was sort of\nmaximally lazy and stopped compilation at every control flow split to avoid\nguessing which path would actually be useful later when executing the code.\nThis was an attempt to reduce the problem of the first JIT generating way too\nmuch machine code. Only later, when execution went down one of the not yet\ncompiled paths would it continue compiling more code. This gives an effect\nsimilar to that of lazy basic block versioning.\nThe fourth JIT generator was a pretty strange prototype, a runtime partial\nevaluator for Prolog, to experiment with various specialization trade-offs. It\nhad an approach that we gave a not at all humble name, called \"perfect\nspecialization\".\nThe fifth JIT generator is the one that we are still using today. Instead of\ngenerating a method-based JIT compiler from our interpreter we switched to\ngenerating a tracing JIT compiler. Tracing JIT compilers were sort of the\nlatest fashion at the time, at least for a little while.\n\n\n\n\n\n2009: Meta-Tracing\nSo, how did that tracing JIT generator work? A tracing JIT generates code by\nobserving and logging the execution of the running program. This yields a\nstraight-line trace of operations, which are then optimized and compiled into\nmachine code.\u00a0Of course most tracing systems mostly focus on tracing loops.\nAs we discovered, it's actually quite simple to apply a tracing JIT to a generic\ninterpreter, by not tracing the execution of the user program directly, but by\ninstead tracing the execution of the interpreter while it is running the user\nprogram (here's the paper we wrote about this approach).\nSo that's what we implemented. Of course we kept the two successful parts of the\nfirst JIT, promote and virtuals (both links go to the papers about these\nfeatures in the meta-tracing context).\n\n\n\n\n\nWhy did we Abandon Partial Evaluation?\nSo one question I get sometimes asked when telling this story is, why did\nwe think that tracing would work better than partial evaluation (PE)? One of the\nhardest parts of compilers in general and partial evaluation based systems in\nparticular is the decision when and how much to inline, how much to specialize,\nas well as the decision when to split control flow paths. In the PE based JIT\ngenerator we never managed to control that question. Either the JIT would\ninline too much, leading to useless compilation of all kinds of unlikely error\ncases. Or it wouldn't inline enough, preventing necessary optimizations.\nMeta tracing solves this problem with a hammer, it doesn't make particularly\ncomplex inlining decisions at all. It instead decides what to inline by\nprecisely following what a real execution through the program is doing. Its\ninlining decisions are therefore very understandable and predictable, and it\nbasically only has one heuristic based on whether the called function contains a\nloop or not: If the called function contains a loop, we'll never inline it, if\nit doesn't we always try to inline it. That predictability is actually what was\nthe most helpful, since it makes it possible for interpreter authors to\nunderstand why the JIT did what it did and to actually influence its inlining\ndecisions by changing the annotations in the interpreter source. It turns out\nthat simple is better than complex.\n\n\n\n\n2009-2011: The PyJIT Eurostars Project\nWhile we were writing all these JIT prototypes, PyPy had sort of reverted back\nto being a volunteer-driven open source project (although some of us, like\nAntonio Cuni and I, had started working for universities and other project\nmembers had other sources of funding). But again, while we did the work it\nbecame clear that to get an actually working fast PyPy with generated JIT we\nwould need actual funding again for the project. So we applied to the EU again,\nthis time for a much smaller project with less money, in the Eurostars\nframework. We got a grant for three participants, merlinux, OpenEnd and Uni\nD\u00fcsseldorf, on the order of a bit more than half a million euro. That money was\nspecifically for JIT development and JIT testing infrastructure.\n\n\n\n\n\nTracing JIT improvements\nWhen writing the grant we had sat together at a sprint and discussed extensively\nand decided that we would not switch JIT generation approaches any more. We all\nliked the tracing approach well enough and thought it was promising. So instead\nwe agreed to try in earnest to make the tracing JIT really practical. So in the\nEurostars project we started with implementing sort of fairly standard JIT\ncompiler optimizations for the meta-tracing JIT, such as:\n\nconstant folding\ndead code elimination\nloop invariant code motion (using LuaJIT's approach)\nbetter heap optimizations\nfaster deoptimization (which is actually a bit of a mess in the\nmeta-approach)\nand dealing more efficiently with Python frames objects and the\nfeatures of Python's debugging facilities\n\n\n\n\n\n2010: speed.pypy.org\nIn 2010, to make sure that we wouldn't accidentally introduce speed regressions\nwhile working on the JIT, we implemented infrastructure to build PyPy and run\nour benchmarks nightly. Then, the https://speed.pypy.org website was implemented\nby Miquel Torres, a volunteer. The website shows the changes in benchmark\nperformance compared to the previous n days. It didn't sound too important at\nfirst, but this was (and is) a fantastic tool, and an amazing motivator over the\nnext years, to keep continually improving performance.\n\n\n\n\n\nContinuous Integration\nThis actually leads me to something else that I'd say we learned, which is that\ncontinuous integration is really awesome, and completely transformative to have\nfor a project. This is not a particularly surprising insight nowadays in the\nopen source community, it's easy to set up continuous integration on Github\nusing Travis or some other CI service. But I still see a lot of research\nprojects that don't have tests, that don't use CI, so I wanted to mention it\nanyway. As I mentioned earlier in the post, PyPy has a quite serious testing\nculture, with unit tests written for new code, regression tests for all bugs,\nand integration tests using the CPython test suite. Those tests are run\nnightly on a number of architectures and operating systems.\nHaving all this kind of careful testing is of course necessary, since PyPy is\nreally trying to be a Python implementation that people actually use, not just\nwrite papers about. But having all this infrastructure also had other benefits,\nfor example it allows us to trust newcomers to the project very quickly.\nBasically after your first patch gets accepted, you immediately get commit\nrights to the PyPy repository. If you screw up, the tests (or the code reviews)\nare probably going to catch it, and that reduction to the barrier to\ncontributing is just super great.\nThis concludes my advertisement for testing in this post.\n\n\n\n\n2010: Implementing Python Objects with Maps\nSo, what else did we do in the Eurostars project, apart from adding traditional\ncompiler optimizations to the tracing JIT and setting up CI infrastructure?\nAnother strand of work, that went on sort of concurrently to the JIT generator\nimprovements, were deep rewrites in the Python runtime, and the Python data\nstructures. I am going to write about two exemplary ones here, maps and storage strategies.\nThe first such rewrite is fairly standard. Python instances are similar to\nJavascript objects, in that you can add arbitrary attributes to them at runtime.\nOriginally Python instances were backed by a dictionary in PyPy, but of course\nin practice most instances of the same class have the same set of attribute\nnames. Therefore we went and implemented Self style maps, which are often\ncalled hidden classes in the JS world to represent instances instead. This\nhas two big benefits, it allows you to generate much better machine code for\ninstance attribute access and makes instances use a lot less memory.\n\n\n\n\n\n2011: Container Storage Strategies\nAnother important change in the PyPy runtime was rewriting the Python container\ndata structures, such as lists, dictionaries and sets. A fairly straightforward\nobservation about how those are used is that in a significant percentage of\ncases they contain type-homogeneous data. As an example it's quite common to\nhave lists of only integers, or lists of only strings. So we changed the list,\ndict and set implementations to use something we called storage strategies. With\nstorage strategies these data structures use a more efficient representations if\nthey contain only primitives of the same type, such as ints, floats, strings.\nThis makes it possible to store the values without boxing them in the underlying\ndata structure. Therefore read and write access are much faster for such type\nhomogeneous containers. Of course when later another data type gets added to\nsuch a list, the existing elements need to all be boxed at that point, which is\nexpensive. But we did a study and found out that that happens quite rarely in\npractice. A lot of that work was done by Lukas Diekmann.\n\n\n\n\n\nDeep Changes in the Runtime are Necessary\nThese two are just two examples for a number of fairly fundamental changes in\nthe PyPy runtime and PyPy data structures, probably the two most important ones,\nbut we did many others. That leads me to another thing we learned. If you want\nto generate good code for a complex dynamic language such as Python, it's\nactually not enough at all to have a good code generator and good compiler\noptimizations. That's not going to help you, if your runtime data-structures\naren't in a shape where it's possible to generate efficient machine code to\naccess them.\nMaybe this is well known in the VM and research community. However it's the main\nmistake that in my opinion every other Python JIT effort has made in the last 10\nyears, where most projects said something along the lines of \"we're not\nchanging the existing CPython data structures at all, we'll just let LLVM\ninline enough C code of the runtime and then it will optimize all the overhead\naway\". That never works very well.\n\n\n\n\nJIT Status 2011\nSo, here we are at the end of the Eurostars project, what's the status of the JIT? Well, it\nseems this meta-tracing stuff really works! We finally started actually\nbelieving in it, when we reached the point in 2010 where self-hosting PyPy was\nactually faster than bootstrapping the VM on CPython. Speeding up the\nbootstrapping process is something that Psyco never managed at all, so we\nconsidered this a quite important achievement. At the end of\nEurostars, we were about 4x faster than CPython on our set of benchmarks.\n\n\n\n\n2012-2017: Engineering and Incremental Progress\n2012 the Eurostars project was finished and PyPy reverted yet another time back\nto be an open source project. From then on, we've had a more diverse set of\nsources of funding: we received some crowd funding via the Software Freedom\nConservancy and contracts of various sizes from companies to implement various\nspecific features, often handled by Baroque Software. Over the next couple of\nyears\nwe revamped various parts of the VM. We improved the GC in major ways. We\noptimized the implementation of the JIT compiler to improve warmup times. We\nimplemented backends for various CPU architectures (including PowerPC and\ns390x). We tried to reduce the number of performance cliffs and make the JIT\nuseful in a broader set of cases.\nAnother strand of work was to push quite significantly to be more\ncompatible with CPython, particularly the Python 3 line as well as extension\nmodule support. Other compatibility improvements we did was making sure that\nvirtualenv works with PyPy, better support for distutils and setuptools and\nsimilar improvements. The continually improving performance as well better\ncompatibility with the ecosystem tools led to the first few users of PyPy in\nindustry.\n\n\n\n\nCPyExt\nAnother very important strand of work that took a lot of effort in recent years\nwas CPyExt. One of the main blockers of PyPy adoption had always been the fact\nthat a lot of people need specific C-extension modules at least in some parts of\ntheir program, and telling them to reimplement everything in Python is just not\na practical solution. Therefore we worked on CPyExt, an emulation layer  to make\nit possible to run CPython C-extension modules in PyPy. Doing that was a very\npainful process, since the CPython extension API leaks a lot of CPython\nimplementation details, so we had to painstakingly emulate all of these details\nto make it possible to run extensions. That this works at all remains completely\namazing to me! But nowadays CPyExt is even getting quite good, a lot of the big\nnumerical libraries such as Numpy and Pandas are now supported (for a while\nwe had worked hard on a reimplementation of Numpy called NumPyPy, but\neventually realized that it would never be complete and useful enough).\nHowever, calling CPyExt modules from PyPy can still be very slow,\nwhich makes it impractical for some applications\nthat's why we are working on it.\nNot thinking about C-extension module emulation earlier in the project history\nwas a pretty bad strategic mistake. It had been clear for a long time that\ngetting people to just stop using all their C-extension modules was never going\nto work, despite our efforts to give them alternatives, such as cffi. So we\nshould have thought of a story for all the existing C-extension modules earlier\nin the project. Not starting CPyExt earlier was mostly a failure of our\nimagination (and maybe a too high pain threshold): We didn't believe this kind\nof emulation was going to be practical, until somebody went and tried it.\n\n\n\n\nPython 3\nAnother main\nfocus of the last couple of years has been to catch up with the CPython 3 line.\nOriginally we had ignored Python 3 for a little bit too long, and were trailing\nseveral versions behind. In 2016 and 2017 we had a grant from the Mozilla open\nsource support program of $200'000 to be able to catch up with Python 3.5. This\nwork is now basically done, and we are starting to target CPython 3.6 and will\nhave to look into 3.7 in the near future.\n\n\n\n\nIncentives of OSS compared to Academia\nSo, what can be learned from those more recent years? One thing we can observe\nis that a lot of the engineering work we did in that time is not really science\nas such. A lot of the VM techniques we implemented are kind of well known, and\ncatching up with new Python features is also not particularly deep researchy\nwork. Of course this kind of work is obviously super necessary if you want\npeople to use your VM, but it would be very hard to try to get research funding\nfor it. PyPy managed quite well over its history to balance phases of more\nresearch oriented work, and more product oriented ones. But getting this balance\nsomewhat right is not easy, and definitely also involves a lot of luck. And, as\nhas been discussed a lot, it's actually very hard to find funding for open\nsource work, both within and outside of academia.\n\n\nMeta-Tracing really works!\nLet me end with what, in my opinion, is the main positive technical result of PyPy the\nproject. Which is that the whole idea of using a meta-tracing JIT can really\nwork! Currently PyPy is about 7 times faster than CPython on a broad set of\nbenchmarks. Also, one of the very early motivations for using a meta-jitting\napproach in PyPy, which was to not have to adapt the JIT to new versions of\nCPython proved to work: indeed we didn't have to change anything in the JIT\ninfrastructure to support Python 3.\nRPython has also worked and improved performance for a number of other\nlanguages. Some of these interpreters had wildly different architectures.\nAST-based interpreters, bytecode based, CPU emulators, really inefficient\nhigh-level ones that allocate continuation objects all the time, and so on. This\nshows that RPython also gives you a lot of freedom in deciding how you want to\nstructure the interpreter and that it can be applied to languages of quite\ndifferent paradigms.\nI'll end with a list of the people that have contributed code to PyPy over its\nhistory, more than 350 of them. I'd like to thank all of them and the various\nroles they played. To the next 15 years!\n\n\n\n\n\n\nAcknowledgements\nA lot of people helped me with this blog post. Tim Felgentreff made me give the\nkeynote, which lead me to start collecting the material. Samuele Pedroni\ngave essential early input when I just started planning the talk, and also gave\nfeedback on the blog post. Maciej Fija\u0142kowski gave me feedback on the post, in\nparticular important insight about the more recent years of the project. Armin\nRigo discussed the talk slides with me, and provided details about the early\nexpectations about the first JIT's hoped-for performance. Antonio Cuni gave\nsubstantial feedback and many very helpful suggestions for the blog post.\nMichael Hudson-Doyle also fixed a number of mistakes in the post and rightfully\ncomplained about the lack of mention of the GC. Christian Tismer provided\naccess to his copy of early Python-de mailing list posts. Matti Picus pointed\nout a number of things I had forgotten and fixed a huge number of typos and\nawkward English, including my absolute inability to put commas correctly.\nAll remaining errors are of course my own.\n\n\nupdate: fixed confusing wording in the maps section.",
      "tags": "roadmap",
      "url": "https://www.pypy.org/posts/2018/09/the-first-15-years-of-pypy-3412615975376972020.html"
    },
    {
      "title": "Repeating a Matrix Multiplication Benchmark",
      "text": "I watched the Hennessy & Patterson's Turing award lecture recently:\n\n\n\nIn it, there's a slide comparing the performance of various matrix\nmultiplication implementations, using Python (presumably CPython) as a baseline\nand comparing that against various C implementations (I couldn't find the\nlinked paper yet):\n\n\n\nI expected the baseline speedup of switching from CPython to C to be\nhigher and I also wanted to know what performance PyPy gets, so I did my own\nbenchmarks. This is a problem that Python is completely unsuited for, so it\nshould give very exaggerated results.\nThe usual disclaimers apply: All benchmarks are lies, benchmarking of\nsynthetic workloads even more so. My implementation is really naive (though I\ndid optimize it a little bit to help CPython), don't use any\nof this code\nfor anything real. The benchmarks ran on my rather old Intel i5-3230M laptop\nunder Ubuntu 17.10.\nWith that said, my results were as follows:\n\n\n\nImplementation\ntime\nspeedup over CPython\nspeedup over PyPy\n\n\n\n\nCPython\n512.588 \u00b1 2.362 s\n1 \u00d7\n\n\n\nPyPy\n8.167 \u00b1 0.007 s\n62.761 \u00b1  0.295 \u00d7\n1 \u00d7\n\n\n'naive' C\n2.164 \u00b1 0.025 s\n236.817 \u00b1  2.918 \u00d7\n3.773 \u00b1 0.044 \u00d7\n\n\nNumPy\n0.171 \u00b1 0.002 s\n2992.286 \u00b1 42.308 \u00d7\n47.678 \u00b1 0.634 \u00d7\n\nThis is running 1500x1500 matrix multiplications with (the same) random matrices. Every\nimplementation is run 50 times in a fresh process. The results are averaged,\nthe errors are bootstrapped 99% confidence intervals.\nSo indeed the speedup that I got of switching from CPython to C is quite a bit higher than\n47x! PyPy is much better than CPython, but of course can't really compete\nagainst GCC. And then the real professionals (numpy/OpenBLAS) are in a whole\n'nother league. The speedup of the AVX numbers in the slide above is even\nhigher than my NumPy numbers, which I assume is the result of my old CPU with\ntwo cores, vs. the 18 core CPU with AVX support.\nLesson confirmed: leave matrix multiplication to people who\nactually know what they are doing.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/06/repeating-matrix-multiplication-8641748742577945875.html"
    },
    {
      "title": "How to ignore the annoying Cython warnings in PyPy 6.0",
      "text": "If you install any Cython-based module in PyPy 6.0.0, it is very likely that you get a warning like this:\n>>>> import numpy\n/data/extra/pypy/6.0.0/site-packages/numpy/random/__init__.py:99: UserWarning: __builtin__.type size changed, may indicate binary incompatibility. Expected 888, got 408\n  from .mtrand import *\n\n\nThe TL;DR version is: the warning is a false alarm, and you can hide it by doing:\n$ pypy -m pip install pypy-fix-cython-warning\n\n\nThe package does not contain any module, only a\u00a0.pth\u00a0file which installs a warning filter at startup.\n\nTechnical details\n\nThis happens because whenever Cython compiles a pyx file, it generates C code which does a sanity check on the C size of\u00a0PyType_Type. PyPy versions up to 5.10 are buggy and report the incorrect size, so Cython includes a workaround to compare it with the incorrect value, when on PyPy.\n\nPyPy 6 fixed the bug and now\u00a0PyType_Type\u00a0reports the correct size; however, Cython still tries to compare it with the old, buggy value, so it (wrongly) emits the warning.\n\nCython 0.28.2 includes a fix for it, so that C files generated by it no longer emit the warning. However, most packages are distributed with pre-cythonized C files. For example,\u00a0numpy-1.14.2.zip\u00a0include C files which were generated by Cython 0.26.1: if you compile it you still get the warning, even if you locally installed a newer version of Cython.\n\nThere is not much that we can do on the PyPy side, apart for waiting for all the Cython-based packages to do a new release which include C files generated by a newer Cython.\u00a0 In the mean time, installing this module will silence the\u00a0warning.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/04/how-to-ignore-annoying-cython-warnings-1007636731207810779.html"
    },
    {
      "title": "PyPy2.7 and PyPy3.5 v6.0 dual release",
      "text": "The PyPy team is proud to release both PyPy2.7 v6.0 (an interpreter supporting\nPython 2.7 syntax), and a PyPy3.5 v6.0 (an interpreter supporting Python\n3.5 syntax). The two releases are both based on much the same codebase, thus\nthe dual release.\nThis release is a feature release following our previous 5.10 incremental\nrelease in late December 2017. Our C-API compatibility layer cpyext is\nnow much faster (see the blog post) as well as more complete. We have made\nmany other improvements in speed and CPython compatibility. Since the changes\naffect the included python development header files, all c-extension modules must\nbe recompiled for this version.\nUntil we can work with downstream providers to distribute builds with PyPy, we\nhave made packages for some common packages available as wheels. You may\ncompile yourself using pip install --no-build-isolation <package>, the\nno-build-isolation is currently needed for pip v10.\nFirst-time python users are often stumped by silly typos and omissions when\ngetting started writing code. We have improved our parser to emit more friendly\nsyntax errors,  making PyPy not only faster but more friendly.\nThe GC now has hooks to gain more insights into its performance\nThe default Matplotlib TkAgg backend now works with PyPy, as do pygame and pygobject.\nWe updated the cffi module included in PyPy to version 1.11.5, and the\ncppyy backend to 0.6.0. Please use these to wrap your C and C++ code,\nrespectively, for a JIT friendly experience.\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nWe strongly recommend updating.\nThe Windows PyPy3.5 release is still considered beta-quality. There are open\nissues with unicode handling especially around system calls and c-extensions.\nThe utf8 branch that changes internal representation of unicode to utf8 did not\nmake it into the release, so there is still more goodness coming. We also\nbegan working on a Python3.6 implementation, help is welcome.\nYou can download the v6.0 releases here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject. If PyPy is not quite good enough for your needs, we are available for\ndirect consulting work.\nWe would also like to thank our contributors and encourage new people to join\nthe project. PyPy has many layers and we need help with all of them: PyPy\nand RPython documentation improvements, tweaking popular modules to run\non pypy, or general help with making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThe PyPy release supports:\n\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\n\n\nWhat else is new?\n\nPyPy 5.10 was released in Dec, 2017.\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2018/04/pypy27-and-pypy35-v60-dual-release-7416552143474607997.html"
    },
    {
      "title": "Improving SyntaxError in PyPy",
      "text": "For the last year, my halftime job has been to teach non-CS uni students\nto program in Python. While doing that, I have been trying to see what common\nstumbling blocks exist for novice programmers. There are many\nthings that could be said here, but a common theme that emerges is\nhard-to-understand error messages. One source of such error messages,\nparticularly when starting out, is SyntaxErrors.\nPyPy's parser (mostly following the architecture of CPython) uses a\nregular-expression-based tokenizer with some cleverness to deal with\nindentation, and a simple LR(1) parser. Both of these components obviously\nproduce errors for invalid syntax, but the messages are not very helpful. Often,\nthe message is just \"invalid syntax\", without any hint of what exactly is wrong.\nIn the last couple of weeks I have invested a little bit of effort to make them a\ntiny bit better. They will be part of the upcoming PyPy 6.0 release. Here are\nsome examples of what changed.\n\nMissing Characters\nThe first class of errors occurs when a token is missing, often there is only one\nvalid token that the parser expects. This happens most commonly by leaving out\nthe ':' after control flow statements (which is the syntax error I personally\nstill make at least a few times a day). In such situations, the parser will now\ntell you which character it expected:\n\n>>>> # before\n>>>> if 1\n  File \"<stdin>\", line 1\n    if 1\n       \nSyntaxError: invalid syntax\n>>>>\n\n>>>> # after\n>>>> if 1\n  File \"<stdin>\", line 1\n    if 1\n       \nSyntaxError: invalid syntax (expected ':')\n>>>>\n\nAnother example of this feature:\n\n>>>> # before\n>>>> def f:\n  File \"<stdin>\", line 1\n    def f:\n        \nSyntaxError: invalid syntax\n>>>>\n\n>>>> # after\n>>>> def f:\n  File \"<stdin>\", line 1\n    def f:\n         \nSyntaxError: invalid syntax (expected '(')\n>>>>\n\n\n\nParentheses\nAnother source of errors are unmatched parentheses. Here, PyPy has always had\nslightly better error messages than CPython:\n\n>>> # CPython\n>>> )\n  File \"<stdin>\", line 1\n    )\n    \nSyntaxError: invalid syntax\n>>>\n\n>>>> # PyPy\n>>> )\n  File \"<stdin>\", line 1\n    )\n    \nSyntaxError: unmatched ')'\n>>>>\n\nThe same is true for parentheses that are never closed (the call to eval is\nneeded to get the error, otherwise the repl will just wait for more input):\n\n>>> # CPython\n>>> eval('(')\n  File \"<string>\", line 1\n    (\n    \nSyntaxError: unexpected EOF while parsing\n>>>\n\n>>>> # PyPy\n>>>> eval('(')\n  File \"<string>\", line 1\n    (\n    \nSyntaxError: parenthesis is never closed\n>>>>\n\nWhat I have now improved is the case of parentheses that are matched wrongly:\n\n>>>> # before\n>>>> (1,\n.... 2,\n.... ]\n  File \"<stdin>\", line 3\n    ]\n    \nSyntaxError: invalid syntax\n>>>>\n\n>>>> # after\n>>>> (1,\n.... 2,\n.... ]\n  File \"<stdin>\", line 3\n    ]\n    \nSyntaxError: closing parenthesis ']' does not match opening parenthesis '(' on line 1\n>>>>\n\n\n\nConclusion\nObviously these are just some very simple cases, and there is still a lot of\nroom for improvement (one huge problem is that only a single SyntaxError is\never shown per parse attempt, but fixing that is rather hard).\nIf you have a favorite unhelpful SyntaxError message you love to hate, please\ntell us in the comments and we might try to improve it. Other kinds of\nnon-informative error messages are also always welcome!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/04/improving-syntaxerror-in-pypy-5733639208090522433.html"
    },
    {
      "title": "Leysin Winter Sprint 2018: review",
      "text": "Like every year, the PyPy developers and a couple of newcomers\n      gathered in Leysin, Switzerland, to share their thoughts and\n      contribute to the development of PyPy.\n    As always, we had interesting discussions about how we could\n      improve PyPy, to make it the first choice for even more\n      developers. We also made some progress with current issues, like\n      compatibility with Python 3.6 and improving the performance of\n      CPython extension modules, where we fixed a lot of bugs and gained\n      new insights about where and how we could tweak PyPy.\n    \n     We were very happy about the number of new people who joined us\n      for the first time, and hope they enjoyed it as much as everyone\n      else. \n    \n    Topics\n    We worked on the following topics (and more!):\n    \n      Introductions for newcomers\n      Python 3.5 and 3.6 improvements\n      CPyExt performance improvements and GC implementation\n      \n      JIT: guard-compatible implementation\n      \n      Pygame performance improvements\n      Unicode/UTF8 implementation\n      \n      CFFI tutorial/overview rewrite\n      \n      py3 test runners refactoring\n      RevDB improvements\n      \n    \n    The weather was really fine for most of the week, with only\n    occasional snow and fog. We started our days with a short (and\n    sometimes not so short) planning session and enjoyed our dinners in\n    the great restaurants in the area. Some of us even started earlier\n    and continued till late night. It was a relaxed, but also very\n    productive atmosphere. On our break day on Wednesday, we enjoyed the\n    great conditions and went skiing and hiking.\n    Attendees\n    \n      Arianna\n      Jean-Daniel\n      \n      Stefan Beyer\n      Floris Bruynooghe\n      \n      Antonio Cuni\n      Ren\u00e9 Dudfield\n      Manuel Jacob\n      Ronan Lamy\n      Remi Meier\n      Matti Picus\n      \n      Armin Rigo\n      Alexander Schremmer\n      \n    \n    Leysin is easily reachable by Geneva Airport, so feel free to join\n    us next time!\n    \n    \n    Cheers,\n      Stefan",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/03/leysin-winter-sprint-2018-review-3988364248531980164.html"
    },
    {
      "title": "PyPy 5.10.1 bugfix release for python 3.5",
      "text": "We have released a bug fix PyPy3.5-v5.10.1\ndue to the following issues:\n\n\n\nFix time.sleep(float('nan')) which would hang on Windows\nFix missing errno constants on Windows\nFix issue 2718 for the REPL on Linux\nFix an overflow in converting int secs to nanosecs (issue 2717 )\nUsing kwarg 'flag' to os.setxattr had no effect\nFix the winreg module for unicode entries in the registry on Windows\n\n\n\nNote that many of these fixes are for our new beta version of PyPy3.5 on Windows. There may be more unicode problems in the Windows beta version,\nespecially concerning directory- and file-names with non-ASCII\ncharacters.\n\nOn macOS, we recommend you wait for the\nHomebrew package to prevent issues with third-party packages. For other supported platforms our downloads are available now.\nThanks to those who reported the issues.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\n\nThis PyPy 3.5 release supports:\n\n\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, macOS 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\nPlease update, and continue to help us make PyPy better.\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2018/01/pypy-5101-bugfix-release-for-python-35-8485250762789380657.html"
    },
    {
      "title": "Leysin Winter sprint: 17-24 March 2018",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the thirteenth\ntime.  This is a fully public sprint: newcomers and topics other than\nthose proposed below are welcome.\n\n(Note: this sprint is independent from the suggested April-May sprint in\nPoland.)\n\nGoals and topics of the sprint\n\nThe list of topics is open, but here is our current list:\n\n\n\n\n\n\n\n\n\n cffi tutorial/overview rewrite\n py3 test runners are too complicated\n make win32 builds green\n make packaging more like cpython/portable builds\n get CI builders for PyPy into mainstream projects (Numpy, Scipy, lxml, uwsgi)\n get more of scientific stack working (tensorflow?)\n cpyext performance improvements\n General 3.5 and 3.6 improvements\n JIT topics: guard-compatible, and the subsequent research project to save and reuse traces across processes\n finish unicode-utf8\n update www.pypy.org, speed.pypy.org (web devs needed)\n\n\nAs usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off (for ski or anything else).\n\nExact times\n\nWork days: starting March 18th (~noon), ending March 24th (~noon).\n\nPlease see announcement.txt for more information.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2018/01/leysin-winter-sprint-17-24-march-2018-7141092581585849418.html"
    },
    {
      "title": "PyPy2.7 and PyPy3.5 v5.10 dual release",
      "text": "The PyPy team is proud to release both PyPy2.7 v5.10 (an interpreter supporting\nPython 2.7 syntax), and a final PyPy3.5 v5.10 (an interpreter for Python\n3.5 syntax). The two releases are both based on much the same codebase, thus\nthe dual release.\nThis release is an incremental release with very few new features, the main\nfeature being the final PyPy3.5 release that works on linux and OS X with beta\nwindows support. It also includes fixes for vmprof cooperation with greenlets.\nCompared to 5.9, the 5.10 release contains mostly bugfixes and small improvements.\nWe have in the pipeline big new features coming for PyPy 6.0 that did not make\nthe release cut and should be available within the next couple months.\nAs always, this release is 100% compatible with the previous one and fixed\nseveral issues and bugs raised by the growing community of PyPy users.\nAs always, we strongly recommend updating.\nThere are quite a few important changes that are in the pipeline that did not\nmake it into the 5.10 release. Most important are speed improvements to cpyext\n(which will make numpy and pandas a bit faster) and utf8 branch that changes\ninternal representation of unicode to utf8, which should help especially the\nPython 3.5 version of PyPy.\nThis release concludes the Mozilla Open Source grant for having a compatible\nPyPy 3.5 release and we're very grateful for that.  Of course, we will continue\nto improve PyPy 3.5 and probably move to 3.6 during the course of 2018.\nYou can download the v5.10 releases here:\n\nhttps://pypy.org/download.html\nWe would like to thank our donors for the continued support of the PyPy\nproject.\nWe would also like to thank our contributors and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on pypy, or general help\nwith making RPython's JIT even better.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7 and CPython 3.5. It's fast (PyPy and CPython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython\ncan do for them.\nThe PyPy release supports:\n\n\nx86 machines on most common operating systems\n(Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\nChangelog\n\nimprove ssl handling on windows for pypy3 (makes pip work)\nimprove unicode handling in various error reporters\nfix vmprof cooperation with greenlets\nfix some things in cpyext\ntest and document the cmp(nan, nan) == 0 behaviour\ndon't crash when calling sleep with inf or nan\nfix bugs in _io module\ninspect.isbuiltin() now returns True for functions implemented in C\nallow the sequences future-import, docstring, future-import for CPython bug compatibility\nIssue #2699: non-ascii messages in warnings\nposix.lockf\nfixes for FreeBSD platform\nadd .debug files, so builds contain debugging info, instead of being stripped\nimprovements to cppyy\nissue #2677 copy pure c PyBuffer_{From,To}Contiguous from cpython\nissue #2682, split firstword on any whitespace in sqlite3\nctypes: allow ptr[0] = foo when ptr is a pointer to struct\nmatplotlib will work with tkagg backend once matplotlib pr #9356 is merged\nimprovements to utf32 surrogate handling\ncffi version bump to 1.11.2\n\nMaciej Fijalkowski, Matti Picus and the whole PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2017/12/pypy27-and-pypy35-v510-dual-release-3223396318213306071.html"
    },
    {
      "title": "How to make your code 80 times faster",
      "text": "I often hear people who are happy because PyPy makes their code 2 times faster\nor so. Here is a short personal story which shows PyPy can go well beyond\nthat.\n\nDISCLAIMER: this is not a silver bullet or a general recipe: it worked in\nthis particular case, it might not work so well in other cases. But I think it\nis still an interesting technique. Moreover, the various steps and\nimplementations are showed in the same order as I tried them during the\ndevelopment, so it is a real-life example of how to proceed when optimizing\nfor PyPy.\n\nSome months ago I played a bit with evolutionary algorithms: the ambitious\nplan was to automatically evolve a logic which could control a (simulated)\nquadcopter, i.e. a PID controller (spoiler: it doesn't fly).\n\nThe idea is to have an initial population of random creatures: at each\ngeneration, the ones with the best fitness survive and reproduce with small,\nrandom variations.\n\nHowever, for the scope of this post, the actual task at hand is not so\nimportant, so let's jump straight to the code. To drive the quadcopter, a\nCreature has a run_step method which runs at each delta_t (full\ncode):\nclass Creature(object):\n    INPUTS = 2  # z_setpoint, current z position\n    OUTPUTS = 1 # PWM for all 4 motors\n    STATE_VARS = 1\n    ...\n\n    def run_step(self, inputs):\n        # state: [state_vars ... inputs]\n        # out_values: [state_vars, ... outputs]\n        self.state[self.STATE_VARS:] = inputs\n        out_values = np.dot(self.matrix, self.state) + self.constant\n        self.state[:self.STATE_VARS] = out_values[:self.STATE_VARS]\n        outputs = out_values[self.STATE_VARS:]\n        return outputs\n\n\ninputs is a numpy array containing the desired setpoint and the current\nposition on the Z axis;\noutputs is a numpy array containing the thrust to give to the motors. To\nstart easy, all the 4 motors are constrained to have the same thrust, so\nthat the quadcopter only travels up and down the Z axis;\nself.state contains arbitrary values of unknown size which are passed from\none step to the next;\nself.matrix and self.constant contains the actual logic. By putting\nthe \"right\" values there, in theory we could get a perfectly tuned PID\ncontroller. These are randomly mutated between generations.\n\nrun_step is called at 100Hz (in the virtual time frame of the simulation). At each\ngeneration, we test 500 creatures for a total of 12 virtual seconds each. So,\nwe have a total of 600,000 executions of run_step at each generation.\n\nAt first, I simply tried to run this code on CPython; here is the result:\n$ python -m ev.main\nGeneration   1: ... [population = 500]  [12.06 secs]\nGeneration   2: ... [population = 500]  [6.13 secs]\nGeneration   3: ... [population = 500]  [6.11 secs]\nGeneration   4: ... [population = 500]  [6.09 secs]\nGeneration   5: ... [population = 500]  [6.18 secs]\nGeneration   6: ... [population = 500]  [6.26 secs]\n\nWhich means ~6.15 seconds/generation, excluding the first.\n\nThen I tried with PyPy 5.9:\n$ pypy -m ev.main\nGeneration   1: ... [population = 500]  [63.90 secs]\nGeneration   2: ... [population = 500]  [33.92 secs]\nGeneration   3: ... [population = 500]  [34.21 secs]\nGeneration   4: ... [population = 500]  [33.75 secs]\n\nOuch! We are ~5.5x slower than CPython. This was kind of expected: numpy is\nbased on cpyext, which is infamously slow.  (Actually, we are working on\nthat and on the cpyext-avoid-roundtrip branch we are already faster than\nCPython, but this will be the subject of another blog post.)\n\nSo, let's try to avoid cpyext. The first obvious step is to use numpypy\ninstead of numpy (actually, there is a hack to use just the micronumpy\npart). Let's see if the speed improves:\n$ pypy -m ev.main   # using numpypy\nGeneration   1: ... [population = 500]  [5.60 secs]\nGeneration   2: ... [population = 500]  [2.90 secs]\nGeneration   3: ... [population = 500]  [2.78 secs]\nGeneration   4: ... [population = 500]  [2.69 secs]\nGeneration   5: ... [population = 500]  [2.72 secs]\nGeneration   6: ... [population = 500]  [2.73 secs]\n\nSo, ~2.7 seconds on average: this is 12x faster than PyPy+numpy, and more than\n2x faster than the original CPython. At this point, most people would be happy\nand go tweeting how PyPy is great.\n\nIn general, when talking of CPython vs PyPy, I am rarely satisfied with a 2x\nspeedup: I know that PyPy can do much better than this, especially if you\nwrite code which is specifically optimized for the JIT. For a real-life\nexample, have a look at capnpy benchmarks, in which the PyPy version is\n~15x faster than the heavily optimized CPython+Cython version (both have been\nwritten by me, and I tried hard to write the fastest code for both\nimplementations).\n\nSo, let's try to do better. As usual, the first thing to do is to profile and\nsee where we spend most of the time. Here is the vmprof profile. We spend a\nlot of time inside the internals of numpypy, and allocating tons of temporary\narrays to store the results of the various operations.\n\nAlso, let's look at the jit traces and search for the function run:\nthis is loop in which we spend most of the time, and it is composed of 1796\noperations.  The operations emitted for the line np.dot(...) +\nself.constant are listed between lines 1217 and 1456. Here is the excerpt\nwhich calls np.dot(...); most of the ops are cheap, but at line 1232 we\nsee a call to the RPython function descr_dot; by looking at the\nimplementation we see that it creates a new W_NDimArray to store the\nresult, which means it has to do a malloc():\n\n\n\nThe implementation of the + self.constant part is also interesting:\ncontrary the former, the call to W_NDimArray.descr_add has been inlined by\nthe JIT, so we have a better picture of what's happening; in particular, we\ncan see the call to __0_alloc_with_del____ which allocates the\nW_NDimArray for the result, and the raw_malloc which allocates the\nactual array. Then we have a long list of 149 simple operations which set the\nfields of the resulting array, construct an iterator, and finally do a\ncall_assembler: this is the actual logic to do the addition, which was\nJITtted independently; call_assembler is one of the operations to do\nJIT-to-JIT calls:\n\n\n\nAll of this is very suboptimal: in this particular case, we know that the\nshape of self.matrix is always (3, 2): so, we are doing an incredible\namount of work, including calling\u00a0malloc() twice for the temporary arrays, just to\ncall two functions which ultimately do a total of 6 multiplications\nand 6 additions.  Note also that this is not a fault of the JIT: CPython+numpy\nhas to do the same amount of work, just hidden inside C calls.\n\nOne possible solution to this nonsense is a well known compiler optimization:\nloop unrolling.  From the compiler point of view, unrolling the loop is always\nrisky because if the matrix is too big you might end up emitting a huge blob\nof code, possibly uselss if the shape of the matrices change frequently: this\nis the main reason why the PyPy JIT does not even try to do it in this case.\n\nHowever, we know that the matrix is small, and always of the same\nshape. So, let's unroll the loop manually:\nclass SpecializedCreature(Creature):\n\n    def __init__(self, *args, **kwargs):\n        Creature.__init__(self, *args, **kwargs)\n        # store the data in a plain Python list\n        self.data = list(self.matrix.ravel()) + list(self.constant)\n        self.data_state = [0.0]\n        assert self.matrix.shape == (2, 3)\n        assert len(self.data) == 8\n\n    def run_step(self, inputs):\n        # state: [state_vars ... inputs]\n        # out_values: [state_vars, ... outputs]\n        k0, k1, k2, q0, q1, q2, c0, c1 = self.data\n        s0 = self.data_state[0]\n        z_sp, z = inputs\n        #\n        # compute the output\n        out0 = s0*k0 + z_sp*k1 + z*k2 + c0\n        out1 = s0*q0 + z_sp*q1 + z*q2 + c1\n        #\n        self.data_state[0] = out0\n        outputs = [out1]\n        return outputs\n\nIn the actual code there is also a sanity check which asserts that the\ncomputed output is the very same as the one returned by Creature.run_step.\n\nSo, let's try to see how it performs. First, with CPython:\n$ python -m ev.main\nGeneration   1: ... [population = 500]  [7.61 secs]\nGeneration   2: ... [population = 500]  [3.96 secs]\nGeneration   3: ... [population = 500]  [3.79 secs]\nGeneration   4: ... [population = 500]  [3.74 secs]\nGeneration   5: ... [population = 500]  [3.84 secs]\nGeneration   6: ... [population = 500]  [3.69 secs]\n\nThis looks good: 60% faster than the original CPython+numpy\nimplementation. Let's try on PyPy:\nGeneration   1: ... [population = 500]  [0.39 secs]\nGeneration   2: ... [population = 500]  [0.10 secs]\nGeneration   3: ... [population = 500]  [0.11 secs]\nGeneration   4: ... [population = 500]  [0.09 secs]\nGeneration   5: ... [population = 500]  [0.08 secs]\nGeneration   6: ... [population = 500]  [0.12 secs]\nGeneration   7: ... [population = 500]  [0.09 secs]\nGeneration   8: ... [population = 500]  [0.08 secs]\nGeneration   9: ... [population = 500]  [0.08 secs]\nGeneration  10: ... [population = 500]  [0.08 secs]\nGeneration  11: ... [population = 500]  [0.08 secs]\nGeneration  12: ... [population = 500]  [0.07 secs]\nGeneration  13: ... [population = 500]  [0.07 secs]\nGeneration  14: ... [population = 500]  [0.08 secs]\nGeneration  15: ... [population = 500]  [0.07 secs]\n\nYes, it's not an error. After a couple of generations, it stabilizes at around\n~0.07-0.08 seconds per generation. This is around 80 (eighty) times faster\nthan the original CPython+numpy implementation, and around 35-40x faster than\nthe naive PyPy+numpypy one.\n\nLet's look at the trace again: it no longer contains expensive calls, and\ncertainly no more temporary malloc() s. The core of the logic is between\nlines 386-416, where we can see that it does fast C-level multiplications and\nadditions: float_mul and float_add are translated straight into\nmulsd and addsd x86 instructions.\n\nAs I said before, this is a very particular example, and the techniques\ndescribed here do not always apply: it is not realistic to expect an 80x\nspeedup on arbitrary code, unfortunately. However, it clearly shows the potential of PyPy when\nit comes to high-speed computing. And most importantly, it's not a toy\nbenchmark which was designed specifically to have good performance on PyPy:\nit's a real world example, albeit small.\n\nYou might be also interested in the talk I gave at last EuroPython, in which I\ntalk about a similar topic: \"The Joy of PyPy JIT: abstractions for free\"\n(abstract, slides and video).\n\n\n\nHow to reproduce the results\n$ git clone https://github.com/antocuni/evolvingcopter\n$ cd evolvingcopter\n$ {python,pypy} -m ev.main --no-specialized --no-numpypy\n$ {python,pypy} -m ev.main --no-specialized\n$ {python,pypy} -m ev.main",
      "tags": "jit,profiling,speed",
      "url": "https://www.pypy.org/posts/2017/10/how-to-make-your-code-80-times-faster-1424098117108093942.html"
    },
    {
      "title": "(Cape of) Good Hope for PyPy",
      "text": "Hello from the other side of the world (for most of you)!\n\nWith the excuse of coming to PyCon ZA during the last two weeks Armin,\nRonan, Antonio and sometimes Maciek had a very nice and productive sprint in\nCape Town, as pictures show :). We would like to say a big thank you to\nKiwi.com, which sponsored part of the travel costs via its awesome Sourcelift\nprogram to help Open Source projects.\n\n\n\nArmin, Anto and Ronan at Cape Point\n\n\nArmin, Ronan and Anto spent most of the time hacking at cpyext, our CPython\nC-API compatibility layer: during the last years, the focus was to make it\nworking and compatible with CPython, in order to run existing libraries such\nas numpy and pandas. However, we never paid too much attention to performance,\nso the net result is that with the latest released version of PyPy, C\nextensions generally work but their speed ranges from \"slow\" to \"horribly\nslow\".\n\nFor example, these very simple microbenchmarks measure the speed of\ncalling (empty) C functions, i.e. the time you spend to \"cross the border\"\nbetween RPython and C.  (Note: this includes the time spent doing the loop in regular Python code.) These are the results on CPython, on PyPy 5.8, and on\nour newest in-progress version:\n\n$ python bench.py     # CPython\nnoargs      : 0.41 secs\nonearg(None): 0.44 secs\nonearg(i)   : 0.44 secs\nvarargs     : 0.58 secs\n\n\n\n$ pypy-5.8 bench.py   # PyPy 5.8\nnoargs      : 1.01 secs\nonearg(None): 1.31 secs\nonearg(i)   : 2.57 secs\nvarargs     : 2.79 secs\n\n\n\n$ pypy bench.py       # cpyext-refactor-methodobject branch\nnoargs      : 0.17 secs\nonearg(None): 0.21 secs\nonearg(i)   : 0.22 secs\nvarargs     : 0.47 secs\n\n\n\n\n\nSo yes: before the sprint, we were ~2-6x slower than CPython. Now, we are\nfaster than it!\nTo reach this result, we did various improvements, such as:\n\n\n\nteach the JIT how to look (a bit) inside the cpyext module;\nwrite specialized code for calling METH_NOARGS, METH_O and\nMETH_VARARGS functions; previously, we always used a very general and\nslow logic;\nimplement freelists to allocate the cpyext versions of int and\ntuple objects, as CPython does;\nthe cpyext-avoid-roundtrip branch: crossing the RPython/C border is\nslowish, but the real problem was (and still is for many cases) we often\ncross it many times for no good reason. So, depending on the actual API\ncall, you might end up in the C land, which calls back into the RPython\nland, which goes to C, etc. etc. (ad libitum).\n\n\nThe branch tries to fix such nonsense: so far, we fixed only some cases, which\nare enough to speed up the benchmarks shown above.  But most importantly, we\nnow have a clear path and an actual plan to improve cpyext more and\nmore. Ideally, we would like to reach a point in which cpyext-intensive\nprograms run at worst at the same speed of CPython.\n\nThe other big topic of the sprint was Armin and Maciej doing a lot of work on the\nunicode-utf8 branch: the goal of the branch is to always use UTF-8 as the\ninternal representation of unicode strings. The advantages are various:\n\n\n\ndecoding a UTF-8 stream is super fast, as you just need to check that the\nstream is valid;\nencoding to UTF-8 is almost a no-op;\nUTF-8 is always more compact representation than the currently\nused UCS-4. It's also almost always more compact than CPython 3.5 latin1/UCS2/UCS4 combo;\nsmaller representation means everything becomes quite a bit faster due to lower cache pressure.\n\n\nBefore you ask: yes, this branch contains special logic to ensure that random\naccess of single unicode chars is still O(1), as it is on both CPython and the\ncurrent PyPy.\nWe also plan to improve the speed of decoding even more by using modern processor features, like SSE and AVX. Preliminary results show that decoding can be done 100x faster than the current setup.\n\n\nIn summary, this was a long and profitable sprint, in which we achieved lots\nof interesting results. However, what we liked even more was the privilege of\ndoing commits from awesome places such as the top of Table Mountain:\n\n\n\nOur sprint venue today #pypy pic.twitter.com/o38IfTYmAV\n\u2014 Ronan Lamy (@ronanlamy) 4 ottobre 2017\n\n\n\n\n\nThe panorama we looked at instead of staring at cpyext code",
      "tags": "cpyext,profiling,speed,sprint,unicode",
      "url": "https://www.pypy.org/posts/2017/10/cape-of-good-hope-for-pypy-hello-from-3656631725712879033.html"
    },
    {
      "title": "PyPy v5.9 Released, Now Supports Pandas, NumPy",
      "text": "The PyPy team is proud to release both PyPy3.5 v5.9 (a beta-quality interpreter for Python\n3.5 syntax) and PyPy2.7 v5.9 (an interpreter supporting\nPython 2.7 syntax). \n\n\n\nNumPy and Pandas now work on PyPy2.7 (together with Cython 0.27.1). Many other modules\nbased on C-API extensions work on PyPy as well.\n\n\n\nCython 0.27.1 (released very recently) supports more projects with PyPy, both\non PyPy2.7 and PyPy3.5 beta. Note version 0.27.1 is now the minimum\nversion that supports this version of PyPy, due to some interactions with\nupdated C-API interface code.\n\n\n\n\nWe optimized the JSON parser for recurring string keys, which should decrease\nmemory use by up to 50% and increase parsing speed by up to 15% for large JSON files\nwith many repeating dictionary keys (which is quite common).\n\n\n\nCFFI, which is part of the PyPy release, has been updated to 1.11.1,\nimproving an already great package for interfacing with C. CFFI now supports\ncomplex arguments in API mode, as well as char16_t and char32_t and has\nimproved support for callbacks.\n\n\n\nIssues in the C-API compatibility layer that appeared as excessive memory\nuse were cleared up and other incompatibilities were resolved. The C-API\ncompatibility layer does slow down code which crosses the python-c interface\noften. Some fixes are in the pipelines for some of the performance issues, and we still recommend\nusing pure python on PyPy or interfacing via CFFI.\u00a0 \n\n\nPlease let us know if your use case is slow, we have ideas how to make things\nfaster but need real-world examples (not micro-benchmarks) of problematic code.\n\n\nWork sponsored by a Mozilla grant continues on PyPy3.5; we continue on the path to the goal of a complete python 3.5 implementation. Of course the bug fixes and performance enhancements\nmentioned above are part of both PyPy2.7 and PyPy3.5 beta.\n\n\nAs always, this release fixed many other issues and bugs raised by the\ngrowing community of PyPy users. We strongly recommend updating.\n\n\nYou can download the v5.9 releases here (note that we provide PyPy3.5 binaries for only Linux 64bit for now):\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors and contributors, and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on PyPy, or general help\nwith making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 (stdlib version 2.7.13), and CPython 3.5 (stdlib version 3.5.3). It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThe PyPy 2.7 release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux \n\n\n\n\nWhat else is new?\n\nPyPy 5.8 was released in June, 2017.\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2017/10/pypy-v59-released-now-supports-pandas-2261195727261691228.html"
    },
    {
      "title": "Let's remove the Global Interpreter Lock",
      "text": "Hello everyone\nThe Python community has been discussing removing the Global Interpreter Lock for\na long time.\nThere have been various attempts at removing it:\nJython or IronPython successfully removed it with the help of the underlying\nplatform, and some have yet to bear fruit, like gilectomy. Since our February sprint in Leysin,\nwe have experimented with the topic of GIL removal in the PyPy project.\nWe believe that the work done in IronPython or Jython can be reproduced with\nonly a bit more effort in PyPy. Compared to that, removing the GIL in CPython is a much\nharder topic, since it also requires tackling the problem of multi-threaded reference\ncounting. See the section below for further details.\nAs we announced at EuroPython, what we have so far is a GIL-less PyPy\nwhich can run very simple multi-threaded, nicely parallelized, programs.\nAt the moment, more complicated programs probably segfault. The\nremaining 90% (and another 90%) of work is with putting locks in strategic\nplaces so PyPy does not segfault during concurrent accesses to\ndata structures.\nSince such work would complicate the PyPy code base and our day-to-day work,\nwe would like to judge the interest of the community and the commercial\npartners to make it happen (we are not looking for individual\ndonations at this point).  We estimate a total cost of $50k,\nout of which we already have backing for about 1/3 (with a possible 1/3\nextra from the STM money, see below).  This would give us a good\nshot at delivering a good proof-of-concept working PyPy with no GIL. If we can get a $100k\ncontract, we will deliver a fully working PyPy interpreter with no GIL as a release,\npossibly separate from the default PyPy release.\nPeople asked several questions, so I'll try to answer the technical parts\nhere.\nWhat would the plan entail?\nWe've already done the work on the Garbage Collector to allow doing multi-\nthreaded programs in RPython.  \"All\" that is left is adding locks on mutable\ndata structures everywhere in the PyPy codebase. Since it would significantly complicate\nour workflow, we require real interest in that topic, backed up by\ncommercial contracts in order to justify the added maintenance burden.\nWhy did the STM effort not work out?\nSTM was a research project that proved that the idea is possible. However,\nthe amount of user effort that is required to make programs run in a\nparallelizable way is significant, and we never managed to develop tools\nthat would help in doing so.  At the moment we're not sure if more work\nspent on tooling would improve the situation or if the whole idea is really doomed.\nThe approach also ended up adding significant overhead on single threaded programs,\nso in the end it is very easy to make your programs slower.  (We have some money\nleft in the donation pot for STM which we are not using; according to the rules, we\ncould declare the STM attempt failed and channel that money towards the present\nGIL removal proposal.)\nWouldn't subinterpreters be a better idea?\nPython is a very mutable language - there are tons of mutable state and\nbasic objects (classes, functions,...) that are compile-time in other\nlanguage but runtime and fully mutable in Python.  In the end, sharing\nthings between subinterpreters would be restricted to basic immutable\ndata structures, which defeats the point. Subinterpreters suffers from the same problems as\nmultiprocessing with no additional benefits.\nWe believe that reducing mutability to implement subinterpreters is not viable without seriously impacting the\nsemantics of the language (a conclusion which applies to many other\napproaches too).\nWhy is it easier to do in PyPy than CPython?\nRemoving the GIL in CPython has two problems:\n\nhow do we guard access to mutable  data structures with locks and\nwhat to do with reference counting that needs to be guarded.\n\nPyPy only has the former problem; the latter doesn't exist,\ndue to a different garbage collector approach.  Of course the first problem\nis a mess too, but at least we are already half-way there. Compared to Jython\nor IronPython, PyPy lacks some data structures that are provided by JVM or .NET,\nwhich we would need to implement, hence the problem is a little harder\nthan on an existing multithreaded platform. However, there is good research\nand we know how that problem can be solved.\nBest regards,\nMaciej Fijalkowski",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/08/lets-remove-global-interpreter-lock-748023554216649595.html"
    },
    {
      "title": "Binary wheels for PyPy",
      "text": "Hi,\n\nthis is a short blog post, just to announce the existence of this Github repository, which contains binary PyPy wheels for some selected packages. The availability of binary wheels means that you can install the packages much more quickly, without having to wait for compilation.\n\n\nAt the moment of writing, these packages are available:\n\n\nnumpy\nscipy\npandas\npsutil\nnetifaces\n\n\nFor now, we provide only wheels built on Ubuntu, compiled for PyPy 5.8.\nIn particular, it is worth noting that they are not\u00a0manylinux1 wheels, which means they could not work on other Linux distributions. For more information, see the explanation in the README of the above repo.\n\nMoreover, the existence of the wheels does not guarantee that they work correctly 100% of the time. they still depend on cpyext, our C-API emulation layer, which is still work-in-progress, although it has become better and better during the last months. Again, the wheels are there only to save compilation time.\n\nTo install a package from the wheel repository, you can invoke pip like this:\n\n$ pip install --extra-index https://antocuni.github.io/pypy-wheels/ubuntu numpy\n\n\n\nHappy installing!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/07/binary-wheels-for-pypy-8718353804433344916.html"
    },
    {
      "title": "PyPy v5.8 released",
      "text": "The PyPy team is proud to release both PyPy2.7 v5.8 (an interpreter supporting\nPython 2.7 syntax), and a beta-quality PyPy3.5 v5.8 (an interpreter for Python\n3.5 syntax). The two releases are both based on much the same codebase, thus\nthe dual release.  Note that PyPy3.5 supports Linux 64bit only for now.\n\nThis new PyPy2.7 release includes the upstream stdlib version 2.7.13, and\nPyPy3.5 includes the upstream stdlib version 3.5.3.\n\nWe fixed critical bugs in the shadowstack rootfinder garbage collector\nstrategy that crashed multithreaded programs and very rarely showed up\neven in single threaded programs.\n\nWe added native PyPy support to profile frames in the vmprof statistical\nprofiler.\n\nThe struct module functions pack* and unpack* are now much faster,\nespecially on raw buffers and bytearrays. Microbenchmarks show a 2x to 10x\nspeedup. Thanks to Gambit Research for sponsoring this work.\n\nThis release adds (but disables by default) link-time optimization and\nprofile guided optimization of the base interpreter, which may make\nunjitted code run faster. To use these, translate with appropriate\noptions.  Be aware of issues with gcc toolchains, though.\n\nPlease let us know if your use case is slow, we have ideas how to make things\nfaster but need real-world examples (not micro-benchmarks) of problematic code.\n\nWork sponsored by a Mozilla grant continues on PyPy3.5; numerous fixes from\nCPython were ported to PyPy and PEP 489 was fully implemented. Of course the\nbug fixes and performance enhancements mentioned above are part of both PyPy\n2.7 and PyPy 3.5.\n\nCFFI, which is part of the PyPy release, has been updated to an unreleased 1.10.1,\nimproving an already great package for interfacing with C.\n\nAnyone using NumPy 1.13.0, must upgrade PyPy to this release since we implemented some previously missing C-API functionality. Many other c-extension modules now work with PyPy, let us know if yours does not.\n\nAs always, this release fixed many issues and bugs raised by the\ngrowing community of PyPy users. We strongly recommend updating.\n\nYou can download the v5.8 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors and contributors, and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on PyPy, or general help\nwith making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThe PyPy 2.7 release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux \n\n\n\n\n\nWhat else is new?\n\nPyPy 5.7 was released in March, 2017.\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release,sponsors",
      "url": "https://www.pypy.org/posts/2017/06/pypy-v58-released-739876359584854017.html"
    },
    {
      "title": "PyPy 5.7.1 bugfix released",
      "text": "We have released a bugfix PyPy2.7-v5.7.1 and PyPy3.5-v5.7.1 beta (Linux 64bit),\ndue to the following issues:\n\n\n\ncorrectly handle an edge case in dict.pop (issue 2508)\nfix a regression to correctly handle multiple inheritance in a C-API type\nwhere the second base is an app-level class with a __new__ function\nfix a regression to fill a C-API type\u2019s tp_getattr slot from a\n__getattr__ method (issue 2523)\n\n\n\nThanks to those who reported issues and helped test out the fixes\n\nYou can download the v5.7.1 release here:\n\n\nhttps://pypy.org/download.html\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThe PyPy 2.7 release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2017/04/pypy-571-bugfix-released-8519267986159880133.html"
    },
    {
      "title": "Native profiling in VMProf",
      "text": "We are happy to announce a new release for the PyPI package vmprof.\nIt is now able to capture native stack frames on Linux and Mac OS X to show you bottle necks in compiled code (such as CFFI modules, Cython or C Python extensions). It supports PyPy, CPython versions 2.7, 3.4, 3.5 and 3.6. Special thanks to Jetbrains for funding the native profiling support.\n\n\n\n\n\n\nWhat is vmprof?\n\nIf you have already worked with vmprof you can skip the next two section. If not, here is a short introduction:\n\nThe goal of vmprof package is to give you more insight into your program. It is a statistical profiler. Another prominent profiler you might already have worked with is cProfile. It is bundled with the Python standard library.\n\nvmprof's distinct feature (from most other profilers) is that it does not significantly slow down your program execution. The employed strategy is statistical, rather than deterministic. Not every function call is intercepted, but it samples stack traces and memory usage at a configured sample rate (usually around 100hz). You can imagine that this creates a lot less contention than doing work before and after each function call.\n\nAs mentioned earlier cProfile gives you a complete profile, but it needs to intercept every function call (it is a deterministic profiler). Usually this means that you have to capture and record every function call, but this takes an significant amount time.\n\n The overhead vmprof consumes is roughly 3-4% of your total program runtime or even less if you reduce the sampling frequency. Indeed it lets you sample and inspect much larger programs. If you failed to profile a large application with cProfile, please give vmprof a shot.\n\nvmprof.com or PyCharm\n\n\n\nThere are two major alternatives to the command-line tools shipped with vmprof:\n\nA web service on vmprof.com\nPyCharm Professional Edition \n\n\nWhile the command line tool is only good for quick inspections, vmprof.com\n and PyCharm compliment each other providing deeper insight into your \nprogram. With PyCharm you can view the per-line profiling results inside\n the editor. With the vmprof.com you get a\u00a0handy visualization of the profiling results as a flame chart and memory usage graph.\n\n\n\n\n\nSince the PyPy Team runs and maintains the service on vmprof.com (which is by the way free and open-source), I\u2019ll explain some more details here. On vmprof.com you can inspect the generated profile interactively instead of looking at console output. What is sent to vmprof.com? You can find details here.\n\nFlamegraph: Accumulates and displays the most frequent codepaths. It allows you to quickly and accurately identify hot spots in your code. The flame graph below is a very short run of richards.py (Thus it shows a lot of time spent in PyPy's JIT compiler).\n\n\n\n\n\nList all functions (optionally sorted): the equivalent of the vmprof command line output in the web.\n\n\n\n\n\u00a0Memory curve: A line plot that shows how how many MBytes have been consumed over the lifetime of your program (see more info in the section below).\n\n\n\nNative programs\n\nThe new feature introduced in vmprof 0.4.x allows you to look beyond the Python level. As you might know, Python maintains a stack of frames to save the execution. Up to now the vmprof profiles only contained that level of information. But what if you program jumps to native code (such as calling gzip compression on a large file)? Up to now you would not see that information.\n\nMany packages make use of the CPython C API (which we discurage, please lookup cffi for a better way to call C). Have you ever had the issue that you know that your performance problems reach down to, but you could not profile it properly? Now you can!\n\n Let's inspect a very simple Python program to find out why a program is significantly slower on Linux than on Mac:\n\nimport numpy as np\nn = 1000\na = np.random.random((n, n))\nb = np.random.random((n, n))\nc = np.dot(np.abs(a), b)\n\n\nTake two NxN random matrix objects and create a dot product. The first argument to the dot product provides the absolute value of the random matrix.\n\n\nRunPythonNumPyOSn=... Took \n [1]CPython 3.5.2NumPy 1.12.1Mac OS X, 10.12.3n=5000~9 sec\n [2]CPython 3.6.0NumPy 1.12.1Linux 64, Kernel 4.9.14n=1000~26 sec\n\n\nNote that the Linux machine operates on a 5 times smaller matrix, still it takes much longer. What is wrong? Is Linux slow? CPython 3.6.0? Well no, lets inspect and [1] and [2] (shown below in that order).\n\n\n\n\n\n[2] runs on Linux, spends nearly all of the time in PyArray_MatrixProduct2, if you compare to [1] on Mac OS X, you'll see that a lot of time is spent in generating the random numbers and the rest in cblas_matrixproduct.\n\nBlas has a very efficient implementation so you can achieve the same on Linux if you install a blas implementation (such as openblas).\n\nUsually you can spot potential program source locations that take a lot of time and might be the first starting point to resolve performance issues.\n\nBeyond Python programs \n\nIt is not unthinkable that the strategy can be reused for native programs. Indeed this can already be done by creating a small cffi wrapper around an entry point of a compiled C program. It would even work for programs compiled from other languages (e.g. C++ or Fortran). The resulting function names are the full symbol name embedded into either the executable symboltable or extracted from the dwarf debugging information. Most of those will be compiler specific and contain some cryptic information.\n\nMemory profiling\nWe thankfully received a code contribution from the company Blue Yonder. They have built a memory profiler (for Linux and Mac OS X) on top of vmprof.com that displays the memory consumption for the runtime of your process.\n\nYou can run it the following way:\n\n$ python -m vmprof --mem --web script.py\n\nBy adding --mem, vmprof will capture memory information and display it in the dedicated view on vmprof.com. You can view it by by clicking the 'Memory' switch in the flamegraph view.\n\nThere is more\n\nSome more minor highlights contained in 0.4.x:\n\nVMProf support for Windows 64 bit (No native profiling)\nVMProf can read profiles generated by another host system\nVMProf is now bundled in several binary wheel for fast and easy installation (Mac OS X, Linux 32/64 for CPython 2.7, 3.4, 3.5, 3.6)\n\nFuture plans - Profile Streaming\n\nvmprof has not reached the end of development. There are many features we could implement. But there is one feature that could be a great asset to many Python developers.\n\nContinuous delivery of your statistical profile, or in short, profile streaming. One of the great strengths of vmprof is that is consumes very little overhead. It is not a crazy idea to run this in production.\n\nIt would require a smart way to stream the profile in the background to vmprof.com and new visualizations to look at much more data your Python service produces.\n\nIf that sounds like a solid vmprof improvement, don't hesitate to get in touch with us (e.g. IRC #pypy, mailing list pypy-dev, or comment below)\n\nYou can help! \n\nThere are some immediate things other people could help with. Either by donating time or money (yes we have occasional contributors which is great)!\n\nWe gladly received code contribution for the memory profiler. But it was not enough time to finish the migration completely. Sadly it is a bit brittle right now.\nWe would like to spend more time on other visualizations. This should include to give a much better user experience on vmprof.com (like a tutorial that explains the visualization that we already have).\u00a0\nBuild Windows 32/64 bit wheels (for all CPython versions we currently support)\n\nWe are also happy to accept google summer of code projects on vmprof for new visualizations and other improvements. If you qualify and are interested, don't hesitate to ask!\n\nRichard Plangger (plan_rich) and the PyPy Team\n\n[1] Mac OS X https://vmprof.com/#/567aa150-5927-4867-b22d-dbb67ac824ac\n[2] Linux64 https://vmprof.com/#/097fded2-b350-4d68-ae93-7956cd10150c",
      "tags": "profiling,vmprof",
      "url": "https://www.pypy.org/posts/2017/04/native-profiling-in-vmprof-6949065546884243105.html"
    },
    {
      "title": "PyPy2.7 and PyPy3.5 v5.7 - two in one release",
      "text": "The PyPy team is proud to release both PyPy2.7 v5.7 (an interpreter supporting\nPython v2.7 syntax), and a beta-quality PyPy3.5 v5.7 (an interpreter for Python\nv3.5 syntax). The two releases are both based on much the same codebase, thus\nthe dual release.  Note that PyPy3.5 only supports Linux 64bit for now.\n\nThis new PyPy2.7 release includes the upstream stdlib version 2.7.13, and PyPy3.5 (our first in the 3.5 series) includes the upstream stdlib version 3.5.3.\n\nWe continue to make incremental improvements to our C-API compatibility layer (cpyext). PyPy2 can now import and run many C-extension packages, among the most notable are Numpy, Cython, and Pandas. Performance may be slower than CPython, especially for frequently-called short C functions. Please let us know if your use case is slow, we have ideas how to make things faster but need real-world examples (not micro-benchmarks) of problematic code.\n\nWork proceeds at a good pace on the PyPy3.5 version due to a grant from the Mozilla Foundation, hence our first 3.5.3 beta release. Thanks Mozilla !!! While we do not pass all tests yet, asyncio works and as these benchmarks show it already gives a nice speed bump. We also backported the f\"\" formatting from 3.6 (as an exception; otherwise \u201cPyPy3.5\u201d supports the Python 3.5 language).\n\nCFFI has been updated to 1.10, improving an already great package for interfacing with C.\n\nWe now use shadowstack as our default gcrootfinder even on Linux. The alternative, asmgcc, will be deprecated at some future point. While about 3% slower, shadowstack is much more easily maintained and debuggable. Also, the performance of shadowstack has been improved in general: this should close the speed gap between other platforms and Linux.\n\nAs always, this release fixed many issues and bugs raised by the growing community of PyPy users. We strongly recommend updating.\n\nYou can download the v5.7 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n\n\u00a0\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and CPython 3.5. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThe PyPy 2.7 release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\n\n\u00a0\n\nWhat else is new?\n\n(since the releases of PyPy 2.7 and 3.3 at the end of 2016)\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2017/03/pypy27-and-pypy35-v57-two-in-one-release-4736633226245374150.html"
    },
    {
      "title": "Leysin Winter Sprint Summary",
      "text": "Today\n is the last day of our yearly sprint event in Leysin. We had lots of \nideas on how to enhance the current state of PyPy, we went skiing and \nhad interesting discussions around virtual machines, the Python \necosystem, and other real world problems.\n\u00a0\n\n\nWhy don't you join us next time?\n\n\nA usual PyPy sprints day goes through the following stages:\n\n\n\n\n\u00a0Planning Session: Tasks from previous days that have seen progress or \nare completed are noted in a shared document. Everyone adds new tasks \nand then assigns themselves to one or more tasks (usually in pairs). As \nsoon as everybody is happy with their task and has a partner to work \nwith, the planning session is concluded and the work can start.\nDiscussions: A sprint is a good occasion to discuss difficult \nand important topics in person. We usually sit down in a separate area \nin the sprint room and discuss until a) nobody wants to discuss anymore \nor b) we found a solution to the problem. The good thing is that usally \nthe outcome is b).\nLunch: For lunch we prepare sandwiches and other finger food.\nContinue working until dinner, which we eat at a random restaurant in Leysin.\nGoto 1 the next day, if sprint has not ended.\n\n\n\nSprints\n are open to everybody and help newcomers to get started with PyPy (we usually\n pair you with a developer familiar with PyPy). They are perfect to \ndiscuss and find solutions to problems we currently face. If you are \neager to join next year, please don't hesitate to register next year \naround January.\n\n\u00a0\n\n\nSprint Summary\u00a0 \u00a0\nSprint goals included to work on the following topics: \n\n\nWork towards releasing PyPy 3.5 (it will be released soon)\nCPython Extensionsion (CPyExt) modules on PyPy\nHave fun in winter sports (a side goal)\n\n\n\n\nHighlights\n\n\n\n\n\n\n\nWe have spent lots of time debugging and fixing memory issues on CPyExt.\n In particular, we fixed a serious memory leak where taking a memoryview\n would prevent numpy arrays from ever being freed. More work is still required to ensure that our GC always releases arrays in a timely \nmanner.\nFruitful discussions and progress about how to flesh out some details about the unicode representation in PyPy. Our current goal is to use utf-8 as the unicode representation internally and have fast vectorized operations (indexing, check if valid, ...).\nPyPy will participate in GSoC 2017 and we will try to allocate more resources to that than last year.\nProfile and think about some details how to reduce the starting size of the interpreter. The starting point would be to look at the parser and reduce the amount of strings to keep alive.\nFound a topic for a student's master thesis: correctly freeing cpyext reference cycles.\nRun lots of Python3 code on top of PyPy3 and resolve issues we found along the way.\nInitial work on making RPython thread-safe without a GIL.\n\n\n\n\nList of attendees\n\n\n- Stefan Beyer\n\n- Antonio Cuni\n\n- Maciej Fijalkowski\n\n- Manuel Jacob\n\n- Ronan Lamy\n\n- Remi Meier\n\n- Richard Plangger\n\n- Armin Rigo\n\n- Robert Zaremba\n\n\u00a0\n\n\u00a0 \n\n\n\n\n\n\n\n\nWe\n would like to thank our donors for the continued support of the PyPy \nproject and we looking forward to next years sprint in Leysin.\n\n\n\n\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/03/leysin-winter-sprint-summary-4587213628578490701.html"
    },
    {
      "title": "Async HTTP benchmarks on PyPy3",
      "text": "Hello everyone,\n\n\n\nSince Mozilla announced funding, we've been working quite hard on delivering you a working Python 3.5.\n\n\u00a0\n\nWe are almost ready to release an alpha version of PyPy 3.5. Our goal is to release it shortly after the sprint. Many modules have already been ported and\u00a0 it can probably run many Python 3 programs already. We are happy to receive any feedback after the next release.\u00a0 \n\n\n\nTo show that the heart (asyncio) of Python 3 is already working we have prepared some benchmarks. They are done by Pawe\u0142 Piotr Przeradowski @squeaky_pl for a HTTP workload on serveral asynchronous IO libraries, namely the relatively new asyncio and curio libraries and the battle-tested tornado, gevent and Twisted libraries. To see the benchmarks check out https://github.com/squeaky-pl/zenchmarks and the instructions for reproducing can be found inside README.md in the repository. Raw results can be obtained from https://github.com/squeaky-pl/zenchmarks/blob/master/results.csv.\n\n\n\nThe\n purpose of the presented benchmarks is showing that the upcoming PyPy release \nis already working with unmodified code that runs on CPython 3.5. PyPy \nalso manages to make them run significantly faster.\n\n\n\nThe\n benchmarks consist of HTTP servers implemented on the top of the mentioned \nlibraries. All the servers are single-threaded relying on underlying \nevent loops to provide concurrency. Access logging was disabled to \nexclude terminal I/O from the results. The view code consists of a \nlookup in a dictionary mapping ASCII letters to verses from the famous \nZen of Python. If a verse is found the view returns it, otherwise a 404 \nNot Found response is served. The 400 Bad Request and 500 Internal \nServer Error cases are also handled.\n\n\n\nThe workload was generated with the wrk HTTP benchmarking tool. It is run with one thread opening up to 100 \nconcurrent connections for 2 seconds and repeated 1010 times to get \nconsecutive measures. There is a Lua script provided\n that instructs wrk to continuously send 24 different requests that hit \ndifferent execution paths (200, 404, 400) in the view code. Also it is \nworth noting that wrk will only count 200 responses as successful so the actual request per second throughput is higher.\n\n\n\nFor your convenience all the used libraries versions are vendored into the benchmark repository. There is also a precompiled portable version of wrk provided\n that should run on any reasonably recent (10 year old or newer) Linux \nx86_64 distribution. The benchmark was performed on a public cloud scaleway x86_64 server launched in a Paris data center. The server was running \nUbuntu 16.04.01 LTS and reported Intel(R) Xeon(R) CPU D-1531 @ 2.20GHz \nCPU. CPython 3.5.2 (shipped by default in Ubuntu) was benchmarked \nagainst a pypy-c-jit-90326-88ef793308eb-linux64 snapshot of the 3.5 compatibility branch of PyPy.\n\n\n\n\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nWe want to thank Mozilla for supporting our work!\n\n\n\nCheers,\n\nfijal, squeaky_pl and the PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/03/async-http-benchmarks-on-pypy3-1092124994927894138.html"
    },
    {
      "title": "Leysin Winter Sprint: 25/26th Feb. - 4th March 2017",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the twelveth time.\nThis is a fully public sprint: newcomers and topics other than those\nproposed below are welcome.\n\nGoals and topics of the sprint\nThe list of topics is very open.\n\nThe main topic is Python 3.5 support in PyPy, as most py3.5\ncontributors should be present.  It is also a good topic if you have\nno or limited experience with PyPy contribution: we can easily find\nsomething semi-independent that is not done in py3.5 so far, and\ndo pair-programming with you.\nAny other topic is fine too: JIT compiler optimizations, CFFI,\nthe RevDB reverse debugger, improving to speed of your program on\nPyPy, etc.\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off (for ski or anything else).\n\n\n\nExact times\nWork days: starting 26th Feb (~noon), ending March 4th (~noon).\nI have pre-booked the week from Saturday Feb 25th to Saturday March 4th.\nIf it is possible for you to arrive Sunday before mid-afternoon, then\nyou should get a booking from Sunday only.  The break day should be\naround Wednesday.\nIt is fine to stay a few more days on either side, or conversely to book\nfor a part of that time only.\n\n\nLocation & Accomodation\n\nLeysin, Switzerland, \"same place as before\".\n\n\n\nLet me refresh your\nmemory: both the sprint venue and the lodging will be in a\npair of chalets built specifically for bed & breakfast:\nhttps://www.ermina.ch/.  The place has a good ADSL Internet connection\nwith wireless installed.  You can also arrange your own lodging\nelsewhere (as long as you are in Leysin, you cannot be more than a 15\nminutes walk away from the sprint venue).\nPlease confirm that you are coming so that we can adjust the\nreservations as appropriate.\nThe options of rooms are a bit more limited than on previous years\nbecause the place for bed-and-breakfast is shrinking; but we should\nstill have enough room for us.  The price is around 60 CHF, breakfast\nincluded, in shared rooms (3 or 4 people).  If there are people that\nwould prefer a double or single room, please contact me and we'll see\nwhat choices you have.  There are also a choice of hotels in Leysin.\nPlease register by Mercurial:\n\nhttps://bitbucket.org/pypy/extradoc/\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/leysin-winter-2017/\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\nYou need a Swiss-to-(insert country here) power adapter.  There will be\nsome Swiss-to-EU adapters around, and at least one EU-format power strip.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2017/01/leysin-winter-sprint-2526th-feb-4th-3831779797804484935.html"
    },
    {
      "title": "PyPy2.7 v5.6 released - stdlib 2.7.12 support, C-API improvements, and more",
      "text": "We have released PyPy2.7 v5.6 [0], about two months after PyPy2.7 v5.4. This new PyPy2.7 release includes the upstream stdlib version 2.7.12.\n\nWe continue to make incremental improvements to our C-API compatibility layer (cpyext). We pass all but 12 of the over-6000 tests in the upstream NumPy test suite, and have begun examining what it would take to support Pandas and PyQt. \n\nWork proceeds at a good pace on the PyPy3.5 version due to a grant from the Mozilla Foundation, and some of those changes have been backported to PyPy2.7 where relevant.\n\nThe PowerPC and s390x backend have been enhanced with the capability to use SIMD instructions for micronumpy loops.\n\nWe changed timeit to now report average +/- standard deviation, which is better than the misleading minimum value reported in CPython.\n\nWe now support building PyPy with OpenSSL 1.1 in our built-in _ssl module, as well as maintaining support for previous versions.\n\nCFFI has been updated to 1.9, improving an already great package for interfacing with C.\n\nAs always, this release fixed many issues and bugs raised by the growing community of PyPy users. We strongly recommend updating. You can download the PyPy2.7 v5.6 release here:\n\n\nhttps://pypy.org/download.html\n\nDownstream packagers have been hard at work. The Debian package is already available, and the portable PyPy versions are also ready, for those who wish to run PyPy on other Linux distributions like RHEL/Centos 5.\n\nWe would like to thank our donors for the continued support of the PyPy project.\n\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThis release supports:\n\n\n\nx86 machines on most common operating systems (Linux 32/64 bits, Mac OS X 64 bits, Windows 32 bits, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\n\n\nWhat else is new?\n\n(since the release of PyPy 5.4 in August, 2016)\n\nThere are many incremental improvements to RPython and PyPy, the complete listing is here.\n\n\u00a0 \nPlease update, and continue to help us make PyPy better.\n\nCheers, The PyPy team\n\n[0] We skipped 5.5 since we share a code base with PyPy3, and PyPy3.3-v.5.5-alpha was released last month",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/11/pypy27-v56-released-stdlib-2712-support-5671090852400583673.html"
    },
    {
      "title": "Vectorization extended. PowerPC and s390x",
      "text": "We are happy to announce that JIT support in both the PowerPC backend and the\ns390x backend have been enhanced. Both can now vectorize loops via SIMD\ninstructions. Special thanks to IBM for funding this work.\n\nIf you are not familiar with this topic you can read more details\u00a0here.\n\n\nThere are many more enhancements under the hood. Most notably, all pure operations are now delayed until the latest possible point. In some cases indices have been calculated more than once or they needed an additional register, because the old value is still used. Additionally it is now possible to load quadword-aligned memory in both PPC and s390x (x86 currently cannot do that).\n\nNumPy & CPyExt\nThe community and core developers have been moving CPyExt towards a complete, but emulated, layer for CPython C extensions. This is great, because the one restriction preventing the wider deployment of PyPy in several scenarios will hopefully be removed. However, we advocate not to use CPyExt, but rather to not write C code at all (let PyPy speed up your Python code) or use cffi.\n\nThe work done here to support vectorization helps micronumpy (NumPyPy) to speed up operations for PPC and s390x. So why is PyPy supporting both NumPyPy and NumPy, do we actually need both? Yes, there are places where gcc can beat the JIT, and places where the tight integration between NumPyPy and PyPy is more performant. We do have plans to integrate both, hijacking the C-extension method calls to use NumPyPy where we know NumPyPy can be faster.\n\nJust to give you an idea why this is a benefit:\n\nNumPy arrays can carry custom dtypes and apply user defined python functions on the arrays. How could one optimize this kind of scenario? In a traditional setup, you cannot. But as soon as NumPyPy is turned on, you can suddenly JIT compile this code and vectorize it.\n\nAnother example is element access that occurs frequently, or any other calls that cross between Python and the C level frequently.\n\nBenchmarks\nLet's have a look at some benchmarks reusing\u00a0mikefc's numpy benchmark suite\u00a0(find the forked version here).\u00a0I only ran a subset of microbenchmarks, showing that the core functionality is\nfunctioning properly. Additionally it has been rewritten to use\u00a0perf\u00a0instead of the timeit stdlib module.\n\n\nSetup\nx86 runs on a Intel i7-2600 clocked at 3.40GHz using 4 cores. PowerPC runs on the Power 8 clocked at 3.425GHz providing 160 cores. Last but not least the mainframe machine clocked up to 4 GHz, but fully virtualized (as it is common for such machines). Note that PowerPC is a non private remote machine. It is used by many users and it is crowded with processes. It is hard to extract a stable benchmark there.\n\nx86 ran on Fedora 24 (kernel version of 4.8.4), PPC ran on Fedora 21 (kernel version 3.17.4) and s390x ran on Redhat Linux 7.2 (kernel version 3.10.0). Respectivley, numpy on cpython had openblas available on x86, no blas implementation were present on s390x and PPC provided blas and lapack.\n\nAs you can see all machines run very different configurations. It does not make sense to compare across platforms, but rather implementations on the same platform.\n\n\n\n\n\n\n\nBlue shows CPython 2.7.10+ available on that platform using the latest NumPy (1.11). Micro NumPy is used for PyPy. PyPy+ indicates that the vectorization optimization is turned on.\nAll bar charts show the median value of all runs (5 samples, 100 loops, 10 inner loops, for the operations on vectors (not matrices) the loops are set to 1000). PyPy additionally gets 3 extra executions to warmup the JIT.\n\nThe comparison is really comparing speed of machine code. It compares the PyPy's JIT output vs GCC's output. It has little to do with the speed of the interpreter.\n\nBoth new SIMD backends speedup the numeric kernels. Some times it is near to the speed of CPython, some times it is faster. The maximum parallelism very much depends on the extension emitted by the compiler. All three SIMD backends have the same vector register size (which is 128 bit). This means that all three behave similar but ppc and s390x gain more because they can load 128bit of memory from quadword aligned memory.\n\n\nFuture directions\nPython is achieving rapid adoption in data science. This is currently a trend emerging in Europe, and Python is already heavily used for data science in the USA many other places around the world.\n\n\nPyPy can make a valuable contribution for data scientists, helping them to rapidly write scientific programs in Python and run them at near native speed. If you happen to be in that situation, we are eager to hear you feedback or resolve your issues and also work together to improve the performance of your,\ncode. Just get in touch!\n\n\nRichard Plangger (plan_rich) and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/11/vectorization-extended-powerpc-and-s390x-4042433015460084057.html"
    },
    {
      "title": "PyPy3 5.5.0 released",
      "text": "We're pleased to announce the release of PyPy3 v5.5.0. Coming four months after PyPy3.3 v5.2, it improves compatibility with Python 3.3 (3.3.5). We strongly recommend updating from previous PyPy3 versions.\n\nWe would like to thank all of the people who donated to the py3k proposal for supporting the work that went into this release.\n\nYou can download the PyPy3.3 v5.5.0 release here:\u00a0https://pypy.org/download.html\n\nImproved Python 3.3.5 support.\n\nos.get_terminal_size(), time.monotonic(), str.casefold()\u00a0\nfaulthandler module\nThere are still some missing features such as a PEP 393-like space efficient string representation and including performance regressions (e.g. issue #2305). The focus for this release has been updating to 3.3 compatibility. Windows is also not yet supported.\n\nensurepip is also included (it's only included in CPython 3 >= 3.4).\nBuffer interface improvements (numpy on top of cpyext)\nSeveral JIT improvements (force-virtual-state, residual calls)\nSearch path for libpypy-c.so has changed (helps with cffi embedding on linux distributions)\nImprove the error message when the user forgot the \"self\" argument of a method\nMany more small improvements, please head over to our documentation for more information\n\n\nTowards Python 3.5\n\n\nWe have started to work on Python 3.5, which is a version used by many software projects. It seems to get wide adoption. We are happy to be part of the\u00a0Mozilla Open Source Support (MOSS) initiative.\n\n\n\nNevertheless we want to give our users the chance to use PyPy in their Python 3 projects, thus we have prepared this release.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\u00a0CPython 2.7.10 and 3.3.5. It's fast due to its integrated tracing JIT\u00a0compiler.\n We also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems except Windows\u00a0\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux\u00a0\nbig- and little-endian variants of PPC64 running Linux\u00a0\ns390x running Linux\n\nPlease try it out and let us know what you think. We welcome feedback, we know\nyou are using PyPy, please tell us about it!\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/10/pypy3-550-released-8069558680221199646.html"
    },
    {
      "title": "RevDB released, v5.4.1",
      "text": "Hi all,\n\n\nThe first beta version of RevDB is out!  Remember that RevDB is a reverse debugger for Python.  The idea is that it is a debugger that can run forward and backward in time, letting you more easily understand your subtle bug in your big Python program.\n\n\nRevDB should work on almost any Python program.  Even if you are normally only using CPython, trying to reproduce the bug with RevDB is similar to trying to run the program on a regular PyPy---usually it just works, even if not quite always.\n\n\nNews from the alpha version in the previous blog post include notably support for:\n\nThreads.\nCPyExt, the compatibility layer of PyPy that can run CPython C extension modules.\n\nas well as many other improvements.\n\n\nYou need to build it yourself for now.  It is tested on 64-bit Linux.  32-bit Linux, OS/X, and other POSIX platforms should all either work out of the box or be just a few fixes away (contributions welcome).  Win32 support is a lot more involved but not impossible.\n\n\nSee https://bitbucket.org/pypy/revdb/ for more information!\n\nArmin",
      "tags": "releaserevdb",
      "url": "https://www.pypy.org/posts/2016/09/revdb-released-v541-6719768292347391304.html"
    },
    {
      "title": "PyPy 5.4.1 bugfix released",
      "text": "We have released a bugfix for PyPy2.7-v5.4.0, released last week, due to the following issues:\n\n\n\nUpdate list of contributors in documentation and LICENSE file, this was unfortunately left out of 5.4.0. My apologies to the new contributors\nAllow tests run with -A to find libm.so even if it is a script not a dynamically loadable file\nBump sys.setrecursionlimit() when translating PyPy, for translating with CPython\nTweak a float comparison with 0 in backendopt.inline to avoid rounding errors\nFix for an issue for translating the sandbox\nFix for and issue where unicode.decode('utf8', 'custom_replace') messed up the last byte of a unicode string sometimes\nUpdate built-in cffi to version 1.8.1\nExplicitly detect that we found as-yet-unsupported OpenSSL 1.1, and crash translation with a message asking for help porting it\nFix a regression where a PyBytesObject was forced (converted to a RPython object) when not required, reported as issue #2395\n\n\nThanks to those who reported the issues.\n\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\nPlease update, and continue to help us make PyPy better.\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/09/pypy-541-bugfix-released-3217566297258542810.html"
    },
    {
      "title": "PyPy2 v5.4 released - incremental improvements and enhancements",
      "text": "We have released PyPy2.7 v5.4, a little under two months after PyPy2.7 v5.3.\nThis new PyPy2.7 release includes incremental improvements to our C-API\ncompatibility layer (cpyext), enabling us to pass over 99% of the upstream\nnumpy test suite.\n\nWe updated built-in cffi support to version 1.8,\nwhich now supports the \u201climited API\u201d mode for c-extensions on\nCPython >=3.2.\n\n\nWe improved tooling for the PyPy JIT, and expanded VMProf\nsupport to OpenBSD and Dragon Fly BSD\n\n\nAs always, this release fixed many issues and bugs raised by the\ngrowing community of PyPy users. We strongly recommend updating.\n\n\nYou can download the PyPy2 v5.4 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for their continued support of the PyPy\nproject. We would also like to thank our contributors and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, testing and adapting popular modules to run on PyPy, or general help\nwith making RPython\u2019s JIT even better.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (PyPy and CPython 2.7 performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux\nbig- and little-endian variants of PPC64 running Linux\ns390x running Linux\n\n\n\nWhat is New?\n\n(since the release of PyPy 5.3 in June, 2016)\nThere are many incremental improvements to RPython and PyPy, the complete listing is here. Mozilla generously sponsored work toward python 3.5 compatibility, and we are beginning to see some cross-over improvements of RPython and PyPy2.7 as a result.\n\nPlease update, and continue to help us make PyPy better.\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/08/pypy2-v54-released-incremental-3611318295736669599.html"
    },
    {
      "title": "PyPy Tooling Upgrade: JitViewer and VMProf",
      "text": "We are happy to announce a major JitViewer (JV) update.\nJV allows you to inspect RPython's internal compiler representation (the language in which PyPy is implemented) including the generated machine code of your program. It can graphically show you details of the JIT compiled code and helps you pinpoint issues in your program.\n\nVMProf is a statistical CPU profiler for python imposing very little overhead at runtime.\n\nBoth VMProf and JitViewer share a common goal: Present useful information for your python program.\nThe combination of both can reveal more information than either alone.\nThat is the reason why they are now both packaged together.\nWe also updated vmprof.com\u00a0with various bug fixes and changes including an all new interface to JV.\n\nThis work was done with the goal of improving tooling and libraries around the Python/PyPy/RPython ecosystem.\nSome of the tools we have developed:\n\n\nCFFI - Foreign Function Interface that avoids CPyExt (CFFI docs)\nRevDB - A reverse debugger for python (RevDB blog post)\n\n\nand of course the tools we discuss here:\n\n\nVMProf - A statistical CPU profiler (VMProf docs)\nJitViewer - Visualization of the log file produced by RPython (JitLog docs)\n\n\n\nA \"brand new\" JitViewer\n\nJitViewer has two pieces: you create a log file when running your program, and then use a graphic tool to view what happened.\n\nThe old logging format was a hard-to-maintain, plain-text-logging facility. Frequent changes often broke internal tools.\nAdditionally, the logging output of a long running program required a lot of disk space.\n\nOur new binary format encodes data densely, makes use of some compression (gzip), and tries to remove repetition where possible.\nIt also supports versioning for future proofing and can be extended easily.\n\nAnd *drumroll* you no longer need to install a tool to view the log yourself\nanymore! The whole system moved to vmprof.com and you can use it any time.\n\nSounds great. But what can you do with it? Here are two examples for a PyPy user:\n\nPyPy crashed? Did you discover a bug?\n\nFor some hard to find bugs it is often necessary to look at the compiled code. The old\nprocedure often required you to upload a plain text file which was hard to parse and to look through.\n\nA better way to share a crash report is to install the ``vmprof`` module from PyPi and execute either of the two commands:\n\n# this program does not crash, but has some weird behaviour\n$ pypy -m jitlog --web <your program args>\n...\nPyPy Jitlog: https://vmprof.com/#/<hash>/traces\n# this program segfaults\n$ pypy -m jitlog -o /tmp/log <your program args>\n...\n<Segfault>\n$ pypy -m jitlog --upload /tmp/log\nPyPy Jitlog: https://vmprof.com/#/<hash>/traces\n\n\nProviding the link in the bug report allows PyPy developers to browse and identify potential issues.\n\n\nSpeed issues\n\nVMProf is a great tool to find hot spots that consume a lot of time in your program. As soon as you have identified code that runs slowly, you can switch to jitlog and maybe pinpoint certain aspects that do not behave as expected. You will find an overview, and are able to browse the generated code. If you cannot make sense of all that, you can just share the link with us and we can have a look too.\n\nFuture direction\n\nWe hope that the new release will help both PyPy developers and PyPy users resolve potential issues and easily point them out.\n\nHere are a few ideas what might come in the next few releases:\n\n\n\n\u00a0Combination of CPU profiles and the JITLOG (sadly did not make it into the current release).\nExtend vmprof.com to be able to query vmprof/jitlog. An example query for vmprof: 'methods.callsites() > 5' andfor the jitlog would be 'traces.contains('call_assembler').hasbridge('*my_func_name*')'.\nExtend the jitlog to capture the information of the optimization stage.\n\n\n\nRichard Plangger (plan_rich) and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/08/pypy-tooling-upgrade-jitviewer-and-5107430577468391432.html"
    },
    {
      "title": "PyPy gets funding from Mozilla for Python 3.5 support",
      "text": "\"Python 2.x versus Python 3.x\": this is by now an old question.  In the eyes of some people Python 2 is here to stay, and in the eyes of others Python has long been 3 only.\n\nPyPy's own position is that PyPy will support Python 2.7 forever---the RPython language in which PyPy is written is a subset of  2.7, and we have no plan to upgrade that.  But at the same time, we want to support 3.x.  This is particularly true now: a relatively recent development is that Python 3.5 seems to attract more and more people.  The \"switch\" to Python 3.x might be starting to happen.\n\nCorrespondingly, PyPy has been searching for a while for a way to support a larger-scale development effort.  The goal is to support not just any old version of Python 3.x, but Python 3.5, as this seems to be the version that people are switching to.  PyPy is close to supporting all of Python 3.3 now; but the list of what is new in Python 3.4 and 3.5 is far, far longer than anyone imagines.  The long-term goal is also to get a version of \"PyPy3\" that is as good as \"PyPy2\" is, including its performance and its cpyext layer (CPython C API interoperability), for example.\n\nSo, the end result: Mozilla recently decided to award $200,000 to Baroque Software to work on PyPy as part of its Mozilla Open Source Support (MOSS) initiative.  This money will be used to implement the Python 3.5 features in PyPy. Within the next year, we plan to use the money to pay four core PyPy developers half-time to work on the missing features and on some of the big performance and cpyext issues. This should speed up the progress of catching up with Python 3.x significantly. We are extremely thankful to Mozilla for supporting us in this way, and will keep you updated on the progress via this blog.",
      "tags": "sponsors",
      "url": "https://www.pypy.org/posts/2016/08/pypy-gets-funding-from-mozilla-for-5569307998787871200.html"
    },
    {
      "title": "Reverse debugging for Python",
      "text": "RevPDB\nA \"reverse debugger\" is a debugger where you can go forward and\nbackward in time.  It is an uncommon feature, at least in the open\nsource world, but I have no idea why.  I have used undodb-gdb and\nrr, which are reverse debuggers for C code, and I can only say that\nthey saved me many, many days of poking around blindly in gdb.\nThe PyPy team is pleased to give you \"RevPDB\", a reverse-debugger\nsimilar to rr but for Python.\nAn example is worth a thousand words.  Let's say your big Python\nprogram has a bug that shows up inconsistently.  You have nailed it\ndown to something like:\n\nstart x.py, which does stuff (maybe involving processing files,\nanswering some web requests that you simulate from another terminal,\netc.);\nsometimes, after a few minutes, your program's state becomes\ninconsistent and you get a failing assert or another exception.\n\nThis is the case where RevPDB is useful.\nRevPDB is available only on 64-bit Linux and OS/X right now, but should\nnot be too hard to port to other OSes.  It is very much alpha-level!\n(It is a debugger full of bugs.  Sorry about that.)  I believe it is\nstill useful---it helped me in one real use case already.\n\n\nHow to get RevPDB\nThe following demo was done with an alpha version for 64-bit Linux,\ncompiled for Arch Linux.  I won't provide the binary; it should be\neasy enough to retranslate (much faster than a regular PyPy because it\ncontains neither a JIT nor a custom GC).  Grab the PyPy sources from\nMercurial, and then:\n\nhg update reverse-debugger\n# or \"hg update ff376ccacb36\" for exactly this demo\ncd pypy/goal\n../../rpython/bin/rpython -O2 --revdb targetpypystandalone.py  \\\n                  --withoutmod-cpyext --withoutmod-micronumpy\n\nand possibly rename the final pypy-c to pypy-revdb to avoid\nconfusion.\nOther platforms than 64-bit Linux and OS/X need some fixes before they work.\n\n\nDemo\nFor this demo, we're going to use this x.py as the \"big program\":\n\nimport os\n\nclass Foo(object):\n    value = 5\n\nlst1 = [Foo() for i in range(100)]\nlst1[50].value += 1\nfor x in lst1:\n    x.value += 1\n\nfor x in lst1:\n    if x.value != 6:\n        print 'oops!'\n        os._exit(1)\n\nOf course, it is clear what occurs in this small example: the check\nfails on item 50.  For this demo, the check has been written with\nos._exit(1), because this exits immediately the program.  If it\nwas written with an assert, then its failure would execute things\nin the traceback module afterwards, to print the traceback; it\nwould be a minor mess just to find the exact point of the failing\nassert.  (This and other issues are supposed to be fixed in the\nfuture, but for now it is alpha-level.)\nAnyway, with a regular assert and a regular post-mortem pdb,\nwe could observe that x.value is indeed 7 instead of 6 when the\nassert fails.  Imagine that the program is much bigger: how would we\nfind the exact chain of events that caused this value 7 to show up on\nthis particular Foo object?  This is what RevPDB is for.\nFirst, we need for now to disable Address Space Layout Randomization\n(ASLR), otherwise replaying will not work.  This is done once with the\nfollowing command line, which changes the state until the next\nreboot:\n\necho 0 | sudo tee /proc/sys/kernel/randomize_va_space\n\nUPDATE: the above is no longer necessary from revision ff376ccacb36.\nRun x.py with RevPDB's version of PyPy instead of the regular\ninterpreter (CPython or PyPy):\n\nPYPYRDB=log.rdb ./pypy-revdb x.py\n\nThis pypy-revdb executable is like a slow PyPy executable, running\n(for now) without a JIT.  This produces a file log.rdb which\ncontains a complete log of this execution.  (If the bug we are\ntracking occurs rarely, we need to re-run it several times until we\nget the failure.  But once we got the failure, then we're done with\nthis step.)\nStart:\n\nrpython/translator/revdb/revdb.py log.rdb\n\nWe get a pdb-style debugger.  This revdb.py is a normal Python\nprogram, which you run with an unmodified Python; internally, it looks\ninside the log for the path to pypy-revdb and run it as needed (as\none forking subprocess, in a special mode).\nInitially, we are at the start of the program---not at the end, like\nwe'd get in a regular debugger:\n\nFile \"<builtin>/app_main.py\", line 787 in setup_bootstrap_path:\n(1)$\n\nThe list of commands is available with help.\nGo to the end with continue (or c):\n\n(1)$ continue\nFile \"/tmp/x.py\", line 14 in <module>:\n...\n  lst1 = [Foo() for i in range(100)]\n  lst1[50].value += 1\n  for x in lst1:\n      x.value += 1\n\n  for x in lst1:\n      if x.value != 6:\n          print 'oops!'\n>         os._exit(1)\n(19727)$\n\nWe are now at the beginning of the last executed line.  The number\n19727 is the \"time\", measured in number of lines executed.  We can go\nbackward with the bstep command (backward step, or bs), line\nby line, and forward again with the step command.  There are also\ncommands bnext, bcontinue and bfinish and their forward\nequivalents.  There is also \"go TIME\" to jump directly to the specified\ntime.  (Right now the debugger only stops at \"line start\"\nevents, not at function entry or exit, which makes some cases a bit\nsurprising: for example, a step from the return statement of\nfunction foo() will jump directly to the caller's caller, if the\ncaller's current line was return foo() + 2, because no \"line\nstart\" event occurs in the caller after foo() returns to it.)\nWe can print Python expressions and statements using the p\ncommand:\n\n(19727)$ p x\n$0 = <__main__.Foo object at 0xfffffffffffeab3e>\n(19727)$ p x.value\n$1 = 7\n(19727)$ p x.value + 1\n8\n\nThe \"$NUM =\" prefix is only shown when we print an object that\nreally exists in the debugged program; that's why the last line does\nnot contain it.  Once a $NUM has been printed, then we can use\nit in further expressions---even at a different point time.  It\nbecomes an anchor that always refers to the same object:\n\n(19727)$ bstep\n\nFile \"/tmp/x.py\", line 13 in <module>:\n...\n\n  lst1 = [Foo() for i in range(100)]\n  lst1[50].value += 1\n  for x in lst1:\n      x.value += 1\n\n  for x in lst1:\n      if x.value != 6:\n>         print 'oops!'\n          os._exit(1)\n(19726)$ p $0.value\n$1 = 7\n\nIn this case, we want to know when this value 7 was put in this\nattribute.  This is the job of a watchpoint:\n\n(19726)$ watch $0.value\nWatchpoint 1 added\nupdating watchpoint value: $0.value => 7\n\nThis watchpoint means that $0.value will be evaluated at each line.\nWhen the repr() of this expression changes, the watchpoint activates\nand execution stops:\n\n(19726)$ bcontinue\n[searching 19629..19726]\n[searching 19338..19629]\n\nupdating watchpoint value: $0.value => 6\nReverse-hit watchpoint 1: $0.value\nFile \"/tmp/x.py\", line 9 in <module>:\n  import os\n\n  class Foo(object):\n      value = 5\n\n  lst1 = [Foo() for i in range(100)]\n  lst1[50].value += 1\n  for x in lst1:\n>     x.value += 1\n\n  for x in lst1:\n      if x.value != 6:\n          print 'oops!'\n          os._exit(1)\n(19524)$\n\nNote that using the $NUM syntax is essential in watchpoints.  You\ncan't say \"watch x.value\", because the variable x will go out\nof scope very soon when we move forward or backward in time.  In fact\nthe watchpoint expression is always evaluated inside an environment\nthat contains the builtins but not the current locals and globals.\nBut it also contains all the $NUM, which can be used to refer to\nknown objects.  It is thus common to watch $0.attribute if $0\nis an object, or to watch len($1) if $1 is some list.  The\nwatch expression can also be a simple boolean: for example, \"watch\n$2 in $3\" where $3 is some dict and $2 is some object that\nyou find now in the dict; you would use this to find out the time when\n$2 was put inside $3, or removed from it.\nUse \"info watchpoints\" and \"delete <watchpointnum>\" to manage\nwatchpoints.\nThere are also regular breakpoints, which you set with \"b\nFUNCNAME\".  It breaks whenever there is a call to a function that\nhappens to have the given name.  (It might be annoying to use for a\nfunction like __init__() which has many homonyms.  There is no\nsupport for breaking on a fully-qualified name or at a given line\nnumber for now.)\nIn our demo, we stop at the line x.value += 1, which is where the\nvalue was changed from 6 to 7.  Use bcontinue again to stop at the\nline lst1[50].value += 1, which is where the value was changed from\n5 to 6.  Now we know how this value attribute ends up being 7.\n\n(19524)$ bcontinue\n[searching 19427..19524]\n[searching 19136..19427]\n\nupdating watchpoint value: $0.value => 5\nReverse-hit watchpoint 1: $0.value\nFile \"/tmp/x.py\", line 7 in <module>:\n  import os\n\n  class Foo(object):\n      value = 5\n\n  lst1 = [Foo() for i in range(100)]\n> lst1[50].value += 1\n  for x in lst1:\n      x.value += 1\n\n  for x in lst1:\n      if x.value != 6:\n...\n(19422)$\n\nTry to use bcontinue yet another time.  It will stop now just before\n$0 is created.  At that point in time, $0 refers to\nan object that does not exist yet, so the watchpoint now evaluates to\nan error message (but it continues to work as before, with that error\nmessage as the string it currently evaluates to).\n\n(19422)$ bcontinue\n[searching 19325..19422]\n\nupdating watchpoint value: $0.value => RuntimeError:\n               '$0' refers to an object created later in time\nReverse-hit watchpoint 1: $0.value\nFile \"/tmp/x.py\", line 6 in <module>:\n  import os\n\n  class Foo(object):\n      value = 5\n\n> lst1 = [Foo() for i in range(100)]\n  lst1[50].value += 1\n  for x in lst1:\n      x.value += 1\n\n  for x in lst1:\n...\n(19371)$\n\nIn big programs, the workflow is similar, just more complex.  Usually\nit works this way: we find interesting points in time with some\ncombination of watchpoints and some direct commands to move around.\nWe write down on a piece of (real or virtual) paper these points in\nhistory, including most importantly their time, so that we can\nconstruct an ordered understanding of what is going on.\nThe current revdb can be annoying and sometimes even crash; but\nthe history you reconstruct can be kept.  All the times and\nexpressions printed are still valid when you restart revdb.  The\nonly thing \"lost\" is the $NUM objects, which you need to print\nagain.  (Maybe instead of $0, $1, ...  we should use $<big\nnumber>, where the big number identifies uniquely the object by its\ncreation time.  These numbers would continue to be valid even after\nrevdb is restarted.  They are more annoying to use than just\n$0 though.)\nScreencast: Here's a (slightly typo-y) screencast of cfbolz using the reverse debugger:\n\n\n\nCurrent issues\nGeneral issues:\n\nIf you are using revdb on a log that took more than a few\nminutes to record, then it can be painfully slow.  This is because\nrevdb needs to replay again big parts of the log for some\noperations.\nThe pypy-revdb is currently missing the following modules:\nthread (implementing multithreading is possible, but not done\nyet);\ncpyext (the CPython C API compatibility layer);\nmicronumpy (minor issue only);\n_continuation (for greenlets).\n\n\nDoes not contain a JIT, and does not use our fast garbage\ncollectors.  You can expect pypy-revdb to be maybe 3 times\nslower than CPython.\nOnly works on Linux and OS/X.  There is no fundamental reason for\nthis restriction, but it is some work to fix.\nReplaying a program uses a lot more memory; maybe 15x as much than\nduring the recording.  This is because it creates many forks.  If\nyou have a program that consumes 10% of your RAM or more, you will\nneed to reduce MAX_SUBPROCESSES in process.py.\n\nReplaying also comes with a bunch of user interface issues:\n\nAttempted to do I/O or access raw memory: we get this whenever\ntrying to print some expression that cannot be evaluated with\nonly the GC memory---or which can, but then the __repr__()\nmethod of the result cannot.  We need to reset the state with\nbstep + step before we can print anything else.  However,\nif only the __repr__() crashes, you still see the $NUM =\nprefix, and you can use that $NUM afterwards.\nid() is globally unique, returning a reproducible 64-bit number,\nso sometimes using id(x) is a workaround for when using x\ndoesn't work because of Attempted to do I/O issues (e.g.  p\n[id(x) for x in somelist]).\nas explained in the demo, next/bnext/finish/bfinish might jump\naround a bit non-predictably.\nsimilarly, breaks on watchpoints can stop at apparently unexpected\nplaces (when going backward, try to do \"step\" once).  The issue is\nthat it can only stop at the beginning of every line.  In the\nextreme example, if a line is foo(somelist.pop(getindex())),\nthen somelist is modified in the middle.  Immediately before\nthis modification occurs, we are in getindex(), and\nimmediately afterwards we are in foo().  The watchpoint will\nstop the program at the end of getindex() if running backward,\nand at the start of foo() if running forward, but never\nactually on the line doing the change.\nwatchpoint expressions must not have any side-effect at all.  If\nthey do, the replaying will get out of sync and revdb.py will\ncomplain about that.  Regular p expressions and statements can\nhave side-effects; these effects are discarded as soon as you move\nin time again.\nsometimes even \"p import foo\" will fail with Attempted to do\nI/O.  Use instead \"p import sys; foo = sys.modules['foo']\".\nuse help to see all commands.  backtrace can be useful.\nThere is no up command; you have to move in time instead,\ne.g. using bfinish to go back to the point where the current\nfunction was called.\n\n\n\nHow RevPDB is done\nIf I had to pick the main advantage of PyPy over CPython, it is that\nwe have got with the RPython translation toolchain a real place for\nexperimentation.  Every now and then, we build inside RPython some\nfeature that gives us an optionally tweaked version of the PyPy\ninterpreter---tweaked in a way that would be hard to do with CPython,\nbecause it would require systematic changes everywhere.  The most\nobvious and successful examples are the GC and the JIT.  But there\nhave been many other experiments along the same lines, from the\nso-called stackless transformation in the early days, to the STM\nversion of PyPy.\nRevPDB works in a similar way.  It is a version of PyPy in which some\noperations are systematically replaced with other operations.\nTo keep the log file at a reasonable size, we duplicate the content of\nall GC objects during replaying---by repeating the same actions on\nthem, without writing anything in the log file.  So that means that in\nthe pypy-revdb binary, the operations that do arithmetic or\nread/write GC-managed memory are not modified.  Most operations are\nlike that.  However, the other operations, the ones that involve\neither non-GC memory or calls to external C functions, are tweaked.\nEach of these operations is replaced with code that works in two\nmodes, based on a global flag:\n\nin \"recording\" mode, we log the result of the operation (but not the\narguments);\nin \"replaying\" mode, we don't really do the operation at all, but\ninstead just fetch the result from the log.\n\nHopefully, all remaining unmodified operations (arithmetic and GC\nload/store) are completely deterministic.  So during replaying, every\ninteger or non-GC pointer variable will have exactly the same value as\nit had during recording.  Interestingly, it means that if the\nrecording process had a big array in non-GC memory, then in the\nreplaying process, the array is not allocated at all; it is just\nrepresented by the same address, but there is nothing there.  When we\nrecord \"read item 123 from the array\", we record the result of the\nread (but not the \"123\").  When we replay, we're seeing again the same\n\"read item 123 from the array\" operation.  At that point, we don't\nread anything; we just return the result from the log.  Similarly,\nwhen recording a \"write\" to the array, we record nothing (this write\noperation has no result); so that when replaying, we redo nothing.\nNote how that differs from anything managed by GC memory: GC objects\n(including GC arrays) are really allocated, writes really occur, and\nreads are redone.  We don't touch the log in this case.\n\n\nOther reverse debuggers for Python\nThere are already some Python experiments about reverse debugging.\nThis is also known as \"omniscient debugging\".  However, I claim that\nthe result they get to is not very useful (for the purpose presented\nhere).  How they work is typically by recording changes to some\nobjects, like lists and dictionaries, in addition to recording the\nhistory of where your program passed through.  However, the problem of\nPython is that lists and dictionaries are not the end of the story.\nThere are many, many, many types of objects written in C which are\nmutable---in fact, the immutable ones are the exception.  You can try\nto systematically record all changes, but it is a huge task and easy\nto forget a detail.\nIn other words it is a typical use case for tweaking the RPython\ntranslation toolchain, rather than tweaking the CPython (or PyPy)\ninterpreter directly.  The result that we get here with RevPDB is more\nsimilar to rr anyway, in that only a relatively small number of\nexternal events are recorded---not every single change to every single\nlist and dictionary.\nSome links:\n\nepdb: https://github.com/native-human/epdb\npode: https://github.com/rodsenra/pode\n\nFor C:\n\nrr: https://rr-project.org/\nundodb-gdb: https://undo.io/\n\n\n\nFuture work\nAs mentioned above, it is alpha-level, and only works on Linux and OS/X.\nSo the plans for the immediate future are to fix the various\nissues described above, and port to more operating systems.  The core of the system\nis in the C file and headers in rpython/translator/revdb/src-revdb.\nFor interested people, there is also the Duhton interpreter and its\nreverse-debugger branch, which is where I prototyped the RPython\nconcept before moving to PyPy.  The basics should work for any\ninterpreter written in RPython, but they require some specific code to\ninterface with the language; in the case of PyPy, it is in\npypy/interpreter/reverse_debugging.py.\nIn parallel, there are various user interface improvements that people\ncould be interested in, like a more \"pdb++\" experience.  (And the script\nat rpython/translator/revdb/revdb.py should be moved out into some\nmore \"official\" place, and the reverse-debugger branch should be\nmerged back to default.)\nI would certainly welcome any help!\n-+- Armin",
      "tags": "revdb",
      "url": "https://www.pypy.org/posts/2016/07/reverse-debugging-for-python-8854823774141612670.html"
    },
    {
      "title": "PyPy2 v5.3 released - major C-extension support improvements",
      "text": "We have released PyPy2.7 v5.3, about six weeks after PyPy 5.1 and a week after\nPyPy3.3 v5.2 alpha 1, the first PyPy release targeting 3.3\ncompatibility. This new PyPy2.7 release includes major improvements for the\nC-API compatibility layer. In addition to complete support\nfor lxml, we now pass most (more than 95%) of the upstream numpy test suite. We can build and run scipy and matplotlib as well. Most of the failures have to do with (ab) use of the C-API, for instance writing to a read-only pointer obtained from PyString_AsString().\n\nNote that the C-API compatibility layer is significantly slower than CPython, as explained in the blog post about the new strategy for reflection of C objects into the PyPy interpreter.\n\nWe updated cffi to version 1.7 (incremental changes which provide a nicer developer experience, documented here). We would encourage developers to move their C-extension modules to cffi, but are willing to help you work through issues with existing code; come to #pypy on IRC and let us know how we can help you help us do better.\n\nYou can download the PyPy2 v5.3 release here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for their continued support of the PyPy\nproject. We would also like to thank our contributors and\nencourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on PyPy, or general help\nwith making RPython\u2019s JIT even better.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (PyPy and CPython 2.7 performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD)\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux\nbig- and little-endian variants of PPC64 running Linux\ns390x running Linux\n\n\n\nOther Highlights\n\n(since the release of PyPy 5.1 in April, 2016)\n\n\nNew features:\n\n\nMerge a major expansion of the C-API support in cpyext, also expand cpyext tests to allow running them after translation as well as untranslated\n\n\nInstead of \u201cGIL not held when a CPython C extension module\ncalls PyXxx\u201d, we now silently acquire/release the GIL.  Helps with\nC extension modules that call some PyXxx() functions without\nholding the GIL (arguably, they are theoretically buggy).\n\n\nSupport command line -v to trace import statements\n\n\nRevive traceviewer, a tool to use pygame to view traces\n\n\n\n\n\n\nNumpy via our internal _numpypy module:\n\nImplement ufunc.outer\nMove PyPy-specific numpypy headers to a subdirectory (also changed the repo\naccordingly)\n\n\u00a0\n\n\nPerformance improvements:\n\nUse bitstrings to compress lists of descriptors that are attached to an\nEffectInfo\nRemove most of the _ovf, _zer and _val operations from RPython.  Kills\nquite some code internally, and allows the JIT to do better\noptimizations: for example, app-level code like x / 2 or x % 2\ncan now be turned into x >> 1 or x & 1, even if x is possibly\nnegative.\nRework the way registers are moved/spilled in before_call()\n\n\n\n\nInternal refactorings:\n\nRefactor code to better support Python3-compatible syntax\nReduce the size of generated C sources during translation by\neliminating many many unused struct declarations (Issue #2281)\nReduce the size of generated code by using the same function objects in\nall generated subclasses\nShare cpyext Py* function wrappers according to the signature, shrinking the\ntranslated libpypy.so by about 10% (without the JIT)\n\n\n\nPlease update, and continue to help us make PyPy better.\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/06/pypy2-v53-released-major-c-extension-7708576047190172431.html"
    },
    {
      "title": "PyPy3.3 v5.2 alpha 1 released",
      "text": "We're pleased to announce the first alpha release of PyPy3.3 v5.2. This is the\nfirst release of PyPy which targets Python 3.3 (3.3.5) compatibility.We would like to thank all of the people who donated to the py3k proposal\nfor supporting the work that went into this and future releases.You can download the PyPy3.3 v5.2 alpha 1 release here:https://pypy.org/download.html#python-3-3-5-compatible-pypy3-3-v5-2HighlightsPython 3.3.5 support!Being an early alpha release, there are some missing features such as a\nPEP 393-like space efficient string representation and known issues\nincluding performance issues (e.g. issue #2305). The focus for this\nrelease has been updating to 3.3 compatibility. Windows is also not yet\nsupported.\n\nensurepip is also included (it's only included in CPython 3 >= 3.4).\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.10 and one day 3.3.5. It's fast due to its integrated tracing JIT\ncompiler.We also welcome developers of other dynamic languages to see what RPython\ncan do for them.This release supports:x86 machines on most common operating systems except Windows\n(Linux 32/64, Mac OS X 64, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\nPlease try it out and let us know what you think. We welcome feedback, we know\nyou are using PyPy, please tell us about it!We'd especially like to thank these people for their contributions to this\nrelease:Manuel Jacob, Ronan Lamy, Mark Young, Amaury Forgeot d'Arc, Philip Jenvey,\nMartin Matusiak, Vasily Kuznetsov, Matti Picus, Armin Rigo and many others.CheersThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/05/pypy33-v52-alpha-1-released-1725927506363370346.html"
    },
    {
      "title": "PyPy 5.1.1 bugfix released",
      "text": "We have released a bugfix for PyPy 5.1, due to a regression in installing third-party packages depending on numpy (using our numpy fork available at https://bitbucket.org/pypy/numpy ).Thanks to those who reported the issue. We also fixed a regression in translating PyPy which increased the memory required to translate. Improvement will be noticed by downstream packagers and those who translate rather thandownload pre-built binaries.\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.We also welcome developers of other dynamic languages to see what RPython can do for them.This release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\nPlease update, and continue to help us make PyPy better.CheersThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/05/pypy-511-bugfix-released-7586640750680293200.html"
    },
    {
      "title": "PyPy 5.1 released",
      "text": "We have released PyPy 5.1, about a month after PyPy 5.0.\n\nThis release includes more improvement to warmup time and memory requirements, extending the work done on PyPy 5.0. We have seen an additional reduction of about 20% in memory requirements, and up to 30% warmup time improvement, more detail in the blog post.\n\nWe also now have full support for the IBM s390x. Since this support is in RPython, any dynamic language written using RPython, like PyPy, will automagically be supported on that architecture.\n\nWe updated cffi to 1.6 (cffi 1.6 itself will be released shortly), and continue to improve support for the wider python ecosystem using the PyPy interpreter.\n\nYou can download the PyPy 5.1 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (PyPy and CPython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\n\nThis release supports:\n\nx86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux,\nbig- and little-endian variants of PPC64 running Linux,\ns390x running Linux\n\n\n\n\nOther Highlights\n\n(since the release of PyPy 5.0 in March, 2016\n\n\n\nNew features:\n\nA new jit backend for the IBM s390x, which was a large effort over the past few months.\nAdd better support for PyUnicodeObject in the C-API compatibility layer\nSupport GNU/kFreeBSD Debian ports in vmprof\nAdd __pypy__._promote\nMake attrgetter a single type for CPython compatibility\n\n\n\n\nBug Fixes\n\nCatch exceptions raised in an exit function\nFix a corner case in the JIT\nFix edge cases in the cpyext refcounting-compatible semantics (more work on cpyext compatibility is coming in the cpyext-ext branch, but isn\u2019t ready yet)\nTry harder to not emit NEON instructions on ARM processors without NEON support\nImprove the rpython posix module system interaction function calls\nDetect a missing class function implementation instead of calling a random function\nCheck that PyTupleObjects do not contain any NULLs at the point of conversion to W_TupleObjects\nIn ctypes, fix _anonymous_ fields of instances\nFix JIT issue with unpack() on a Trace which contains half-written operations\nFix sandbox startup (a regression in 5.0)\nFix possible segfault for classes with mangled mro or __metaclass__\nFix isinstance(deque(), Hashable) on the pure python deque\nFix an issue with forkpty()\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy\n\n\n\n\nNumpy:\n\nImplemented numpy.where for a single argument\nIndexing by a numpy scalar now returns a scalar\nFix transpose(arg) when arg is a sequence\nRefactor include file handling, now all numpy ndarray, ufunc, and umath functions exported from libpypy.so are declared in pypy_numpy.h, which is included only when building our fork of numpy\nAdd broadcast\n\n\n\n\nPerformance improvements:\n\nImprove str.endswith([tuple]) and str.startswith([tuple]) to allow JITting\nMerge another round of improvements to the warmup performance\nCleanup history rewriting in pyjitpl\nRemove the forced minor collection that occurs when rewriting the assembler at the start of the JIT backend\nPort the resource module to cffi\n\n\u00a0\n\n\nInternal refactorings:\n\nUse a simpler logger to speed up translation\nDrop vestiges of Python 2.5 support in testing\nUpdate rpython functions with ones needed for py3k\n\n\n\n\n\n\n\n\n\n\nPlease update, and continue to help us make PyPy better.\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/04/pypy-51-released-4979856639628970409.html"
    },
    {
      "title": "PyPy Enterprise Edition",
      "text": "With the latest additions, PyPy's JIT now supports the Z architecture on Linux. The newest architecture revision (also known as s390x, or colloquially referred to as \"big iron\") is the 64-bit extension for IBM mainframes. Currently only Linux 64 bit is supported (not z/OS nor TPF).\nThis is the fourth assembler backend supported by PyPy in addition to x86 (32 and 64), ARM (32-bit only) and PPC64 (both little- and big-endian). It might seem that we kind of get a hang of new architectures. Thanks to IBM for funding this work!\n\n\nHistory \nWhen I went to university one lecture covered the prediction of Thomas Watson in 1943. His famous quote \"I think there is a world market for maybe five computers ...\", turned out not to be true. \n\nHowever, even 70 years later, mainframes are used more often than you think. They back critical tasks requiring a high level of stability/security and offer high hardware and computational utilization rates by virtualization.\n\nWith the new PyPy JIT backend we are happy to present a fast Python virtual machine for mainframes and contribute more free software running on s390x.\n\n\nMeta tracing\nEven though the JIT backend has been tested on PyPy, it is not restricted to\u00a0 the Python programming language. Do you have a great idea for a DSL, or another language that should run on mainframes? Go ahead and just implement your interpreter using RPython.\n\n\nHow do I get a copy?\nPyPy can be built using the usual instructions found here. As soon as the next PyPy version has been released we will provide binaries. Until then you can just grab a nightly here.We are currently busy to get the next version of PyPy ready, so an official release will be rolled out soon.\n\n\nComparing s390x to x86\nThe goal of this comparison is not to scientifically evaluate the benefits/disadvantages on s390x, but rather to see that PyPy's architecture delivers the same benefits as it does on other platforms. Similar to the comparison done for PPC I ran the benchmarks using the same setup. The first column is the speedup of the PyPy JIT VM compared to the speedup of a pure PyPy interpreter 1). Note that the s390x's OS was virtualized.\n\n\u00a0 Label \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x86\u00a0\u00a0\u00a0\u00a0 s390x\u00a0\u00a0\u00a0\u00a0\u00a0 s390x (run 2)\n\n\u00a0 ai\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 13.7 \u00a0\u00a0\u00a0\u00a0 12.4\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 11.9\u00a0 bm_chameleon\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.5 \u00a0\u00a0\u00a0\u00a0\u00a0 6.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 6.8\u00a0 bm_dulwich_log\u00a0\u00a0\u00a0\u00a0\u00a0 5.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.1\u00a0 bm_krakatau\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.5 \u00a0\u00a0\u00a0\u00a0\u00a0 2.0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.0\u00a0 bm_mako\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.4 \u00a0\u00a0\u00a0\u00a0\u00a0 5.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.9\u00a0 bm_mdp\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.0 \u00a0\u00a0\u00a0\u00a0\u00a0 3.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.8\u00a0 chaos\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 56.9 \u00a0\u00a0\u00a0\u00a0 52.6\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 53.4\u00a0 crypto_pyaes\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 62.5 \u00a0\u00a0\u00a0\u00a0 64.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 64.2\u00a0 deltablue\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.3 \u00a0\u00a0\u00a0\u00a0\u00a0 3.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.6\u00a0 django\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 28.8 \u00a0\u00a0\u00a0\u00a0 22.6\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 21.7\u00a0 eparse\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.3 \u00a0\u00a0\u00a0\u00a0\u00a0 2.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.6\u00a0 fannkuch\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.1 \u00a0\u00a0\u00a0\u00a0\u00a0 9.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 10.1\u00a0 float\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 13.8 \u00a0\u00a0\u00a0\u00a0 12.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 13.8\u00a0 genshi_text\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 16.4 \u00a0\u00a0\u00a0\u00a0 10.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 10.9\u00a0 genshi_xml\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.2 \u00a0\u00a0\u00a0\u00a0\u00a0 7.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.2\u00a0 go\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 6.7 \u00a0\u00a0\u00a0\u00a0\u00a0 6.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 11.2\u00a0 hexiom2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 24.3 \u00a0\u00a0\u00a0\u00a0 23.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 23.5\u00a0 html5lib\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.4 \u00a0\u00a0\u00a0\u00a0\u00a0 5.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.7\u00a0 json_bench\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 28.8 \u00a0\u00a0\u00a0\u00a0 27.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 28.1\u00a0 meteor-contest\u00a0\u00a0\u00a0\u00a0\u00a0 5.1 \u00a0\u00a0\u00a0\u00a0\u00a0 4.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.4\u00a0 nbody_modified\u00a0\u00a0\u00a0\u00a0 20.6 \u00a0\u00a0\u00a0\u00a0 19.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 19.4\u00a0 pidigits\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.0 \u00a0\u00a0\u00a0\u00a0 -1.1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -1.0\u00a0 pyflate-fast\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.0 \u00a0\u00a0\u00a0\u00a0\u00a0 8.7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.5\u00a0 pypy_interp\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.3 \u00a0 \u00a0\u00a0\u00a0 4.2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.4\u00a0 raytrace-simple\u00a0\u00a0\u00a0 69.0 \u00a0\u00a0\u00a0 100.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 93.4\u00a0 richards\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 94.1 \u00a0\u00a0\u00a0\u00a0 96.6\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 84.3\u00a0 rietveld\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.2 \u00a0\u00a0\u00a0\u00a0\u00a0 2.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.7\u00a0 slowspitfire\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.8 \u00a0\u00a0\u00a0\u00a0\u00a0 3.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.2\u00a0 spambayes\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.0 \u00a0\u00a0\u00a0\u00a0\u00a0 4.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.8\u00a0 spectral-norm\u00a0\u00a0\u00a0\u00a0\u00a0 41.9 \u00a0\u00a0\u00a0\u00a0 39.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 42.6\u00a0 spitfire\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3.8 \u00a0\u00a0\u00a0\u00a0\u00a0 3.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.3\u00a0 spitfire_cstringio\u00a0 7.6 \u00a0\u00a0\u00a0\u00a0\u00a0 7.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.2\u00a0 sympy_expand\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 2.9 \u00a0\u00a0\u00a0\u00a0\u00a0 1.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.8\u00a0 sympy_integrate\u00a0\u00a0\u00a0\u00a0 4.3 \u00a0\u00a0\u00a0\u00a0\u00a0 3.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.0\u00a0 sympy_str\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.5 \u00a0\u00a0\u00a0\u00a0\u00a0 1.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.3\u00a0 sympy_sum\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 6.2 \u00a0\u00a0\u00a0\u00a0\u00a0 5.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.9\u00a0 telco\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 61.2 \u00a0\u00a0\u00a0\u00a0 48.5\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 54.8\u00a0 twisted_iteration\u00a0 55.5 \u00a0\u00a0\u00a0\u00a0 41.9\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 43.8\u00a0 twisted_names\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 8.2 \u00a0\u00a0\u00a0\u00a0\u00a0 9.3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.7\u00a0 twisted_pb\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 12.1 \u00a0\u00a0\u00a0\u00a0 10.4\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 10.2\u00a0 twisted_tcp\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.9 \u00a0\u00a0\u00a0\u00a0\u00a0 4.8\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 5.2\n\u00a0 Geometric mean:\u00a0\u00a0\u00a0 9.31\u00a0\u00a0\u00a0\u00a0\u00a0 9.10\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 9.43\n\nAs you can see the benefits are comparable on both platforms.\nOf course this is scientifically not good enough, but it shows a tendency. s390x can achieve the same results as you can get on x86. \n\nAre you running your business application on a mainframe? We would love to get some feedback. Join us in IRC tell us if PyPy made your application faster! \n\nplan_rich & the PyPy Team\n\n1) PyPy revision for the benchmarks: 4b386bcfee54",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/04/pypy-enterprise-edition-3688275697656890948.html"
    },
    {
      "title": "Warmup improvements: more efficient trace representation",
      "text": "Hello everyone.\nI'm pleased to inform that we've finished another round of\nimprovements to the warmup performance of PyPy. Before I go\ninto details, I'll recap the achievements that we've done since we've started\nworking on the warmup performance. I picked a random PyPy from November 2014\n(which is definitely before we started the warmup work) and compared it with\na recent one, after 5.0. The exact revisions are respectively ffce4c795283\nand cfbb442ae368. First let's compare pure warmup benchmarks that\ncan be found in our benchmarking suite. Out of those,\npypy-graph-alloc-removal numbers should be taken with a grain of salt,\nsince other work could have influenced the results.\nThe rest of the benchmarks mentioned is bottlenecked purely by warmup times.\nYou can see how much your program spends in warmup running\nPYPYLOG=jit-summary:- pypy your-program.py under \"tracing\" and \"backend\"\nfields (in the first three lines). An example looks like that:\n\n[e00c145a41] {jit-summary\nTracing:        71      0.053645 <- time spent tracing & optimizing\nBackend:        71      0.028659 <- time spent compiling to assembler\nTOTAL:                  0.252217 <- total run time of the program\n\nThe results of the benchmarks\n\n\n\n\n\n\n\n\n\n\nbenchmark\ntime - old\ntime - new\nspeedup\nJIT time - old\nJIT time - new\n\nfunction_call\n1.86\n1.42\n1.3x\n1.12s\n0.57s\n\nfunction_call2\n5.17s\n2.73s\n1.9x\n4.2s\n1.6s\n\nbridges\n2.77s\n2.07s\n1.3x\n1.5s\n0.8s\n\npypy-graph-alloc-removal\n2.06s\n1.65s\n1.25x\n1.25s\n0.79s\n\n\n\nAs we can see, the overall warmup benchmarks got up to 90% faster with\nJIT time dropping by up to 2.5x. We have more optimizations in the pipeline,\nwith an idea how to transfer some of the JIT gains into more of a total program\nruntime by jitting earlier and more eagerly.\n\nDetails of the last round of optimizations\nNow the nitty gritty details - what did we actually do? I covered a lot of\nwarmup improvements in the past blog posts so I'm going to focus on\nthe last change, the jit-leaner-frontend branch. This last change is simple, instead of using\npointers to store the \"operations\" objects created during tracing, we use a compact list of\n16-bit integers (with 16bit pointers in between). On 64bit machine the memory wins are\ntremendous - the new representation is 4x more efficient to use 16bit pointers than full 64bit pointers.\nAdditionally, the smaller representation has much better cache behavior and much less\npointer chasing in memory. It also has a better defined lifespan, so we don't need to\nbother tracking them by the GC, which also saves quite a bit of time.\nThe change sounds simple, but the details in the underlaying data mean that\neverything in the JIT had to be changed which took quite a bit of effort :-)\nGoing into the future on the JIT front, we have an exciting set of optimizations,\nranging from faster loops through faster warmup to using better code generation\ntechniques and broadening the kind of program that PyPy speeds up. Stay tuned\nfor the updates.\nWe would like to thank our commercial partners for making all of this possible.\nThe work has been performed by baroquesoftware and would not be possible\nwithout support from people using PyPy in production. If your company uses\nPyPy and want it to do more or does not use PyPy but has performance problems\nwith the Python installation, feel free to get in touch with me, trust me using\nPyPy ends up being a lot cheaper than rewriting everything in go :-)\nBest regards,\nMaciej Fijalkowski",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/04/warmup-improvements-more-efficient-7082900097299909512.html"
    },
    {
      "title": "PyPy 5.0.1 bugfix released",
      "text": "PyPy 5.0.1\n\nWe have released a bugfix for PyPy 5.0, after reports that the newly released\nlxml 3.6.0, which now supports PyPy 5.0 +, can crash on large files.\nThanks to those who reported the crash. Please update, downloads are available\nat\n\npypy.org/download.html\n\n\nThe changes between PyPy 5.0 and 5.0.1 are only two bug fixes: one in\ncpyext, which fixes notably (but not only) lxml; and another for a\ncorner case of the JIT.\n\n\nWhat is PyPy?\n\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It\u2019s fast (PyPy and CPython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\n\nWe also welcome developers of other\ndynamic languages to see what RPython can do for them.\n\nThis release supports x86 machines on most common operating systems\n(Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, FreeBSD),\nnewer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux, and the\nbig- and little-endian variants of PPC64 running Linux.\n\n\nPlease update, and continue to help us make PyPy better.\n\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/03/pypy-501-bugfix-released-2218405735970044084.html"
    },
    {
      "title": "PyPy 5.0 released",
      "text": "PyPy 5.0\nWe have released PyPy 5.0, about three months after PyPy 4.0.1. We encourage all users of PyPy to update to this version.\n\nYou can download the PyPy 5.0 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n\u00a0\n\nFaster and Leaner\n\nWe continue to improve the warmup time and memory usage of JIT-related metadata. The exact effects depend vastly on the program you\u2019re running and can range from insignificant to warmup being up to 30% faster and memory dropping by about 30%. \n\n\u00a0\n\nC-API Upgrade\n\nWe also merged a major upgrade to our C-API layer (cpyext), simplifying the interaction between c-level objects and PyPy interpreter level objects. As a result, lxml  (prerelease) with its cython compiled component passes all tests on PyPy. The new cpyext is also much faster. This major refactoring will soon be followed by an expansion of our C-API compatibility.\n\n\u00a0\n\nProfiling with vmprof supported on more platforms\n\n\nvmprof has been a go-to profiler for PyPy on linux for a few releases and we\u2019re happy to announce that thanks to the cooperation with jetbrains, vmprof now works on Linux, OS X and Windows on both PyPy and CPython.\n\n\n\u00a0\n\nCFFI\nWhile not applicable only to PyPy, cffi is arguably our most significant contribution to the python ecosystem. PyPy 5.0 ships with cffi-1.5.2 which now allows embedding PyPy (or CPython) in a C program.\n\n\n\u00a0\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThis release supports x86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, freebsd), newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux, and 64 bit PowerPC hardware, specifically Linux running the big- and little-endian variants of ppc64.\n\n\n\u00a0\n\nOther Highlights (since 4.0.1 released in November 2015)\n\nNew features:\nSupport embedding PyPy in a C-program via cffi and static callbacks in cffi.\nThis deprecates the old method of embedding PyPy\nRefactor vmprof to work cross-operating-system, deprecate using buggy\nlibunwind on Linux platforms. Vmprof even works on Windows now.\nSupport more of the C-API type slots, like tp_getattro, and fix C-API\nmacros, functions, and structs such as _PyLong_FromByteArray(),\nPyString_GET_SIZE, f_locals in PyFrameObject, Py_NAN, co_filename in\nPyCodeObject\nUse a more stable approach for allocating PyObjects in cpyext. (see\nblog post). Once the PyObject corresponding to a PyPy object is created,\nit stays around at the same location until the death of the PyPy object.\nDone with a little bit of custom GC support.  It allows us to kill the\nnotion of \u201cborrowing\u201d inside cpyext, reduces 4 dictionaries down to 1, and\nsignificantly simplifies the whole approach (which is why it is a new\nfeature while technically a refactoring) and allows PyPy to support the\npopulart lxml module (as of the next release) with no PyPy specific\npatches needed\nMake the default filesystem encoding ASCII, like CPython\nUse hypothesis in test creation, which is great for randomizing tests\n\n\u00a0\n\n\nBug Fixes\nBackport always using os.urandom for uuid4 from cpython and fix the JIT as well\n(issue #2202)\nMore completely support datetime, optimize timedelta creation\nFix for issue #2185 which caused an inconsistent list of operations to be\ngenerated by the unroller, appeared in a complicated DJango app\nFix an elusive issue with stacklets on shadowstack which showed up when\nforgetting stacklets without resuming them\nFix entrypoint() which now acquires the GIL\nFix direct_ffi_call() so failure does not bail out before setting CALL_MAY_FORCE\nFix (de)pickling long values by simplifying the implementation\nFix RPython rthread so that objects stored as threadlocal do not force minor\nGC collection and are kept alive automatically. This improves perfomance of\nshort-running Python callbacks and prevents resetting such object between\ncalls\nSupport floats as parameters to itertools.isslice()\nCheck for the existence of CODESET, ignoring it should have prevented PyPy\nfrom working on FreeBSD\nFix for corner case (likely shown by Krakatau) for consecutive guards with\ninterdependencies\nFix applevel bare class method comparisons which should fix pretty printing\nin IPython\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy\n\n\u00a0\n\n\nNumpy:\nUpdates to numpy 1.10.2 (incompatibilities and not-implemented features\nstill exist)\nSupport dtype=((\u2018O\u2019, spec)) union while disallowing record arrays with\nmixed object, non-object values\nRemove all traces of micronumpy from cpyext if \u2013withoutmod-micronumpy option used\nSupport indexing filtering with a boolean ndarray\nSupport partition() as an app-level function, together with a cffi wrapper\nin pypy/numpy, this now provides partial support for partition()\n\n\u00a0\n\n\nPerformance improvements:\nOptimize global lookups\nImprove the memory signature of numbering instances in the JIT. This should\nmassively decrease the amount of memory consumed by the JIT, which is\nsignificant for most programs. Also compress the numberings using variable-\nsize encoding\nOptimize string concatenation\nUse INT_LSHIFT instead of INT_MUL when possible\nImprove struct.unpack by casting directly from the underlying buffer.\nUnpacking floats and doubles is about 15 times faster, and integer types\nabout 50% faster (on 64 bit integers). This was then subsequently\nimproved further in optimizeopt.py.\nOptimize two-tuple lookups in mapdict, which improves warmup of instance\nvariable access somewhat\nReduce all guards from int_floordiv_ovf if one of the arguments is constant\nIdentify permutations of attributes at instance creation, reducing the\nnumber of bridges created\nGreatly improve re.sub() performance\n\n\u00a0\n\n\nInternal refactorings:\nRefactor and improve exception analysis in the annotator\nRemove unnecessary special handling of space.wrap().\nSupport list-resizing setslice operations in RPython\nTweak the trace-too-long heuristic for multiple jit drivers\nRefactor bookkeeping (such a cool word - three double letters) in the\nannotater\nRefactor wrappers for OS functions from rtyper to rlib and simplify them\nSimplify backend loading instructions to only use four variants\nSimplify GIL handling in non-jitted code\nRefactor naming in optimizeopt\nChange GraphAnalyzer to use a more precise way to recognize external\nfunctions and fix null pointer handling, generally clean up external\nfunction handling\nRemove pure variants of getfield_gc_* operations from the JIT by\ndetermining purity while tracing\nRefactor databasing\nSimplify bootstrapping in cpyext\nRefactor rtyper debug code into python.rtyper.debug\nSeperate structmember.h from Python.h Also enhance creating api functions\nto specify which header file they appear in (previously only pypy_decl.h)\nFix tokenizer to enforce universal newlines, needed for Python 3 support\n\n\n\nPlease try it out and let us know what you think. We welcome feedback, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2016/03/pypy-50-released-5730569530415927220.html"
    },
    {
      "title": "C-API Support update",
      "text": "As you know, PyPy can emulate the CPython C API to some extent. In this post I will describe an important optimization that we merged to improve the performance and stability of the C-API emulation layer.\n\nThe C-API is implemented by passing around PyObject * pointers in the C code.  The problem with providing the same interface with PyPy is that\nobjects don't natively have the same PyObject * structure at all; and\nadditionally their memory address can change.  PyPy handles the\ndifference by maintaining two sets of objects.  More precisely, starting\nfrom a PyPy object, it can allocate on demand a PyObject structure\nand fill it with information that points back to the original PyPy\nobjects; and conversely, starting from a C-level object, it can allocate\na PyPy-level object and fill it with information in the opposite\ndirection.\n\nI have merged a rewrite of the interaction between C-API C-level objects\nand PyPy's interpreter level objects.  This is mostly a simplification\nbased on a small hack in our garbage collector.  This hack makes the\ngarbage collector aware of the reference-counted PyObject\nstructures.  When it considers a pair consisting of a PyPy object and a\nPyObject, it will always free either none or both of them at the\nsame time.  They both stay alive if either there is a regular GC\nreference to the PyPy object, or the reference counter in the\nPyObject is bigger than zero.\n\nThis gives a more stable result.  Previously, a PyPy object might grow a\ncorresponding PyObject, loose it (when its reference counter goes to\nzero), and later have another corresponding PyObject re-created at a\ndifferent address.  Now, once a link is created, it remains alive until\nboth objects die.\n\nThe rewrite significantly simplifies our previous code (which used to be\nbased on at least 4 different dictionaries), and should make using the\nC-API somewhat faster (though it is still slower than using pure\npython or cffi).\n\nA side effect of this work is that now PyPy actually supports the upstream lxml package---which is is one\nof the most popular packages on PyPI.  (Specifically, you need version\n3.5.0 with this pull\nrequest to remove old PyPy-specific hacks that were not really\nworking.  See\ndetails.)  At this point, we no longer recommend using the\ncffi-lxml alternative: although it may still be faster, it might be\nincomplete and old.\n\nWe are actively working on extending our C-API support, and hope to soon\nmerge a branch to support more of the C-API functions (some numpy news\ncoming!).  Please try\nit out and let us know how it works for you.\n\nArmin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/02/c-api-support-update-8582726091670983181.html"
    },
    {
      "title": "Using CFFI for embedding",
      "text": "Introduction\n\nCFFI has been a great success so far to call C libraries in your\nPython programs, in a way that is both simple and that works across\nCPython 2.x and 3.x and PyPy.\n\nThis post assumes that you know what CFFI is and how to use it in\nAPI mode (ffi.cdef(), ffi.set_source(), ffi.compile()).\nA quick overview can be found in this paragraph.\n\nThe major news of CFFI 1.4, released last december, was that you can\nnow declare C functions with extern \"Python\" in the cdef().\nThese magic keywords make the function callable from C (where it is\ndefined automatically), but calling it will call some Python code\n(which you attach with the @ffi.def_extern() decorator).  This is\nuseful because it gives a more straightforward, faster and\nlibffi-independent way to write callbacks.  For more details, see the\ndocumentation.\n\nYou are, in effect, declaring a static family of C functions which\ncall Python code.  The idea is to take pointers to them, and pass them\naround to other C functions, as callbacks.  However, the idea of a set\nof C functions which call Python code opens another path: embedding\nPython code inside non-Python programs.\n\nEmbedding\n\nEmbedding is traditionally done using the CPython C API: from C code,\nyou call Py_Initialize() and then some other functions like\nPyRun_SimpleString().  In the simple cases it is, indeed, simple\nenough; but it can become a complicated story if you throw in\nsupporting application-dependent object types; and a messy story if\nyou add correctly running on multiple threads, for example.\nMoreover, this approach is specific to CPython (2.x or 3.x).  It does\nnot work at all on PyPy, which has its own very different, minimal\nembedding API.\n\nThe new-and-coming thing about CFFI 1.5, meant as replacement of the\nabove solutions, is direct embedding support---with no fixed API at\nall.  The idea is to write some Python script with a cdef() which\ndeclares a number of extern \"Python\" functions.  When running the\nscript, it creates the C source code and compiles it to a\ndynamically-linked library (.so on Linux).  This is the same as in\nthe regular API-mode usage.  What is new is that these extern\n\"Python\" can now also be exported from the .so, in the C\nsense.  You also give a bit of initialization-time Python code\ndirectly in the script, which will be compiled into the .so too.\nThis library can now be used directly from any C program (and it is\nstill importable in Python).  It exposes the C API of your choice,\nwhich you specified with the extern \"Python\" declarations.  You\ncan use it to make whatever custom API makes sense in your particular\ncase.  You can even directly make a \"plug-in\" for any program that\nsupports them, just by exporting the API expected for such plugins.\n\nTrying it out on CPython\n\nThis is still being finalized, but please try it out.  You can\nsee embedding.py directly online for a quick glance.  Or\nsee below the instructions on Linux with CPython 2.7 (CPython 3.x and\nnon-Linux platforms are still a work in progress right now, but this\nshould be quickly fixed):\n\nget the branch static-callback-embedding of CFFI:\n\nhg clone https://foss.heptapod.net/cffi/cffi\nhg up static-callback-embedding\n\n\nmake the _cffi_backend.so:\n\npython setup_base.py build_ext -f -i\n\n\nrun embedding.py in the demo directory:\n\ncd demo\nPYTHONPATH=.. python embedding.py\n\n\nthis produces _embedding_cffi.c.  Run gcc to build it.  On Linux:\n\ngcc -shared -fPIC _embedding_cffi.c -o _embedding_cffi.so  \\\n    -lpython2.7 -I/usr/include/python2.7\n\n\ntry out the demo C program in embedding_test.c:\n\ngcc embedding_test.c _embedding_cffi.so\nPYTHONPATH=.. LD_LIBRARY_PATH=. ./a.out\n\n\n\nNote that if you get ImportError: cffi extension module\n'_embedding_cffi' has unknown version 0x2701, it means that the\n_cffi_backend module loaded is a pre-installed one instead of the\nmore recent one in \"..\".  Be sure to use PYTHONPATH=.. for now.  (Some installations manage to be confused enough to load the system-wide cffi even if another version is in the PYTHONPATH.  I think a virtualenv can be used to work around this issue.)\n\nTry it out on PyPy\n\nVery similar steps can be followed on PyPy, but it requires the\ncffi-static-callback-embedding branch of PyPy, which you must\nfirst translate from sources.  The difference is then that you need to\nadapt the first gcc command line: replace -lpython2.7 with\n-lpypy-c and to fix the -I path (and possibly add a -L\npath).\n\nMore details\n\nHow it works, more precisely, is by automatically initializing CPython/PyPy\nthe first time any of the extern \"Python\"\nfunctions is called from the C program.  This is done using locks in case of multi-threading,\nso several threads can concurrently do this \"first call\".  This should work even if two\ndifferent threads call the first time a function from two different\nembedded CFFI extensions that happen to be linked with the same program.  Explicit initialization is\nnever needed.\n\nThe custom initialization-time Python code you put in\nffi.embedding_init_code() is executed at that time.  If this code\nstarts to be big, you can move it to independent modules or packages.\nThen the initialization-time Python code only needs to import them.  In\nthat case, you have to carefully set up sys.path if the modules are\nnot installed in the usual Python way.\nIf the Python code is big and full of dependencies, a better alternative\nwould be to use virtualenv.  How to do that is not fully fleshed out so\nfar.  You can certainly run the whole program with the environment\nvariables set up by the virtualenv's activate script first.  There\nare probably other solutions that involve using gcc's\n-Wl,-rpath=\\$ORIGIN/ or -Wl,-rpath=/fixed/path/ options to load\na specific libpython or libypypy-c library.  If you try it out and it\ndoesn't work the way you would like, please complain :-)\nAnother point: right now this does not support CPython's notion of\nmultiple subinterpreters.  The logic creates a single global Python\ninterpreter, and runs everything in that context.  Maybe a future\nversion would have an explicit API to do that \u2014 or maybe it should be\nthe job of a 3rd-party extension module to provide a Python interface\nover the notion of subinterpreters...\nMore generally, any feedback is appreciated.\nHave fun,\nArmin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/01/using-cffi-for-embedding-8493496761738752124.html"
    },
    {
      "title": "Leysin Winter Sprint (20-27th February 2016)",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the eleventh time.\nThis is a fully public sprint: newcomers and topics other than those\nproposed below are welcome.\n\nGoals and topics of the sprint\nThe details depend on who is here and ready to work.  The list of\ntopics is mostly the same as last year (did PyPy became a mature\nproject with only long-term goals?):\n\ncpyext (CPython C API emulation layer): various speed and\ncompleteness topics\ncleaning up the optimization step in the JIT, change the register\nallocation done by the JIT's backend, or more improvements to the\nwarm-up time\nfinish vmprof - a statistical profiler for CPython and PyPy\nPy3k (Python 3.x support), NumPyPy (the numpy module)\nSTM (Software Transaction Memory), notably: try to come up with\nbenchmarks, and measure them carefully in order to test and improve\nthe conflict reporting tools, and more generally to figure out how\npractical it is in large projects to avoid conflicts\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off for ski.\n\n\n\nExact times\nI have booked the week from Saturday 20 to Saturday 27.  It is fine to\nleave either the 27 or the 28, or even stay a few\nmore days on either side.  The plan is to work full days between the 21\nand the 27.  You are of course allowed to show up for a part of that\ntime only, too.\n\n\nLocation & Accomodation\nLeysin, Switzerland, \"same place as before\".  Let me refresh your\nmemory: both the sprint venue and the lodging will be in a\npair of chalets built specifically for bed & breakfast:\nhttps://www.ermina.ch/.  The place has a good ADSL Internet connection\nwith wireless installed.  You can also arrange your own lodging\nelsewhere (as long as you are in Leysin, you cannot be more than a 15\nminutes walk away from the sprint venue).\nPlease confirm that you are coming so that we can adjust the\nreservations as appropriate.\nThe options of rooms are a bit more limited than on previous years\nbecause the place for bed-and-breakfast is shrinking: what is\nguaranteed is only one double-bed room and a bigger room with 5-6\nindividual beds (the latter at 50-60 CHF per night, breakfast\nincluded).  If there are more people that would prefer a single room,\nplease contact me and we'll see what choices you have.  There are a\nchoice of hotels, many of them reasonably priced for Switzerland.\nPlease register by Mercurial:\n\nhttps://bitbucket.org/pypy/extradoc/\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/leysin-winter-2016\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\nYou need a Swiss-to-(insert country here) power adapter.  There will be\nsome Swiss-to-EU adapters around, and at least one EU-format power strip.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2016/01/leysin-winter-sprint-20-27th-february-1737200016169608469.html"
    },
    {
      "title": "PyPy 4.0.1 released please update",
      "text": "PyPy 4.0.1\n\nWe have released PyPy 4.0.1, three weeks after PyPy 4.0.0. We have fixed a few critical bugs in the JIT compiled code, reported by users. We therefore encourage all users of PyPy to update to this version. There are a few minor enhancements in this version as well.\n\nYou can download the PyPy 4.0.1 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better.\n\n \n\n\n\u00a0\n\nCFFI update\n\nWhile not applicable only to PyPy, cffi is arguably our most significant contribution to the python ecosystem. PyPy 4.0.1 ships with cffi-1.3.1 with the improvements it brings.\n\n\n\u00a0\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThis release supports x86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, freebsd), newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux, and the big- and little-endian variants of ppc64 running Linux.\n\n\n\u00a0\n\nOther Highlights (since 4.0.0 released three weeks ago)\n\n\n\nBug Fixes\nFix a bug when unrolling double loops in JITted code\nFix multiple memory leaks in the ssl module, one of which affected CPython as well (thanks to Alex Gaynor for pointing those out)\nUse pkg-config to find ssl headers on OS-X\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy\n\n\nNew features\nInternal cleanup of RPython class handling\nSupport stackless and greenlets on PPC machines\nImprove debug logging in subprocesses: use PYPYLOG=jit:log.%d for example to have all subprocesses write the JIT log to a file called \u2018log.%d\u2019, with \u2018%d\u2019 replaced with the subprocess\u2019 PID.\nSupport PyOS_double_to_string in our cpyext capi compatibility layer\n\n\nNumpy\nImprove support for __array_interface__\nPropagate most NAN mantissas through float16-float32-float64 conversions\n\n\nPerformance improvements and refactorings\nImprovements in slicing byte arrays\nImprovements in enumerate()\nSilence some warnings while translating\n\n\n\nPlease update, and continue to help us make PyPy better.\n\nCheers \nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/11/pypy-401-released-please-update-2652340737298251005.html"
    },
    {
      "title": "PyPy 4.0.0 Released - A Jit with SIMD Vectorization and More",
      "text": "PyPy 4.0.0\nWe\u2019re pleased and proud to unleash PyPy 4.0.0, a major update of the PyPy python 2.7.10 compatible interpreter with a Just In Time compiler. We have improved warmup time and memory overhead used for tracing, added vectorization for numpy and general loops where possible on x86 hardware (disabled by default), refactored rough edges in rpython, and increased functionality of numpy.\nYou can download the PyPy 4.0.0 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy project.\nWe would also like to thank our contributors (7 new ones since PyPy 2.6.0) and encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on PyPy, or general help with making RPython\u2019s JIT even better.\n\n\nNew Version Numbering\n\n\nSince the past release, PyPy 2.6.1, we decided to update the PyPy 2.x.x versioning directly to PyPy 4.x.x, to avoid confusion with CPython 2.7 and 3.5. Note that this version of PyPy uses the stdlib and implements the syntax of CPython 2.7.10.\n\n\nVectorization\n\n\nRichard Plangger began work in March and continued over a Google Summer of Code to add a vectorization step to the trace optimizer. The step recognizes common constructs and emits SIMD code where possible, much as any modern compiler does. This vectorization happens while tracing running code,  so it is actually easier at run-time to determine the availability of possible vectorization than it is for ahead-of-time compilers.\nAvailability of SIMD hardware is detected at run time, without needing to precompile various code paths into the executable.\nThe first version of the vectorization has been merged in this release, since it is so new it is off by default. To enable the vectorization in built-in JIT drivers (like numpy ufuncs), add \u2013jit vec=1, to enable all implemented vectorization add \u2013jit vec_all=1\nBenchmarks and a summary of this work appear here\n\n\nInternal Refactoring: Warmup Time Improvement and Reduced Memory Usage\n\n\nMaciej Fijalkowski and Armin Rigo refactored internals of Rpython that now allow PyPy to more efficiently use guards in jitted code. They also rewrote unrolling, leading to a warmup time improvement of 20% or so. The reduction in guards also means a reduction in the use of memory, also a savings of around 20%.\n\n\n\nNumpy\n\nOur implementation of numpy continues to improve. ndarray and the numeric dtypes are very close to feature-complete; record, string and unicode dtypes are mostly supported.  We have reimplemented numpy linalg, random and fft as cffi-1.0 modules that call out to the same underlying libraries that upstream numpy uses. Please try it out, especially using the new vectorization (via \u2013jit vec=1 on the command line) and let us know what is missing for your code.\n\n\n\nCFFI\n\nWhile not applicable only to PyPy, cffi is arguably our most significant contribution to the python ecosystem. Armin Rigo continued improving it, and PyPy reaps the benefits of cffi-1.3: improved manangement of object lifetimes, __stdcall on Win32, ffi.memmove(), and percolate const, restrict keywords from cdef to C code.\n\n\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\nWe also welcome developers of other dynamic languages to see what RPython can do for them.\nThis release supports x86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, freebsd), as well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\nWe also introduce support for the 64 bit PowerPC hardware, specifically Linux running the big- and little-endian variants of ppc64.\n\n\nOther Highlights (since 2.6.1 release two months ago)\n\nBug Fixes\nApplied OPENBSD downstream fixes\nFix a crash on non-linux when running more than 20 threads\nIn cffi, ffi.new_handle() is more cpython compliant\nAccept unicode in functions inside the _curses cffi backend exactly like cpython\nFix a segfault in itertools.islice()\nUse gcrootfinder=shadowstack by default, asmgcc on linux only\nFix ndarray.copy() for upstream compatability when copying non-contiguous arrays\nFix assumption that lltype.UniChar is unsigned\nFix a subtle bug with stacklets on shadowstack\nImprove support for the cpython capi in cpyext (our capi compatibility layer). Fixing these issues inspired some thought about cpyext in general, stay tuned for more improvements\nWhen loading dynamic libraries, in case of a certain loading error, retry loading the library assuming it is actually a linker script, like on Arch and Gentoo\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy\n\n\nNew features:\nAdd an optimization pass to vectorize loops using x86 SIMD intrinsics.\nSupport __stdcall on Windows in CFFI\nImprove debug logging when using PYPYLOG=???\nDeal with platforms with no RAND_egd() in OpenSSL\n\n\nNumpy:\nAdd support for ndarray.ctypes\nFast path for mixing numpy scalars and floats\nAdd support for creating Fortran-ordered ndarrays\nFix casting failures in linalg (by extending ufunc casting)\nRecognize and disallow (for now) pickling of ndarrays with objects embedded in them\n\n\nPerformance improvements and refactorings:\nReuse hashed keys across dictionaries and sets\nRefactor JIT interals to improve warmup time by 20% or so at the cost of a minor regression in JIT speed\nRecognize patterns of common sequences in the JIT backends and optimize them\nMake the garbage collecter more incremental over external_malloc() calls\nShare guard resume data where possible which reduces memory usage\nFast path for zip(list, list)\nReduce the number of checks in the JIT for lst[a:]\nMove the non-optimizable part of callbacks outside the JIT\nFactor in field immutability when invalidating heap information\nUnroll itertools.izip_longest() with two sequences\nMinor optimizations after analyzing output from vmprof and trace logs\nRemove many class attributes in rpython classes\nHandle getfield_gc_pure* and getfield_gc_* uniformly in heap.py\nImprove simple trace function performance by lazily calling fast2locals and locals2fast only if truly necessary \n\n\n\n\n\n\n\n\nPlease try it out and let us know what you think. We welcome feedback, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/10/pypy-400-released-jit-with-simd-8282134928733384063.html"
    },
    {
      "title": "Automatic SIMD vectorization support in PyPy",
      "text": "Hi everyone,\n\nit took some time to catch up with the JIT refacrtorings merged in this summer. But, (drums) we are happy to announce that:\n\n\nThe next release of PyPy,\u00a0 \"PyPy 4.0.0\", will ship the new auto vectorizer\nThe goal of this project was to increase the speed of numerical applications in both the NumPyPy library and for arbitrary Python programs. In PyPy we have focused a lot on improvements in the 'typical python workload',  which usually involves object and string manipulations, mostly for web development. We're hoping with this work that we'll continue improving the other very important Python use case - numerics.\n\n\nWhat it can do! \nIt targets numerics only. It \nwill not execute object manipulations faster, but it is capable of \nenhancing common vector and matrix operations.\nGood news is that it is not specifically targeted for the NumPy library and the PyPy \nvirtual machine. Any interpreter (written in RPython) is able make use \nof the vectorization. For more information about that take a look here, or consult the documentation. For the time being it is not turn on by default, so be sure to enable it by specifying --jit vec=1\u00a0before running your program.\n\nIf your language (written in RPython) contains many array/matrix operations, you can easily integrate the optimization by adding the parameter 'vec=1' to the JitDriver.\n\n\nNumPyPy Improvements\n\nLet's take a look at the core functions of the NumPyPy library (*). \nThe following tests tests show the speedup of the core functions commonly used in Python code interfacing with NumPy, on CPython with NumPy, on the PyPy 2.6.1 relased several weeks ago, and on PyPy 15.11 to be released soon. Timeit was used to test the time needed to run the operation in the plot title on various vector (lower case) and square matrix (upper case) sizes displayed on the X axis. The Y axis shows the speedup compared to CPython 2.7.10. This means that higher is better.\u00a0\n\n\n\n\n\n\n\n\nIn comparison to PyPy 2.6.1, the speedup greatly improved. The hardware support really strips down the runtime of the vector and matrix operations. There is another operation we would like to highlight: the dot product.\nIt is a very common operation in numerics and PyPy now (given a moderate sized matrix and vector) decreases the time spent in that operation. See for yourself:\n\n\n\n\n\nThese are nice improvements in the NumPyPy library and we got to a competitive level only making use of SSE4.1.\n\n\nFuture work\u00a0\u00a0 \n\nThis is not the end of the road. The GSoC project showed that it is possible to implement this optimization in PyPy. There might be other improvements we can make to carry this further:\n\nCheck alignment at runtime to increase the memory throughput of the CPU\nSupport the AVX vector extension which (at least) doubles the size of the vector register\nHandle each and every corner case in Python traces to enable it\u00a0 globally\nDo not rely only on loading operations to trigger the analysis, there might be cases where combination of floating point values could be done in parallel \n\nCheers,\nThe PyPy Team\n\n(*) The benchmark code can be found here it was run using this configuration: i7-2600 CPU @ 3.40GHz (4 cores).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/10/automatic-simd-vectorization-support-in-639063580401330508.html"
    },
    {
      "title": "PowerPC backend for the JIT",
      "text": "Hi all,\n\nPyPy's JIT now supports the 64-bit PowerPC architecture!  This is the\nthird architecture supported, in addition to x86 (32 and 64) and ARM\n(32-bit only).  More precisely, we support Linux running the big- and the\nlittle-endian variants of ppc64.  Thanks to IBM for funding this work!\n\nThe new JIT backend has been merged into \"default\".  You should be able\nto translate PPC versions\nas usual\ndirectly on the machines.  For\nthe foreseeable future, I will compile and distribute binary versions\ncorresponding to the official releases (for Fedora), but of course I'd\nwelcome it if someone else could step in and do it.  Also, it is unclear\nyet if we will run a buildbot.\n\nTo check that the result performs well, I logged in a ppc64le machine\nand ran the usual benchmark suite of PyPy (minus sqlitesynth: sqlite\nwas not installed on that machine).  I ran it twice at a difference of\n12 hours, as an attempt to reduce risks caused by other users suddenly\nusing the machine.  The machine was overall relatively quiet.  Of\ncourse, this is scientifically not good enough; it is what I could come\nup with given the limited resources.\n\nHere are the results, where the numbers are speed-up factors between the\nnon-jit and the jit version of PyPy.  The first column is x86-64, for\nreference.  The second and third columns are the two ppc64le runs.  All\nare Linux.  A few benchmarks are not reported here because the runner\ndoesn't execute them on non-jit (however, apart from sqlitesynth, they\nall worked).\n\n\n    ai                        13.7342        16.1659     14.9091\n    bm_chameleon               8.5944         8.5858        8.66\n    bm_dulwich_log             5.1256         5.4368      5.5928\n    bm_krakatau                5.5201         2.3915      2.3452\n    bm_mako                    8.4802         6.8937      6.9335\n    bm_mdp                     2.0315         1.7162      1.9131\n    chaos                     56.9705        57.2608     56.2374\n    sphinx\n    crypto_pyaes               62.505         80.149     79.7801\n    deltablue                  3.3403         5.1199      4.7872\n    django                    28.9829         23.206       23.47\n    eparse                     2.3164         2.6281       2.589\n    fannkuch                   9.1242        15.1768     11.3906\n    float                     13.8145        17.2582     17.2451\n    genshi_text               16.4608        13.9398     13.7998\n    genshi_xml                 8.2782         8.0879      9.2315\n    go                         6.7458        11.8226     15.4183\n    hexiom2                   24.3612        34.7991     33.4734\n    html5lib                   5.4515         5.5186       5.365\n    json_bench                28.8774        29.5022     28.8897\n    meteor-contest             5.1518         5.6567      5.7514\n    nbody_modified            20.6138        22.5466     21.3992\n    pidigits                   1.0118          1.022      1.0829\n    pyflate-fast               9.0684        10.0168     10.3119\n    pypy_interp                3.3977         3.9307      3.8798\n    raytrace-simple           69.0114       108.8875    127.1518\n    richards                  94.1863       118.1257    102.1906\n    rietveld                   3.2421         3.0126      3.1592\n    scimark_fft\n    scimark_lu\n    scimark_montecarlo\n    scimark_sor\n    scimark_sparsematmul\n    slowspitfire               2.8539         3.3924      3.5541\n    spambayes                  5.0646         6.3446       6.237\n    spectral-norm             41.9148        42.1831     43.2913\n    spitfire                   3.8788         4.8214       4.701\n    spitfire_cstringio          7.606         9.1809      9.1691\n    sqlitesynth\n    sympy_expand               2.9537         2.0705      1.9299\n    sympy_integrate            4.3805         4.3467      4.7052\n    sympy_str                  1.5431         1.6248      1.5825\n    sympy_sum                  6.2519          6.096      5.6643\n    telco                     61.2416        54.7187     55.1705\n    trans2_annotate\n    trans2_rtype\n    trans2_backendopt\n    trans2_database\n    trans2_source\n    twisted_iteration         55.5019        51.5127     63.0592\n    twisted_names              8.2262         9.0062      10.306\n    twisted_pb                12.1134         13.644     12.1177\n    twisted_tcp                4.9778          1.934      5.4931\n\n    GEOMETRIC MEAN               9.31           9.70       10.01\n\n\nThe last line reports the geometric mean of each column.  We see that\nthe goal was reached: PyPy's JIT actually improves performance by a\nfactor of around 9.7 to 10 times on ppc64le.  By comparison, it \"only\"\nimproves performance by a factor 9.3 on Intel x86-64.  I don't know why,\nbut I'd guess it mostly means that a non-jitted PyPy performs slightly\nbetter on Intel than it does on PowerPC.\n\nWhy is that?  Actually, if we do the same comparison with an ARM\ncolumn too, we also get higher numbers there than on Intel.\nWhen we discovered that a few years ago, we guessed that\non ARM running the whole interpreter in\nPyPy takes up a lot of resources, e.g. of instruction cache, which the\nJIT's assembler doesn't need any more after the process is warmed up.\nAnd caches are much bigger on Intel.  However, PowerPC is much closer\nto Intel, so this argument doesn't work for PowerPC.\nBut there are other more subtle\nvariants of it.  Notably, Intel is doing crazy things about branch\nprediction, which likely helps a big interpreter---both the non-JITted\nPyPy and CPython, and both for the interpreter's main loop itself and\nfor the numerous indirect branches that depend on the types of the\nobjects.  Maybe the PowerPC is as good as Intel, and so this argument\ndoesn't work either.  Another one would be:\non PowerPC I did notice that gcc itself is not\nperfect at optimization.  During development of this backend, I often\nlooked at assembler produced by gcc, and there are a number of small\ninefficiencies there.  All these are factors that slow down the\nnon-JITted version of PyPy, but don't influence the speed of the\nassembler produced just-in-time.\n\nAnyway, this is just guessing.  The fact remains that PyPy can now\nbe used on PowerPC machines.  Have fun!\n\nA bient\u00f4t,\n\nArmin.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/10/powerpc-backend-for-jit-3014100267884692148.html"
    },
    {
      "title": "PyPy memory and warmup improvements (2) - Sharing of Guards",
      "text": "Hello everyone!\nThis is the second part of the series of improvements in warmup time and\nmemory consumption in the PyPy JIT. This post covers recent work on sharing guard\nresume data that was recently merged to trunk. It will be a part\nof the next official PyPy release. To understand what it does, let's\nstart with a loop for a simple example:\n\nclass A(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def call_method(self, z):\n        return self.x + self.y + z\n\ndef f():\n    s = 0\n    for i in range(100000):\n        a = A(i, 1 + i)\n        s += a.call_method(i)\n\nAt the entrance of the loop, we have the following set of operations:\n\nguard(i5 == 4)\nguard(p3 is null)\np27 = p2.co_cellvars\np28 = p2.co_freevars\nguard_class(p17, 4316866008, descr=<Guard0x104295e08>)\np30 = p17.w_seq\nguard_nonnull(p30, descr=<Guard0x104295db0>)\ni31 = p17.index\np32 = p30.strategy\nguard_class(p32, 4317041344, descr=<Guard0x104295d58>)\np34 = p30.lstorage\ni35 = p34..item0\n\nThe above operations gets executed at the entrance, so each time we call f(). They ensure\nall the optimizations done below stay valid. Now, as long as nothing\nout of the ordinary happens, they only ensure that the world around us never changed. However, if e.g. someone puts new\nmethods on class A, any of the above guards might fail. Despite the fact that it's a very unlikely\ncase, PyPy needs to track how to recover from such a situation. Each of those points needs to keep the full\nstate of the optimizations performed, so we can safely deoptimize them and reenter the interpreter.\nThis is vastly wasteful since most of those guards never fail, hence some sharing between guards\nhas been performed.\nWe went a step further - when two guards are next to each other or the\noperations in between them don't have side effects, we can safely redo the operations or to simply\nput, resume in the previous guard. That means every now and again we execute a few\noperations extra, but not storing extra info saves quite a bit of time and memory. This is similar to the approach that LuaJIT takes, which is called sparse snapshots.\n\n\nI've done some measurements on annotating & rtyping translation of pypy, which\nis a pretty memory hungry program that compiles a fair bit. I measured, respectively:\n\ntotal time the translation step took (annotating or rtyping)\ntime it took for tracing (that excludes backend time for the total JIT time) at\nthe end of rtyping.\nmemory the GC feels responsible for after the step. The real amount of memory\nconsumed will always be larger and the coefficient of savings is in 1.5-2x mark\n\nHere is the table:\n\n\n\n\n\n\n\n\n\n\nbranch\ntime annotation\ntime rtyping\nmemory annotation\nmemory rtyping\ntracing time\n\n\n\ndefault\n317s\n454s\n707M\n1349M\n60s\n\nsharing\n302s\n430s\n595M\n1070M\n51s\n\nwin\n4.8%\n5.5%\n19%\n26%\n17%\n\n\n\nObviously pypy translation is an extreme example - the vast majority of the code out there\ndoes not have that many lines of code to be jitted. However, it's at the very least\na good win for us :-)\nWe will continue to improve the warmup performance and keep you posted!\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/10/pypy-memory-and-warmup-improvements-2-4598780879518640015.html"
    },
    {
      "title": "PyPy warmup improvements",
      "text": "Hello everyone!\nI'm very pleased to announce that we've just managed to merge\nthe optresult branch.\nUnder this cryptic name is the biggest JIT refactoring we've done in a couple\nyears, mostly focused on the warmup time and memory impact of PyPy.\nTo understand why we did that, let's look back in time - back when we\ngot the first working JIT prototype in 2009 we were focused exclusively\non achieving peak performance with some consideration towards memory usage, but\nwithout serious consideration towards warmup time. This means we accumulated\nquite a bit of technical debt over time that we're trying, with difficulty,\nto address right now. This branch mostly does not affect the peak performance\n- it should however help you with short-living scripts, like test runs.\nWe identified warmup time to be one of the major pain points for pypy users,\nalong with memory impact and compatibility issues with CPython C extension\nworld. While we can't address all the issues at once, we're trying to address\nthe first two in the work contributing to this blog post. I will write\na separate article on the last item separately.\nTo see how much of a problem warmup is for your program, you can run your\nprogram with PYPYLOG=jit-summary:- environment variable set.\nThis should show you something like this:\n\n(pypy-optresult)fijal@hermann:~/src/botbot-web$ PYPYLOG=jit-summary:- python orm.py 1500\n[d195a2fcecc] {jit-summary\nTracing:            781     2.924965\nBackend:            737     0.722710\nTOTAL:                      35.912011\nops:                1860596\nrecorded ops:       493138\n  calls:            81022\nguards:             131238\nopt ops:            137263\nopt guards:         35166\nforcings:           4196\nabort: trace too long:      22\nabort: compiling:   0\nabort: vable escape:        22\nabort: bad loop:    0\nabort: force quasi-immut:   0\nnvirtuals:          183672\nnvholes:            25797\nnvreused:           116131\nTotal # of loops:   193\nTotal # of bridges: 575\nFreed # of loops:   6\nFreed # of bridges: 75\n[d195a48de18] jit-summary}\n\nThis means that the total (wall clock) time was 35.9s, out of which we spent\n2.9s tracing 781 loops and 0.72s compiling them. The remaining couple were\naborted (trace too long is normal, vable escape means someone called\nsys._getframe() or equivalent). You can do the following things:\n\ncompare the numbers with pypy --jit off and see at which number of\niterations pypy jit kicks in\nplay with the thresholds:\npypy --jit threshold=500,function_threshold=400,trace_eagerness=50 was\nmuch better in this example. What this does is to lower the threshold\nfor tracing loops from default of 1039 to 400, threshold for tracing\nfunctions from the start from 1619 to 500 and threshold for tracing bridges\nfrom 200 to 50. Bridges are \"alternative paths\" that JIT did not take that\nare being additionally traced. We believe in sane defaults, so we'll try\nto improve upon those numbers, but generally speaking there is no one-size\nfits all here.\nif the tracing/backend time stays high, come and complain to us with\nbenchmarks, we'll try to look at them\n\nWarmup, as a number, is notoriously hard to measure. It's a combination of:\n\npypy running interpreter before jitting\npypy needing time to JIT the traces\nadditional memory allocations needed during tracing to accomodate bookkeeping\ndata\nexiting and entering assembler until there is enough coverage of assembler\n\nWe're working hard on making a better assesment at this number, stay tuned :-)\n\nSpeedups\nOverall we measured about 50% speed improvement in the optimizer, which reduces\nthe overall warmup time between 10% and 30%. The very\nobvious warmup benchmark got a speedup from 4.5s to 3.5s, almost\n30% improvement. Obviously the speedups on benchmarks would vastly\ndepend on how much warmup time is there in those benchmarks. We observed\nannotation of pypy to decreasing by about 30% and the overall translation\ntime by about 7%, so your mileage may vary.\nOf course, as usual with the large refactoring of a crucial piece of PyPy,\nthere are expected to be bugs. We are going to wait for the default branch\nto stabilize so you should see warmup improvements in the next release.\nIf you're not afraid to try, nightlies will already have them.\nWe're hoping to continue improving upon warmup time and memory impact in the\nfuture, stay tuned for improvements.\n\n\nTechnical details\nThe branch does \"one\" thing - it changes the underlying model of how operations\nare represented during tracing and optimizations. Let's consider a simple\nloop like:\n\n[i0, i1]\ni2 = int_add(i0, i1)\ni3 = int_add(i2, 1)\ni4 = int_is_true(i3)\nguard_true(i4)\njump(i3, i2)\n\nThe original representation would allocate a Box for each of i0 - i4\nand then store those boxes in instances of ResOperation. The list of such\noperations would then go to the optimizer. Those lists are big - we usually\nremove 90% of them during optimizations, but they can be a couple thousand\nelements. Overall, allocating those big lists takes a toll on warmup time,\nespecially due to the GC pressure. The branch removes the existance of Box\ncompletely, instead using a link to ResOperation itself. So say in the above\nexample, i2 would refer to its producer - i2 = int_add(i0, i1) with\narguments getting special treatment.\nThat alone reduces the GC pressure slightly, but a reduced number\nof instances also lets us store references on them directly instead\nof going through expensive dictionaries, which were used to store optimizing\ninformation about the boxes.\nCheers!\nfijal & arigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/09/pypy-warmup-improvements-8349465374608676233.html"
    },
    {
      "title": "PyPy 2.6.1 released",
      "text": "PyPy 2.6.1\nWe\u2019re pleased to announce PyPy 2.6.1, an update to PyPy 2.6.0 released June 1.\nWe have fixed many issues, updated stdlib to 2.7.10, cffi to version 1.3, extended support for\nthe new vmprof statistical profiler for multiple threads, and increased\nfunctionality of numpy.\nYou can download the PyPy 2.6.1 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject, and our volunteers and contributors.\n\nWe would also like to encourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and RPython documentation\nimprovements, tweaking popular modules to run on pypy, or general help with making\nRPython\u2019s JIT even better.\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\n\nThis release supports x86 machines on most common operating systems\n(Linux 32/64, Mac OS X 64, Windows 32, OpenBSD, freebsd),\nas well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\n\nWe also welcome developers of other\ndynamic languages to see what RPython can do for them.\n\n\n\nHighlights\n\nBug Fixes\nRevive non-SSE2 support\nFixes for detaching _io.Buffer*\nOn Windows, close (and flush) all open sockets on exiting\nDrop support for ancient macOS v10.4 and before\nClear up contention in the garbage collector between trace-me-later and pinning\nIssues reported with our previous release were resolved after reports from users on\nour issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at\n#pypy.\n\n\nNew features:\ncffi was updated to version 1.3\nThe python stdlib was updated to 2.7.10 from 2.7.9\nvmprof now supports multiple threads and OS X\nThe translation process builds cffi import libraries for some stdlib\npackages, which should prevent confusion when package.py is not used\nbetter support for gdb debugging\nfreebsd should be able to translate PyPy \u201cout of the box\u201d with no patches\n\n\nNumpy:\nBetter support for record dtypes, including the align keyword\nImplement casting and create output arrays accordingly (still missing some corner cases)\nSupport creation of unicode ndarrays\nBetter support ndarray.flags\nSupport axis argument in more functions\nRefactor array indexing to support ellipses\nAllow the docstrings of built-in numpy objects to be set at run-time\nSupport the buffered nditer creation keyword\n\n\nPerformance improvements:\nDelay recursive calls to make them non-recursive\nSkip loop unrolling if it compiles too much code\nTweak the heapcache\nAdd a list strategy for lists that store both floats and 32-bit integers.\nThe latter are encoded as nonstandard NaNs.  Benchmarks show that the speed\nof such lists is now very close to the speed of purely-int or purely-float\nlists.\nSimplify implementation of ffi.gc() to avoid most weakrefs\nMassively improve the performance of map() with more than\none sequence argument\n\n\n\nPlease try it out and let us know what you think. We welcome\nsuccess stories, experiments,  or benchmarks, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/08/pypy-261-released-3638960649983103796.html"
    },
    {
      "title": "PyPy and ijson - a guest blog post",
      "text": "This gem was posted in the ijson issue tracker after some discussion on #pypy, and Dav1dde kindly allowed us to repost it here:\n\n\"So, I was playing around with parsing huge JSON files (19GiB, testfile is ~520MiB) and wanted to try a sample code with PyPy, turns out, PyPy needed ~1:30-2:00 whereas CPython 2.7 needed ~13 seconds (the pure python implementation on both pythons was equivalent at ~8 minutes). \"Apparantly ctypes is really bad performance-wise, especially on PyPy. So I made a quick CFFI mockup: https://gist.github.com/Dav1dde/c509d472085f9374fc1d\n\nBefore:\nCPython 2.7: \u00a0\u00a0\u00a0 python -m emfas.server size dumps/echoprint-dump-1.json \u00a0\u00a0\u00a0 11.89s user 0.36s system 98% cpu 12.390 total\u00a0\nPYPY: \u00a0\u00a0\u00a0 python -m emfas.server size dumps/echoprint-dump-1.json \u00a0\u00a0\u00a0 117.19s user 2.36s system 99% cpu 1:59.95 total \nAfter (CFFI): CPython 2.7: \u00a0\u00a0\u00a0\u00a0 python jsonsize.py ../dumps/echoprint-dump-1.json\u00a0\u00a0\u00a0\u00a0  8.63s user 0.28s system 99% cpu 8.945 total\u00a0\nPyPy: \u00a0\u00a0\u00a0\u00a0 python jsonsize.py ../dumps/echoprint-dump-1.json \u00a0\u00a0\u00a0\u00a0 4.04s user 0.34s system 99% cpu 4.392 total\n\"\n\n\nDav1dd goes into more detail in the issue itself, but we just want to emphasize a few significant points from this brief interchange:\n\nHis CFFI implementation is faster than the ctypes one even on CPython 2.7.\nPyPy + CFFI is faster than CPython even when using C code to do the heavy parsing.\n\n\u00a0The PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/06/pypy-and-ijson-guest-blog-post-8143007374752482637.html"
    },
    {
      "title": "PyPy 2.6.0 release",
      "text": "PyPy 2.6.0 - Cameo Charm\n\nWe\u2019re pleased to announce PyPy 2.6.0, only two months after PyPy 2.5.1. We are particulary happy to update cffi to version 1.1, which makes the popular ctypes-alternative even easier to use, and to support the new vmprof statistical profiler.\n\n\n\nYou can download the PyPy 2.6.0 release here:\n\n\n\n\nhttps://pypy.org/download.html\n\n\n\n\nWe would like to thank our donors for the continued support of the PyPy project, and for those who donate to our three sub-projects, as well as our volunteers and contributors.\n\n\n\nThanks also to Yury V. Zaytsev and David Wilson who recently started running nightly builds on Windows and MacOSX buildbots.\n\n\n\nWe\u2019ve shown quite a bit of progress, but we\u2019re slowly running out of funds. Please consider donating more, or even better convince your employer to donate, so we can finish those projects! The three sub-projects are:\n\n\n\nPy3k (supporting Python 3.x): We have released a Python 3.2.5 compatible version we call PyPy3 2.4.0, and are working toward a Python 3.3 compatible version\nSTM (software transactional memory): We have released a first working version, and continue to try out new promising paths of achieving a fast multithreaded Python\nNumPy which requires installation of our fork of upstream numpy, available on bitbucket\n\n\n\n\nWe would also like to encourage new people to join the project. PyPy has many layers and we need help with all of them: PyPy and RPython documentation improvements, tweaking popular modules to run on pypy, or general help with making RPython\u2019s JIT even better. Nine new people contributed since the last release, you too could be one of them.\n\n\nWhat is PyPy?\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\n\n\nThis release supports x86 machines on most common operating systems (Linux 32/64, Mac OS X 64, Windows, OpenBSD, freebsd), as well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\n\n\n\nWhile we support 32 bit python on Windows, work on the native Windows 64 bit python is still stalling, we would welcome a volunteer to handle that. We also welcome developers with other operating systems or dynamic languages to see what RPython can do for them.\n\n\n\n\n\nHighlights\n\nPython compatibility:\n\nImprove support for TLS 1.1 and 1.2\nWindows downloads now package a pypyw.exe in addition to pypy.exe\nSupport for the PYTHONOPTIMIZE environment variable (impacting builtin\u2019s __debug__ property)\nIssues reported with our previous release were resolved after reports from users on our issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at #pypy.\n\n\nNew features:\n\nAdd preliminary support for a new lightweight statistical profiler vmprof, which has been designed to accomodate profiling JITted code\n\n\nNumpy:\n\nSupport for object dtype via a garbage collector hook\nSupport for .can_cast and .min_scalar_type as well as beginning a refactoring of the internal casting rules\nBetter support for subtypes, via the __array_interface__, __array_priority__, and __array_wrap__ methods (still a work-in-progress)\nBetter support for ndarray.flags\n\n\nPerformance improvements:\n\nSlight improvement in frame sizes, improving some benchmarks\nInternal refactoring and cleanups leading to improved JIT performance\n\n\nImproved IO performance of zlib and bz2 modules\nWe continue to improve the JIT\u2019s optimizations. Our benchmark suite is now over 7 times faster than cpython\n\n\n\n\n\n\n\nPlease try it out and let us know what you think. We welcome success stories, experiments,  or benchmarks, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/06/pypy-260-release-8983050552628070433.html"
    },
    {
      "title": "CFFI 1.0.1 released",
      "text": "CFFI 1.0.1 final has now been released for CPython!  CFFI is a (CPython and PyPy) module to interact with C code from Python.\nThe main news from CFFI 0.9 is the new way to build extension modules:\nthe \"out-of-line\" mode, where you have a separate build script.  When\nthis script is executed, it produces the extension module.  This comes\nwith associated Setuptools support that fixes the headache of\ndistributing your own CFFI-using packages.  It also massively cuts\ndown the import times.\nAlthough this is a major new version, it should be fully\nbackward-compatible: existing projects should continue to work, in\nwhat is now called the \"in-line mode\".\nThe documentation has been reorganized and split into a few pages.\nFor more information about this new \"out-of-line\" mode, as well as\nmore general information about what CFFI is and how to use it, read the Goals and proceed to\nthe Overview.\nUnlike the 1.0 beta 1 version (ffi.dlopen(), instead of only\nffi.verify().\nPyPy support: PyPy needs integrated support for efficient JITting,\nso you cannot install a different version of CFFI on top of an\nexisting PyPy.  You need to wait for the upcoming PyPy 2.6 to use\nCFFI 1.0---or get a nightly build.\nMy thanks again to the PSF (Python Software Foundation) for their\nfinancial support!\n\nUPDATE:Bug with the first example \"ABI out-of-line\": variadic functions (like printf, ending in a \"...\" argument) crash.  Fixed in CFFI 1.0.2.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/05/cffi-101-released-756545636419794802.html"
    },
    {
      "title": "CFFI 1.0 beta 1",
      "text": "Finally!  CFFI 1.0 is almost ready.  CFFI gives Python developers a convenient way to call external C libraries.  Here \"Python\" == \"CPython or PyPy\", but this post is mostly about the CPython side of CFFI, as the PyPy version is not ready yet.\nOn CPython, you can download the version\n\"1.0.0b1\" either by looking for the cffi-1.0 branch in\nthe repository, or by\nsaying\n\npip install \"cffi>=1.0.dev0\"\n\n(Until 1.0 final is ready,\npip install cffi will still give you version 0.9.2.)\nThe main news: you can now explicitly generate and compile a CPython C\nextension module from a \"build\" script.  Then in the rest of your\nprogram or library, you no longer need to import cffi at all.\nInstead, you simply say:\n\nfrom _my_custom_module import ffi, lib\n\nThen you use ffi and lib just like you did in your\nverify()-based project in CFFI 0.9.2.  (The lib is what used to\nbe the result of verify().)  The details of how you use them\nshould not have changed at all, so that the rest of your program should\nnot need any update.\n\nBenefits\nThis is a big step towards standard practices for making and\ndistributing Python packages with C extension modules:\n\non the one hand, you need an explicit compilation step, triggered\nhere by running the \"build\" script;\non the other hand, what you gain in return is better control over\nwhen and why the C compilation occurs, and more standard ways to write\ndistutils- or setuptools-based setup.py files (see below).\n\nAdditionally, this completely removes one of the main drawbacks of using\nCFFI to interface with large C APIs: the start-up time.  In some cases\nit could be extreme on slow machines (cases of 10-20 seconds on ARM\nboards occur commonly).  Now, the import above is instantaneous.\nIn fact, none of the pure Python cffi package is needed any more at\nruntime (it needs only an internal extension module from CFFI, which\ncan be installed by doing \"pip install cffi-runtime\" [*] if you only need that).\nThe ffi object you get by the import above is of a\ncompletely different class written entirely in C.  The two\nimplementations might get merged in the future; for now they are\nindependent, but give two compatible APIs.  The differences are that\nsome methods like cdef() and verify() and set_source() are\nomitted from the C version, because it is supposed to be a complete FFI\nalready; and other methods like new(), which take as parameter a\nstring describing a C type, are faster now because that string is parsed\nusing a custom small-subset-of-C parser, written in C too.\n\n\nIn practice\nCFFI 1.0 beta 1 was tested on CPython 2.7 and 3.3/3.4, on Linux and to\nsome extent on Windows and OS/X.  Its PyPy version is not ready yet,\nand the only docs available so far are those below.\nThis is beta software, so there might be bugs and details may change.  We are interested in hearing any feedback (irc.freenode.net #pypy) or bug reports.\nTo use the new features, create a source file that is not imported by the rest of\nyour project, in which you place (or move) the code to build the FFI\nobject:\n\n# foo_build.py\nimport cffi\nffi = cffi.FFI()\n\nffi.cdef(\"\"\"\n    int printf(const char *format, ...);\n\"\"\")\n\nffi.set_source(\"_foo\", \"\"\"\n    #include <stdio.h>\n\"\"\")   # and other arguments like libraries=[...]\n\nif __name__ == '__main__':\n    ffi.compile()\n\nThe ffi.set_source() replaces the ffi.verify() of CFFI 0.9.2.\nCalling it attaches the given source code to the ffi object, but this call doesn't\ncompile or return anything by itself.  It may be placed above the ffi.cdef()\nif you prefer.  Its first argument is the name of the C extension module\nthat will be produced.\nActual compilation (including generating the complete C sources) occurs\nlater, in one of two places: either in ffi.compile(), shown above,\nor indirectly from the setup.py, shown next.\nIf you directly execute the file foo_build.py above, it will\ngenerate a local file _foo.c and compile it to _foo.so (or the\nappropriate extension, like _foo.pyd on Windows).  This is the\nextension module that can be used in the rest of your program by saying\n\"from _foo import ffi, lib\".\n\n\nDistutils\nIf you want to distribute your program, you write a setup.py using\neither distutils or setuptools.  Using setuptools is generally\nrecommended nowdays, but using distutils is possible too.  We show it\nfirst:\n\n# setup.py\nfrom distutils.core import setup\nimport foo_build\n\nsetup(\n    name=\"example\",\n    version=\"0.1\",\n    py_modules=[\"example\"],\n    ext_modules=[foo_build.ffi.distutils_extension()],\n)\n\nThis is similar to the CFFI 0.9.2 way.  It only works if cffi was\ninstalled previously, because otherwise foo_build cannot be\nimported.  The difference is that you use ffi.distutils_extension()\ninstead of ffi.verifier.get_extension(), because there is no longer\nany verifier object if you use set_source().\n\n\nSetuptools\nThe modern way is to write setup.py files based on setuptools, which\ncan (among lots of other things) handle dependencies.  It is what you\nnormally get with pip install, too.  Here is how you'd write it:\n\n# setup.py\nfrom setuptools import setup\n\nsetup(\n    name=\"example\",\n    version=\"0.1\",\n    py_modules=[\"example\"],\n    setup_requires=[\"cffi>=1.0.dev0\"],\n    cffi_modules=[\"foo_build:ffi\"],\n    install_requires=[\"cffi-runtime\"],    # see [*] below\n)\n\nNote that \"cffi\" is mentioned on three lines here:\n\nthe first time is in setup_requires, which means that cffi will\nbe locally downloaded and used for the setup.\nthe second mention is a custom cffi_modules argument.  This\nargument is handled by cffi as soon as it is locally downloaded.  It\nshould be a list of \"module:ffi\" strings, where the ffi part\nis the name of the global variable in that module.\nthe third mention is in install_requires.  It means that in\norder to install this example package, \"cffi-runtime\" must also be\ninstalled.  This is (or will be) a PyPI entry that only contains a\ntrimmed down version of CFFI, one that does not include the pure\nPython \"cffi\" package and its dependencies.  None of it is needed at\nruntime.\n\n[*] NOTE: The \"cffi-runtime\" PyPI entry is not ready yet.  For now, use \"cffi>=1.0.dev0\" instead.  Considering PyPy, which has got a built-in \"_cffi_backend\" module, the \"cffi-runtime\" package could never be upgraded there; but it would still be nice if we were able to upgrade the \"cffi\" pure Python package on PyPy.  This might require some extra care in writing the interaction code.  We need to sort it out now...\n\n\nThanks\nSpecial thanks go to the PSF (Python Software Foundation) for their\nfinancial support, without which this work---er... it might likely have occurred anyway, but at an unknown future date :-)\n(For reference, the amount I asked for (and got) is equal to one\nmonth of what a Google Summer of Code student gets, for work that will\ntake a bit longer than one month. At least I personally am running mostly\non such money, and so I want to thank the PSF again for their\ncontribution to CFFI---and while I'm at it, thanks to all other\ncontributors to PyPy---for making this job more than an unpaid hobby on\nthe side :-)\n\nArmin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/05/cffi-10-beta-1-4375652711495636911.html"
    },
    {
      "title": "PyPy-STM 2.5.1 released",
      "text": "PyPy-STM 2.5.1 - Mawhrin-Skel\n\nWe're pleased to announce PyPy-STM 2.5.1, codenamed Mawhrin-Skel.\nThis is the second official release of PyPy-STM.  You can download\nthis release here (64-bit Linux only):\n\nhttps://pypy.org/download.html\nDocumentation:\n\nhttps://pypy.readthedocs.org/en/latest/stm.html\nPyPy is an implementation of the Python programming language which focuses\non performance. So far we've been relentlessly optimizing for the single\ncore/process scenario. PyPy STM brings to the table a version of PyPy\nthat does not have the infamous Global Interpreter Lock, hence can run\nmultiple threads on multiple cores. Additionally it comes with a set\nof primitives that make writing multithreaded applications a lot easier,\nas explained below (see TransactionQueue) and in the documentation.\nInternally, PyPy-STM is based on the Software Transactional Memory\nplug-in called stmgc-c7.  This version comes with a relatively\nreasonable single-core overhead but scales only up to around 4 cores\non some examples; the next version of the plug-in, stmgc-c8, is in\ndevelopment and should address that limitation (as well as reduce the\noverhead).  These versions only support 64-bit Linux; we'd welcome\nsomeone to port the upcoming stmgc-c8 to other (64-bit) platforms.\nThis release passes all regular PyPy tests, except for a few\nspecial cases.  In other words, you should be able to drop in\nPyPy-STM instead of the regular PyPy and your program should still\nwork.  See current status for more information.\nThis work was done by Remi Meier and Armin Rigo.  Thanks to all donors\nfor crowd-funding the STM work so far!  As usual, it took longer\nthan we would have thought.  I really want to thank the people that\nkept making donations anyway.  Your trust is greatly appreciated!\n\n\nWhat's new?\nCompared to the July 2014 release, the main addition is a way to\nget reports about STM conflicts.  This is an essential new feature.\nTo understand why this is so important, consider that if you already\nplayed around with the previous release, chances are that you didn't\nget very far.  It probably felt like a toy: on very small examples it\nwould nicely scale, but on any larger example it would not scale at\nall.  You didn't get any feedback about why, but the underlying reason\nis that, in a typical large example, there are some STM conflicts that\noccur all the time and that won't be immediately found just by\nthinking.  This prevents any parallelization.\nNow PyPy-STM is no longer a black box: you have a way to learn about\nthese conflicts, fix them, and try again.  The tl;dr version is to run:\n\n    PYPYSTM=stmlog ./pypy-stm example.py\n    ./print_stm_log.py stmlog\n\nMore details in the STM user guide.\n\n\n\nPerformance\nThe performance is now more stable than it used to be.  More\nprecisely, the best case is still \"25%-40% single-core slow-down with\nvery good scaling up to 4 threads\", but the average performance seems\nnot too far from that.  There are still dark spots --- notably, the\nJIT is still slower to warm up, though it was improved a lot.  These\nare documented in the current status section.  Apart from\nthat, we should not get more than 2x single-core slow-down in the\nworst case.  Please report such cases as bugs!\n\n\n\nTransactionQueue\nAs explained before, PyPy-STM is more than \"just\" a Python without\nGIL.  It is a Python in which you can do minor tweaks to your\nexisting, non-multithreaded programs and get them to use multiple\ncores.  You identify medium- or large-sized, likely-independent parts\nof the code and to ask PyPy-STM to run these parts in parallel.  An\nexample would be every iteration of some outermost loop over all items\nof a dictionary.  This is done with a new API:\ntransaction.TransactionQueue().  See help(TransactionQueue) or\nread more about it in the STM user guide.\nThis is not a 100% mechanical change: very likely, you need to hunt\nfor and fix \"STM conflicts\" that prevent parallel execution (see\ndocs).  However, at all points your program runs correctly, and you\ncan stop the hunt when you get acceptable performance.  You don't get\ndeadlocks or corrupted state.\n\nThanks for reading!\nArmin, Remi, Fijal",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/03/pypy-stm-251-released-1342113838236225773.html"
    },
    {
      "title": "PyPy 2.5.1 Released",
      "text": "PyPy 2.5.1 - Pineapple Bromeliad\nWe\u2019re pleased to announce PyPy 2.5.1, Pineapple Bromeliad following on the heels of 2.5.0. You can download the PyPy 2.5.1 release here:\n\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject, and for those who donate to our three sub-projects, as well as our\nvolunteers and contributors.\nWe\u2019ve shown quite a bit of progress, but we\u2019re slowly running out of funds.\nPlease consider donating more, or even better convince your employer to donate,\nso we can finish those projects! The three sub-projects are:\n\n\n\n\nPy3k (supporting Python 3.x): We have released a Python 3.2.5 compatible version we call PyPy3 2.4.0, and are working toward a Python 3.3 compatible version\n\u00a0\n\n\n\nSTM (software transactional memory): We have released a first working version,\nand continue to try out new promising paths of achieving a fast multithreaded Python\n\n\n\n\nNumPy which requires installation of our fork of upstream numpy,\navailable on bitbucket\n\n\nWe would also like to encourage new people to join the project. PyPy has many\nlayers and we need help with all of them: PyPy and Rpython documentation\nimprovements, tweaking popular modules to run on pypy, or general help with making\nRpython\u2019s JIT even better.\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\n\n\nThis release supports x86 machines on most common operating systems\n(Linux 32/64, Mac OS X 64, Windows, and OpenBSD),\nas well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\n\n\nWhile we support 32 bit python on Windows, work on the native Windows 64\nbit python is still stalling, we would welcome a volunteer\nto handle that.\n\n\n\n\nHighlights\n\nThe past months have seen pypy mature and grow, as rpython becomes the goto\nsolution for writing fast dynamic language interpreters. Our separation of\nRpython from the python interpreter PyPy is now much clearer in the\nPyPy documentation  and we now have seperate RPython documentation.\nTell us what still isn\u2019t clear, or even better help us improve the documentation. \n\n\n\n\nWe merged version 2.7.9 of python\u2019s stdlib. From the python release notice:\nThe entirety of Python 3.4\u2019s ssl module has been backported.\nSee PEP 466 for justification.\nHTTPS certificate validation using the system\u2019s certificate store is now\nenabled by default. See PEP 476 for details.\nSSLv3 has been disabled by default in httplib and its reverse dependencies\ndue to the POODLE attack.\nThe ensurepip module has been backported, which provides the pip\npackage manager in every Python 2.7 installation. See PEP 477.\n\n\n\nThe garbage collector now ignores parts of the stack which did not change\nsince the last collection, another performance boost \n\n\nerrno and LastError are saved around cffi calls so things like pdb will not\noverwrite it \n\n\nWe continue to asymptotically approach a score of 7 times faster than cpython\non our benchmark suite, we now rank 6.98 on latest runs \n\n\nIssues reported with our previous release were resolved after reports from users on\nour issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at\n#pypy.\n\nPlease try it out and let us know what you think. We welcome\nsuccess stories, experiments,  or benchmarks, we know you are using PyPy, please tell us about it!\n\nCheers\n\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/03/pypy-251-released-5657064769385723517.html"
    },
    {
      "title": "Pydgin: Using RPython to Generate Fast Instruction-Set Simulators",
      "text": "Note: This is a guest blog post by Derek Lockhart and Berkin Ilbeyi from\nComputer Systems Laboratory of Cornell University.\nIn this blog post I'd like to describe some recent work on using the RPython\ntranslation toolchain to generate fast instruction set simulators.\nOur open-source framework, Pydgin [a], provides a domain-specific\nlanguage (DSL) embedded in Python for concisely describing instruction set\narchitectures [b] and then uses these descriptions to generate fast,\nJIT-enabled simulators.\nPydgin will be presented at the IEEE International Symposium on Performance\nAnalysis of Systems and Software (ISPASS) and in this post we provide a\npreview of that work.\nIn addition, we discuss some additional progress updates that occurred after\nthe publishing deadline and will not appear in the final paper [1].\nOur area of research expertise is computer architecture, which is perhaps an\nunfamiliar topic for some readers of the PyPy blog.\nBelow we provide some brief background on hardware simulation in the field of\ncomputer architecture, as well as some context as to why instruction set\nsimulators in particular are such an important tool.\n\nSimulators: Designing Hardware with Software\nFor computer architects in both academia and industry, a key step in designing\nnew computational hardware (e.g., CPUs, GPUs, and mobile system-on-chips) is\nsimulation [c] of the target system.\nWhile numerous models for simulation exist, three classes are particularly\nimportant in hardware design.\nFunctional Level models simulate the behavior of the target system.\nThese models are useful for creating a \"golden\" reference which can serve as an\nexecutable specification or alternatively as an emulation platform for software\ndevelopment.\nCycle Level models aim to simulate both the behavior and the approximate\ntiming of a hardware component.\nThese models help computer architects explore design tradeoffs and quickly\ndetermine things like how big caches should be, how many functional units are\nneeded to meet throughput targets, and how the addition of a custom accelerator\nblock may impact total system performance.\nRegister-Transfer Level (RTL) models specify the behavior, timing, and\nresources (e.g., registers, wires, logic gates) of a hardware component.\nRTL models are bit-accurate hardware specifications typically written in a\nhardware description language (HDL) such as Verilog or VHDL.\nOnce verified through extensive simulation, HDL specifications can be passed\ninto synthesis and place-and-route tools to estimate area/energy/timing or to\ncreate FPGA or ASIC prototypes.\nAn instruction set simulator (ISS) is a special kind of\nfunctional-level model that simulates the behavior of a processor or\nsystem-on-chip (SOC).  ISSs serve an important role in hardware design\nbecause they model the instruction set architecture (ISA) interface: the\ncontractual boundary between hardware designers and software developers.\nISSs allow hardware designers to quickly experiment with adding new processor\ninstructions while also allowing software developers to build new compilers,\nlibraries, and applications long before physical silicon is available.\n\n\nInstruction-Set Simulators Must be Fast and Productive\nInstruction-set simulators are more important than ever because the ISA\nboundary has become increasingly fluid.\nWhile Moore's law has continued to deliver larger numbers of transistors\nwhich computer architects can use to build increasingly complex chips, limits\nin Dennard scaling have restricted how these transistors can be used [d].\nIn more simple terms, thermal constraints (and energy constraints in mobile\ndevices) have resulted in a growing interest in pervasive specialization:\nusing custom accelerators to more efficiently perform compute intensive tasks.\nThis is already a reality for designers of mobile SOCs who continually add new\naccelerator blocks and custom processor instructions in order to achieve higher\nperformance with less energy consumption.\nISSs are indispensable tools in this SOC design process for both hardware\narchitects building the silicon and software engineers developing the software\nstack on top of it.\nAn instruction set simulator has two primary responsibilities: 1) accurately\nemulating the external execution behavior of the target, and 2) providing\nobservability by accurately reproducing the target's internal state (e.g.,\nregister values, program counter, status flags) at each time step.\nHowever, other qualities critical to an effective ISS are simulation\nperformance and designer productivity.\nSimulation performance is important because shorter simulation times allow\ndevelopers to more quickly execute and verify large software applications.\nDesigner productivity is important because it allows hardware architects to\neasily experiment with adding new instructions and estimate their impact on\napplication performance.\nTo improve simulation performance, high-performance ISSs use dynamic binary\ntranslation (DBT) as a mechanism to translate frequently visited blocks of\ntarget instructions into optimized sequences of host instructions.\nTo improve designer productivity, many design toolchains automatically generate\nISSs from an architectural description language (ADL): a special\ndomain-specific language for succinctly specifying instruction encodings and\ninstruction semantics of an ISA.\nVery few existing systems have managed to encapsulate the design complexity of\nDBT engines such that high-performance, DBT-accelerated ISSs could be\nautomatically generated from ADLs [e].\nUnfortunately, tools which have done so are either proprietary software or\nleave much to be desired in terms of performance or productivity.\n\n\nWhy RPython?\nOur research group learned of the RPython translation toolchain through our\nexperiences with PyPy, which we had used in conjunction with our Python\nhardware modeling framework to achieve significant improvements in simulation\nperformance [2].\nWe realized that the RPython translation toolchain could potentially be adapted\nto create fast instruction set simulators since the process of interpreting\nexecutables comprised of binary instructions shared many similarities with the\nprocess of interpreting bytecodes in a dynamic-language VM.\nIn addition, we were inspired by PyPy's meta-tracing approach to JIT-optimizing\nVM design which effectively separates the process of specifying a language\ninterpreter from the optimization machinery needed to achieve good performance.\nExisting ADL-driven ISS generators have tended to use domain-specific\nlanguages that require custom parsers or verbose C-based syntax that\ndistracts from the instruction specification.\nCreating an embedded-ADL within Python provides several benefits over these\nexisting approaches including a gentler learning curve for new users, access to\nbetter debugging tools, and easier maintenance and extension by avoiding a\ncustom parser.\nAdditionally, we have found that the ability to directly execute Pydgin\nISA descriptions in a standard Python interpreter such as CPython or PyPy\nsignificantly helps debugging and testing during initial ISA exploration.\nPython's concise, pseudocode-like syntax also manages to map quite closely to\nthe pseudocode specifications provided by many ISA manuals [f].\n\n\nThe Pydgin embedded-ADL\nDefining a new ISA in the Pydgin embedded-ADL requires four primary pieces of\ninformation: the architectural state (e.g. register file, program counter,\ncontrol registers), the bit encodings of each instruction, the instruction\nfields, and the semantic definitions for each instruction. Pydgin aims to make\nthis process as painless as possible by providing helper classes and functions\nwhere possible.\nFor example, below we provide a truncated example of the ARMv5 instruction\nencoding table. Pydgin maintains encodings of all instructions in a centralized\nencodings data structure for easy maintenance and quick lookup. The\nuser-provided instruction names and bit encodings are used to automatically\ngenerate decoders for the simulator. Unlike many ADLs, Pydgin does not require\nthat the user explicitly specify instruction types or mask bits for field\nmatching because the Pydgin decoder generator can automatically infer decoder\nfields from the encoding table.\n\nencodings = [\n  ['adc',      'xxxx00x0101xxxxxxxxxxxxxxxxxxxxx'],\n  ['add',      'xxxx00x0100xxxxxxxxxxxxxxxxxxxxx'],\n  ['and',      'xxxx00x0000xxxxxxxxxxxxxxxxxxxxx'],\n  ['b',        'xxxx1010xxxxxxxxxxxxxxxxxxxxxxxx'],\n  ['bl',       'xxxx1011xxxxxxxxxxxxxxxxxxxxxxxx'],\n  ['bic',      'xxxx00x1110xxxxxxxxxxxxxxxxxxxxx'],\n  ['bkpt',     '111000010010xxxxxxxxxxxx0111xxxx'],\n  ['blx1',     '1111101xxxxxxxxxxxxxxxxxxxxxxxxx'],\n  ['blx2',     'xxxx00010010xxxxxxxxxxxx0011xxxx'],\n  # ...\n  ['teq',      'xxxx00x10011xxxxxxxxxxxxxxxxxxxx'],\n  ['tst',      'xxxx00x10001xxxxxxxxxxxxxxxxxxxx'],\n]\n\nA major goal of Pydgin was ensuring instruction semantic definitions map to ISA\nmanual specifications as much as possible. The code below shows one such\ndefinition for the ARMv5 add instruction.\nA user-defined Instruction class (not shown) specifies field names that can\nbe used to conveniently access bit positions within an instruction (e.g.\nrd, rn, S).\nAdditionally, users can choose to define their own helper functions, such as\nthe condition_passed function, to create more concise syntax that better\nmatches the ISA manual.\n\ndef execute_add( s, inst ):\n  if condition_passed( s, inst.cond() ):\n    a,   = s.rf[ inst.rn() ]\n    b, _ = shifter_operand( s, inst )\n    result = a + b\n    s.rf[ inst.rd() ] = trim_32( result )\n\n    if inst.S():\n      if inst.rd() == 15:\n        raise FatalError('Writing SPSR not implemented!')\n      s.N = (result >> 31)&1\n      s.Z = trim_32( result ) == 0\n      s.C = carry_from( result )\n      s.V = overflow_from_add( a, b, result )\n\n    if inst.rd() == 15:\n      return\n\n  s.rf[PC] = s.fetch_pc() + 4\n\nCompared to the ARM ISA Reference manual shown below, the Pydgin instruction\ndefinition is a fairly close match. Pydgin's definitions could certainly be\nmade more concise by using a custom DSL, however, this would lose many of the\ndebugging benefits afforded to a well-supported language such as Python and\nadditionally require using a custom parser that would likely need modification\nfor each new ISA.\n\nif ConditionPassed(cond) then\n   Rd = Rn + shifter_operand\n   if S == 1 and Rd == R15 then\n     if CurrentModeHasSPSR() then CPSR = SPSR\n   else UNPREDICTABLE else if S == 1 then\n     N Flag = Rd[31]\n     Z Flag = if Rd == 0 then 1 else 0\n     C Flag = CarryFrom(Rn + shifter_operand)\n     V Flag = OverflowFrom(Rn + shifter_operand)\n\nCreating an ISS that can run real applications is a rather complex task, even\nfor a bare metal simulator with no operating system such as Pydgin.\nEach system call in the C library must be properly implemented, and\nbootstrapping code must be provided to set up the program stack and\narchitectural state.\nThis is a very tedious and error prone process which Pydgin tries to\nencapsulate so that it remains as transparent to the end user as possible.\nIn future versions of Pydgin we hope to make bootstrapping more painless and\nsupport a wider variety of C libraries.\n\n\n\n\nPydgin Performance\nIn order to achieve good simulation performance from Pydgin ISSs, significant\nwork went into adding appropriate JIT annotations to the Pydgin library\ncomponents.\nThese optimization hints, which allow the JIT generated by the RPython\ntranslation toolchain to produce more efficient code, have been specifically\nselected for the unique properties of ISSs.\nFor the sake of brevity, we do not talk about the exact optimizations here but\na detailed discussion can be found in the ISPASS paper [1].\nIn the paper we evaluate two ISSs, one for a simplified MIPS ISA and another\nfor the ARMv5 ISA, whereas below we only discuss results for the ARMv5 ISS.\nThe performance of Pydgin-generated ARMv5 ISSs were compared against\nseveral reference ISSs: the gem5 ARM atomic simulator (gem5),\ninterpretive and JIT-enabled versions of SimIt-ARM (simit-nojit and\nsimit-jit), and QEMU.\nAtomic models from the gem5 simulator were chosen for comparison due their wide\nusage amongst computer architects [g].\nSimIt-ARM was selected because it is currently the highest performance\nADL-generated DBT-ISS publicly available.\nQEMU has long been held as the gold-standard for DBT simulators due to its\nextremely high performance, however, QEMU is generally intended for usage as an\nemulator rather than a simulator [c] and therefore achieves its excellent\nperformance at the cost of observability.\nUnlike QEMU, all other simulators in our study faithfully track architectural\nstate at an instruction level rather than block level.\nPydgin ISSs were generated with and without JITs using the RPython translation\ntoolchain in order to help quantify the performance benefit of the meta-tracing\nJIT.\nThe figure below shows the performance of each ISS executing applications from\nthe SPEC CINT2006 benchmark suite [h].\nBenchmarks were run to completion on the high-performance DBT-ISSs\n(simit-jit, pydgin-jit, and QEMU), but were terminated after only\n10 billion simulated instructions for the non-JITed interpretive ISSs\n(these would require many hours, in some cases days, to run to completion).\nSimulation performance is measured in MIPS [i] and plotted on a log\nscale due to the wide variance in performance.\nThe WHMEAN group summarizes each ISS's performance across all benchmarks\nusing the weighted harmonic mean.\n\n\n\nA few points to take away from these results:\n\nISSs without JITs (gem5, simit-nojit, and pydgin-nojit) demonstrate\nrelatively consistent performance across applications, whereas ISSs with JITs\n(simit-jit, pydgin-jit, and QEMU) demonstrate much greater\nperformance variability from application-to-application.\nThe gem5 atomic model demonstrates particularly miserable performance, only\n2-3 MIPS!\nQEMU lives up to its reputation as a gold-standard for simulator performance,\nleading the pack on nearly every benchmark and reaching speeds of 240-1120\nMIPS.\npydgin-jit is able to outperform simit-jit on four of the\napplications, including considerable performance improvements of 1.44\u20131.52\u00d7\nfor the applications 456.hmmer, 462.libquantum, and 471.omnetpp\n(managing to even outperform QEMU on 471.omnetpp).\nsimit-jit is able to obtain much more consistent performance (230-459\nMIPS across all applications) than pydgin-jit (9.6-659 MIPS).  This is\ndue to simit-jit's page-based approach to JIT optimization compared to\npydgin-jit's tracing-based approach.\n464.h264ref displays particularly bad pathological behavior in Pydgin\u2019s\ntracing JIT and is the only application to perform worse on pydgin-jit\nthan pydgin-nojit (9.6 MIPS vs. 21 MIPS).\n\nThe pathological behavior demonstrated by 464.h264ref was of particular\nconcern because it caused pydgin-jit to perform even worse than having no\nJIT at all. RPython JIT logs indicated that the reason for this performance\ndegradation was a large number of tracing aborts due to JIT traces growing too\nlong. However, time limitations before the publication deadline prevented us\nfrom investigating this issue thoroughly.\nSince the deadline we've applied some minor bug fixes and made some small\nimprovements in the memory representation.\nMore importantly, we've addressed the performance degradation in 464.h264ref\nby increasing trace lengths for the JIT.\nBelow we show how the performance of 464.h264ref changes as the\ntrace_limit parameter exposed by the RPython JIT is varied from the default\nsize of 6000 operations.\n\n\n\n\nBy quadrupling the trace limit we achieve an 11x performance improvement in\n464.h264ref.\nThe larger trace limit allows the JIT to optimize long code paths that were\npreviously triggering trace aborts, greatly helping amortize the costs of\ntracing.\nNote that arbitrarily increasing this limit can potentially hurt performance if\nlonger traces are not able to detect optimizable code sequences.\nAfter performing similar experiments across the applications in the SPEC\nCINT2006 benchmark suite, we settled on a trace limit of 400,000 operations.\nIn the figure below we show how the updated Pydgin ISS (pydgin-400K) improves\nperformance across all benchmarks and fixes the performance degradation\npreviously seen in 464.h264ref. Note that the non-JITted simulators have been\nremoved for clarity, and simulation performance is now plotted on a\nlinear scale to more clearly distinguish the performance gap between\neach ISS.\n\n\n\nWith these improvements, we are now able to beat simit-jit on all but two\nbenchmarks. In future work we hope to further close the gap with QEMU as well.\n\n\nConclusions and Future Work\nPydgin demonstrates that the impressive work put into the RPython translation\ntoolchain, designed to simplify the process of building fast dynamic-language\nVMs, can also be leveraged to build fast instruction set simulators.\nOur prototype ARMv5 ISS shows that Pydgin can generate ISSs with performance\ncompetitive to SimIt-ARM while also providing a more productive development\nexperience: RPython allowed us to develop Pydgin with only four person-months\nof work.\nAnother significant benefit of the Pydgin approach is that any performance\nimprovements applied to the RPython translation toolchain immediately benefit\nPydgin ISSs after a simple software download and retranslation.\nThis allows Pydgin to track the continual advances in JIT technology introduced\nby the PyPy development team.\nPydgin is very much a work in progress. There are many features we would like\nto add, including:\n\nmore concise syntax for accessing arbitrary instruction bits\nsupport for other C libraries such as glibc, uClibc, and musl\n(we currently only support binaries compiled with newlib)\nsupport for self-modifying code\nfeatures for more productive debugging of target applications\nISS descriptions for other ISAs such as RISC-V, ARMv8, and x86\nautomatic generation of compilers and toolchains from Pydgin descriptions\n\nIn addition, we think there are opportunities for even greater performance\nimprovements with more advanced techniques such as:\n\nautomatic generation of optimized instruction decoders\noptimizations for floating-point intensive applications\nmultiple tracing-JITs for parallel simulation of multicore SOCs\na parallel JIT compilation engine as proposed by Bo\u0308hm et al. [3]\n\nWe hope that Pydgin can be of use to others, so if you try it out please let us\nknow what you think. Feel free to contact us if you find any of the above\ndevelopment projects interesting, or simply fork the project on GitHub and hack\naway!\n-- Derek Lockhart and Berkin Ilbeyi\n\n\nAcknowledgements\n We would like to sincerely thank Carl Friedrich Bolz and Maciej Fijalkowski for their feedback on the Pydgin publication and their guidance on improving the JIT performance of our simulators. We would also like to thank for the whole PyPy team for their incredible work on the PyPy and the RPython translation toolchain. Finally, thank you to our research advisor, Prof. Christopher Batten, and the sponsors of this work which include the National Science Foundation, the Defense Advanced Research Projects Agency, and Intel Corporation.\n\n\nFootnotes\n\n\n\n[a]Pydgin loosely stands for [Py]thon [D]SL for [G]enerating\n[In]struction set simulators and is pronounced the same as \u201cpigeon\u201d. The\nname is inspired by the word \u201cpidgin\u201d which is a grammatically simplified\nform of language and captures the intent of the Pydgin embedded-ADL.\nhttps://github.com/cornell-brg/pydgin\n\n\n\n\n\n[b]Popular instruction set architectures (ISAs) include MIPs, ARM,\nx86, and more recently RISC-V\n\n\n\n\n\n[c](1, 2) For a good discussion of simulators vs. emulators, please see the\nfollowing post on StackOverflow:\nhttps://stackoverflow.com/questions/1584617/simulator-or-emulator-what-is-the-difference\n\n\n\n\n\n[d]https://en.wikipedia.org/wiki/Dark_silicon\n\n\n\n\n\n[e]Please see the Pydgin paper for a more detailed discussion of prior work.\n\n\n\n\n\n[f]For more examples of Pydgin ISA specifications, please see the ISPASS\npaper [1] or the Pydgin source code on GitHub.\nPydgin instruction definitions for a simple MIPS-inspired ISA can be\nfound here:\n\nhttps://github.com/cornell-brg/pydgin/blob/master/parc/isa.py\n\nPydgin instruction definitions for a simplified ARMv5 ISA can be found\nhere:\n\nhttps://github.com/cornell-brg/pydgin/blob/master/arm/isa.py\n\n\n\n\n\n\n\n[g]gem5 is a cycle-level simulation framework that contains both\nfunctional-level (atomic) and cycle-level processor models. Although\nprimarily used for detailed, cycle-approximate processor simulation,\ngem5's atomic model is a popular tool for many ISS tasks.\n\nhttps://www.m5sim.org/SimpleCPU\n\n\n\n\n\n\n\n[h]All performance measurements were taken on an unloaded server-class\nmachine.\n\n\n\n\n\n[i]Millions of instructions per second.\n\n\n\n\nReferences\n\n\n\n[1](1, 2, 3) Derek Lockhart, Berkin Ilbeyi, and Christopher Batten. \"Pydgin:\nGenerating Fast Instruction Set Simulators from Simple Architecture\nDescriptions with Meta-Tracing JIT Compilers.\" IEEE Int'l Symp. on\nPerformance Analysis of Systems and Software (ISPASS), Mar. 2015.\n\nhttps://csl.cornell.edu/~cbatten/pdfs/lockhart-pydgin-ispass2015.pdf\nhttps://github.com/cornell-brg/pydgin\n\n\n\n\n\n\n\n[2]Derek Lockhart, Gary Zibrat, and Christopher Batten. \"PyMTL: A Unified\nFramework for Vertically Integrated Computer Architecture Research.\" 47th\nACM/IEEE Int'l Symp. on Microarchitecture (MICRO-47), Dec. 2014.\n\nhttps://csl.cornell.edu/~cbatten/pdfs/lockhart-pymtl-micro2014.pdf\nhttps://github.com/cornell-brg/pymtl\n\n\n\n\n\n\n\n[3]I. Bo\u0308hm, B. Franke, and N. Topham. Generalized Just-In-Time Trace\nCompilation Using a Parallel Task Farm in a Dynamic Binary Translator.\nACM SIGPLAN Conference on Programming Language Design and Implementation\n(PLDI), Jun 2011.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/03/pydgin-using-rpython-to-generate-fast-1514065178985838697.html"
    },
    {
      "title": "Experiments in Pyrlang with RPython",
      "text": "Pyrlang is an Erlang BEAM bytecode interpreter written in RPython.\nIt implements approximately 25% of BEAM instructions. It can support\ninteger calculations (but not bigint), closures, exception handling,\nsome operators to atom, list and tuple, user modules, and multi-process\nin single core. Pyrlang is still in development.\nThere are some differences between BEAM and the VM of PyPy:\n\nBEAM is a register-based VM, whereas the VM in PyPy is stack-based.\nThere is no traditional call-stack in BEAM. The Y register in BEAM is\nsimilar to a call-stack, but the Y register can sometimes store some\nvariables.\nThere are no typical language-level threads and OS-level threads in\nBEAM; only language-level processes, whose behavior is very similar\nto the actor model.\n\nRegarding bytecode dispatch loop, Pyrlang uses a while loop to fetch\ninstructions and operands, call the function corresponding to every\ninstruction, and jump back to the head of the while loop. Due to the\ndifferences between the RPython call-stack and BEAM\u2019s Y register, we\ndecided to implement and manage the Y register by hand. On the other\nhand, PyPy uses RPython\u2019s call stack to implement Python\u2019s call stack.\nAs a result, the function for the dispatch loop in PyPy calls itself\nrecursively. This does not happen in Pyrlang.\nThe Erlang compiler (erlc) usually compiles the bytecode instructions\nfor function invocation into CALL (for normal invocation) and CALL_ONLY\n(for tail recursive invocation). You can use a trampoline semantic to\nimplement it:\n\nCALL instruction: The VM pushes the current instruction pointer (or\ncalled-program counter in PyPy) to the Y register, and jumps to the\ndestination label. When encountering a RETURN instruction, the VM\npops the instruction pointer from the Y register and returns to the\nlocation of the instruction pointer to continue executing the outer\nfunction.\nCALL_ONLY instruction: The VM simply jumps to the destination label,\nwithout any modification of the Y register. As a result, the tail\nrecursive invocation never increases the Y register.\n\nThe current implementation only inserts the JIT hint of can_enter_jit\nfollowing the CALL_ONLY instruction. This means that the JIT only\ntraces the tail-recursive invocation in Erlang code, which has a very\nsimilar semantic to the loop in imperative programming languages like\nPython.\nWe have also written a single scheduler to implement the language level\nprocess in a single core. There is a runable queue in the scheduler. On\neach iteration, the scheduler pops one element (which is a process\nobject with dispatch loop) from the queue, and executes the dispatch\nloop of the process object. In the dispatch loop, however, there is a\ncounter-call \u201creduction\u201d inside the dispatch loop. The reduction\ndecrements during the execution of the loop, and when the reduction\nbecomes 0, the dispatch loop terminates. Then the scheduler pushes that\nelement into the runable queue again, and pops the next element for the\nqueue, and so on.\nWe are planning to implement a multi-process scheduler for multi-core\nCPUs, which will require multiple schedulers and even multiple runable\nqueues for each core, but that will be another story. :-)\n\nMethods\nWe wrote two benchmark programs of Erlang:\n\nFACT: A benchmark to calculate the factorial in a tail-recursive\nstyle, but because we haven\u2019t implemented big int, we do a remainder\ncalculation to the argument for the next iteration, so the number\nnever overflows.\nREVERSE: The benchmark creates a reversed list of numbers, such as\n[20000, 19999, 19998, \u2026], and applies a bubble sort to it.\n\n\n\nResults\n\nThe Value of Reduction\nWe used REVERSE to evaluate the JIT with different values of\nreduction:\n\n\nThe X axis is the value of reduction, and the Y axis is the execution\ntime (by second).\nIt seems that when the value of reduction is small, the reduction\ninfluences the performance significantly, but when reduction becomes\nlarger, it only increases the speed very slightly. In fact, we use 2000\nas the default reduction value (as well as the reduction value in the\nofficial Erlang interpreter).\nSurprisingly, the trace is always generated even when the reduction is\nvery small, such as 0, which means the dispatch loop can only run for a\nvery limited number of iterations, and the language level process\nexecutes fewer instructions than an entire loop in one switch of the\nscheduler). The generated trace is almost the same, regardless of\ndifferent reduction values.\nActually, the RPython JIT only cares what code it meets, but does not\ncare who executes it, thus the JIT always generates the results above.\nThe trace even can be shared among different threads if they execute the\nsame code.\nThe overhead at low reduction value may be due to the scheduler, which\nswitches from different processes too frequently, or from the\ntoo-frequent switching between bytecode interpreter and native code, but\nnot from JIT itself.\nHere is more explanation from Armin Rigo:\n\n\u201cThe JIT works well because you\u2019re using a scheme where some counter\nis decremented (and the soft-thread interrupted when it reaches\nzero) only once in each app-level loop. The soft-thread switch is\ndone by returning to some scheduler, which will resume a different\nsoft-thread by calling it. It means the JIT can still compile each\nof the loops as usual, with the generated machine code containing\nthe decrease-and-check-for-zero operation which, when true, exits\nthe assembler.\"\n\n\nFair Process Switching vs. Unfair Process Switching\nWe are also concerned about the timing for decreasing reduction value.\nIn our initial version of Pyrlang, we decrease reduction value at every\nlocal function invocation, module function invocation, and BIF (built-in\nfunction) invocation, since this is what the official Erlang interpreter\ndoes. However, since the JIT in RPython basically traces the target\nlanguage loop (which is the tail recursive invocation in Pyrlang) it is\ntypically better to keep the loop whole during a switch of the language\nlevel process. We modified Pyrlang, and made the reduction decrement\nonly occur after CALL_ONLY, which is actually the loop boundary of the\ntarget language.\nOf course, this strategy may cause an \u201cunfair\u201d execution among language\nlevel processes. For example, if one process has only a single\nlong-sequence code, it executes until the end of the code. On the other\nhand, if a process has a very short loop, it may be executed by very\nlimited steps then be switched out by the scheduler. However, in the\nreal world, this \u201cunfairness\u201d is usually considered acceptable, and is\nused in many VM implementations including PyPy for improving the overall\nperformance.\nWe compared these two versions of Pyrlang in the FACT benchmark. The\nreduction decrement is quite different because there are some BIF\ninvocations inside the loop. In the old version the process can be\nsuspended at loop boundaries or other function invocation, but in the\nnew version, it can be suspended only at loop boundaries.\nWe show that the strategy is effective, removing around 7% of the\noverhead. We have also compared it in REVERSE, but since there are no\nextra invocations inside the trace, it cannot provide any performance\nimprovement. In the real world, we believe there is usually more than\none extra invocation inside a single loop, so this strategy is effective\nfor most cases.\n\n\nComparison with Default Erlang and HiPE\nWe compared the performance of Pyrlang with the default Erlang\ninterpreter and the HiPE (High Performance Erlang) complier. HiPE is an\nofficial Erlang compiler that can compile Erlang source code to native\ncode. The speed of Erlang programs obviously improves but loses its\ngenerality instead.\nPlease note that Pyrlang is still in development, so in some situations\nit does less work than the default Erlang interpreter, such as not\nchecking integer overflow when dealing with big integer, and not\nchecking and adding locks when accessing message queues in the\nlanguage-level process, so is therefore faster. The final version of\nPyrlang may be slower.\nWe used the two benchmark programs above, and made sure both of them are\nexecuted for more than five seconds to cover the JIT warm-up time for\nRPython. The experiment environment is a OS X 10.10 machine with 3.5GHZ\n6-core Intel Xeon E5 CPU and 14GB 1866 MHz DDR3 ECC memory.\nLet\u2019s look at the result of FACT. The graph shows that Pyrlang runs\n177.41% faster on average than Erlang, and runs at almost the same speed\nas HiPE. However, since we haven\u2019t implemented big integer in Pyrlang,\nthe arithmetical operators do not do any extra overflow checking. It is\nreasonable that the final version for Pyrlang will be slower than the\ncurrent version and HiPE.\n\nAs for REVERSE, the graph shows that Pyrlang runs 45.09% faster than\nErlang, but 63.45% slower than HiPE on average. We think this is\nreasonable because there are only few arithmetical operators in this\nbenchmark so the speeds of these three implementations are closer.\nHowever, we observed that at the scale of 40,000, the speed of Pyrlang\nslowed down significantly (111.35% slower than HiPE) compared with the\nother two scales (56.38% and 22.63% slower than HiPE).\nUntil now we can only hypothesize why Pyrlang slows down at that scale.\nWe guess that the overhead might be from GC. This is because the BEAM\nbytecode provides some GC hints to help the default Erlang compiler to\nperform some GC operations immediately. For example, using GC_BIF\ninstead of a BIF instruction tells the VM that there may be a GC\nopportunity, and tells the VM how many live variables should be around\none instruction. In Pyrlang we do not use these kinds of hints but rely\non RPython\u2019s GC totally. When there are a huge number of objects during\nruntime, (as for REVERSE, it should be the Erlang list object) the speed\ntherefore slows down.\n\nRuochen Huang",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/02/experiments-in-pyrlang-with-rpython-8103387814587972227.html"
    },
    {
      "title": "linalg support in pypy/numpy",
      "text": "Introduction\nPyPy's numpy support has matured enough that it can now support the  lapack/blas libraries through the numpy.linalg module. To install the  version of numpy this blog post refers to, install PyPy version 2.5.0 or  newer, and run this:\n\npypy -m pip install git+https://bitbucket.org/pypy/numpy.git\n\n\nThis update is a major step forward for PyPy's numpy support. Many of  the basic matrix operations depend on linalg, even matplotlib requires  it to display legends (a pypy-friendly version of matplotlib 1.3 is  available  at https://github.com/mattip/matplotlib).\n\nA number of improvements and adaptations, some of which are in the newly-released PyPy 2.5.0, made this possible:\n\nSupport for an extended frompyfunc(), which in the PyPy  version supports much of the ufunc API (signatures, multiple dtypes)  allowing creation of pure-python, jit-friendly ufuncs. An additional  keyword allows choosing between out = func(in) or func(in, out) ufunc signatures. More explanation follows.\nSupport for GenericUfuncs via PyPy's (slow) capi-compatibility  layer. The underlying mechanism actually calls the internal  implementation of frompyfunc().\nA cffi version of _umath_linalg. Since cffi uses dlopen()  to call into shared objects, we added support in the numpy build system  to create non-python shared libraries from source code in the numpy  tree. We also rewrote parts of the c-based _umath_linalg.c.src in python, renamed numpy's umath_linalg capi module to umath_linag_capi, and use it as a shared object through cffi.\n\n\n\nStatus\nWe have not completely implemented all the linalg features. dtype  resolution via casting is missing, especially for complex ndarrays,  which leads to slight numerical errors where numpy uses a more precise  type for intermediate calculations. Other missing features in PyPy's  numpy support may have implications for complete linalg support.\n\nSome OSX users have noticed they need to update pip to version 6.0.8 to overcome a regression in pip, and it is not clear if we support all combinations of blas/lapack implementations on all platforms.\n\nOver  the next few weeks we will be ironing out these issues.\n\n\nPerformance\nA simple benchmark is shown below, but let's state the obvious:  PyPy's JIT and the iterators built into PyPy's ndarray implementation  will in most cases be no faster than CPython's numpy. The JIT can help  where there is a mixture of python and numpy-array code. We do have  plans to implement lazy evaluation and to further optimize PyPy's  support for numeric python, but numpy is quite good at what it does.\n\n\nHowTo for PyPy's extended frompyfunc \nThe magic enabling blas support is a rewrite of the _umath_linalg c-based module as a cffi-python module that creates ufuncs via frompyfunc. We extended the numpy frompyfunc to allow it to function as a replacement for the generic ufunc available in numpy only through the c-api.\n\nWe start with the basic frompyfunc, which wraps a python function into a ufunc:\n\u00a0\ndef times2(in0):\n    return in0 * 2\nufunc = frompyfunc(times2, 1, 1)\n\n\nIn cpython's numpy the dtype of the result is always object, which is  not implemented (yet) in PyPy, so this example will fail. While the  utility of object dtypes can be debated, in the meantime we add a  non-numpy-compatible keyword argument dtypes to frompyfunc. If dtype=['match'] the output dtype will match the dtype of the first input ndarray:\n\nufunc = frompyfunc(times2, 1, 1, dtype=['match'])\nai = arange(24).reshape(3, 4, 2)\nao = ufunc(ai)\nassert  (ao == ai * 2).all()\n\n\nI hear you ask \"why is the dtypes keyword argument a list?\" This is so we can support the Generalized Universal Function API, which allows specifying a number of specialized functions and the input-output dtypes each specialized function accepts.\nNote that the function feeds the values of ai one at a time,  the function operates on scalar values. To support more complicated  ufunc calls, the generalized ufunc API allows defining a signature,  which specifies the layout of the ndarray inputs and outputs. So we extended frompyfunc with a signature keyword as well.\nWe add one further extension to frompyfunc: we allow a Boolean keyword stack_inputs to specify the argument layout of the function itself. If the function is of the form:\n\u00a0\nout0, out1, ... = func(in0, in1,...)\n\n\nthen stack_inputs is False. If it is True the function is of the form:\n\u00a0\nfunc(in0, in1, ... out0, out1, ...)\n\n\nHere is a complete example of using frompyfunc to create a ufunc, based on this link:\n\u00a0\ndef times2(in_array, out_array):\n    in_flat = in_array.flat\n    out_flat = out_array.flat\n    for i in range(in_array.size):\n        out_flat[i] = in_flat[i] * 2\nufunc = frompyfunc([times2, times2], 1, 1,\n                signature='(i)->(i)',\n                dtypes=[dtype(int), dtype(int),\n                        dtype(float), dtype(float),\n                       ],\n                stack_inputs=True,\n                )\nai = arange(10, dtype=int)\nai2 = ufunc(ai)\nassert all(ai2 == ai * 2)\n\n\nUsing this extended syntax, we rewrote the lapack calls into the blas  functions in pure python, no c needed. Benchmarking this approach  actually was much slower than using the upstream umath_linalg  module via cpyext, as can be seen in the following benchmarks. This is  due to the need to copy c-aligned data into Fortran-aligned format. Our __getitem__ and __setitem__ iterators are not as fast as pointer arithmetic in C. So we next tried a hybrid approach: compile and use numpy's umath_linalg python module as a shared object, and call the optimized specific wrapper function from it.\n\n\nBenchmarks\nHere are some benchmarks, running a tight loop of the different versions of linalg.inv(a), where a is a 10x10 double ndarray. The benchmark ran on an i7 processor running ubuntu 14.04 64 bit:\n\n Impl. Time after warmup \n \n CPython 2.7 + numpy 1.10.dev + lapack 8.9 msec/1000 loops \n PyPy 2.5.0  + numpy + lapack via cpyext 8.6 msec/1000 loops \n PyPy 2.5.0  + numpy + lapack via pure python + cffi 19.9 msec/1000 loops \n PyPy 2.5.0  + numpy + lapack via python + c + cffi 9.5 msec/1000 loops\n\n\n\n\n\n\n\n\nWhile no general conclusions may be drawn from a single micro-benchmark, it does indicate that there is some merit in the approach taken. \n\nConclusion\nPyPy's numpy now includes a working linalg module. There are still  some rough corners, but hopefully we have implemented the parts you  need. While the speed of the isolated linalg function is no faster than  CPython and upstream numpy, it should not be significantly slower  either. Your use case may see an improvement if you use a mix of python  and lapack, which is the usual case.\n\nPlease let us know how it goes. We love to hear success stories too.\n\nWe still have challenges at all levels of programming,and are always  looking for people willing to contribute, so stop by on IRC at #pypy.\n\nmattip and the PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/02/linalg-support-in-pypynumpy-1131217944329711855.html"
    },
    {
      "title": "NumPyPy status - January 2015",
      "text": "Hi Everyone\n\nHere is what has been done in January thanks to the funding of NumPyPy,\u00a0I would like to thank all the donors and tell you that you can still donate\u00a0:\n\nI have focused on implementing the object dtype this month, it is now possible to store objects inside ndarrays using the object dtype\nIt is also possible to add an object ndarray to any other ndarray (implementing other operators is trivial)\n\n\nThe next things I plan on working on next are :\n\n\nImplementing the missing operations for object arrays\nImplementing garbage collection support for object arrays (currently, storing an object inside an ndarray doesn't keep the object alive)\nPackaging NumPyPy on PyPI\n\n\nCheers\n\n\nRomain",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/02/numpypy-status-january-2015-5092986229783279944.html"
    },
    {
      "title": "PyPy 2.5.0 released",
      "text": "PyPy 2.5.0 - Pincushion Protea\nWe\u2019re pleased to announce PyPy 2.5, which contains significant performance\nenhancements and bug fixes.\nYou can download the PyPy 2.5.0 release here:\n\n\nhttps://pypy.org/download.html\n\nWe would like to thank our donors for the continued support of the PyPy\nproject, and for those who donate to our three sub-projects, as well as our\nvolunteers and contributors (10 new commiters joined PyPy since the last\nrelease).\nWe\u2019ve shown quite a bit of progress, but we\u2019re slowly running out of funds.\nPlease consider donating more, or even better convince your employer to donate,\nso we can finish those projects! The three sub-projects are:\n\n\n\nPy3k (supporting Python 3.x): We have released a Python 3.2.5 compatible version\n\nwe call PyPy3 2.4.0, and are working toward a Python 3.3 compatible version\n\n\n\nSTM (software transactional memory): We have released a first working version,\nand continue to try out new promising paths of achieving a fast multithreaded Python\n\n\nNumPy which requires installation of our fork of upstream numpy,\navailable on bitbucket\n\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It\u2019s fast (pypy and cpython 2.7.x performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines on most common operating systems\n(Linux 32/64, Mac OS X 64, Windows, and OpenBSD),\nas well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\nWhile we support 32 bit python on Windows, work on the native Windows 64\nbit python is still stalling, we would welcome a volunteer\nto handle that.\n\n\nHighlights\n\nThe past months have seen pypy mature and grow, as rpython becomes the goto\nsolution for writing fast dynamic language interpreters. Our separation of\nrpython and the python interpreter PyPy is now much clearer in the\nPyPy documentation  and we now have separate RPython documentation.\nWe have improved warmup time as well as jitted code performance: more than 10%\ncompared to pypy-2.4.0.\nWe no longer zero-out memory allocated in the gc nursery by default, work that\nwas started during a GSoC.\nPassing objects between C and PyPy has been improved. We are now able to pass\nraw pointers to C (without copying) using pinning. This improves I/O;\nbenchmarks that use networking intensively improved by about 50%. File()\noperations still need some refactoring but are already showing a 20%\nimprovement on our benchmarks. Let us know if you see similar improvements.\nOur integrated numpy support gained much of the GenericUfunc api in order to\nsupport the lapack/blas linalg module of numpy. This dovetails with work in the\npypy/numpy repository to support linalg both through the (slower) cpyext capi\ninterface and also via (the faster) pure python cffi interface, using an\nextended frompyfunc() api. We will soon post a seperate blog post specifically\nabout linalg and PyPy.\nDictionaries are now ordered by default, see the blog post\nOur nightly translations use \u2013shared by default, including on OS/X and linux\nWe now more carefully handle errno (and GetLastError, WSAGetLastError) tying\nthe handlers as close as possible to the external function call, in non-jitted\nas well as jitted code.\nIssues reported with our previous release were resolved after reports from users on\nour issue tracker at https://foss.heptapod.net/pypy/pypy/-/issues or on IRC at\n#pypy.\n\nWe have further improvements on the way: rpython file handling,\nfinishing numpy linalg compatibility, numpy object dtypes, a better profiler,\nas well as support for Python stdlib 2.7.9.\nPlease try it out and let us know what you think. We especially welcome\nsuccess stories, we know you are using PyPy, please tell us about it!\nCheers\nThe PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2015/02/pypy-250-released-247160062953533060.html"
    },
    {
      "title": "Faster, more memory efficient and more ordered dictionaries on PyPy",
      "text": "Hello everyone!\nAs of today, we merged the latest branch that brings better dictionaries to PyPy by default. The work is based on an idea by Raymond Hettinger on python-dev, with prior work done notably in Java.\u00a0 It was done by Maciej Fija\u0142kowski and Armin Rigo, with Laurence Tratt recently prodding us to finish it.\u00a0 (Earlier work going in a similar direction include Alex Gaynor's work on ordered dicts in Topaz, which was also used in the Hippy VM.\u00a0 Each of these pieces of work is itself based on the original dict implementation in RPython, whose origins fade in the Subversion prehistory of PyPy.)\u00a0 Coincidentally, a very similar idea has been implemented in Zend PHP very recently. Zend implementation description.\nThis post covers the basics of design and implementation as well as some basic benchmarks.\n\n\nDictionaries are now ordered!\nOne surprising part is that the new design, besides being more\nmemory efficient, is ordered by design: it preserves the\ninsertion order.\u00a0 This is not forbidden by the Python language, which allows any order.\u00a0 It makes the collections.OrderedDict subclass much faster than before: it is now a thin subclass of dict.\u00a0 Obviously, we recommend that any portable Python program continues to use OrderedDict when ordering is important.\u00a0 Note that a non-portable program might rely on more: for example, a **keywords argument now receives the keywords in the same order as the one in which they were given in the call.\u00a0 (Whether such a thing might be called a language design change or not is a bit borderline.)\u00a0 The point is that Python programs that work on CPython or previous versions of PyPy should continue to work on PyPy.\nThere is one exception, though.\u00a0 The iterators of the OrderedDict subclass are now working just like the ones of the dict builtin: they will raise RuntimeError when iterating if the dictionary was modified.\u00a0 In the CPython design, the class OrderedDict explicitly doesn't worry about that, and instead you get some result that might range from correct to incorrect to crashes (i.e. random Python exceptions).\n\n\nOriginal PyPy dictionary design\nOriginally, PyPy dictionaries, as well as CPython dictionaries\nare implemented as follows (simplified view):\n\nstruct dict {\n   long num_items;\n   dict_entry* items;\u00a0\u00a0 /* pointer to array */\n}\n\nstruct dict_entry {\n   long hash;\n   PyObject* key;\n   PyObject* value;\n}\n\nWhere items is a sparse array, with 1/3 to 1/2 of the items being NULL.\nThe average space occupied by a dictionary is 3 * WORD * 12/7 plus some small constant (the smallest dict has 8 entries, which is\n8 * 3 * WORD + 2 * WORD = 26 WORDs).\n\n\nNew PyPy dictionary design\nThe new PyPy dictionary is split in two arrays:\n\nstruct dict {\n    long num_items;\n    variable_int *sparse_array;\n    dict_entry* compact_array;\n}\n\nstruct dict_entry {\n    long hash;\n    PyObject *key;\n    PyObject *value;\n}\n\nHere, compact_array stores all the items in order of insertion, while sparse_array is a 1/2 to 2/3 full array of integers. The integers themselves are of the smallest size necessary for indexing the compact_array. So if compact_array has less than 256 items, then sparse_array will be made of bytes; if less than 216, it'll be two-byte integers; and so on.\nThis design saves quite a bit of memory. For example, on 64bit systems we can, but almost never, use indexing of more than 4 billion elements; and for small dicts, the extra sparse_array takes very little space.\u00a0 For example a 100 element dict, would be on average for the original design on 64bit: 100 * 12/7 * WORD * 3 =~ 4100 bytes, while on new design it's 100 * 12/7 + 3 * WORD * 100 =~ 2600 bytes, quite a significant saving.\n\n\nGC friendliness\nThe obvious benefit of having more compact dictionaries is an increased cache friendliness. In modern CPUs cache misses are much more costly than doing additional simple work, like having an additional level of (in-cache) indirection. Additionally, there is a GC benefit coming from it. When doing a minor collection, the GC has to visit all the GC fields in old objects that can point to young objects. In the case of large arrays, this can prove problematic since the array grows and with each minor collection we need to visit more and more GC pointers. In order to avoid it, large arrays in PyPy employ a technique called \"card marking\" where the GC only visits \"cards\" or subsets of arrays that were modified between collections. The problem with dictionaries was that by design modifications in a dictionary occur randomly, hence a lot of cards used to get invalidated. In the new design, however, new items are typically appended to the compact_array, hence invalidate much fewer cards --- which improves GC performance.\u00a0 (The new sparse_array is an array of integers, so it does not suffer from the same problems.)\n\n\nDeletion\nDeleting entries from dictionaries is not very common, but important in a few use cases.\u00a0 To preserve order, when we delete an entry, we mark the entry as removed but don't otherwise shuffle the remaining entries.\u00a0 If we repeat this operation often enough, there will be a lot of removed entries in the (originally compact) array.\u00a0 At this point, we need to do a \"packing\" operation, which moves all live entries to the start of the array (and then reindexes the sparse array, as the positions changed).\u00a0 This works well, but there are use cases where previously no reindexing was ever needed, so it makes these cases a bit slower (for example when repeatedly adding and removing keys in equal number).\n\n\nBenchmarks\nThe PyPy speed benchmarks show mostly small effect. The microbenchmarks that we did show large improvements on large and very large dictionaries (particularly, building dictionaries of at least a couple 100s of items is now twice faster) and break-even on small ones (between 20% slower and 20% faster depending very much on the usage patterns and sizes of dictionaries). The new dictionaries enable various optimization possibilities which we're going to explore in the near future.\nCheers,\nfijal, arigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/01/faster-more-memory-efficient-and-more-4096950404745375390.html"
    },
    {
      "title": "Leysin Winter Sprint (20-28th February 2015)",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the tenth time.\nThis is a fully public sprint: newcomers and topics other than those\nproposed below are welcome.\n\nGoals and topics of the sprint\n\nThe details depend on who is here and ready to work.  We might touch\ntopics such as:\n\n\ncleaning up the optimization step in the JIT, change the register\nallocation done by the JIT's backend, or improvements to the\nwarm-up time\n\nSTM (Software Transaction Memory), notably: try to come up with\nbenchmarks, and measure them carefully in order to test and improve\nthe conflict reporting tools, and more generally to figure out how\npractical it is in large projects to avoid conflicts\n\nvmprof - a statistical profiler for CPython and PyPy work, including\nmaking it more user friendly.\n\nPy3k (Python 3.x support), NumPyPy (the numpy module)\n\nadded: cffi 1.0, trying out pygame+cffi on Raspberry Pi devices\n\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off for ski.\n\n\n\n\nExact times\n\nFor a change, and as an attempt to simplify things, I specified the\ndates as 20-28 Februrary 2015, where 20 and 28 are travel days.  We will\nwork full days between the 21 and the 27.  You are of course allowed to\nshow up for a part of that time only, too.\n\nLocation and Accomodation\n\nLeysin, Switzerland, \"same place as before\".  Let me refresh your\nmemory: both the sprint venue and the lodging will be in a very spacious\npair of chalets built specifically for bed & breakfast:\nErmina.  The place has a good ADSL Internet connection\nwith wireless installed.  You can of course arrange your own lodging\nanywhere (as long as you are in Leysin, you cannot be more than a 15\nminutes walk away from the sprint venue), but I definitely recommend\nlodging there too -- you won't find a better view anywhere else (though\nyou probably won't get much worse ones easily, either :-)\n\nPlease confirm that you are coming so that we can adjust the\nreservations as appropriate.  In the past, the rates were around 60 CHF a\nnight all included in 2-person rooms, with breakfast.  Now, the rooms\navailable are either single-person (or couple), or rooms for 3 persons.\nThe latter choice is recommended and should be under 60 CHF per person.\n\nPlease register by Mercurial, or on the pypy-dev mailing list if you do not yet have check-in rights.\n\nYou need a Swiss-to-(insert country here) power adapter.  There will be\nsome Swiss-to-EU adapters around, and at least one EU-format power strip.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2015/01/leysin-winter-sprint-20-28th-february-2590212640945547308.html"
    },
    {
      "title": "September donations and thank you to the Python Software Foundation!",
      "text": "Hello everyone!\nWe would like to show you a short update on the PyPy funding.\nWe gathered a total of $15,986 in the month of September and as per\nearlier agreement, the Python Software Foundation donated $10,000\nto PyPy. We would like to thank everyone participating and the PSF in\nparticular for supporting the PyPy project and making our work possible!\nWe've been working hard on the goals outlined in the funding proposals.\n\nPyPy Python 3 support has been in beta for a while and it's already\nbeing used by many people, as seen per the number of reported bugs.\nWe're currently supporting 3.2, planning on moving towards 3.4 in the\nfuture.\nSoftware Transactional Memory has been a successful research project,\nwith first real world results shown during the Warsaw sprint.\nMore detailed update on numpy will be published soon. A little spoiler is\nthat we're planning on addressing matplotlib, scipy and the larger ecosystem\nto some extent. Stay tuned!\n\nAgain, thanks to everyone who donated and happy Thanksgiving to everyone\non that side of the world!\nCheers,\nfijal and the entire PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/11/september-donations-and-thank-you-to-4531550307707104017.html"
    },
    {
      "title": "Tornado without a GIL on PyPy STM",
      "text": "This post is by Konstantin Lopuhin, who tried PyPy STM during the\nWarsaw sprint.\nPython has a GIL, right? Not quite - PyPy STM is a python implementation\nwithout a GIL, so it can scale CPU-bound work to several cores.\nPyPy STM is developed by Armin Rigo and Remi Meier,\nand supported by community donations.\nYou can read more about it in the\ndocs.\nAlthough PyPy STM is still a work in progress, in many cases it can already\nrun CPU-bound code faster than regular PyPy, when using multiple cores.\nHere we will see how to slightly modify Tornado IO loop to use\ntransaction\nmodule.\nThis module is described\nin the docs and is really simple to use - please see an example there.\nAn event loop of Tornado, or any other asynchronous\nweb server, looks like this (with some simplifications):\n\nwhile True:\n    for callback in list(self._callbacks):\n        self._run_callback(callback)\n    event_pairs = self._impl.poll()\n    self._events.update(event_pairs)\n    while self._events:\n        fd, events = self._events.popitem()\n        handler = self._handlers[fd]\n        self._handle_event(fd, handler, events)\n\nWe get IO events, and run handlers for all of them, these handlers can\nalso register new callbacks, which we run too. When using such a framework,\nit is very nice to have a guaranty that all handlers are run serially,\nso you do not have to put any locks. This is an ideal case for the\ntransaction module - it gives us guaranties that things appear\nto be run serially, so in user code we do not need any locks. We just\nneed to change the code above to something like:\n\nwhile True:\n    for callback in list(self._callbacks):\n        transaction.add(                # added\n            self._run_callback, callback)\n    transaction.run()                   # added\n    event_pairs = self._impl.poll()\n    self._events.update(event_pairs)\n    while self._events:\n        fd, events = self._events.popitem()\n        handler = self._handlers[fd]\n        transaction.add(                # added\n            self._handle_event, fd, handler, events)\n    transaction.run()                   # added\n\nThe actual commit is\nhere,\n- we had to extract a little function to run the callback.\n\nPart 1: a simple benchmark: primes\nNow we need a simple benchmark, lets start with\nthis\n- just calculate a list of primes up to the given number, and return it\nas JSON:\n\ndef is_prime(n):\n    for i in xrange(2, n):\n        if n % i == 0:\n            return False\n    return True\n\nclass MainHandler(tornado.web.RequestHandler):\n    def get(self, num):\n        num = int(num)\n        primes = [n for n in xrange(2, num + 1) if is_prime(n)]\n        self.write({'primes': primes})\n\nWe can benchmark it with siege:\n\nsiege -c 50 -t 20s https://localhost:8888/10000\n\nBut this does not scale. The CPU load is at 101-104 %, and we handle 30 %\nless request per second. The reason for the slowdown is STM overhead,\nwhich needs to keep track of all writes and reads in order to detect conflicts.\nAnd the reason for using only one core is, obviously, conflicts!\nFortunately, we can see what this conflicts are, if we run code like this\n(here 4 is the number of cores to use):\n\nPYPYSTM=stm.log ./primes.py 4\n\nThen we can use print_stm_log.py\nto analyse this log. It lists the most expensive conflicts:\n\n14.793s lost in aborts, 0.000s paused (1258x STM_CONTENTION_INEVITABLE)\nFile \"/home/ubuntu/tornado-stm/tornado/tornado/httpserver.py\", line 455, in __init__\n    self._start_time = time.time()\nFile \"/home/ubuntu/tornado-stm/tornado/tornado/httpserver.py\", line 455, in __init__\n    self._start_time = time.time()\n...\n\nThere are only three kinds of conflicts, they are described in\nstm source,\nHere we see that two threads call into external function to get current time,\nand we can not rollback any of them, so one of them must wait till the other\ntransaction finishes.\nFor now we can hack around this by disabling this timing - this is only\nneeded for internal profiling in tornado.\nIf we do it, we get the following results (but see caveats below):\n\n\n\n\n\n\n\n\nImpl.\nreq/s\n\n\n\nPyPy 2.4\n14.4\n\nCPython\u00a02.7\n3.2\n\nPyPy-STM 1\n9.3\n\nPyPy-STM 2\n16.4\n\nPyPy-STM 3\n20.4\n\nPyPy\u00a0STM\u00a04\n24.2\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\nAs we can see, in this benchmark PyPy STM using just two cores\ncan beat regular PyPy!\nThis is not linear scaling, there are still conflicts left, and this\nis a very simple example but still, it works!\nBut its not that simple yet :)\nFirst, these are best-case numbers after long (much longer than for regular\nPyPy) warmup. Second, it can sometimes crash (although removing old pyc files\nfixes it). Third, benchmark meta-parameters are also tuned.\nHere we get relatively good results only when there are a lot of concurrent\nclients - as a results, a lot of requests pile up, the server is not keeping\nwith the load, and transaction module is busy with work running this piled up\nrequests. If we decrease the number of concurrent clients, results get slightly worse.\nAnother thing we can tune is how heavy is each request - again, if we ask\nprimes up to a lower number, then less time is spent doing calculations,\nmore time is spent in tornado, and results get much worse.\nBesides the time.time() conflict described above, there are a lot of others.\nThe bulk of time is lost in these two conflicts:\n\n14.153s lost in aborts, 0.000s paused (270x STM_CONTENTION_INEVITABLE)\nFile \"/home/ubuntu/tornado-stm/tornado/tornado/web.py\", line 1082, in compute_etag\n    hasher = hashlib.sha1()\nFile \"/home/ubuntu/tornado-stm/tornado/tornado/web.py\", line 1082, in compute_etag\n    hasher = hashlib.sha1()\n\n13.484s lost in aborts, 0.000s paused (130x STM_CONTENTION_WRITE_READ)\nFile \"/home/ubuntu/pypy/lib_pypy/transaction.py\", line 164, in _run_thread\n    got_exception)\n\nThe first one is presumably calling into some C function from stdlib, and we get\nthe same conflict as for time.time() above, but is can be fixed on PyPy\nside, as we can be sure that computing sha1 is pure.\nIt is easy to hack around this one too, just removing etag support, but if\nwe do it, performance is much worse, only slightly faster than regular PyPy,\nwith the top conflict being:\n\n83.066s lost in aborts, 0.000s paused (459x STM_CONTENTION_WRITE_WRITE)\nFile \"/home/arigo/hg/pypy/stmgc-c7/lib-python/2.7/_weakrefset.py\", line 70, in __contains__\nFile \"/home/arigo/hg/pypy/stmgc-c7/lib-python/2.7/_weakrefset.py\", line 70, in __contains__\n\nComment by Armin: It is unclear why this happens so far.  We'll investigate...\nThe second conflict (without etag tweaks) originates\nin the transaction module, from this piece of code:\n\nwhile True:\n    self._do_it(self._grab_next_thing_to_do(tloc_pending),\n                got_exception)\n    counter[0] += 1\n\nComment by Armin: This is a conflict in the transaction module itself; ideally,\nit shouldn't have any, but in order to do that we might need a little bit\nof support from RPython or C code.  So this is pending improvement.\nTornado modification used in this blog post is based on 3.2.dev2.\nAs of now, the latest version is 4.0.2, and if we\napply\nthe same changes to this version, then we no longer get any scaling on this benchmark,\nand there are no conflicts that take any substantial time.\nComment by Armin: There are two possible reactions to a conflict.  We can either\nabort one of the two threads, or (depending on the circumstances) just\npause the current thread until the other one commits, after which the\nthread will likely be able to continue.  The tool ``print_stm_log.py``\ndid not report conflicts that cause pauses.  It has been fixed very\nrecently.  Chances are that on this test it would report long pauses and\npoint to locations that cause them.\n\n\nPart 2: a more interesting benchmark: A-star\nAlthough we have seen that PyPy STM is not all moonlight and roses,\nit is interesting to see how it works on a more realistic application.\nastar.py\nis a simple game where several players move on a map\n(represented as a list of lists of integers),\nbuild and destroy walls, and ask server to give them\nshortest paths between two points\nusing A-star search, adopted from ActiveState recipie.\nThe benchmark bench_astar.py\nis simulating players, and tries to put the main load on A-star search,\nbut also does some wall building and destruction. There are no locks\naround map modifications, as normal tornado is executing all callbacks\nserially, and we can keep this guaranty with atomic blocks of PyPy STM.\nThis is also an example of a program that is not trivial\nto scale to multiple cores with separate processes (assuming\nmore interesting shared state and logic).\nThis benchmark is very noisy due to randomness of client interactions\n(also it could be not linear), so just lower and upper bounds for\nnumber of requests are reported\n\n\n\n\n\n\nImpl.\nreq/s\n\n\n\nPyPy 2.4\n5 .. 7\n\nCPython 2.7\n0.5 .. 0.9\n\nPyPy-STM 1\n2 .. 4\n\nPyPy STM 4\n2 .. 6\n\n\n\nClearly this is a very bad benchmark, but still we can see that scaling is worse\nand STM overhead is sometimes higher.\nThe bulk of conflicts come from the transaction module (we have seen it\nabove):\n\n91.655s lost in aborts, 0.000s paused (249x STM_CONTENTION_WRITE_READ)\nFile \"/home/ubuntu/pypy/lib_pypy/transaction.py\", line 164, in _run_thread\n    got_exception)\n\nAlthough it is definitely not ready for production use, you can already try\nto run things, report bugs, and see what is missing in user-facing tools\nand libraries.\nBenchmarks setup:\n\nAmazon c3.xlarge (4 cores) running Ubuntu 14.04\npypy-c-r74011-stm-jit for the primes benchmark (but it has more bugs\nthan more recent versions), and\npypy-c-r74378-74379-stm-jit\nfor astar benchmark (put it inside pypy source checkout at 38c9afbd253c)\nhttps://bitbucket.org/kostialopuhin/tornado-stm-bench at 65144cda7a1f",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/11/tornado-without-gil-on-pypy-stm-7284102716557557428.html"
    },
    {
      "title": "PyPy IO improvements",
      "text": "Hello everyone!\nWe've wrapped up the Warsaw sprint, so I would like to describe some\nbranches which have been recently merged and which improved the I/O and the\nGC: gc_no_cleanup_nursery and gc-incminimark-pinning.\nThe first branch was started by Wenzhu Man for her Google Summer of Code\nand finished by Maciej Fija\u0142kowski and Armin Rigo.\nThe PyPy GC works by allocating new objects in the young object\narea (the nursery), simply by incrementing a pointer. After each minor\ncollection, the nursery has to be cleaned up. For simplicity, the GC used\nto do it by zeroing the whole nursery.\nThis approach has bad effects on the cache, since you zero a large piece of\nmemory at once and do unnecessary work for things that don't require zeroing\nlike large strings. We mitigated the first problem somewhat with incremental\nnursery zeroing, but this branch removes the zeroing completely, thus\nimproving the string handling and recursive code (since jitframes don't\nrequires zeroed memory either). I measured the effect on two examples:\na recursive implementation of  fibonacci and gcbench,\nto measure GC performance.\nThe results for fibonacci and gcbench are below (normalized to cpython\n2.7). Benchmarks were run 50 times each (note that the big standard\ndeviation comes mostly from the warmup at the beginning, true figures\nare smaller):\n\n\n\n\n\n\n\n\n\nbenchmark\nCPython\nPyPy 2.4\nPyPy non-zero\n\nfibonacci\n4.8+-0.15 (1.0x)\n0.59+-0.07 (8.1x)\n0.45+-0.07 (10.6x)\n\ngcbench\n22+-0.36 (1.0x)\n1.34+-0.28 (16.4x)\n1.02+-0.15 (21.6x)\n\n\n\n\nThe second branch was done by Gregor Wegberg for his master thesis and finished\nby Maciej Fija\u0142kowski and Armin Rigo. Because of the way it works, the PyPy GC from\ntime to time moves the objects in memory, meaning that their address can change.\nTherefore, if you want to pass pointers to some external C function (for\nexample, write(2) or read(2)), you need to ensure that the objects they are\npointing to will not be moved by the GC (e.g. when running a different thread).\nPyPy up to 2.4 solves the problem by copying the data into or from a non-movable buffer, which\nis obviously inefficient.\nThe branch introduce the concept of \"pinning\", which allows us to inform the\nGC that it is not allowed to move a certain object for a short period of time.\nThis introduces a bit of extra complexity\nin the garbage collector, but improves the I/O performance quite drastically,\nbecause we no longer need the extra copy to and from the non-movable buffers.\nIn this benchmark, which does I/O in a loop,\nwe either write a number of bytes from a freshly allocated string into\n/dev/null or read a number of bytes from /dev/full. I'm showing the results\nfor PyPy 2.4, PyPy with non-zero-nursery and PyPy with non-zero-nursery and\nobject pinning. Those are wall times for cases using os.read/os.write\nand file.read/file.write, normalized against CPython 2.7.\nBenchmarks were done using PyPy 2.4 and revisions 85646d1d07fb for\nnon-zero-nursery and 3d8fe96dc4d9 for non-zero-nursery and pinning.\nThe benchmarks were run once, since the standard deviation was small.\n\n\n\nThe Y axis is speed, normalized to CPython, the more the better\n\nWhat we can see is that os.read and os.write both improved greatly\nand outperforms CPython now for each combination. file operations are\na little more tricky, and while those branches improved the situation a bit,\nthe improvement is not as drastic as in os versions.  It really should not\nbe the case and it showcases how our file buffering is inferior to CPython.\nWe plan on removing our own buffering and using FILE* in C in the near future,\nso we should outperform CPython on those too (since our allocations are cheaper).\nIf you look carefully in the benchmark, the write function is copied three times.\nThis hack is intended to avoid JIT overspecializing the assembler code, which happens\nbecause the buffering code was written way before the JIT was done. In fact, our buffering\nis hilariously bad, but if stars align correctly it can be JIT-compiled to something\nthat's not half bad. Try removing the hack and seeing how the performance of the last\nbenchmark drops :-) Again, this hack should be absolutely unnecessary once we remove\nour own buffering, stay tuned for more.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/11/pypy-io-improvements-1042070332447047674.html"
    },
    {
      "title": "PyPy3 2.4.0 released",
      "text": "We're pleased to announce the availability of PyPy3 2.4.0!\n\nThis release contains several bugfixes and enhancements. Among the user-facing improvements specific to PyPy3:\nBetter Windows compatibility, e.g. the nt module functions _getfinalpathname\u00a0& _getfileinformation are now supported (the former is required for the popular pathlib library for example)\nVarious fsencode PEP 383 related fixes to the posix module (readlink, uname,\u00a0ttyname and ctermid) and improved locale handling\nSwitched the default binary name on POSIX distributions from 'pypy' to 'pypy3' (which symlinks to to 'pypy3.2')\nFixed a couple different crashes related to parsing Python 3 source code\n\nAnd improvements shared with the recent PyPy 2.4.0 release:\ninternal refactoring in string and GIL handling which led to significant speedups\nimproved handling of multiple objects (like sockets) in long-running  programs. They are collected and released more efficiently, reducing  memory use. In simpler terms - we closed what looked like a memory leak\nWindows builds now link statically to zlib, expat, bzip, and openssl-1.0.1i\nMany issues were resolved since the 2.3.1 release in June\n\nYou can download PyPy3 2.4.0 here https://pypy.org/download.html.\n\nPyPy\u00a0is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and 3.2.5. It's fast (pypy 2.4 and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nThis  release supports x86 machines running Linux 32/64, Mac OS X 64,   Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7,   with VFPv3) running Linux.\u00a0 \nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here.\n\nPlease  try it out and let us know what you think. We especially welcome  success stories, please tell us about how it has helped you!\n\nCheers, The PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2014/10/pypy3-240-released-5007750685927360190.html"
    },
    {
      "title": "Couchbase contribution to PyPy",
      "text": "Hello everyone!\nWe always offer to put on the blog info about our sponsors who donate substantial amounts of money. So far most people decided to stay anonymous, so this is the first blog post describing our sponsor and his relationship to PyPy, hopefully not the last. We'll also publish a full blog post about the PSF-matched fundraiser soon. This is a guest post by Brent Woodruff from Couchbase.\n\n\n\n\nCouchbase is a leading NoSQL document database that provides a flexible data model, high performance, scalability, and high availability. Couchbase is a commercially supported open source project. Visit us at https://www.couchbase.com and https://github.com/couchbase.\n\n\nCouchbase Inc. donated $2000.00, and employees of Couchbase personally contributed a disclosed additional $230.00, towards Pypy progress during the September funding drive. These funds will see a match from the Python Software Foundation.\n\nPypy is primarily used by Couchbase employees to perform product analysis and troubleshooting using internally developed tools. Every customer of Couchbase benefits from the use of Pypy; both due to the rapid development provided by Python, and the speed of the resulting tools provided by the Pypy JIT interpreter.\n\n\u201cPyPy is great - it gave us a 4x speedup in our CPU-intensive internal application over CPython\u201d\n-Dave Rigby and Daniel Owen, Couchbase Engineers\n\n\nAdditionally, Couchbase has a preliminary CFFI based Couchbase client available for Pypy users.",
      "tags": "sponsors",
      "url": "https://www.pypy.org/posts/2014/10/couchbase-contribution-to-pypy-2360892117372790069.html"
    },
    {
      "title": "PyPy 2.4.0 released, 9 days left in funding drive",
      "text": "We're pleased to announce the availability of PyPy 2.4.0; faster, fewer bugs, and updated to the python 2.7.8 stdlib.\n\nThis release contains several bugfixes and enhancements. Among the user-facing improvements:\n\ninternal refactoring in string and GIL handling which led to significant speedups\nimproved handling of multiple objects (like sockets) in long-running  programs. They are collected and released more efficiently, reducing  memory use. In simpler terms - we closed what looked like a memory leak\nWindows builds now link statically to zlib, expat, bzip, and openssl-1.0.1i\nMany issues were resolved since the 2.3.1 release in June \n\n\nYou can download PyPy 2.4.0 here https://pypy.org/download.html.\n\nWe would like to also point out that in September, the Python Software Foundation will match funds for any donations up to $10k, so head over to our website and help this mostly-volunteer effort out.\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and 3.2.5. It's fast (pypy 2.4 and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler. \n\nThis  release supports x86 machines running Linux 32/64, Mac OS X 64,   Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7,   with VFPv3) running Linux.\u00a0 \nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here.\n\nPlease  try it out and let us know what you think. We especially welcome  success stories, please tell us about how it has helped you!\n\nCheers, The PyPy Team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2014/09/pypy-240-released-9-days-left-in-7722154416024407111.html"
    },
    {
      "title": "PyPy 2.4-beta just in time for PSF's funding drive",
      "text": "We're pleased to announce the availability of PyPy 2.4-beta1; faster, fewer bugs, and updated to the python 2.7.8 stdlib.\n\nThis release contains several bugfixes and enhancements. Among the user-facing improvements:\n\ninternal refactoring in string and GIL handling which led to significant speedups\nimproved handling of multiple objects (like sockets) in long-running programs. They are collected and released more efficiently, reducing memory use. In simpler terms - we closed what looked like a memory leak\nWindows builds now link statically to zlib, expat, bzip, and openssl-1.0.1i\nMany issues were resolved since the 2.3.1 release in June \n\n\nYou can download the PyPy 2.4-beta1 release here https://pypy.org/download.html.\n\nWe would like to also point out that in\nSeptember, the Python Software Foundation will match funds for\nany donations up to $10k, so head over to our website and help this mostly-volunteer effort out.\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7 and 3.2.5. It's fast (pypy 2.4 and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nThis\n release supports x86 machines running Linux 32/64, Mac OS X 64,  \nWindows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7,  \nwith VFPv3) running Linux.\u00a0 \nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here.\n\nPlease\n try it out and let us know what you think. We especially welcome \nsuccess stories, please tell us about how it has helped you!\n\nCheers, The PyPy Team\n\nNews Flash from the beta release cycle:\n\nNote that the beta release mistakenly identifies itself in sys.pypy_version_info as releaselevel=='final', please do not mistake this for a final version\nThe beta can hit a \"Illegal instruction\" exception in jitted code on ARMv6 processors like the RaspberryPi. This will be fixed for the release.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/09/pypy-24-beta-just-in-time-for-psfs-5956090195665204063.html"
    },
    {
      "title": "Python Software Foundation Matching Donations this Month",
      "text": "We're extremely excited to announce that for the month of September, any amount\nyou donate to PyPy will be match (up to $10,000) by the Python Software\nFoundation.This includes any of our ongoing fundraisers: NumPyPy, STM, Python3, or our\ngeneral fundraising.Here are some of the things your previous donations have helped accomplish:Getting PyPy3 completed (currently 3.2, with 3.3 work underway)\nNew research and production engineering on STM for PyPy\nLots of progress on NumPy for PyPy\nSignificant performance improvements\nYou can see a preview of what's coming in our next 2.4 release in the draft\nrelease notes.Thank you to all the individuals and companies which have donated so far.So please, donate today: https://pypy.org/(Please be aware that the donation progress bars are not live updating, so\ndon't be afraid if your donation doesn't show up immediately).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/09/python-software-foundation-matching-2230529993193139046.html"
    },
    {
      "title": "A Field Test of Software Transactional Memory Using the RSqueak Smalltalk VM",
      "text": "Extending the Smalltalk RSqueakVM with STM\nby Conrad Calmez, Hubert Hesse, Patrick Rein and Malte Swart supervised by Tim Felgentreff and Tobias Pape\n\nIntroduction\nAfter pypy-stm we can announce that through the RSqueakVM (which used to be called SPyVM) a second VM implementation supports software transactional memory. RSqueakVM is a Smalltalk implementation based on the RPython toolchain. We have added STM support based on the STM tools from RPython (rstm). The benchmarks indicate that linear scale up is possible, however in some situations the STM overhead limits speedup.\nThe work was done as a master's project at the Software Architechture Group of Professor Robert Hirschfeld at at the Hasso Plattner Institut at the University of Potsdam. We - four students - worked about one and a half days per week for four months on the topic. The RSqueakVM was originally developped during a sprint at the University of Bern. When we started the project we were new to the topic of building VMs / interpreters.\nWe would like to thank  Armin, Remi and the #pypy IRC channel who supported us over the course of our project. We also like to thank Toni Mattis and Eric Seckler, who have provided us with an initial code base.\n\nIntroduction to RSqueakVM\nAs the original Smalltalk implementation, the RSqueakVM executes a given Squeak Smalltalk image, containing the Smalltalk code and a snapshot of formerly created objects and active execution contexts. These execution contexts are scheduled inside the image (greenlets) and not mapped to OS threads. Thereby the non-STM RSqueakVM runs on only one OS thread.\n\nChanges to RSqueakVM\nThe core adjustments to support STM were inside the VM and transparent from the view of a Smalltalk user. Additionally we added Smalltalk code to influence the behavior of the STM. As the RSqueakVM has run in one OS thread so far, we added the capability to start OS threads. Essentially, we added an additional way to launch a new Smalltalk execution context (thread). But in contrast to the original one this one creates a new native OS thread, not a Smalltalk internal green thread.\n\nSTM (with automatic transaction boundaries) already solves the problem of concurrent access on one value as this is protected by the STM transactions (to be more precise one instruction). But there are cases were the application relies on the fact that a bigger group of changes is executed either completely or not at all (atomic). Without further information transaction borders could be in the middle of such a set of atomic statements. rstm allows to aggregate multiple statements into one higher level transaction. To let the application mark the beginning and the end of these atomic blocks (high-level transactions), we added two more STM specific extensions to Smalltalk.\n\n\nBenchmarks\nRSqueak was executed in a single OS thread so far. rstm enables us to execute the VM using several OS threads. Using OS threads we expected a speed-up in benchmarks which use multiple threads. We measured this speed-up by using two benchmarks: a simple parallel summation where each thread sums up a predefined interval and an implementation of Mandelbrot where each thread computes a range of predefined lines.\n\nTo assess the speed-up, we used one RSqueakVM compiled with rstm enabled, but once running the benchmarks with OS threads and once with Smalltalk green threads. The workload always remained the same and only the number of threads increased. To assess the overhead imposed by the STM transformation we also ran the green threads version on an unmodified RSqueakVM. All VMs were translated with the JIT optimization and all benchmarks were run once before the measurement to warm up the JIT. As the JIT optimization is working it is likely to be adoped by VM creators (the baseline RSqueakVM did that) so that results with this optimization are more relevant in practice than those without it. We measured the execution time by getting the system time in Squeak. The results are:\n\nParallel Sum Ten Million\n\n\n\n\n\n\nBenchmark Parallel Sum 10,000,000\n\n\n Thread Count RSqueak green threads RSqueak/STM green threads RSqueak/STM OS threads Slow down from  RSqueak green threads to RSqueak/STM green threads Speed up from RSqueak/STM green threads to RSQueak/STM OS Threads \n \n   1   168.0 ms   240.0 ms   290.9 ms   0.70   0.83  \n   2   167.0 ms   244.0 ms   246.1 ms   0.68   0.99  \n   4   167.8 ms   240.7 ms   366.7 ms   0.70   0.66  \n   8   168.1 ms   241.1 ms   757.0 ms   0.70   0.32  \n   16   168.5 ms   244.5 ms   1460.0 ms   0.69   0.17  \n \n\n\n\nParallel Sum One Billion\n\n\n\n\n\n\nBenchmark Parallel Sum 1,000,000,000\n\n\n\nThread CountRSqueak green threadsRSqueak/STM green threadsRSqueak/STM OS threadsSlow down from  RSqueak green threads to RSqueak/STM green threadsSpeed up from RSqueak/STM green threads to RSQueak/STM OS Threads\n\n   1   16831.0 ms   24111.0 ms   23346.0 ms   0.70   1.03  \n   2   17059.9 ms   24229.4 ms   16102.1 ms   0.70   1.50  \n   4   16959.9 ms   24365.6 ms   12099.5 ms   0.70   2.01  \n   8   16758.4 ms   24228.1 ms   14076.9 ms   0.69   1.72  \n   16   16748.7 ms   24266.6 ms   55502.9 ms   0.69   0.44  \n\n\n\n\n\nMandelbrot Iterative\n\n\n\n\n\n\nBenchmark Mandelbrot\n\n\n Thread Count RSqueak green threads RSqueak/STM green threads RSqueak/STM OS threads Slow down from  RSqueak green threads to RSqueak/STM green threads Speed up from RSqueak/STM green threads to RSqueak/STM OS Threads \n \n   1   724.0 ms   983.0 ms   1565.5 ms   0.74   0.63  \n   2   780.5 ms   973.5 ms   5555.0 ms   0.80   0.18  \n   4   781.0 ms   982.5 ms   20107.5 ms   0.79   0.05  \n   8   779.5 ms   980.0 ms   113067.0 ms   0.80   0.01\n\n\n\n\n\nDiscussion of benchmark results\nFirst of all, the ParallelSum benchmarks show that the parallelism is actually paying off, at least for sufficiently large embarrassingly parallel problems. Thus RSqueak can also benefit from rstm.\nOn the other hand, our Mandelbrot implementation shows the limits of our current rstm integration. We implemented two versions of the algorithm one using one low-level array and one using two nested collections. In both versions, one job only calculates a distinct range of rows and both lead to a slowdown. The summary of the state of rstm transactions shows that there are a lot of inevitable transactions (transactions which must be completed). One reason might be the interactions between the VM and its low-level extensions, so called plugins. We have to investigate this further.\n\nLimitations\nAlthough the current VM setup is working well enough to support our benchmarks, the VM still has limitations. First of all, as it is based on rstm, it has the current limitation of only running on 64-bit Linux.\nBesides this, we also have two major limitations regarding the VM itself. First, the atomic interface exposed in Smalltalk is currently not working, when the VM is compiled using the just-in-time compiler transformation. Simple examples such as concurrent parallel sum work fine while more complex benchmarks such as chameneos fail. The reasons for this are currently beyond our understanding. Second, Smalltalk supports green threads, which are threads which are managed by the VM and are not mapped to OS threads. We currently support starting new Smalltalk threads as OS threads instead of starting them as green threads. However, existing threads in a Smalltalk image are not migrated to OS threads, but remain running as green threads.\n\nFuture work for STM in RSqueak\nThe work we presented showed interesting problems, we propose the following problem statements for further analysis:\n\nInevitable transactions in benchmarks. This looks like it could limit other applications too so it should be solved.\nCollection implementation aware of STM: The current implementation of collections can cause a lot of STM collisions due to their internal memory structure. We believe it could bear potential for performance improvements,  if we replace these collections in an STM enabled interpreter with implementations with less STM collisions. As already proposed by Remi Meier, bags, sets and lists are of particular interest.\nFinally, we exposed STM through languages features such as the atomic method, which is provided through the VM. Originally, it was possible to model STM transactions barriers implicitly by using clever locks, now its exposed via the atomic keyword. From a language design point of view, the question arises whether this is a good solution and what features an stm-enabled interpreter must provide to the user in general? Of particular interest are for example, access to the transaction length and hints for transaction borders to and their  performance impact.\n\n\n\nDetails for the technically inclined\n\nAdjustments to the interpreter loop were minimal.\nSTM works on bytecode granularity that means, there is a implicit transaction border after every bytecode executed. Possible alternatives: only break transactions after certain  bytecodes, break transactions on one abstraction layer above, e.g. object methods (setter, getter).\nrstm calls were exposed using primtives (a way to expose native code in Smalltalk), this was mainly used for atomic.\nStarting and stopping OS threads is exposed via primitives as well. Threads are started from within the interpreter.\nFor Smalltalk enabled STM code we currently have different image versions. However another way to add, load and replace code to the Smalltalk code base is required to make a switch between STM and non-STM code simple.\n\n\n\nDetails on the project setup\nFrom a non-technical perspective, a problem we encountered was the huge roundtrip times (on our machines up to 600s, 900s with JIT enabled). This led to a tendency of bigger code changes (\"Before we compile, let's also add this\"), lost flow (\"What where we doing before?\") and different compiled interpreters in parallel testing (\"How is this version different from the others?\") As a consequence it was harder to test and correct errors. While this is not as much of a problem for other RPython VMs, RSqueakVM needs to execute the entire image, which makes running it untranslated even slower.\n\nSummary\nThe benchmarks show that speed up is possible, but also that the STM overhead in some situations can eat up the speedup. The  resulting STM-enabled VM still has some limitations: As rstm is  currently only running on 64-bit Linux the RSqueakVM is doing so as  well. Eventhough it is possible for us now to create new threads that  map to OS threads within the VM, the migration of exiting Smalltalk threads keeps being problematic.\nWe showed that an existing VM code base can benefit of STM in terms of scaling up. Further it was relatively easy to enable STM support. This may also be valuable to VM developers considering to get STM support for their VMs.",
      "tags": "Smalltalk,Squeak,stm",
      "url": "https://www.pypy.org/posts/2014/08/a-field-test-of-software-transactional-5659022209916605798.html"
    },
    {
      "title": "PyPy-STM: first \"interesting\" release",
      "text": "Hi all,\n\nPyPy-STM is now reaching a point where we can say it's good enough to be\na GIL-less Python.  (We don't guarantee there are no more bugs, so please\nreport them :-)  The first official STM release:\n\n\npypy-stm-2.3-r2-linux64\n(UPDATE: this is release r2, fixing a systematic segfault at start-up on some systems)\n\n\nThis corresponds roughly to PyPy 2.3 (not 2.3.1).  It requires 64-bit\nLinux.  More precisely, this release is built for Ubuntu 12.04 to 14.04;\nyou can also rebuild it\nfrom source by getting the branch stmgc-c7.  You need\nclang to compile, and you need a patched\nversion of llvm.\n\nThis version's performance can reasonably be compared with a regular\nPyPy, where both include the JIT.  Thanks for following the meandering progress of PyPy-STM over the past three years --- we're finally getting somewhere really interesting!  We cannot thank enough all contributors to the previous PyPy-STM money pot that made this possible.  And, although this blog post is focused on the results from that period of time, I have of course to remind you that we're running a second call for donation for future work, which I will briefly mention again later.\n\nA recap of what we did to get there: around the start of the year we found a new model, a \"redo-log\"-based STM which uses a couple of hardware tricks to not require chasing pointers, giving it (in this context) exceptionally cheap read barriers.  This idea was developed over the following months and (relatively) easily integrated with the JIT compiler.  The most recent improvements on the Garbage Collection side are closing the gap with a regular PyPy (there is still a bit more to do there).  There is some preliminary user documentation.\n\nToday, the result of this is a PyPy-STM that is capable of running pure Python code on multiple threads in parallel, as we will show in the benchmarks that follow.  A quick warning: this is only about pure Python code.  We didn't try so far to optimize the case where most of the time is spent in external libraries, or even manipulating \"raw\" memory like array.array or numpy arrays.  To some extent there is no point because the approach of CPython works well for this case, i.e. releasing the GIL around the long-running operations in C.  Of course it would be nice if such cases worked as well in PyPy-STM --- which they do to some extent; but checking and optimizing that is future work.\n\nAs a starting point for our benchmarks, when running code that\nonly uses one thread, we get a slow-down between 1.2 and 3: at worst,\nthree times as slow; at best only 20% slower than a regular\nPyPy.  This worst case has been brought down --it used to be 10x-- by\nrecent work on \"card marking\", a useful GC technique that is also\npresent in the regular PyPy (and about which I don't find any blog post;\nmaybe we should write one :-)  The main remaining issue is fork(), or\nany function that creates subprocesses: it works, but is very slow.  To\nremind you of this fact, it prints a line to stderr when used.\n\nNow the real main part: when you run multithreaded code, it scales very nicely with two\nthreads, and less-than-linearly but still not badly with three or four\nthreads.  Here is an artificial example:\n\n    total = 0\n    lst1 = [\"foo\"]\n    for i in range(100000000):\n        lst1.append(i)\n        total += lst1.pop()\n\nWe run this code N times, once in each of N threads\n(full\nbenchmark).  Run times, best of three:\n\n\n\nNumber of threads\n    Regular PyPy (head)\n    PyPy-STM\nN = 1\n    real 0.92s \nuser+sys 0.92s\n    real 1.34s \nuser+sys 1.34s\nN = 2\n    real 1.77s \nuser+sys 1.74s\n    real 1.39s \nuser+sys 2.47s\nN = 3\n    real 2.57s \nuser+sys 2.56s\n    real 1.58s \nuser+sys 4.106s\nN = 4\n    real 3.38s \nuser+sys 3.38s\n    real 1.64s \nuser+sys 5.35s\n\n\n(The \"real\" time is the wall clock time.  The \"user+sys\" time is the\nrecorded CPU time, which can be larger than the wall clock time if\nmultiple CPUs run in parallel.  This was run on a 4x2 cores machine.\nFor direct comparison, avoid loops that are so trivial\nthat the JIT can remove all allocations from them: right now\nPyPy-STM does not handle this case well.  It has to force a dummy allocation\nin such loops, which makes minor collections occur much more frequently.)\n\nFour threads is the limit so far: only four threads can be executed in\nparallel.  Similarly, the memory usage is limited to 2.5 GB of GC\nobjects.  These two limitations are not hard to increase, but at least\nincreasing the memory limit requires fighting against more LLVM bugs.\n(Include here snark remarks about LLVM.)\n\nHere are some measurements from more real-world benchmarks.  This time,\nthe amount of work is fixed and we parallelize it on T threads.  The first benchmark is just running translate.py on a trunk PyPy.  The last\nthree benchmarks are here.\n\n\n\nBenchmark\n    PyPy 2.3\n    (PyPy head)\n    PyPy-STM, T=1\n    T=2\n    T=3\n    T=4\ntranslate.py --no-allworkingmodules\n(annotation step)\n    184s\n    (170s)\n    386s (2.10x)\n    n/a\nmultithread-richards\n5000 iterations\n    24.2s\n    (16.8s)\n    52.5s (2.17x)\n    37.4s (1.55x)\n    25.9s (1.07x)\n    32.7s (1.35x)\nmandelbrot\ndivided in 16-18 bands\n    22.9s\n    (18.2s)\n    27.5s (1.20x)\n    14.4s (0.63x)\n    10.3s (0.45x)\n    8.71s (0.38x)\nbtree\n    2.26s\n    (2.00s)\n    2.01s (0.89x)\n    2.22s (0.98x)\n    2.14s (0.95x)\n    2.42s (1.07x)\n\n\nThis shows various cases that can occur:\n\nThe mandelbrot example runs with minimal overhead and very good parallelization.\nIt's dividing the plane to compute in bands, and each of the T threads receives the\nsame number of bands.\n\nRichards, a classical benchmark for PyPy (tweaked to run the iterations\nin multiple threads), is hard to beat on regular PyPy:\nwe suspect that the difference is due to the fact that a lot of\npaths through the loops don't allocate, triggering the issue already\nexplained above.  Moreover, the speed of Richards was again improved\ndramatically recently, in trunk.\n\nThe translation benchmark measures the time translate.py\ntakes to run the first phase only, \"annotation\" (for now it consumes too much memory\nto run translate.py to the end).  Moreover the timing starts only after the large number of\nsubprocesses spawned at the beginning (mostly gcc).  This benchmark is not parallel, but we\ninclude it for reference here.  The slow-down factor of 2.1x is still too much, but\nwe have some idea about the reasons: most likely, again the Garbage Collector, missing the regular PyPy's\nvery fast small-object allocator for old objects.  Also, translate.py\nis an example of application that could, with\nreasonable efforts, be made largely parallel in the future using atomic blocks.\n\nAtomic blocks are also present in the btree benchmark.  I'm not completely sure\nbut it seems that, in this case, the atomic blocks create too many\nconflicts between the threads for actual parallization: the base time is very good,\nbut running more threads does not help at all.\n\n\nAs a summary, PyPy-STM looks already useful to run CPU-bound multithreaded\napplications.  We are certainly still going to fight slow-downs, but it\nseems that there are cases where 2 threads are enough to outperform a regular\nPyPy, by a large margin.  Please try it out on your own small examples!\n\nAnd, at the same time, please don't attempt to retrofit threads inside\nan existing large program just to benefit from PyPy-STM!\nOur goal is not to send everyone down the obscure route of multithreaded\nprogramming and its dark traps.  We are going finally to shift our main\nfocus on the phase 2 of our\nresearch (donations welcome): how to enable a better way of writing multi-core programs.\nThe starting point is to fix and test atomic blocks.  Then we will have to\ndebug common causes of conflicts and fix them or work around them; and\ntry to see how common frameworks like Twisted can be adapted.\n\nLots of work ahead, but lots of work behind too :-)\n\nArmin (thanks Remi as well for the work).",
      "tags": "releasestm",
      "url": "https://www.pypy.org/posts/2014/07/pypy-stm-first-interesting-release-8684276541915333814.html"
    },
    {
      "title": "PyPy3 2.3.1 - Fulcrum",
      "text": "We're pleased to announce the first stable release of PyPy3. PyPy3\ntargets Python 3 (3.2.5) compatibility.We would like to thank all of the people who donated to the py3k proposal\nfor supporting the work that went into this.You can download the PyPy3 2.3.1 release here:https://pypy.org/download.html#pypy3-2-3-1HighlightsThe first stable release of PyPy3: support for Python 3!\nThe stdlib has been updated to Python 3.2.5\nAdditional support for the u'unicode' syntax (PEP 414) from Python 3.3\nUpdates from the default branch, such as incremental GC and various JIT\nimprovements\nResolved some notable JIT performance regressions from PyPy2:\nRe-enabled the previously disabled collection (list/dict/set) strategies\nResolved performance of iteration over range objects\nResolved handling of Python 3's exception __context__ unnecessarily forcing\nframe object overhead\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.6 or 3.2.5. It's fast due to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64, Windows,\nand OpenBSD,\nas well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.While we support 32 bit python on Windows, work on the native Windows 64\nbit python is still stalling, we would welcome a volunteer\nto handle that.How to use PyPy?We suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.Cheers,\nthe PyPy team",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2014/06/pypy3-231-fulcrum-3765964217640322884.html"
    },
    {
      "title": "PyPy 2.3.1 - Terrestrial Arthropod Trap Revisited",
      "text": "We're pleased to announce PyPy 2.3.1, a feature-and-bugfix improvement over our recent 2.3 release last month.\n\nThis release contains several bugfixes and enhancements among the user-facing improvements:\nThe built-in struct module was renamed to _struct, solving issues with IDLE and other modules\nSupport for compilation with gcc-4.9\nA CFFI-based version of the gdbm module is now included in our binary bundle\nMany issues were resolved since the 2.3 release on May 8 \n\nYou can download the PyPy 2.3.1 release here:\n\nhttps://pypy.org/download.html\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (pypy 2.3.1 and cpython 2.7.x performance comparison) due to its integrated tracing JIT compiler.\n\nThis release supports x86 machines running Linux 32/64, Mac OS X 64,  Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7,  with VFPv3) running Linux.\u00a0 \nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here.\n\nPlease try it out and let us know what you think. We especially welcome success stories, please tell us about how it has helped you!\n\nCheers, The PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/06/pypy-231-terrestrial-arthropod-trap-5076300474324870908.html"
    },
    {
      "title": "PyPy 2.3 - Terrestrial Arthropod Trap",
      "text": "We\u2019re pleased to announce PyPy 2.3, which targets version 2.7.6 of the Python language. This release updates the stdlib from 2.7.3, jumping directly to 2.7.6.\n\nThis release also contains several bugfixes and performance improvements, many generated by real users finding corner cases.\u00a0CFFI\u00a0has made it easier than ever to use existing C code with both cpython and PyPy, easing the transition for packages like\u00a0cryptography,\u00a0Pillow(Python Imaging Library [Fork]), a basic port of\u00a0pygame-cffi, and others.\n\nPyPy can now be embedded in a hosting application, for instance inside\u00a0uWSGI\n\nYou can download the PyPy 2.3 release here:\n\nhttps://pypy.org/download.html\n\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (pypy 2.3 and cpython 2.7.x performance comparison; note that cpython's speed has not changed since 2.7.2) due to its integrated tracing JIT compiler.\n\nThis release supports x86 machines running Linux 32/64, Mac OS X 64, Windows, and OpenBSD, as well as newer ARM hardware (ARMv6 or ARMv7, with VFPv3) running Linux.\u00a0\n\nWe would like to thank our donors for the continued support of the PyPy project.\n\nThe complete release notice is here\n\nCheers, The PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/05/pypy-23-terrestrial-arthropod-trap-9057496904945555741.html"
    },
    {
      "title": "NumPy on PyPy - Status Update",
      "text": "Work on NumPy on PyPy continued in March, though at a lighter pace than the previous few months. Progress was made on both compatibility and speed fronts. Several behavioral issues reported to the bug tracker were resolved. The most significant of these was probably the correction of casting to built-in Python types. Previously, int/long conversions of numpy scalars such as inf/nan/1e100 would return bogus results. Now, they raise or return values, as appropriate.\n\nOn the speed front, enhancements to the PyPy JIT were made to support virtualizing the raw_store/raw_load memory operations used in numpy arrays. Further work remains here in virtualizing the alloc_raw_storage when possible. This will allow scalars to have storages but still be virtualized when possible in loops.\n\nAside from continued work on compatibility/speed of existing code, we also hope to begin implementing the C-level components of other numpy modules such as mtrand, nditer, linalg, and so on. Several approaches could be taken to get C-level code in these modules working, ranging from reimplementing in RPython to interfacing with existing code with CFFI, if possible. The appropriate approach depends on many factors and will probably vary from module to module.To try out PyPy + NumPy, grab a nightly PyPy and install our NumPy fork. Feel free to report comments/issues to IRC, our mailing list, or bug tracker. Thanks to the contributors to the NumPy on PyPy\u00a0proposal for supporting this work.",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2014/04/numpy-on-pypy-status-update-1103134247318103282.html"
    },
    {
      "title": "STM results and Second Call for Donations",
      "text": "Hi all,\n\nWe now have a preliminary version of PyPy-STM\nwith the JIT, from the new STM documentation\npage.  This PyPy-STM is still not quite useful, failing to top the\nperformance of a regular PyPy by a small margin on most benchmarks, but\nit's definitely getting there :-)  The overheads with the JIT are still\na bit too high.  (I've been tracking an obscure bug since days.\nIt turned out to be a simple buffer overflow.  But if anybody has\na clue about why a hardware watchpoint in gdb, set on one of the garbled\nmemory locations, fails to trigger but the memory ends up being modified\nanyway... and, it turns out, by just a regular pointer write... ideas\nwelcome.)\n\nBut I go off-topic :-)  The main point of this post is to announce the\n2nd Call for Donation about\nSTM.  We achieved most of the goals laid out in the first call.  We\neven largely overachieved them in terms of raw performance, even if\nthere are many cases that are unreasonably slow for now.  So, after the\nsuccessful research, we are launching a second proposal about the\ndevelopment part of the project:\n\nPolish PyPy-STM to get a consistently reasonable speed, 25%-40%\nslower than a regular JITted PyPy when running single-threaded code.  Of\ncourse it is supposed to scale nicely as long as there are no\nuser-visible conflicts.\n\nFocus on developing the Python-facing interface: both internal things\n(e.g. do dictionaries need to be more TM-friendly in general?) as well\nas directly visible things (e.g. some profiler-like interface to explore\ncommon conflicts in a program).\n\nRegular multithreaded code should benefit out of the box, but the\nfinal goal is to explore and tweak some existing non-multithreaded\nframeworks and improve their TM-friendliness.  So existing programs\nusing Twisted or Stackless, for example, should run on multiple cores\nwithout any major change.\n\nSee the full call for more\ndetails!  I'd like to thank Remi Meier for getting involved.  And a big\nthank you to everybody who contributed money on the first call.  It\ntook more time than anticipated, but it's there in good but rough shape.\nNow it needs a lot of polishing :-)\n\nArmin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2014/04/stm-results-and-second-call-for-1767845182888902777.html"
    },
    {
      "title": "pygame_cffi: pygame on PyPy",
      "text": "The Raspberry Pi aims to be a low-cost educational tool that anyone can use to learn about electronics and programming. Python and pygame are included in the Pi's programming toolkit. And since last year, thanks in part to sponsorship from the Raspberry Pi Foundation, PyPy also works on the Pi (read more here).\nWith PyPy working on the Pi, game logic written in Python stands to gain an awesome performance boost. However, the original pygame is a Python C extension. This means it performs poorly on PyPy and negates any speedup in the Python parts of the game code.\nOne solution to making pygame games run faster on PyPy, and eventually on the Raspberry Pi, comes in the form of pygame_cffi. pygame_cffi uses CFFI to wrap the underlying SDL library instead of a C extension. A few months ago, the Raspberry Pi Foundation sponsored a Cape Town Python User Group hackathon to build a proof-of-concept pygame using CFFI. This hackathon was a success and it produced an early working version of pygame_cffi.\nSo for the last 5 weeks Raspberry Pi has been funding work on pygame_cffi. The goal was a complete implementation of the core modules. We also wanted benchmarks to illuminate performance differences between pygame_cffi on PyPy and pygame on CPython. We are happy to report that those goals were met. So without further ado, here's a rundown of what works.\n\nCurrent functionality\n\nSurfaces support all the usual flags for SDL and OpenGL rendering (more about OpenGL below).\nThe graphics-related modules color, display, font and image, and parts of draw and transform are mostly complete.\nEvents! No fastevent module yet, though.\nMouse and keyboard functionality, as provided by the mouse and key modules, is complete.\nSound functionality, as provided by the mixer and music modules, is complete.\nMiscellaneous modules, cursors, rect, sprite and time are also complete.\n\n\nInvention screenshot:\n\n\n\nMutable mamba screenshot:\n\n\n\nWith the above-mentioned functionality in place we could get 10+ of the pygame examples to work, and a number of PyWeek games. At the time of writing, if a game doesn't work it is most likely due to an unimplemented transform or draw function. That will be remedied soon.\n\n\nPerformance\nIn terms of performance, pygame_cffi on PyPy is showing a lot of promise. It beats pygame on CPython by a significant margin in our events processing and collision detection benchmarks, while blit and fill benchmarks perform similarly. The pygame examples we checked also perform better.\n\n\n\n\n\nHowever, there is still work to be done to identify and eliminate bottlenecks. On the Raspberry Pi performance is markedly worse compared to pygame (barring collision detection). The PyWeek games we tested also performed slightly worse. Fortunately there is room for improvement in various places.\n\nInvention & Mutable Mamba (x86)\n\n\n\nStandard pygame examples (Raspberry Pi)\n\n\n\nHere's a summary of some of the benchmarks. Relative speed refers to the frame rate obtained in pygame_cffi on PyPy relative to pygame on CPython.\n\n\n\n\n\n\nBenchmark\nRelative speed (pypy speedup)\n\n\n\nEvents (x86)\n1.41\n\nEvents (Pi)\n0.58\n\nN2 collision detection on 100 sprites (x86)\n4.14\n\nN2 collision detection on 100 sprites (Pi)\n1.01\n\nBlit 100 surfaces (x86)\n1.06\n\nBlit 100 surfaces (Pi)\n0.60\n\nInvention (x86)\n0.95\n\nMutable Mamba (x86)\n0.72\n\nstars example (x86)\n1.95\n\nstars example (Pi)\n0.84\n\n\n\n\nOpenGL\nSome not-so-great news is that PyOpenGL performs poorly on PyPy since PyOpenGL uses ctypes. This translates into a nasty reduction in frame rate for games that use OpenGL surfaces. It might be worthwhile creating a CFFI-powered version of PyOpenGL as well.\n\n\n\nWhere to now?\nWork on pygame_cffi is ongoing. Here are some things that are in the pipeline:\n\nGet pygame_cffi on PyPy to a place where it is consistently faster than pygame on CPython.\nImplement the remaining modules and functions, starting with draw and transform.\nImprove test coverage.\nReduce the time it takes for CFFI to parse the cdef. This makes the initial pygame import slow.\n\nIf you want to contribute you can find pygame_cffi on Github.\nFeel free to find us on #pypy on freenode or post issues on github.\nCheers,\nRizmari Versfeld",
      "tags": "sponsors",
      "url": "https://www.pypy.org/posts/2014/03/pygamecffi-pygame-on-pypy-8679802461301121984.html"
    },
    {
      "title": "STMGC-C7 with PyPy",
      "text": "Hi all,\n\nHere is one of the first full PyPy's\n(edit: it was r69967+, but the general list of versions is currently here)\ncompiled with the new StmGC-c7\nlibrary.  It has no JIT so far, but it runs some small\nsingle-threaded benchmarks by taking around 40% more time than a\ncorresponding non-STM, no-JIT version of PyPy.  It scales --- up to two\nthreads only, which is the hard-coded maximum so far in the c7 code.\nBut the scaling looks perfect in these small benchmarks without\nconflict: starting two threads each running a copy of the benchmark\ntakes almost exactly the same amount of total time, simply using two\ncores.\n\nFeel free to try it!  It is not actually useful so far, because it is\nlimited to two cores and CPython is something like 2.5x faster.  One of\nthe important next steps is to re-enable the JIT.  Based on our current\nunderstanding of the \"40%\" figure, we can probably reduce it with\nenough efforts; but also, the JIT should be able to easily produce\nmachine code that suffers a bit less than the interpreter from these\neffects.  This seems to mean that we're looking at 20%-ish slow-downs\nfor the future PyPy-STM-JIT.\n\nInteresting times :-)\n\nFor reference, this is what you get by downloading the\nPyPy binary linked above: a Linux 64 binary (Ubuntu 12.04) that\nshould behave mostly like a regular PyPy.  (One main missing feature is\nthat destructors are never called.)  It uses two cores, but obviously\nonly if the Python program you run is multithreaded.  The only new\nbuilt-in feature is with __pypy__.thread.atomic: this gives\nyou a way to enforce that a block of code runs \"atomically\", which means\nwithout any operation from any other thread randomly interleaved.\n\nIf you want to translate it yourself, you need a trunk version of clang\nwith three patches applied.  That's the number of bugs that we couldn't\nfind workarounds for, not the total number of bugs we found by (ab)using\nthe address_space feature...\n\nStay tuned for more!\n\nArmin & Remi",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2014/03/hi-all-here-is-one-of-first-full-pypys-8725931424559481728.html"
    },
    {
      "title": "PyPy on uWSGI",
      "text": "Hello everyone\nThere is an interview with Roberto De Ioris (from uWSGI fame) about embedding PyPy in uWSGI that covers recent addition of a PyPy embedding interface using cffi and the experience with using it. Read The full interview\nCheers\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2014/03/hello-everyone-there-is-interview-with-7561523711224053700.html"
    },
    {
      "title": "NumPy on PyPy - Progress in February",
      "text": "More progress was made on the NumPy front in the past month. On the compatibility front, we now pass ~130 more tests from NumPy's suite since the end of January. Currently, we pass 2336 tests out of 3265 tests run, with many of the failures representing portions of NumPy that we don't plan to implement in the near future (object dtypes, unicode, etc). There are still some failures that do represent issues, such as special indexing cases and failures to respect subclassed ndarrays in return values, which we do plan to resolve. There are also some unimplemented components and ufuncs remaining which we hope to implement, such as nditer and mtrand. Overall, the most common array functionality should be working.\n\nAdditionally, I began to take a look at some of the loops generated by our code. One widely used loop is dot, and we were running about 5x slower than NumPy's C version. I was able to optimize the dot loop and also the general array iterator to get us to ~1.5x NumPy C time on dot operations of various sizes. Further progress in this area could be made by using CFFI to tie into BLAS libraries, when available. Also, work remains in examining traces generated for our other loops and checking for potential optimizations.\n\nTo try out PyPy + NumPy, grab a nightly PyPy and install our NumPy fork. Feel free to report comments/issues to IRC, our mailing list, or bug tracker. Thanks to the contributors to the NumPy on PyPy proposal for supporting this work.\n\nCheers,\nBrian",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2014/03/numpy-status-update-february-1245769841736493525.html"
    },
    {
      "title": "Py3k status update #13",
      "text": "This is the 13th status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.We're just finishing up a cleanup of int/long types. This work helps the py3k\nbranch unify these types into the Python 3 int and restore JIT compilation of\nmachine sized integers.This cleanup also removes multimethods from these types. PyPy has\nhistorically used a clever implementation of multimethod dispatch for declaring\nmethods of the __builtin__ types in RPython.This multimethod scheme provides some convenient features for doing this,\nhowever we've come to the conclusion that it may be more trouble than it's\nworth. A major problem of multimethods is that they generate a large amount of\nstub methods which burden the already lengthy and memory hungry RPython\ntranslation process. Also, their implementation and behavior can be somewhat\ncomplicated/obscure.The alternative to multimethods involves doing the work of the type checking\nand dispatching rules in a more verbose, manual way. It's a little more work in\nthe end but less magical.Recently, Manuel Jacob finished a large cleanup effort of the\nunicode/string/bytearray types that also removed their multimethods. This work\nalso benefits the py3k branch: it'll help with future PEP 393 (or PEP 393\nalternative) work. This effort was partly sponsored by Google's Summer of\nCode: thanks Manuel and Google!Now there's only a couple major pieces left in the multimethod removal (the\nfloat/complex types and special marshaling code) and a few minor pieces that\nshould be relatively easy.In conclusion, there's been some good progress made on py3k and multimethod\nremoval this winter, albeit a bit slower than we would have liked.cheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2014/02/py3k-status-update-13-4630607029125647100.html"
    },
    {
      "title": "Rewrites of the STM core model -- again",
      "text": "Hi all,\n\nA quick note about the Software Transactional Memory (STM) front.\n\nSince the previous\npost, we believe we progressed a lot by discovering an alternative\ncore model for software transactions.  Why do I say \"believe\"?  It's\nbecause it means again that we have to rewrite from scratch the C\nlibrary handling STM.  This is currently work in progress.  Once this is\ndone, we should be able to adapt the existing pypy-stm to run on top of\nit without much rewriting efforts; in fact it should simplify the\ndifficult issues we ran into for the JIT.  So while this is basically\nyet another restart similar to last\nJune's, the difference is that the work that we have already put in the PyPy\npart (as opposed to the C library) remains.\n\nYou can read about the basic ideas of this new C library here.\nIt is still STM-only, not HTM, but because it doesn't constantly move\nobjects around in memory, it would be easier to adapt an HTM version.\nThere are even potential ideas about a hybrid TM, like using HTM but\nonly to speed up the commits.  It is based on a Linux-only system call, remap_file_pages()\n(poll: who heard about it before? :-).  As previously, the work is done\nby Remi Meier and myself.\n\nCurrently, the C library is incomplete, but early experiments show good\nresults in running duhton,\nthe interpreter for a minimal language created for the purpose of\ntesting STM.  Good results means we brought down the slow-downs from\n60-80% (previous version) to around 15% (current version).  This number\nmeasures the slow-down from the non-STM-enabled to the STM-enabled\nversion, on one CPU core; of course, the idea is that the STM version\nscales up when using more than one core.\n\nThis means that we are looking forward to a result that is much better\nthan originally predicted.  The pypy-stm has chances to run at a\none-thread speed that is only \"n%\" slower than the regular pypy-jit, for\na value of \"n\" that is optimistically 15 --- but more likely some number\naround 25 or 50.  This is seriously better than the original estimate,\nwhich was \"between 2x and 5x\".  It would mean that using pypy-stm is\nquite worthwhile even with just two cores.\n\nMore updates later...\n\nArmin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2014/02/rewrites-of-stm-core-model-again-633249729751034512.html"
    },
    {
      "title": "NumPy Status Update - December/January",
      "text": "Work continued on the NumPy + PyPy front steadily in December and more lightly in January. The continued focus was compatibility, targeting incorrect or unimplemented features that appeared in multiple NumPy test suite failures. We now pass ~2/3 of the NumPy test suite. The biggest improvements were made in these areas:\n\n- Bugs in conversions of arrays/scalars to/from native types\n- Fix cases where we would choose incorrect dtypes when initializing or computing results\n- Improve handling of subclasses of ndarray through computations\n- Support some optional arguments for array methods that are used in the pure-python part of NumPy\n- Support additional attributes in arrays, array.flags, and dtypes\n- Fix some indexing corner cases that arise in NumPy testing\n- Implemented part of\u00a0numpy.fft (cffti and cfftf)\n\nLooking forward, we plan to continue improving the correctness of the existing implemented NumPy functionality, while also beginning to look at performance. The initial focus for performance will be to look at areas where we are significantly worse than CPython+NumPy. Those interested in trying these improvements out will need a PyPy nightly, and an install of the PyPy NumPy fork. Thanks again to the NumPy on PyPy donors for funding this work.",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2014/02/numpy-status-update-decemberjanuary-4292961614234099787.html"
    },
    {
      "title": "NumPy Status Update - November",
      "text": "Since the PyPy 2.2 release last month, more progress has been made on the NumPy compatibility front. Initial work has been directed by running the NumPy test suite and targeting failures that appear most frequently, along with fixing the few bugs reported on the bug tracker.\n\nImprovements were made in these areas:\n- Many missing/broken scalar functionalities were added/fixed. The scalar API should match up more closely with arrays now.\n- Some missing dtype functionality was added (newbyteorder, hasobject, descr, etc)\n- Support for optional arguments (axis, order) was added to some ndarray functions\n- Fixed some corner cases for string/record types\n\nMost of these improvements went onto trunk after 2.2 was split, so if you're interested in trying them out or running into problems on 2.2, try the\nnightly.\n\nThanks again to the NumPy on PyPy donors who make this continued progress possible.\n\nCheers,\nBrian",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/12/numpy-status-update-november-364321959153372759.html"
    },
    {
      "title": "PyGame CFFI",
      "text": "One of the RaspberryPi's goals is to be a fun toolkit for school children (and adults!) to learn programming and electronics with. Python and pygame are part of this toolkit. Recently the RaspberryPi Foundation funded parts of the effort of porting of pypy to the Pi -- making Python programs on the Pi faster!\nUnfortunately pygame is written as a Python C extension that wraps SDL which means performance of pygame under pypy remains mediocre. To fix this pygame needs to be rewritten using cffi to wrap SDL instead.\nRaspberryPi sponsored a CTPUG (Cape Town Python User Group) hackathon to put together a proof-of-concept pygame-cffi. The day was quite successful - we got a basic version of the bub'n'bros client working on pygame-cffi (and on PyPy). The results can be found on github with contributions from the five people present at the sprint.\nWhile far from complete, the proof of concept does show that there are no major obstacles to porting pygame to cffi and that cffi is a great way to bind your Python package to C libraries.\nAmazingly, we managed to have machines running all three major platforms (OS X, Linux and Windows) at the hackathon so the code runs on all of them!\nWe would like to thank the Praekelt foundation for providing the venue and The Raspberry Pi foundation for providing food and drinks!\nCheers,\nSimon Cross, Jeremy Thurgood, Neil Muller, David Sharpe and fijal.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/12/pygame-cffi-8991437796535033699.html"
    },
    {
      "title": "PyPy Leysin Winter Sprint (11-19st January 2014)",
      "text": "The next PyPy sprint will be in Leysin, Switzerland, for the ninth time.\nThis is a fully public sprint: newcomers and topics other than those\nproposed below are welcome.\nGoals and topics of the sprint\n\nPy3k: work towards supporting Python 3 in PyPy\nNumPyPy: work towards supporting the numpy module in PyPy\nSTM: work towards supporting Software Transactional Memory\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off for ski.\n\n\nExact times\nFor a change, and as an attempt to simplify things, I specified the\ndates as 11-19 January 2014, where 11 and 19 are travel days.  We will\nwork full days between the 12 and the 18.  You are of course allowed to\nshow up for a part of that time only, too.\nLocation & Accomodation\nLeysin, Switzerland, \"same place as before\".  Let me refresh your\nmemory: both the sprint venue and the lodging will be in a very spacious\npair of chalets built specifically for bed & breakfast:\nhttps://www.ermina.ch/.  The place has a good ADSL Internet connexion\nwith wireless installed.  You can of course arrange your own lodging\nanywhere (as long as you are in Leysin, you cannot be more than a 15\nminutes walk away from the sprint venue), but I definitely recommend\nlodging there too -- you won't find a better view anywhere else (though\nyou probably won't get much worse ones easily, either :-)\nPlease confirm that you are coming so that we can adjust the\nreservations as appropriate.  The rate so far has been around 60 CHF a\nnight all included in 2-person rooms, with breakfast.  There are larger\nrooms too (less expensive per person) and maybe the possibility to get a\nsingle room if you really want to.\nPlease register by Mercurial:\n\nhttps://bitbucket.org/pypy/extradoc/\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/leysin-winter-2014\n\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\nYou need a Swiss-to-(insert country here) power adapter.  There will be\nsome Swiss-to-EU adapters around -- bring a EU-format power strip if you\nhave one.",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/11/pypy-leysin-winter-sprint-11-19st-8860782754173653661.html"
    },
    {
      "title": "PyPy 2.2.1 - Incrementalism.1",
      "text": "We're pleased to announce PyPy 2.2.1, which targets version 2.7.3 of the Python\nlanguage. This is a bugfix release over 2.2.\nYou can download the PyPy 2.2.1 release here:\n\nhttps://pypy.org/download.html\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.2 and cpython 2.7.2 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64, Windows\n32, or ARM (ARMv6 or ARMv7, with VFPv3).\nWork on the native Windows 64 is still stalling, we would welcome a volunteer\nto handle that.\nHighlights\nThis is a bugfix release.  The most important bugs fixed are:\n\nan issue in sockets' reference counting emulation, showing up\nnotably when using the ssl module and calling makefile().\nTkinter support on Windows.\nIf sys.maxunicode==65535 (on Windows and maybe OS/X), the json\ndecoder incorrectly decoded surrogate pairs.\nsome FreeBSD fixes.\n\nNote that CFFI 0.8.1 was released.  Both versions 0.8 and 0.8.1 are\ncompatible with both PyPy 2.2 and 2.2.1.\nCheers,\nArmin Rigo & everybody",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/11/pypy-221-incrementalism1-9197847629771910947.html"
    },
    {
      "title": "CFFI 0.8",
      "text": "Hi all,\n\nCFFI 0.8 for CPython (2.6-3.x) has been released.\n\nQuick download: pip install cffi --upgrade\nDocumentation: https://cffi.readthedocs.org/en/release-0.8/\n\nWhat's new: a number of small fixes; ffi.getwinerror(); integrated support for C99 variable-sized structures; multi-thread safety.\n\n--- Armin\n\nUpdate: CFFI 0.8.1, with fixes on Python 3 on OS/X, and some FreeBSD fixes (thanks Tobias).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/11/cffi-08-6086756821078041950.html"
    },
    {
      "title": "NumPy status update",
      "text": "Here is what has been happening with NumPy in PyPy in October thanks to the people who donated to the NumPyPy proposal:\n\n The biggest change is that we shifted to using an external fork of numpy rather than a minimal numpypy module. The idea is that we will be able to\u00a0reuse\u00a0most of the upstream pure-python numpy\u00a0components, replacing the C modules with appropriate RPython micronumpy pieces at the correct places in the module namespace.\n\n The numpy fork should work just as well as the old numpypy for functionality that existed previously, and also include much new functionality from the pure-python numpy pieces that simply hadn't been imported yet in numpypy. However, this new functionality will not have been \"hand picked\" to only include pieces that work, so you may run into functionality that relies on unimplemented components (which should fail with user-level exceptions).\n\n This setup also allows us to run the entire numpy test suite, which will help in directing future compatibility development. The recent PyPy release includes these changes, so download it and let us know how it works! And if you want to live on the edge, the nightly includes even more numpy progress made in November.\n\n To install the fork, download the latest release, and then install numpy either separately with a virtualenv: pip install git+https://bitbucket.org/pypy/numpy.git; or directly: git clone https://bitbucket.org/pypy/numpy.git; cd numpy; pypy setup.py install.\n\nEDIT: if you install numpy as root, you may need to also import it once as root before it works: sudo pypy -c 'import numpy'\n\n\n Along with this change, progress was made in fixing internal micronumpy bugs and increasing compatibility:\nFixed a bug with strings in record dtypes\nFixed a bug where the multiplication of an ndarray with a Python int or float resulted in loss of the array's dtype\nFixed several segfaults encountered in the numpy test suite (suite should run now without segfaulting)\n\n We also began working on __array_prepare__ and __array_wrap__, which are necessary pieces for a working matplotlib module.\n\n Cheers,\nRomain and Brian",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/11/numpy-status-update-1609808546418002632.html"
    },
    {
      "title": "PyPy 2.2 - Incrementalism",
      "text": "We're pleased to announce PyPy 2.2, which targets version 2.7.3 of the Python language. This release main highlight is the introduction of the incremental garbage collector, sponsored by the Raspberry Pi Foundation.\nThis release also contains several bugfixes and performance improvements.\nYou can download the PyPy 2.2 release here:\nhttps://pypy.org/download.htmlWe would like to thank our donors for the continued support of the PyPy project. We showed quite a bit of progress on all three projects (see below) and we're slowly running out of funds. Please consider donating more so we can finish those projects!  The three projects are:\nPy3k (supporting Python 3.x): the release PyPy3 2.2 is imminent.\nSTM (software transactional memory): a preview will be released very soon, as soon as we fix a few bugs\nNumPy: the work done is included in the PyPy 2.2 release. More details below.\n\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for CPython 2.7. It's fast (pypy 2.2 and cpython 2.7.2 performance comparison) due to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64, Windows 32, or ARM (ARMv6 or ARMv7, with VFPv3).\nWork on the native Windows 64 is still stalling, we would welcome a volunteer to handle that.HighlightsOur Garbage Collector is now \"incremental\".  It should avoid almost all pauses due to a major collection taking place.  Previously, it would pause the program (rarely) to walk all live objects, which could take arbitrarily long if your process is using a whole lot of RAM.  Now the same work is done in steps.  This should make PyPy more responsive, e.g. in games.  There are still other pauses, from the GC and the JIT, but they should be on the order of 5 milliseconds each.\nThe JIT counters for hot code were never reset, which meant that a process running for long enough would eventually JIT-compile more and more rarely executed code.  Not only is it useless to compile such code, but as more compiled code means more memory used, this gives the impression of a memory leak.  This has been tentatively fixed by decreasing the counters from time to time.\nNumPy has been split: now PyPy only contains the core module, called _numpypy.  The numpy module itself has been moved to https://bitbucket.org/pypy/numpy and numpypy disappeared. You need to install NumPy separately with a virtualenv: pip install git+https://bitbucket.org/pypy/numpy.git; or directly: git clone https://bitbucket.org/pypy/numpy.git;\u00a0cd numpy;\u00a0pypy setup.py install.\nnon-inlined calls have less overhead\nThings that use sys.set_trace are now JITted (like coverage)\nJSON decoding is now very fast (JSON encoding was already very fast)\nvarious buffer copying methods experience speedups (like list-of-ints to int[] buffer from cffi)\nWe finally wrote (hopefully) all the missing os.xxx() functions, including os.startfile() on Windows and a handful of rare ones on Posix.\nnumpy has a rudimentary C API that cooperates with cpyext\nCheers,\nArmin Rigo and Maciej Fijalkowski",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/11/pypy-22-incrementalism-4723643710897639332.html"
    },
    {
      "title": "Py3k status update #12",
      "text": "This is the 12th status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.Here's an update on the recent progress:Thank you to everyone who has provided initial feedback on the PyPy3 2.1 beta\n1 release. We've gotten a number of bug reports, most of which have been\nfixed.\nAs usual, we're continually keeping up with changes from the default\nbranch. Oftentimes these merges come at a cost (conflicts and or\nreintegration of py3k changes) but occasionally we get goodies for free, such\nas the recent JIT optimizations and incremental garbage collection.\nWe've been focusing on re-optimizing Python 2 int sized (machine sized)\nintegers:\nWe have a couple of known, notable speed regressions in the PyPy3 beta release\nvs regular PyPy. The major one being with Python 2.x int sized (or machine\nsized) integers.Python 3 drops the distinction between int and long types. CPython 3.x\naccomplishes this by removing the old int type entirely and renaming the long\ntype to int. Initially, we've done the same for PyPy3 for the sake of\nsimplicity and getting everything working.However PyPy's JIT is capable of heavily optimizing these machine sized integer\noperations, so this came with a regression in performance in this area.We're now in the process of solving this. Part of this work also involves some\nhouse cleaning on these numeric types which also benefits the default branch.cheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2013/11/py3k-status-update-12-5307085693947812769.html"
    },
    {
      "title": "Making coverage.py faster under PyPy",
      "text": "If you've ever tried to run your programs with coverage.py under PyPy,\nyou've probably experienced some incredible slowness. Take this simple\nprogram:def f():\n    return 1\n\n\ndef main():\n    i = 10000000\n    while i:\n        i -= f()\n\nmain()\nRunning time coverage.py run test.py five times, and looking at the best\nrun, here's how PyPy 2.1 stacks up against CPython 2.7.5:\n\n\n\n\n\nPython\nTime\nNormalized to CPython\n\n\n\nCPython 2.7.5\n3.879s\n1.0x\n\nPyPy 2.1\n53.330s\n13.7x slower\n\n\nTotally ridiculous. I got turned onto this problem because on one of my\nprojects CPython takes about 1.5 minutes to run our test suite on the build\nbot, but PyPy takes 8-10 minutes.So I sat down to address it. And the results:\n\n\n\n\n\nPython\nTime\nNormalized to CPython\n\n\n\nCPython 2.7.5\n3.879s\n1.0x\n\nPyPy 2.1\n53.330s\n13.7x slower\n\nPyPy head\n1.433s\n2.7x faster\n\n\nNot bad.Technical detailsSo how'd we do it? Previously, using sys.settrace() (which coverage.py\nuses under the hood) disabled the JIT. Except it didn't just disable the JIT,\nit did it in a particularly insidious way \u2014 the JIT had no idea it was being\ndisabled!Instead, every time PyPy discovered that one of your functions was a hotspot,\nit would start tracing to observe what the program was doing, and right when it\nwas about to finish, coverage would run and cause the JIT to abort. Tracing\nis a slow process, it makes up for it by generating fast machine code at the\nend, but tracing is still incredibly slow. But we never actually got to the\n\"generate fast machine code\" stage. Instead we'd pay all the cost of tracing,\nbut then we'd abort, and reap none of the benefits.To fix this, we adjusted some of the heuristics in the JIT, to better show it\nhow sys.settrace(<tracefunc>) works. Previously the JIT saw it as an opaque\nfunction which gets the frame object, and couldn't tell whether or not it\nmessed with the frame object. Now we let the JIT look inside the\n<tracefunc> function, so it's able to see that coverage.py isn't\nmessing with the frame in any weird ways, it's just reading the line number and\nfile path out of it.I asked several friends in the VM implementation and research field if they\nwere aware of any other research into making VMs stay fast when debugging tools\nlike coverage.py are running. No one I spoke to was aware of any (but I\ndidn't do a particularly exhaustive review of the literature, I just tweeted at\na few people), so I'm pleased to say that PyPy is quite possibly the first VM\nto work on optimizing code in debugging mode! This is possible because of our\nyears spent investing in meta-tracing research.Happy testing,\nAlex",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/10/making-coveragepy-faster-under-pypy-935409618297062344.html"
    },
    {
      "title": "Update on STM",
      "text": "Hi all,\nThe sprint in London was a lot of fun and very fruitful. In the last\nupdate on STM, Armin was working on improving and specializing the\nautomatic barrier placement. There is still a lot to do in that area,\nbut that work is merged now. Specializing and improving barrier placement\nis still to be done for the JIT.\nBut that is not all. Right after the sprint, we were able to squeeze\nthe last obvious bugs in the STM-JIT combination. However, the performance\nwas nowhere near to what we want. So until now, we fixed some of the most\nobvious issues. Many come from RPython erring on the side of caution\nand e.g. making a transaction inevitable even if that is not strictly\nnecessary, thereby limiting parallelism. Another problem came from\nincreasing counters everytime a guard fails, which caused transactions\nto conflict on these counter updates. Since these counters do not have\nto be completely accurate, we update them non-transactionally now with\na chance of small errors.\nThere are still many such performance issues of various complexity left\nto tackle: we are nowhere near done. So stay tuned or contribute :)\n\nPerformance\nNow, since the JIT is all about performance, we want to at least\nshow you some numbers that are indicative of things to come.\nOur set of STM benchmarks is very small unfortunately\n(something you can help us out with), so this is\nnot representative of real-world performance. We tried to\nminimize the effect of JIT warm-up in the benchmark results.\nThe machine these benchmarks were executed on has 4 physical\ncores with Hyper-Threading (8 hardware threads).\nRaytracer from stm-benchmarks:\nRender times in seconds for a 1024x1024 image:\n\n\n\n\n\n\n\nInterpreter\nBase time: 1 thread\n8 threads (speedup)\n\n\n\nPyPy-2.1\n2.47\n2.56 (0.96x)\n\nCPython\n81.1\n73.4 (1.1x)\n\nPyPy-STM\n50.2\n10.8 (4.6x)\n\n\n\nFor comparison, disabling the JIT gives 148s on PyPy-2.1 and 87s on\nPyPy-STM (with 8 threads).\nRichards from PyPy repository on the stmgc-c4\nbranch:\nAverage time per iteration in milliseconds:\n\n\n\n\n\n\n\nInterpreter\nBase time: 1 thread\n8 threads (speedup)\n\n\n\nPyPy-2.1\n15.6\n15.4 (1.01x)\n\nCPython\n239\n237 (1.01x)\n\nPyPy-STM\n371\n116 (3.2x)\n\n\n\nFor comparison, disabling the JIT gives 492ms on PyPy-2.1 and 538ms on\nPyPy-STM.\n\nTry it!\nAll this can be found in the PyPy repository on the stmgc-c4\nbranch.\nTry it for yourself, but keep in mind that this is still experimental\nwith a lot of things yet to come. Only Linux x64 is supported right\nnow, but contributions are welcome.\nYou can download a prebuilt binary from here:\nhttps://bitbucket.org/pypy/pypy/downloads/pypy-oct13-stm.tar.bz2\n(Linux x64 Ubuntu >= 12.04).  This was made at revision bafcb0cdff48.\n\nSummary\nWhat the numbers tell us is that PyPy-STM is, as expected,\nthe only of the three interpreters where multithreading gives a large\nimprovement in speed.  What they also tell us is that, obviously, the\nresult is not good enough yet: it still takes longer on a 8-threaded\nPyPy-STM than on a regular single-threaded PyPy-2.1.  However, as you\nshould know by now, we are good at promising speed and delivering it...\nyears later :-)\nBut it has been two years already since PyPy-STM started, and this is\nour first preview of the JIT integration.  Expect major improvements\nsoon: with STM, the JIT generates code that is completely suboptimal in\nmany cases (barriers, allocation, and more).  Once we improve this, the\nperformance of the STM-JITted code should come much closer to PyPy 2.1.\nCheers\nRemi & Armin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2013/10/update-on-stm-7145890443443707910.html"
    },
    {
      "title": "Incremental Garbage Collector in PyPy",
      "text": "Hello everyone.\nWe're pleased to announce that as of today,\nthe default PyPy comes with a GC that has much smaller pauses than yesterday.\nLet's start with explaining roughly what GC pauses are. In CPython each\nobject has a reference count, which is incremented each time we create\nreferences and decremented each time we forget them. This means that objects\nare freed each time they become unreachable. That is only half of the story\nthough. First note that when the last reference to a large tree of\nobjects goes away, you have a pause: all the objects are freed. Your\nprogram is not progressing at all during this pause, and this pause's\nduration can be arbitrarily large. This occurs at deterministic times,\nthough. But consider code like this:\n\nclass A(object):\n     pass\n\na = A()\nb = A()\na.item = b\nb.item = a\ndel a\ndel b\n\nThis creates a reference cycle. It means that while we deleted references to\na and b from the current scope, they still have a reference count of 1,\nbecause they point to each other, even though the whole group has no references\nfrom the outside. CPython employs a cyclic garbage collector which is used to\nfind such cycles. It walks over all objects in memory, starting from some known\nroots, such as type objects, variables on the stack, etc. This solves the\nproblem, but can create noticeable, nondeterministic GC pauses as the heap\nbecomes large and convoluted.\nPyPy essentially has only the cycle finder - it does not bother with reference\ncounting, instead it walks alive objects every now and then (this is a big\nsimplification, PyPy's GC is much more complex than this). Although this might\nsound like a missing feature, it is really one of the reasons why PyPy is so\nfast, because at the end of the day the total time spent in managing the\nmemory is lower in PyPy than CPython. However, as a result, PyPy also has the\nproblem of GC pauses.\nTo alleviate this problem, which is essential for\napplications like games, we started to work on incremental GC, which spreads\nthe walking of objects and cleaning them across the execution time in smaller\nintervals. The work was sponsored by the Raspberry Pi foundation, started\nby Andrew Chambers and finished by Armin Rigo and Maciej Fija\u0142kowski.\n\n\nBenchmarks\nEveryone loves benchmarks. We did not measure any significant speed difference\non our quite extensive benchmark suite on speed.pypy.org. The main\nbenchmark that we used for other comparisons was translating the topaz\nruby interpreter using various versions of PyPy and CPython. The exact\ncommand was python <pypy-checkout>/bin/rpython -O2 --rtype targettopaz.py.\nVersions:\n\ntopaz - dce3eef7b1910fc5600a4cd0afd6220543104823\npypy source - defb5119e3c6\npypy compiled with minimark (non-incremental GC) - d1a0c07b6586\npypy compiled with incminimark (new, incremental GC) - 417a7117f8d7\nCPython - 2.7.3\n\nThe memory usage of CPython, PyPy with minimark and PyPy with incminimark is\nshown here. Note that this benchmark is quite bad for PyPy in general, the\nmemory usage is higher and the amount of time taken is longer. This is due\nto the JIT warmup being both memory hungry and inefficient (see below).\nBut first, the new GC is not worse than the old one.\n\n\nEDIT:Red line is CPython, blue is incminimark (new), green is minimark (old)\n\nThe image was obtained by graphing the output of memusage.py.\nHowever, the GC pauses are significantly smaller. For PyPy the way to\nget GC pauses is to measure time between start and stop while running stuff\nwith PYPYLOG=gc-collect:log pypy program.py, for CPython, the magic\nincantation is gc.set_debug(gc.DEBUG_STATS) and parsing the output.\nFor what is worth, the average and total for CPython, as well as the total\nnumber of events are not directly comparable since it only shows the cyclic\ncollector, not the reference counts. The only comparable thing is the\namount of long pauses and their duration. In the table below, pause duration\nis sorted into 8 buckets, each meaning \"below that or equal to the threshold\".\nThe output is generated using the gcanalyze tool.\nCPython:\n\n\n\n\n\n\n\n\n\n\n\n\n150.1ms\n300.2ms\n450.3ms\n600.5ms\n750.6ms\n900.7ms\n1050.8ms\n1200.9ms\n\n5417\n5\n3\n2\n1\n1\n0\n1\n\n\n\nPyPy minimark (non-incremental GC):\n\n\n\n\n\n\n\n\n\n\n\n\n216.4ms\n432.8ms\n649.2ms\n865.6ms\n1082.0ms\n1298.4ms\n1514.8ms\n1731.2ms\n\n27\n14\n6\n4\n6\n5\n3\n3\n\n\n\nPyPy incminimark (new incremental GC):\n\n\n\n\n\n\n\n\n\n\n\n\n15.7ms\n31.4ms\n47.1ms\n62.8ms\n78.6ms\n94.3ms\n110.0ms\n125.7ms\n\n25512\n122\n4\n1\n0\n0\n0\n2\n\n\n\nAs we can see, while there is still work to be done (the 100ms ones could\nbe split among several steps), we did improve the situation quite drastically\nwithout any actual performance difference.\nNote about the benchmark - we know it's a pretty extreme case of JIT\nwarmup, we know we suck on it, we're working on it and we're not afraid of\nshowing PyPy is not always the best ;-)\n\n\nNitty gritty details\nHere are some nitty gritty details for people really interested in\nGarbage Collection.  This was done as a patch to \"minimark\", our current\nGC, and called \"incminimark\" for now.  The former is a generational\nstop-the-world GC.  New objects are allocated \"young\", which means that\nthey initially live in the \"nursery\", a special zone of a few MB of\nmemory.  When the nursery is full, a \"minor collection\" step moves the\nsurviving objects out of the nursery.  This can be done quickly (a few\nmillisecond) because we only need to walk through the young objects that\nsurvive --- usually a small fraction of all young objects; and also by\nfar not all objects that are alive at this point, but only the young\nones.  However, from time to time this minor collection is followed by a\n\"major collection\": in that step, we really need to walk all objects to\nclassify which ones are still alive and which ones are now dead\n(\"marking\") and free the memory occupied by the dead ones (\"sweeping\").\nYou can read more details here.\nThis \"major collection\" is what gives the long GC pauses.  To fix this\nproblem we made the GC incremental: instead of running one complete\nmajor collection, we split its work into a variable number of pieces and\nrun each piece after every minor collection for a while, until there are\nno more pieces.  The pieces are each doing a fraction of marking, or a\nfraction of sweeping.  It adds some few milliseconds after each of these\nminor collections, rather than requiring hundreds of milliseconds in one\ngo.\nThe main issue is that splitting the major collections means that the\nmain program is actually running between the pieces, and so it can\nchange the pointers in the objects to point to other objects.  This is\nnot a problem for sweeping: dead objects will remain dead whatever the\nmain program does.  However, it is a problem for marking.  Let us see\nwhy.\nIn terms of the incremental GC literature, objects are either \"white\",\n\"gray\" or \"black\".  This is called tri-color marking.  See for example\nthis blog post about Rubinius, or this page about LuaJIT or the wikipedia description.  The\nobjects start as \"white\" at the beginning of marking; become \"gray\" when\nthey are found to be alive; and become \"black\" when they have been fully\ntraversed.  Marking proceeds by scanning grey objects for pointers to\nwhite objects.  The white objects found are turned grey, and the grey\nobjects scanned are turned black.  When there are no more grey objects,\nthe marking phase is complete: all remaining white objects are truly\nunreachable and can be freed (by the following sweeping phase).\nIn this model, the important part is that a black object can never point\nto a white object: if the latter remains white until the end, it will be\nfreed, which is incorrect because the black object itself can still be\nreached.  How do we ensure that the main program, running in the middle\nof marking, will not try to write a pointer to white object into a black\nobject?  This requires a \"write barrier\", i.e. a piece of code that runs\nevery time we set a pointer into an object or array.  This piece of code\nchecks if some (hopefully rare) condition is met, and calls a function\nif that is the case.\nThe trick we used in PyPy is to consider minor collections as part of\nthe whole, rather than focus only on major collections.  The existing\nminimark GC had always used a write barrier of its own to do its job,\nlike any generational GC.  This existing write barrier is used to detect\nwhen an old object (outside the nursery) is modified to point to a young\nobject (inside the nursery), which is essential information for minor\ncollections.  Actually, although this was the goal, the actual write\nbarrier code is simpler: it just records all old objects into which we\nwrite any pointer --- to a young or old object.  As we found out over\ntime, doing so is not actually slower, and might actually be a\nperformance improvement: for example, if the main program does a lot of\nwrites into the same old object, we don't need to check over and over\nagain if the written pointer points to a young object or not.  We just\nrecord the old object in some list the first time, and that's it.\nThe trick is that this unmodified write barrier works for incminimark\ntoo.  Imagine that we are in the middle of the marking phase, running\nthe main program.  The write barrier will record all old objects that\nare being modified.  Then at the next minor collection, all surviving\nyoung objects will be moved out of the nursery.  At this point, as we're\nabout to continue running the major collection's marking phase, we\nsimply add to the list of pending gray objects all the objects that we\njust considered --- both the objects listed as \"old objects that are\nbeing modified\", and the objects that we just moved out of the nursery.\nA fraction from the former list were black object; so this mean that\nthey are turned back from the black to the gray color.  This technique\nimplements nicely, if indirectly, what is called a \"backward write\nbarrier\" in the literature.  The backwardness is about the color that\nneeds to be changed in the opposite of the usual direction \"white ->\ngray -> black\", thus making more work for the GC.  (This is as opposed\nto \"forward write barrier\", where we would also detect \"black -> white\"\nwrites but turn the white object gray.)\nIn summary, I realize that this description is less about how we turned\nminimark into incminimark, and more about how we differ from the\nstandard way of making a GC incremental.  What we really had to do to\nmake incminimark was to write logic that says \"if the major collection\nis in the middle of the marking phase, then add this object to the list\nof gray objects\", and put it at a few places throughout minor\ncollection.  Then we simply split a major collection into increments,\ndoing marking or sweeping of some (relatively arbitrary) number of\nobjects before returning.  That's why, after we found that the existing\nwrite barrier would do, it was not much actual work, and could be done\nwithout major changes.  For example, not a single line from the JIT\nneeded adaptation.  All in all it was relatively painless work. ;-)\nCheers,armin and fijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/10/incremental-garbage-collector-in-pypy-8956893523842234676.html"
    },
    {
      "title": "Numpy Status Update",
      "text": "Hi everyone\n\nThanks to the people who donated money to the numpy proposal, here is what I've been working on recently :\n\n- Fixed conversion from a numpy complex number to a python complex number\n- Implement the rint ufunc\n-\u00a0Make numpy.character usable as a dtype\n-\u00a0Fix ndarray(dtype=str).fill()\n- Various fixes on boolean and fancy indexing\n\nCheers\nRomain",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/09/numpy-status-update-5160363918470470887.html"
    },
    {
      "title": "PyCon South Africa & sprint",
      "text": "Hi all,\n\nFor those of you that happen to be from South Africa: don't miss\nPyCon ZA 2013, next October 3rd and 4th!\nLike last year, a few of us will be there.  There will be the first talk\nabout STM getting ready (a\nblog post about that should follow soon).\n\nMoreover, general sprints will continue on the weekend (5th and 6th).\nAfterwards, Fijal will host a longer PyPy sprint (marathon?) with me\nuntil around the 21th.  You are welcome to it as well!  Write to the mailing list or to fijal directly (fijall\nat gmail.com), or simply in comments of this post.\n\n--- Armin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/09/pycon-south-africa-sprint-6630788654105016762.html"
    },
    {
      "title": "Slides of the PyPy London Demo Evening",
      "text": "The slides of the London demo evening are now online:",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/08/slides-of-pypy-london-demo-evening-5157052112396009739.html"
    },
    {
      "title": "NumPy road forward",
      "text": "Hello everyone.\nThis is the roadmap for numpy effort in PyPy as discussed on the London sprint.\nFirst, the highest on our priority list is to finish the low-level part\nof the numpy module. What\nwe'll do is to finish the RPython part of numpy and provide a pip installable\nnumpypy repository that includes the pure python part of Numpy. This would\ncontain the original Numpy with a few minor changes.\nSecond, we need to work on the JIT support that will make NumPy on PyPy\nfaster. In detail:\n\nreenable the lazy loop evaluation\noptimize bridges, which is depending on optimizer refactorings\nSSE support\n\nOn the compatibility front, there were some independent attempts into\nmaking the following stuff working:\n\nf2py\nC API (in fact, PyArray_* API is partly present in the nightly builds of\nPyPy)\nmatplotlib (both using PyArray_* API and embedding CPython runtime in PyPy)\nscipy\n\nIn order to make all of the above happen faster, it would be helpful to raise\nmore funds. You can donate to PyPy's NumPy project on our website. Note\nthat PyPy is a member of SFC which is a 501(c)(3) US non-profit, so donations\nfrom US companies can be tax-deducted.\nCheers,\nfijal, arigo, ronan, rguillebert, anto and others",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/08/numpy-road-forward-4210065750776753500.html"
    },
    {
      "title": "Preliminary London Demo Evening Agenda",
      "text": "We now have a preliminary agenda for the demo evening in London next week. It takes place on Tuesday, August 27 2013, 18:30-19:30 (BST) at King's College London, Strand. The preliminary agenda is as follows:\n\n\nLaurence Tratt: Welcome from the Software Development Team\nCarl Friedrich Bolz: A Short Introduction to PyPy\nMaciej Fija\u0142kowski: Numpy on PyPy, Present State and Outlook\nLukas Diekmann: Collection Strategies for Fast Containers in PyPy\nArmin Rigo: Software Transactional Memory for PyPy\nEdd Barrett: Unipycation: Combining Prolog and Python\n \n\nAll the talks are lightning talks. Afterwards there will be plenty of time for discussion.\n\nThere's still free spots, if you want to come, please register on the Eventbrite page. Hope to see you there!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/08/preliminary-london-demo-evening-agenda-5254002451136674320.html"
    },
    {
      "title": "Update on STM",
      "text": "Hi all,\n\nA quick update on Software Transactional Memory.  We are\nworking on two fronts.\n\nOn the one hand, the integration of the \"c4\" C library with PyPy is done\nand works well, but is still subject to improvements.  The \"PyPy-STM\"\nexecutable (without the JIT)\nseems to be stable, as far as it has been tested.  It runs a simple\nbenchmark like Richards with a 3.2x slow-down over a regular JIT-less\nPyPy.\n\nThe main factor of this slow-down: the numerous \"barriers\" in\nthe code --- checks that are needed a bit everywhere to verify that a\npointer to an object points to a recent enough version, and if not, to\ngo to the most recent version.  These barriers are inserted automatically\nduring the translation; there is no need for us to manually put 42 million\nbarriers in the source code of PyPy.  But this automatic insertion uses a\nprimitive algorithm right now, which usually ends up putting more barriers than the\ntheoretical optimum.  I (Armin) am trying to improve that --- and progressing:\nlast week the slow-down was around 4.5x.  This is done in the branch\nstmgc-static-barrier.\n\nOn the other hand, Remi is progressing on the JIT integration in\nthe branch stmgc-c4. \nThis has been working in simple cases since a couple of weeks by now, but the\nresulting \"PyPy-JIT-STM\" often crashes.  This is because while the\nbasics are not really hard, we keep hitting new issues that must be\nresolved.\n\nThe basics are that whenever the JIT is about to generate\nassembler corresponding to a load or a store in a GC object, it must\nfirst generate a bit of extra assembler that corresponds to the barrier\nthat we need.  This works fine by now (but could benefit from the same\nkind of optimizations described above, to reduce the number of barriers).\nThe additional issues are all more subtle.  I will describe the current\none as an example: it is how to write constant pointers inside the assembler.\n\nRemember that the STM library classifies objects as either\n\"public\" or \"protected/private\".  A \"protected/private\" object\nis one which has not been seen by another thread so far.\nThis is essential as an optimization, because we know that no\nother thread will access our protected or private objects in parallel,\nand thus we are free to modify their content in place.  By contrast,\npublic objects are frozen, and to do any change, we first need to\nbuild a different (protected) copy of the object.  See this\nblog\npost for more details.\n\nSo far so good, but the JIT will sometimes (actually often) hard-code\nconstant pointers into the assembler it produces.  For example, this is the\ncase when the Python code being JITted creates an instance of a known class;\nthe corresponding assembler produced by the JIT will reserve the memory for\nthe instance and then write the constant type pointer in it.  This type\npointer is a GC object (in the simple model, it's the Python class object;\nin PyPy it's actually the \"map\" object, which is\na different story).\n\nThe problem right now is that this constant pointer may point to a\nprotected object.  This is a problem because the same piece of assembler\ncan later be executed by a different thread.  If it does, then this\ndifferent thread will create instances whose type pointer is bogus: looking\nlike a protected object, but actually protected by a different thread.\nAny attempt to use this type pointer to change anything on the class\nitself will likely crash: the threads will all think they can safely change it\nin-place.  To fix this, we need to make sure we only write pointers to\npublic objects in the assembler.  This is a bit involved because we need\nto ensure that there is a public version of the object to start with.\n\nWhen this is done, we will likely hit the next problem, and the next one;\nbut at some point it should converge (hopefully!) and we'll give you our first\nPyPy-JIT-STM ready to try.  Stay tuned :-)\n\nA bient\u00f4t,\n\nArmin.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2013/08/update-on-stm-8705514488940872802.html"
    },
    {
      "title": "NumPyPy Status Update",
      "text": "Hello everyone\n\nAs expected, nditer is a lot of work. I'm going to pause my work on it for now and focus on simpler and more important things, here is a list of what I implemented :\n\nFixed a bug on 32 bit that made int32(123).dtype == dtype(\"int32\") fail\nFixed a bug on the pickling of array slices\nThe external loop flag is implemented on the nditer class\nThe c_index, f_index and multi_index flags are also implemented\nAdd dtype(\"double\") and dtype(\"str\")\nC-style iteration is available for nditer\n\nCheers\nRomain Guillebert",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/08/numpypy-status-update-3401163348519734658.html"
    },
    {
      "title": "PyPy 2.1 - Considered ARMful",
      "text": "We're pleased to announce PyPy 2.1, which targets version 2.7.3 of the Python\nlanguage. This is the first release with official support for ARM processors in the JIT.\nThis release also contains several bugfixes and performance improvements.You can download the PyPy 2.1 release here:https://pypy.org/download.htmlWe would like to thank the Raspberry Pi Foundation for supporting the work\nto finish PyPy's ARM support.The first beta of PyPy3 2.1, targeting version 3 of the Python language, was\njust released, more details can be found here.\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.1 and cpython 2.7.2 performance comparison)\ndue to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64 or Windows\n32. This release also supports ARM machines running Linux 32bit - anything with\nARMv6 (like the Raspberry Pi) or ARMv7 (like the Beagleboard,\nChromebook, Cubieboard, etc.) that supports VFPv3 should work. Both\nhard-float armhf/gnueabihf and soft-float armel/gnueabi builds are\nprovided. The armhf builds for Raspbian are created using the Raspberry Pi\ncustom cross-compilation toolchain\nbased on gcc-arm-linux-gnueabihf and should work on ARMv6 and\nARMv7 devices running Debian or Raspbian. The armel builds are built\nusing the gcc-arm-linux-gnuebi toolchain provided by Ubuntu and\ncurrently target ARMv7.Windows 64 work is still stalling, we would welcome a volunteer\nto handle that.HighlightsJIT support for ARM, architecture versions 6 and 7, hard- and soft-float ABI\nStacklet support for ARM\nSupport for os.statvfs and os.fstatvfs on unix systems\nImproved logging performance\nFaster sets for objects\nInterpreter improvements\nDuring packaging, compile the CFFI based TK extension\nPickling of numpy arrays and dtypes\nSubarrays for numpy\nBugfixes to numpy\nBugfixes to cffi and ctypes\nBugfixes to the x86 stacklet support\nFixed issue 1533: fix an RPython-level OverflowError for space.float_w(w_big_long_number).\nFixed issue 1552: GreenletExit should inherit from BaseException.\nFixed issue 1537: numpypy __array_interface__\nFixed issue 1238: Writing to an SSL socket in PyPy sometimes failed with a \"bad write retry\" message.\nCheers,David Schneider for the PyPy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/08/pypy-21-considered-armful-7177475722033479233.html"
    },
    {
      "title": "PyPy Demo Evening in London, August 27, 2013",
      "text": "As promised in the London sprint announcement we are organising a PyPy demo\nevening during the London sprint on Tuesday, August 27 2013, 18:30-19:30 (BST). The\ndescription of the event is below. If you want to come, please register on the\nEventbrite page.\n\nPyPy is a fast Python VM. Maybe you've never used PyPy and want to find out\nwhat use it might be for you? Or you and your organisation have been using it\nand you want to find out more about how it works under the hood? If so, this\ndemo session is for you!\nMembers of the PyPy team will give a series of lightning talks on PyPy: its\nbenefits; how it works; research currently being undertaken to make it\nfaster; and unusual uses it can be put to. Speakers will be available\nafterwards for informal discussions. This is the first time an event like\nthis has been held in the UK, and is a unique opportunity to speak to core\npeople. Speakers confirmed thus far include: Armin Rigo, Maciej Fija\u0142kowski,\nCarl Friedrich Bolz, Lukas Diekmann, Laurence Tratt, Edd Barrett.\nThe venue for this talk is the Software Development Team, King's College\nLondon. The main entrance is on the Strand, from where the room for the event\nwill be clearly signposted. Travel directions can be found at\nhttps://www.kcl.ac.uk/campuslife/campuses/directions/strand.aspx\nIf you have any questions about the event, please contact Laurence Tratt",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy-demo-evening-in-london-august-27-3640213278969666664.html"
    },
    {
      "title": "PyPy3 2.1 beta 1",
      "text": "We're pleased to announce the first beta of the upcoming 2.1 release of\nPyPy3. This is the first release of PyPy which targets Python 3 (3.2.3)\ncompatibility.We would like to thank all of the people who donated to the py3k proposal\nfor supporting the work that went into this and future releases.You can download the PyPy3 2.1 beta 1 release here:https://pypy.org/download.html#pypy3-2-1-beta-1HighlightsThe first release of PyPy3: support for Python 3, targetting CPython 3.2.3!There are some known issues including performance regressions (issues\n#1540 & #1541) slated to be resolved before the final release.\n\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3 or 3.2.3. It's fast due to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64 or Windows\n32. Also this release supports ARM machines running Linux 32bit - anything with\nARMv6 (like the Raspberry Pi) or ARMv7 (like Beagleboard,\nChromebook, Cubieboard, etc.) that supports VFPv3 should work.Windows 64 work is still stalling and we would welcome a volunteer to handle\nthat.How to use PyPy?We suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.Cheers,\nthe PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy3-21-beta-1-8647445024868663902.html"
    },
    {
      "title": "PyPy 2.1 beta 2",
      "text": "We're pleased to announce the second beta of the upcoming 2.1 release of PyPy.\nThis beta adds one new feature to the 2.1 release and contains several bugfixes listed below.You can download the PyPy 2.1 beta 2 release here:https://pypy.org/download.htmlHighlightsSupport for os.statvfs and os.fstatvfs on unix systems.\nFixed issue 1533: fix an RPython-level OverflowError for space.float_w(w_big_long_number).\nFixed issue 1552: GreenletExit should inherit from BaseException.\nFixed issue 1537: numpypy __array_interface__\nFixed issue 1238: Writing to an SSL socket in pypy sometimes failed with a \"bad write retry\" message.\ndistutils: copy CPython's implementation of customize_compiler, dont call\nsplit on environment variables, honour CFLAGS, CPPFLAGS, LDSHARED and\nLDFLAGS.\nDuring packaging, compile the CFFI tk extension.\nWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3. It's fast due to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64 or Windows\n32. Also this release supports ARM machines running Linux 32bit - anything with\nARMv6 (like the Raspberry Pi) or ARMv7 (like Beagleboard,\nChromebook, Cubieboard, etc.) that supports VFPv3 should work.Windows 64 work is still stalling, we would welcome a volunteer\nto handle that.How to use PyPy?We suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.Cheers,\nThe PyPy Team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy-21-beta-2-264349571160808803.html"
    },
    {
      "title": "PyPy San Francisco Sprint July 27th 2013",
      "text": "The next PyPy sprint will be in San Francisco, California. It is a public\nsprint, suitable for newcomers. It will run on Saturday July 27th.Some possible things people will be hacking on the sprint:running your software on PyPy\nmaking your software fast on PyPy\nimproving PyPy's JIT\nimproving Twisted on PyPy\nany exciting stuff you can think of\nIf there are newcomers, we'll run an introduction to hacking on PyPy.Location\nThe sprint will be held at the Rackspace Office:620 Folsom St, Ste 100The doors will open at 10AM and run until 6PM.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy-san-francisco-sprint-july-27th-2012-3064530444396960172.html"
    },
    {
      "title": "PyPy London Sprint (August 26 - September 1 2013)",
      "text": "The next PyPy sprint will be in London, United Kingdom for the first\ntime. This is a fully public sprint. PyPy sprints are a very good way\nto get into PyPy development and no prior PyPy knowledge is necessary.\nGoals and topics of the sprint\nFor newcomers:\n\nbring your application/library and we'll help you port it to PyPy,\nbenchmark and profile\ncome and write your favorite missing numpy function\nhelp us work on developer tools like jitviewer\n\nWe'll also work on:\n\nrefactoring the JIT optimizations\nSTM and STM-related topics\nanything else attendees are interested in\n\nExact times\nThe work days should be August 26 - September 1 2013 (Monday-Sunday).\nThe official plans are for people to arrive on the 26th, and\nto leave on the 2nd. There will be a break day in the middle.\nWe'll typically start at 10:00 in the morning.\nLocation\nThe sprint will happen within a room of King's College's Strand\nCampus in Central London, UK. There are some travel instructions how to\nget there. We are being hosted by Laurence Tratt and the Software\nDevelopment Team.\nDemo Session\nIf you don't want to come to the full sprint, but still want to chat a\nbit, we are planning to have a demo session on Tuesday August 27. We\nwill announce this separately on the blog. If you are interested, please\nleave a comment.\nRegistration\nIf you want to attend, please register by adding yourself to the\n\"people.txt\" file in Mercurial:\n\nhttps://bitbucket.org/pypy/extradoc/\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/london-2013\n\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\n\nRemember that you may need a (insert country here)-to-UK power adapter.\nPlease note that UK is not within the Schengen zone, so non-EU and\nnon-Switzerland citizens may require specific visa. Please check travel\nregulations. Also, the UK uses pound sterling (GBP).",
      "tags": "sprint",
      "url": "https://www.pypy.org/posts/2013/07/pypy-london-sprint-august-26-september-5156945690440578388.html"
    },
    {
      "title": "Software Transactional Memory lisp experiments",
      "text": "As covered in the previous blog post, the STM subproject of PyPy has been\nback on the drawing board. The result of this experiment is an STM-aware\ngarbage collector written in C. This is finished by now, thanks to Armin's\nand Remi's work, we have a fully functional garbage collector and a STM system\nthat can be used from any C program with enough effort. Using it is more than\na little mundane, since you have to inserts write and read barriers by hand\neverywhere in your code that reads or writes to garbage collector controlled\nmemory. In the PyPy integration, this manual work is done automatically\nby the STM transformation in the interpreter.\nHowever, to experiment some more, we created a minimal\nlisp-like/scheme-like interpreter\n(called Duhton), that follows closely CPython's implementation strategy.\nFor anyone familiar with CPython's source code, it should be pretty\nreadable. This interpreter works like a normal and very basic lisp variant,\nhowever it comes with a transaction builtin, that lets you spawn transactions\nusing the STM system. We implemented a few demos that let you play with the\ntransaction system. All the demos are running without conflicts, which means\nthere are no conflicting writes to global memory and hence the demos are very\namenable to parallelization. They exercise:\n\narithmetics - demo/many_sqare_roots.duh\nread-only access to globals - demo/trees.duh\nread-write access to local objects - demo/trees2.duh\n\nWith the latter ones being very similar to the classic gcbench. STM-aware\nDuhton can be found in the stmgc repo, while the STM-less Duhton,\nthat uses refcounting, can be found in the duhton repo under the base\nbranch.\nBelow are some benchmarks. Note that this is a little comparing apples to\noranges since the single-threaded duhton uses refcounting GC vs generational\nGC for STM version. Future pypy benchmarks will compare more apples to apples.\nMoreover none of the benchmarks has any conflicts. Time is the total time\nthat the benchmark took (not the CPU time) and there was very little variation\nin the consecutive runs (definitely below 5%).\n\n\n\n\n\n\n\n\n\nbenchmark\n1 thread (refcount)\n1 thread (stm)\n2 threads\n4 threads\n\nsquare\n1.9s\n3.5s\n1.8s\n0.9s\n\ntrees\n0.6s\n1.0s\n0.54s\n0.28s\n\ntrees2\n1.4s\n2.2s\n1.1s\n0.57s\n\n\n\nAs you can see, the slowdown for STM vs single thread is significant\n(1.8x, 1.7x, 1.6x respectively), but still lower than 2x. However the speedup\nfrom running on multiple threads parallelizes the problem almost perfectly.\nWhile a significant milestone, we hope the next blog post will cover\nSTM-enabled pypy that's fully working with JIT work ongoing.\nCheers,\nfijal on behalf of Remi Meier and Armin Rigo",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2013/07/software-transactional-memory-lisp-7777576128992250197.html"
    },
    {
      "title": "PyPy 2.1 beta",
      "text": "We're pleased to announce the first beta of the upcoming 2.1 release of PyPy.\u00a0This beta contains many bugfixes and improvements, numerous improvements to the\u00a0numpy in pypy effort. The main feature being that the ARM processor support is\u00a0not longer considered alpha level.\n\nWe would like to thank the Raspberry Pi\u00a0Foundation for supporting the work to finish PyPy's ARM support.\n\n\nYou can download the PyPy 2.1 beta release here:\n\nhttps://pypy.org/download.html\n\n\n\n\nHighlights\n\nBugfixes to the ARM JIT backend, so that ARM is now an officially\nsupported processor architecture\nStacklet support on ARM\nInterpreter improvements\nVarious numpy improvements\nBugfixes to cffi and ctypes\nBugfixes to the stacklet support\nImproved logging performance\nFaster sets for objects\n\n\n\n\n\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\u00a0CPython 2.7.3. It's fast due to its integrated tracing JIT compiler.\u00a0This release supports x86 machines running Linux 32/64, Mac OS X 64 or Windows\u00a032. Also this release supports ARM machines running Linux 32bit - anything with\u00a0ARMv6 (like the Raspberry Pi) or ARMv7 (like Beagleboard,\u00a0Chromebook, Cubieboard, etc.) that supports VFPv3 should work. Both\u00a0hard-float armhf/gnueabihf and soft-float\u00a0armel/gnueabi builds are\u00a0provided. armhf builds for Raspbian are created using the Raspberry Pi\ncustom cross-compilation toolchain\u00a0based on gcc-arm-linux-gnueabihf and should work on ARMv6\u00a0and\u00a0ARMv7 devices running Debian or Raspbian. armel builds are built\u00a0using the gcc-arm-linux-gnuebi toolchain provided by Ubuntu and\u00a0currently target ARMv7.\n\nWindows 64 work is still stalling, we would welcome a volunteer\u00a0to handle that.\n\n\n\n\nHow to use PyPy?\nWe suggest using PyPy from a virtualenv. Once you have a virtualenv\u00a0installed, you can follow instructions from pypy documentation on how\u00a0to proceed. This document also covers other installation schemes.\n\nCheers,\n\nthe PyPy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/pypy-21-beta-1351105697755187196.html"
    },
    {
      "title": "EuroPython",
      "text": "Hi all,\n\nA short note: if you're at EuroPython right now and wondering if PyPy is\ndead because you don't see the obviously expected talk about PyPy, don't\nworry.  PyPy is still alive and kicking.  The truth is two-fold: (1) we\nmissed the talk deadline (duh!)... but as importantly, (2) for various\nreasons we chose not to travel to Florence this year after our trip to\nPyCon US.  (Antonio Cuni is at Florence but doesn't have a talk about PyPy\neither.)\n\nArmin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/07/europython-8992114341185888806.html"
    },
    {
      "title": "Py3k status update #11",
      "text": "This is the 11th status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.Here's some highlights of the progress made since the previous update:PyPy py3k now matches CPython 3's hash code for\nint/float/complex/Decimal/Fraction\nVarious outstanding unicode identifier related issues were\nresolved. E.g. test_importlib/pep263/ucn/unicode all now fully pass. Various\nusage of identifiers (in particular type and module names) have been fixed to\nhandle non-ascii names -- mostly around display of reprs and exception\nmessages.\nThe unicodedata database has been upgraded to 6.0.0.\nWindows support has greatly improved, though it could still use some more\nhelp (but so does the default branch to a certain degree).\nProbably the last of the parsing related bugs/features have been taken care\nof.\nOf course various other smaller miscellaneous fixes\nThis leaves the branch w/ only about 5 outstanding failures of the stdlib test\nsuite:test_float1 failing test about containment of floats in collections.\ntest_memoryviewVarious failures: requires some bytes/str changes among other things (Manuel\nJacob's has some progress on this on the py3k-memoryview branch)\ntest_multiprocessing1 or more tests deadlock on some platforms\ntest_sys and test_threading2 failing tests for the New GIL's new API\nProbably the biggest feature left to tackle is the New GIL.We're now pretty close to pushing an initial release. We had planned for one\naround PyCon, but having missed that we've put some more effort into the branch\nto provide a more fully-fledged initial release.Thanks to the following for their contributions: Manuel Jacob, Amaury Forgeot\nd'Arc, Karl Ramm, Jason Chu and Christian Hudon.cheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2013/06/py3k-status-update-11-133025715908408072.html"
    },
    {
      "title": "STM on the drawing board",
      "text": "Hi all!\n\nThis is an update about the Software Transactional Memory subproject of\nPyPy.  I have some good news of progress.  Also,\nRemi Meier will\nlikely help me this summer.  He did various\ninvestigations with PyPy-STM for his Master's Thesis and contributed back\na lot of ideas and some code.  Welcome again Remi!\n\nI am also sorry that it seems to advance so slowly.  Beyond the usual\nexcuses --- I was busy with other things, e.g. releasing PyPy 2.0 --- I\nwould like to reassure people: I'm again working on it, and the financial\ncontributions are still there and reserved for STM (almost half the money is\nleft, a big thank you again if you contributed!).\n\nThe real reason for the apparent slowness, though, is that it is really\na research project.  It's possible to either have hard deadlines, or to\nfollow various tracks and keep improving the basics, but not both at the\nsame time.\n\nDuring the past month where I have worked again on STM, I worked still on\nthe second option; and I believe it was worth every second of it.  Let me try\nto convince you :-)\n\nThe main blocker was that the STM subsystem, written in C, and the\nGarbage Collection (GC) subsystem, written in RPython, were getting\nharder and harder to coordinate.  So what I did instead is to give up\nusing RPython in favor of using only C for both.  C is a good language\nfor some things, which includes low-level programming where we must take\ncare of delicate multithreading issues; RPython is not a good fit in\nthat case, and wasn't designed to be.\n\nI started a fresh Mercurial repo\nwhich is basically a stand-alone C library.  This library (in heavy development\nright now!) gives any C\nprogram some functions to allocate and track GC-managed objects, and\ngives an actual STM+GC combination on these objects.  It's possible\n(though rather verbose) to use it directly in C programs, like in a\nsmall example interpreter.  Of course the eventual purpose is to link it\nwith PyPy during translation to C, with all the verbose calls\nautomatically generated.\n\nSince I started this, bringing the GC closer to the STM, I kept finding\nnew ways that the two might interact to improve the performance, maybe\nradically.  Here is a summary of the current ideas.\n\nWhen we run\nmultiple threads, there are two common cases: one is to access (read and write)\nobjects that have only been seen by the current thread; the other is to read\nobjects seen by all threads, like in Python the modules/functions/classes,\nbut not to write to them.  Of course, writing to the same object from\nmultiple threads occurs too, and it is handled correctly (that's the whole\npoint), but it is a relatively rare case.\n\nSo each object is classified as \"public\" or \"protected\" (or \"private\",\nwhen they belong to the current transaction).  Newly created objects, once\nthey are no longer private, remain protected until\nthey are read by a different thread.  Now, the point is to use very\ndifferent mechanisms for public and for protected objects.  Public\nobjects are visible by all threads, but read-only in memory; to change\nthem, a copy must be made, and the changes are written to the copy (the\n\"redolog\" approach to STM).  Protected objects, on the other hand, are\nmodified in-place, with (if necessary) a copy of them being made\nfor the sole purpose of a possible abort of the transaction (the \"undolog\"\napproach).\n\nThis is combined with a generational GC similar to PyPy's --- but here,\neach thread gets its own nursery and does its own \"minor collections\",\nindependently of the others.\n\nSo objects are by default protected; when another thread tries to follow a\npointer to them, then it is that other thread's job to carefully \"steal\"\nthe object and turn it public (possibly making a copy of it if needed,\ne.g. if it was still a young object living in the original nursery).\n\nThe same object can exist temporarily in multiple versions: any number\nof public copies; at most one active protected copy; and optionally one\nprivate copy per thread (this is the copy as currently seen by the\ntransaction in progress on that thread).  The GC cleans up the\nunnecessary copies.\n\nThese ideas are variants and extensions of the same basic idea\nof keeping multiple copies with revision numbers to track them.\nMoreover, \"read barriers\" and \"write barriers\" are used by the C program\ncalling into this library in order to be sure that it is accessing the\nright version of the object.  In the currently investigated variant\nI believe it should be possible to have rather cheap\nread barriers, which would definitely be a major speed improvement over\nthe previous variants.  Actually, as far as I know, it would be a major\nimprovement over most of the other existing STMs: in them, the typical read barrier\ninvolves following chains of pointers, and checking some dictionary to see if this\nthread has a modified local copy of the object.  The difference with a\nread barrier that can resolve most cases in a few CPU cycles should be\nhuge.\n\nSo, this is research :-)  It is progressing, and at some point I'll be\nsatisfied with it and stop rewriting everything; and then the actual\nintegration into PyPy should be straightforward (there is already code\nto detect where the read and write barriers need to be inserted, where\ntransactions can be split, etc.).  Then there is support for the\nJIT to be written, and so on.  But more about it later.\n\nThe purpose of this post was to give you some glimpses into what I'm\nworking on right now.  As usual, no plan for release yet.  But you can\nlook forward to seeing the C library progress.  I'll probably also start\nsoon some sample interpreter in C, to test the waters (likely a\nrevival of duhton).\nIf you know nothing about Python but all about the C-level\nmultithreading issues, now is a good time to get involved :-)\n\nThanks for reading!\n\nArmin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2013/06/stm-on-drawing-board-1028082727566254104.html"
    },
    {
      "title": "NumPyPy status update",
      "text": "Hello everyone,\n\nMay was the first month I was paid to work on NumPyPy (thanks to all who donated!), here is what I worked on during this period :\n\n\nIt is now possible to use subarrays.\nIt is now possible to pickle ndarrays (including those using subarrays), dtypes and scalars, the pickling protocol is the same as numpy's.\n\n\n\n\nFor June, I plan to work on the nditer class, it seems that there's enough work for an entire month.\n\nCheers\nRomain Guillebert",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/06/numpypy-status-update-3846626188716521472.html"
    },
    {
      "title": "PyPy 2.0.2 - Fermi Panini",
      "text": "We're pleased to announce PyPy 2.0.2.  This is a stable bugfix release\nover 2.0 and 2.0.1.  You can download it here:\n\nhttps://pypy.org/download.html\nIt fixes a crash in the JIT when calling external C functions (with\nctypes/cffi) in a multithreaded context.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.0 and cpython 2.7.3 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32.  Support for ARM is progressing but not bug-free yet.\n\n\nHighlights\nThis release contains only the fix described above.  A crash (or wrong\nresults) used to occur if all these conditions were true:\n\nyour program is multithreaded;\nit runs on a single-core machine or a heavily-loaded multi-core one;\nit uses ctypes or cffi to issue external calls to C functions.\n\nThis was fixed in the branch emit-call-x86 (see the example file\nbug1.py).\nCheers,\narigo et. al. for the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/05/pypy-202-fermi-panini-1917947221142595738.html"
    },
    {
      "title": "PyPy 2.0.1 - Bohr Sm\u00f8rrebr\u00f8d",
      "text": "We're pleased to announce PyPy 2.0.1.  This is a stable bugfix release\nover 2.0.  You can download it here:\n\nhttps://pypy.org/download.html\nThe fixes are mainly about fatal errors or crashes in our stdlib.  See\nbelow for more details.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.0 and cpython 2.7.3 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32.  Support for ARM is progressing but not bug-free yet.\n\n\nHighlights\n\nfix an occasional crash in the JIT that ends in RPython Fatal error:\nNotImplementedError.\nid(x) is now always a positive number (except on int/float/long/complex).\nThis fixes an issue in _sqlite.py (mostly for 32-bit Linux).\nfix crashes of callback-from-C-functions (with cffi) when used together\nwith Stackless features, on asmgcc (i.e. Linux only).  Now gevent should\nwork better.\nwork around an eventlet issue with socket._decref_socketios().\n\nCheers,\narigo et. al. for the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/05/pypy-201-bohr-smrrebrd-6316445093061941482.html"
    },
    {
      "title": "Numpy Status Update",
      "text": "Hello Everyone,\n\nI've started to work on NumPyPy since the end of April and here is a short update :\n\n\nI implemented pickling support on ndarrays and dtypes, it will be compatible with numpy's pickling protocol when the \"numpypy\" module will be renamed to \"numpy\".\nI am now working on subarrays.\n\n\n\n\nI would also like to thank everyone who donated and allowed me to work on this.\n\n\n\nCheers,\n\nRomain Guillebert",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/05/numpy-status-update-4176018422530420763.html"
    },
    {
      "title": "PyPy 2.0 - Einstein Sandwich",
      "text": "We're pleased to announce PyPy 2.0. This is a stable release that brings\na swath of bugfixes, small performance improvements and compatibility fixes.\nPyPy 2.0 is a big step for us and we hope in the future we'll be able to\nprovide stable releases more often.\nYou can download the PyPy 2.0 release here:\n\nhttps://pypy.org/download.html\nThe two biggest changes since PyPy 1.9 are:\n\nstackless is now supported including greenlets, which means eventlet\nand gevent should work (but read below about gevent)\nPyPy now contains release 0.6 of cffi as a builtin module, which\nis preferred way of calling C from Python that works well on PyPy\n\nIf you're using PyPy for anything, it would help us immensely if you fill out\nthe following survey: https://bit.ly/pypysurvey This is for the developers\neyes and we will not make any information public without your agreement.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 2.0 and cpython 2.7.3 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32.  Windows 64 work is still stalling, we would welcome a volunteer\nto handle that. ARM support is on the way, as you can see from the recently\nreleased alpha for ARM.\n\n\nHighlights\n\nStackless including greenlets should work. For gevent, you need to check\nout pypycore and use the pypy-hacks branch of gevent.\ncffi is now a module included with PyPy.  (cffi also exists for\nCPython; the two versions should be fully compatible.)  It is the\npreferred way of calling C from Python that works on PyPy.\nCallbacks from C are now JITted, which means XML parsing is much faster.\nA lot of speed improvements in various language corners, most of them small,\nbut speeding up some particular corners a lot.\nThe JIT was refactored to emit machine code which manipulates a \"frame\"\nthat lives on the heap rather than on the stack.  This is what makes\nStackless work, and it could bring another future speed-up (not done yet).\nA lot of stability issues fixed.\nRefactoring much of the numpypy array classes, which resulted in removal of\nlazy expression evaluation. On the other hand, we now have more complete\ndtype support and support more array attributes.\n\nCheers,\nfijal, arigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/05/pypy-20-einstein-sandwich-635158782365435530.html"
    },
    {
      "title": "PyPy 2.0 alpha for ARM",
      "text": "Hello.\nWe're pleased to announce an alpha release of PyPy 2.0 for ARM. This is mostly\na technology preview, as we know the JIT is not yet stable enough for the\nfull release. However please try your stuff on ARM and report back.\nThis is the first release that supports a range of ARM devices - anything with\nARMv6 (like the Raspberry Pi) or ARMv7 (like Beagleboard, Chromebook,\nCubieboard, etc.) that supports VFPv3 should work. We provide builds with\nsupport for both ARM EABI variants: hard-float and some older operating\nsystems soft-float.\nThis release comes with a list of limitations, consider it alpha quality,\nnot suitable for production:\n\nstackless support is missing.\nassembler produced is not always correct, but we successfully managed to\nrun large parts of our extensive benchmark suite, so most stuff should work.\n\nYou can download the PyPy 2.0 alpha ARM release here (including a deb for raspbian):\n\nhttps://pypy.org/download.html\nPart of the work was sponsored by the Raspberry Pi foundation.\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3. It's fast due to its integrated tracing JIT compiler.\nThis release supports ARM machines running Linux 32bit. Both hard-float\narmhf and soft-float armel builds are provided.  armhf builds are\ncreated using the Raspberry Pi custom cross-compilation toolchain based on\ngcc-arm-linux-gnueabihf and should work on ARMv6 and ARMv7 devices running at\nleast debian or ubuntu. armel builds are built using gcc-arm-linux-gnuebi\ntoolchain provided by ubuntu and currently target ARMv7.  If there is interest\nin other builds, such as gnueabi for ARMv6 or without requiring a VFP let us\nknow in the comments or in IRC.\n\n\nBenchmarks\nEverybody loves benchmarks. Here is a table of our benchmark suite\n(for ARM we don't provide it yet on https://speed.pypy.org,\nunfortunately).\nThis is a comparison of Cortex A9 processor with 4M cache and Xeon W3580 with\n8M of L3 cache. The set of benchmarks is a subset of what we run for\nhttps://speed.pypy.org that finishes in reasonable time. The ARM machine\nwas provided by Calxeda.\nColumns are respectively:\n\nbenchmark name\nPyPy speedup over CPython on ARM (Cortex A9)\nPyPy speedup over CPython on x86 (Xeon)\nspeedup on Xeon vs Cortex A9, as measured on CPython\nspeedup on Xeon vs Cortex A9, as measured on PyPy\nrelative speedup (how much bigger the x86 speedup is over ARM speedup)\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\nPyPy vs CPython (arm)\nPyPy vs CPython (x86)\nx86 vs arm (pypy)\nx86 vs arm (cpython)\nrelative speedup\n\nai\n3.61\n3.16\n7.70\n8.82\n0.87\n\nbm_mako\n3.41\n2.11\n8.56\n13.82\n0.62\n\nchaos\n21.82\n17.80\n6.93\n8.50\n0.82\n\ncrypto_pyaes\n22.53\n19.48\n6.53\n7.56\n0.86\n\ndjango\n13.43\n11.16\n7.90\n9.51\n0.83\n\neparse\n1.43\n1.17\n6.61\n8.12\n0.81\n\nfannkuch\n6.22\n5.36\n6.18\n7.16\n0.86\n\nfloat\n5.22\n6.00\n9.68\n8.43\n1.15\n\ngo\n4.72\n3.34\n5.91\n8.37\n0.71\n\nhexiom2\n8.70\n7.00\n7.69\n9.56\n0.80\n\nhtml5lib\n2.35\n2.13\n6.59\n7.26\n0.91\n\njson_bench\n1.12\n0.93\n7.19\n8.68\n0.83\n\nmeteor-contest\n2.13\n1.68\n5.95\n7.54\n0.79\n\nnbody_modified\n8.19\n7.78\n6.08\n6.40\n0.95\n\npidigits\n1.27\n0.95\n14.67\n19.66\n0.75\n\npyflate-fast\n3.30\n3.57\n10.64\n9.84\n1.08\n\nraytrace-simple\n46.41\n29.00\n5.14\n8.23\n0.62\n\nrichards\n31.48\n28.51\n6.95\n7.68\n0.91\n\nslowspitfire\n1.28\n1.14\n5.91\n6.61\n0.89\n\nspambayes\n1.93\n1.27\n4.15\n6.30\n0.66\n\nsphinx\n1.01\n1.05\n7.76\n7.45\n1.04\n\nspitfire\n1.55\n1.58\n5.62\n5.49\n1.02\n\nspitfire_cstringio\n9.61\n5.74\n5.43\n9.09\n0.60\n\nsympy_expand\n1.42\n0.97\n3.86\n5.66\n0.68\n\nsympy_integrate\n1.60\n0.95\n4.24\n7.12\n0.60\n\nsympy_str\n0.72\n0.48\n3.68\n5.56\n0.66\n\nsympy_sum\n1.99\n1.19\n3.83\n6.38\n0.60\n\ntelco\n14.28\n9.36\n3.94\n6.02\n0.66\n\ntwisted_iteration\n11.60\n7.33\n6.04\n9.55\n0.63\n\ntwisted_names\n3.68\n2.83\n5.01\n6.50\n0.77\n\ntwisted_pb\n4.94\n3.02\n5.10\n8.34\n0.61\n\n\n\nIt seems that Cortex A9, while significantly slower than Xeon, has higher\nslowdowns with a large interpreter (CPython) than a JIT compiler (PyPy). This\ncomes as a surprise to me, especially that our ARM assembler is not nearly\nas polished as our x86 assembler. As for the causes, various people mentioned\nbranch predictor, but I would not like to speculate without actually knowing.\n\n\nHow to use PyPy?\nWe suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.\nWe would not recommend using in production PyPy on ARM just quite yet,\nhowever the day of a stable PyPy ARM release is not far off.\nCheers,\nfijal, bivab, arigo and the whole PyPy team",
      "tags": "arm,sponsors",
      "url": "https://www.pypy.org/posts/2013/05/pypy-20-alpha-for-arm-2318299473927531503.html"
    },
    {
      "title": "PyPy 2.0 beta 2 released",
      "text": "We're pleased to announce the 2.0 beta 2 release of PyPy. This is a major\nrelease of PyPy and we're getting very close to 2.0 final, however it includes\nquite a few new features that require further testing. Please test and report\nissues, so we can have a rock-solid 2.0 final. It also includes a performance\nregression of about 5% compared to 2.0 beta 1 that we hope to fix before\n2.0 final. The ARM support is not working yet and we're working hard to\nmake it happen before the 2.0 final. The new major features are:\n\nJIT now supports stackless features, that is greenlets and stacklets. This\nmeans that JIT can now optimize the code that switches the context. It enables\nrunning eventlet and gevent on PyPy (although gevent requires some\nspecial support that's not quite finished, read below).\nThis is the first PyPy release that includes cffi as a core library.\nVersion 0.6 comes included in the PyPy library. cffi has seen a lot of\nadoption among library authors and we believe it's the best way to wrap\nC libaries. You can see examples of cffi usage in _curses.py and\n_sqlite3.py in the PyPy source code.\n\nYou can download the PyPy 2.0 beta 2 release here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3. It's fast (pypy 2.0 beta 2 and cpython 2.7.3\nperformance comparison) due to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32. It also supports ARM machines running Linux, however this is\ndisabled for the beta 2 release.\nWindows 64 work is still stalling, we would welcome a volunteer\nto handle that.\n\n\nHow to use PyPy?\nWe suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.\n\n\nHighlights\n\ncffi is officially supported by PyPy. It comes included in the standard\nlibrary, just use import cffi\nstackless support - eventlet just works and gevent requires pypycore\nand pypy-hacks branch of gevent (which mostly disables cython-based\nmodules)\ncallbacks from C are now much faster. pyexpat is about 3x faster, cffi\ncallbacks around the same\n__length_hint__ is implemented (PEP 424)\na lot of numpy improvements\n\n\n\nImprovements since 1.9\n\nJIT hooks are now a powerful tool to introspect the JITting process that\nPyPy performs\nvarious performance improvements compared to 1.9 and 2.0 beta 1\noperations on long objects are now as fast as in CPython (from\nroughly 2x slower)\nwe now have special strategies for dict/set/list which contain\nunicode strings, which means that now such collections will be both faster\nand more compact.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2013/04/pypy-20-beta-2-released-4858660312787995512.html"
    },
    {
      "title": "So, you want to try PyPy",
      "text": "Hello.\nDuring the PyCon trip multiple people asked me how exactly they could run\ntheir stuff on PyPy to get the speedups. Now, in an ideal world,\nyou would just swap CPython with PyPy, everything would run tons of times\nfaster and everyone would live happily ever after. However, we don't live in\nan ideal world and PyPy does not speed up everything you could\npotentially run. Chances are that you can run your stuff quite a bit faster, but\nit requires quite a bit more R&D than just that. This blog post is an attempt to\nexplain certain steps that might help. So here we go:\n\nDownload and install PyPy. 2.0 beta 1 or upcoming 2.0 beta 2 would be a good\ncandidate; it's not called a beta for stability reasons.\nRun your tests on PyPy. There is absolutely no need for fast software that\ndoes not work. There might be some failures. Usually they're harmless (e.g.\nyou forgot to close the file); either fix them or at least inspect them. In\nshort, make sure stuff works.\nInspect your stack. In particular, C extensions, while sometimes working, are\na potential source of instability and slowness. Fortunately,\nsince the introduction of cffi, the ecosystem of PyPy-compatible software\nhas been growing. Things I know are written with PyPy in mind:\nthe new version of pyOpenSSL will support PyPy via cffi\npsycopg2cffi is the most actively maintained postgres binding for PyPy,\nwith pg8000 reported working\nmysql has a ctypes based implementation (although a cffi-based one would\nbe definitely better)\nPyPy 2.0 beta 2 will come with sqlite-using-cffi\nlxml-cffi\nuWSGI, while working, is almost certainly not the best choice. Try\ntornado, twisted.web, cyclone.io, gunicorn or gevent\n(note: gevent support for PyPy is not quite finished; will write about it\nin a separate blog post, but you can't just use the main branch of gevent)\nconsult (and contribute to) pypy compatibility wiki for details (note\nthat it's community maintained, might be out of date)\n\n\n\n\nHave benchmarks. If you don't have benchmarks, then performance does not\nmatter for you. Since PyPy's warm-up time is bad (and yes, we know, we're\nworking on it), you should leave ample time for warm-ups. Five to ten seconds\nof continuous computation should be enough.\nTry them. If you get lucky, the next step might be to deploy and be happy.\nIf you're unlucky, profile and try to isolate bottlenecks. They might be in\na specific library or they might be in your code. The better you can isolate\nthem, the higher your chances of understanding what's going on.\nDon't take it for granted. PyPy's JIT is very good, but there is a variety\nof reasons that it might not work how you expect it to. A lot of times it\nstarts off slow, but a little optimization can improve the speed as much as\n10x. Since PyPy's runtime is less mature than CPython, there are higher\nchances of finding an obscure corner of the standard library that might be\natrociously slow.\nMost importantly, if you run out of options and you have a reproducible\nexample, please report it. A pypy-dev email, popping into #pypy\non irc.freenode.net, or getting hold of me on twitter are good ways.\nYou can also contact me directly at fijall at gmail.com as well. While\nit's cool if the example is slow, a lot of problems only show up on large\nand convoluted examples. As long as I can reproduce it on my machine or I can\nlog in somewhere, I am usually happy to help.\nI typically use a combination of jitviewer, valgrind and\nlsprofcalltree to try to guess what's going on. These tools are all\nuseful, but use them with care. They usually require quite a bit of\nunderstanding before being useful. Also sometimes they're just plain useless\nand you need to write your own analysis.\n\nI hope this summary of steps to take is useful. We hear a lot of stories\nof people trying PyPy, most of them positive, but some of them negative.\nIf you just post \"PyPy didn't work for me\" on your blog, that's\ncool too, but you're missing an opportunity. The reasons may vary from\nsomething serious like \"this is a bad pattern for PyPy GC\" to something\ncompletely hilarious like \"oh, I left this sys._getframe() somewhere\nin my hot loops for debugging\" or \"I used the logging module which uses\nsys._getframe() all over  the place\".\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/03/so-you-want-to-try-pypy-4702482800824669595.html"
    },
    {
      "title": "Numpy status update and developer announcement",
      "text": "Hello, some good news!\nFirst the update:\n\ndtype support - NumPy on PyPy now supports non-native storage formats.\nDue to a lack of true support for longdoubles in rpython, we decided to back\nout the support of longdouble-as-double which was misleading.\nmissing ndarray attributes - work has been made toward supporting the\ncomplete set of attributes\non ndarrays. We are progressing alphabetically, and have made it to d.\nUnsupported attributes, and unsupported arguments to attribute calls\nwill raise a NotImplementedError.\npickling support for numarray - hasn't started yet, but next on the list\nThere has been some work on exposing FFI routines in numpypy.\nBrian Kearns has made progress in improving the numpypy namespace.\nThe python numpypy submodules now more closely resemble their numpy\ncounterparts. Also, translated _numpypy submodules are now more properly\nmapped to the numpy core c-based submodules, furthering the goal of being\nable to install numpy as a pure-python module with few modifications.\n\nAnd now the good news:\nWhile our funding drive over 2012 did not reach our goal, we still managed to\nraise a fair amount of money in donations. So far we only managed to spend around $10 000 of it.\nWe issued a call for additional developers, and are glad to welcome Romain Guillebert and Ronan Lamy\nto the numpypy team. Hopefully we will be able to report on speedier progress soon.\nCheers,\nMatti Picus, Maciej Fijalkowski",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/03/numpy-status-update-and-developer-1503421654591696377.html"
    },
    {
      "title": "Py3k status update #10",
      "text": "This is the tenth status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.There's been significant progress since the last update: the linux x86-32\nbuildbot now passes 289 out of approximately 354 modules (with 39 skips) of\nCPython's regression test suite.That means there's only 26 test module failures left! The list of major items\nremaining for 3.2 compatibility are now short enough to list here, with their\nrelated tests:Tokenizer support for non-ascii identifiers\ntest_importlib\ntest_pep263\nmemoryview (Manuel Jacob's tackling this on the py3k-memoryview branch)\ntest_memoryview\nmultiprocessing module currently deadlocks\ntest_multiprocessing\nBuggy handling of the new extended unpacking syntax by the compiler:\ntest_unpack_ex\nThe new Global Interpreter Lock and new thread signal handling\ntest_threading\ntest_threadsignals\ntest_sys\nUpgrade unicodedata to 6.0.0 (requires updates to the actual unicodedata\ngeneration script)\ntest_ucn\ntest_unicode\ntest_unicodedata\nCPyExt\ntest_capi (currently crashes)\nUpdate int's hash code to match to CPython (float's is already updated on the\npy3k-newhash branch. note that PyPy 2.x doesn't even totally match\nCPython's hashing)\ntest_decimal\ntest_fractions\ntest_numeric_tower\nMiscellaneous:\ntest_complex\ntest_float\ntest_peepholer\ntest_range\ntest_sqlite (a new cffi based version seems to be coming)\ntest_ssl\ntest_struct\ntest_subprocess\ntest_sys_settrace\ntest_time\nAdditionally there are still a number of failures in PyPy's internal test\nsuite. These tests are usually ran against untranslated versions of PyPy during\ndevelopment. However we've now began running them against a fully translated\nversion of PyPy on the buildbot too (thanks to Amaury for setting this\nup). This further ensures that our tests and implementation are sane.We're getting closer to producing an initial alpha release. Before that happens\nwe'd like to see:further test fixes\nthe results of test runs on other major platforms (e.g. linux x86-64 and osx\nseem to have some additional failures as of now)\nsome basic real world testing\nFinally I'd like to thank Manuel Jacob for his various contributions over the\npast month, including fixing the array and ctypes modules among other things,\nand also Amaury Forgeot d'Arc for his ongoing excellent contributions.cheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2013/03/py3k-status-update-10-6681398990092286007.html"
    },
    {
      "title": "10 years of PyPy",
      "text": "From a software engineering perspective, 10 years is indistinguishable\nfrom infinity, so I don't care what happens 10 years from now -- as\nlong as you don't blame me. :-) - Guido van Rossum, Python creator.\n10 years is indeed a long time. PyPy was created approximately 10 years ago,\nwith the exact date being lost in the annals of the version control system.\nWe've come a long way during those 10 years, from a \"minimal Python\" that\nwas supposed to serve mostly as an educational tool, through to a vehicle for\nacademic research to a high performance VM for Python and beyond.\nSome facts from the PyPy timeline:\n\nIn 2007, at the end of the EU funding period, we promised the JIT was just around the corner.\nIt turned out we misjudged it pretty badly -- the first usable PyPy was released in 2010.\nAt some point we decided to have a JavaScript backend so one could compile RPython programs\nto JavaScript and run them in a browser. Turned out it was a horrible idea.\nAnother option we tried was using RPython to write CPython C extensions. Again, it turned out RPython\nis a bad language and instead we made a fast JIT, so you don't have to write C extensions.\nWe made N attempts to use LLVM.  Seriously, N is 4 or 5.  But we haven't fully given up yet :-)\nThey all run into issues one way or another.\nWe were huge fans of ctypes at the beginning. Up to the point where we tried to make\na restricted subset with static types, called rctypes for RPython. Turned out to be horrible.\nTwice.\nWe were very hopeful about creating a JIT generator from the beginning. But the first one failed miserably,\ngenerating too much assembler. The second failed too. The third first burned down and then failed.\nHowever, we managed to release a working JIT in 2010, against all odds.\nMartijn Faassen used to ask us \"how fast is PyPy\" so we decided to name an option enabling all\noptimizations \"--faassen\".  Then \"--no-faassen\" was naturally added too. Later we\ndecided to grow up and renamed it to \"-O2\", and now \"-Ojit\".\nThe first time the Python interpreter successfully compiled to C, it segfaulted because the code generator used signed chars instead of unsigned chars...\nTo make it more likely to be accepted, the proposal for the EU project contained basically every feature under the sun a language could have. This proved to be annoying, because we had to actually implement all that stuff. Then we had to do a cleanup sprint where we deleted 30% of codebase and 70% of features.\nAt one sprint someone proposed a new software development methodology: 'Terminology-Driven Programming' means to pick a fancy name, then discuss what it could mean, then implement it. Examples: timeshifter, rainbow interpreter, meta-space bubble, hint annotations (all but one of these really existed).\nThere is a conspiracy theory that the reason why translation is so slow is because time is stored away during it, which is later retrieved when an actual program runs to make them appear faster\n\nOverall, it was a really long road.  However, 10 years later we are in\ngood shape.  A quick look on the immediate future: we are approaching\nPyPy 2.0 with stackless+JIT and cffi support,\nthe support for Python 3 is taking shape, non-standard\nextensions like STM are slowly getting ready (more soon), and there are\nseveral non-Python interpreters around the corner (Hippy, Topaz and more).\nCheers,\nfijal, arigo, hodgestar, cfbolz and the entire pypy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/10-years-of-pypy-634401291726575821.html"
    },
    {
      "title": "cppyy status update",
      "text": "The cppyy module\nprovides C++ bindings for PyPy by using the reflection information extracted\nfrom C++ header files by means of the\nReflex package.\nIn order to support C++11, the goal is to move away from Reflex and instead use\ncling, an interactive\nC++ interpreter, as the backend.\nCling is based on llvm's\nclang.\n\nThe use of a real compiler under the hood has the advantage that it is now\npossible to cover every conceivable corner case.\nThe disadvantage, however, is that every corner case actually has to be\ncovered.\nLife is somewhat easier when calls come in from the python interpreter, as\nthose calls have already been vetted for syntax errors and all lookups are\nwell scoped.\nFurthermore, the real hard work of getting sane responses from and for C++\nin an interactive environment is done in cling, not in the bindings.\nNevertheless, it is proving a long road (but for that matter clang does not\nsupport all of C++11 yet), so here's a quick status update showing that good \nprogress is being made.\n\nThe following example is on CPython, not PyPy, but moving a third\n(after Reflex and\nCINT) backend into place\nunderneath cppyy is straightforward compared to developing the backend\nin the first place.\n\nTake this snippet of C++11 code\n(cpp11.C):\n\n    constexpr int data_size() { return 5; }\n\n    auto N = data_size();\n\n    template<class L, class R>\n    struct MyMath {\n       static auto add(L l, R r) -> decltype(l+r) { return l + r; }\n    };\n\n    template class MyMath<int, int>;\n\nAs a practical matter, most usage of new C++11 features will live in\nimplementations, not in declarations, and are thus never seen by the bindings.\nThe above example is therefore somewhat contrived, but it will serve to show\nthat these new declarations actually work.\nThe new features used here are\nconstexpr,\nauto, and\ndecltype.\nHere is how you could use these from CPython, using the\nPyROOT\npackage, which has more than a passing resemblance to cppyy, as one is based\non the other:\n\n    import ROOT as gbl\n    gbl.gROOT.LoadMacro('cpp11.C')\n\n    print 'N =', gbl.N\n    print '1+1 =', gbl.MyMath(int, int).add(1,1)\n\nwhich, when entered into a file\n(cpp11.py) and executed,\nprints the expected results:\n\n    $ python cpp11.py\n    N = 5\n    1+1 = 2\n\nIn the example, the C++ code is compiled on-the-fly, rather than first generating\na dictionary as is needed with Reflex.\nA deployment model that utilizes stored pre-compiled information is foreseen\nto work with larger projects, which may have to pull in headers from many places.\n\nWork is going to continue first on C++03 on cling with CPython (about 85% of\nunit tests currently pass), with a bit of work on C++11 support on the side.\nOnce fully in place, it can be brought into a new backend for cppyy, after \nwhich the remaining parts of C++11 can be fleshed out for both interpreters.\n\nCheers,\nWim Lavrijsen",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/cppyy-status-update-808802896237239604.html"
    },
    {
      "title": "PyCon Silicon Valley and San Francisco visit",
      "text": "Hello everyone.\nWe (Armin Rigo and Maciej Fijalkowski) are visiting San Francisco/Silicon Valley\nfor PyCon and beyond. Alex Gaynor, another core PyPy dev is living there\npermanently. My visiting dates are 12-28 of March, Armin's 11-21st.\nIf you want us to give a  talk at your company or simply catch up with us\nfor a dinner\nplease get in touch. Write to pypy-dev@python.org, if you want this publically\nknown or simply send me a mail at fijall@gmail.com if you don't want it public.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/hello-everyone-4718797989680066222.html"
    },
    {
      "title": "Announcing Topaz, an RPython powered Ruby interpreter",
      "text": "Hello everyone\n\nLast week, Alex Gaynor announced the first public release of\nTopaz,\na Ruby interpreter written in RPython. This is the culmination of a\npart-time effort over the past 10 months to provide a Ruby interpreter\nthat implements enough interesting constructs in Ruby to show that the\nRPython toolchain can produce a Ruby implementation fast enough to\nbeat what is out there.\n\nDisclaimer\n\nObviously the implementation is very incomplete currently in terms of\navailable standard library. We are working on getting it useable. If\nyou want to try it, grab a\nnightly build.\n\nWe have run some benchmarks from the\nRuby benchmark suite\nand the\nmetatracing VMs experiment. The\npreliminary results are promising, but at this point we are missing so\nmany method implementations that most benchmarks won't run yet. So instead of\nperformance, I'm going to talk about the high-level structure of the\nimplementation.\n\nArchitecture\n\nTopaz interprets a custom bytecode set. The basics are similar to\nSmalltalk VMs, with bytecodes for loading and storing locals and\ninstance variables, sending messages, and stack management. Some\nsyntactical features of Ruby, such as defining classes and modules,\nliteral regular expressions, hashes, ranges, etc also have their own\nbytecodes. The third kind of bytecodes are for control flow constructs\nin Ruby, such as loops, exception handling, break, continue, etc.\n\nIn trying to get from Ruby source code to bytecode, we found that the\neasiest way to support all of the Ruby syntax is to write a custom\nlexer and use an RPython port of PLY\n(fittingly called RPly) to create the\nparser from the Ruby yacc grammar.\n\nThe Topaz interpreter uses an ObjectSpace (similar to how PyPy does\nit), to interact with the Ruby world. The object space contains all\nthe logic for wrapping and interacting with Ruby objects from the\nVM. It's __init__ method sets up the core classes, initial globals,\nand creates the main thread (the only one right now, as we do not have\nthreading, yet).\n\nClasses are mostly written in Python. We use ClassDef objects to\ndefine the Ruby hierarchy and attach RPython methods to Ruby via\nClassDef decorators. These two points warrant a little explanation.\n\nHierarchies\n\nAll Ruby classes ultimately inherit from BasicObject. However, most\nobjects are below Object (which is a direct subclass of\nBasicObject). This includes objects of type Fixnum, Float,\nClass, and Module, which may not need all of the facilities of\nfull objects most of the time.\n\nMost VMs treat such objects specially, using tagged pointers to\nrepresent Fixnums, for example. Other VMs (for example from the\nSOM Family)\ndon't. In the latter case, the implementation hierarchy matches the\nlanguage hierarchy, which means that objects like Fixnum share a\nrepresentation with all other objects (e.g. they have class pointers\nand some kind of instance variable storage).\n\nIn Topaz, implementation hierarchy and language hierarchy are\nseparate. The first is defined through the Python inheritance. The\nother is defined through the ClassDef for each Python class, where the\nappropriate Ruby superclass is chosen. The diagram below shows how the\nimplementation class W_FixnumObject inherits directly from\nW_RootObject.  Note that W_RootObject doesn't have any attrs,\nspecifically no storage for instance variables and no map (for\ndetermining the class - we'll get to that). These attributes are\ninstead defined on W_Object, which is what most other implementation\nclasses inherit from. However, on the Ruby side, Fixnum correctly\ninherits (via Numeric and Integer) from Object.\n\n\n\n\nThis simple structural optimization gives a huge speed boost, but\nthere are VMs out there that do not have it and suffer performance\nhits for it.\n\nDecorators\n\nRuby methods can have symbols in its names that are not allowed as\npart of Python method names, for example !, ?, or =, so we\ncannot simply define Python methods and expose them to Ruby by the\nsame name. \n\nFor defining the Ruby method name of a function, as well as argument\nnumber checking, Ruby type coercion and unwrapping of Ruby objects to\ntheir Python equivalents, we use decorators defined on ClassDef. When\nthe ObjectSpace initializes, it builds all Ruby classes from their\nrespective ClassDef objects. For each method in an implementation\nclass that has a ClassDef decorator, a wrapper method is generated and\nexposed to Ruby. These wrappers define the name of the Ruby method,\ncoerce Ruby arguments, and unwrap them for the Python method.\n\nHere is a simple example:\n\n@classdef.method(\"*\", times=\"int\")\ndef method_times(self, space, times):\n    return self.strategy.mul(space, self.str_storage, times)\n\n\nThis defines the method * on the Ruby String class. When this is\ncalled, the first argument is converted into a Ruby Fixnum object\nusing the appropriate coercion method, and then unwrapped into a plain\nPython int and passed as argument to method_times. The wrapper\nmethod also supplies the space argument.\n\nObject Structure\n\nRuby objects have dynamically defined instance variables and may\nchange their class at any time in the program (a concept called\nsingleton class\nin Ruby - it allows each object to have unique behaviour). To still\nefficiently access instance variables, you want to avoid dictionary\nlookups and let the JIT know about objects of the same class that have\nthe same instance variables. Topaz, like PyPy (which got it from\nSelf), implements instances using maps, which transforms dictionary\nlookups into array accesses. See the\nblog post\nfor the details.\n\nThis is only a rough overview of the architecture. If you're\ninterested, get in touch on\n#topaz.freenode.net, follow the\nTopaz Twitter account or contribute\non GitHub.\n\nTim Felgentreff",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/announcing-topaz-rpython-powered-ruby-6662407703061538341.html"
    },
    {
      "title": "CFFI 0.5",
      "text": "Hi all,\n\nA short notice to tell you that CFFI 0.5 was released.  This\ncontains a number of small improvements from 0.4, but seems to otherwise\nbe quite stable since a couple of months --- no change since January 10,\napart from the usual last-minute fixes for Python 3 and for Windows.\n\nHave fun!\n\nArmin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2013/02/cffi-05-1630643916751622710.html"
    },
    {
      "title": "NumPyPy 2013 Developer Position",
      "text": "Introduction\nProposed herein is a part-time fellowship for developing NumPy in PyPy.\nThe work will initially consist of 100 hours\nwith the possibility of extension, until the funds run out.\nDevelopment and improvement of PyPy's NumPyPy (as\nwith most Open Source and Free Software) is done as a collaborative process\nbetween volunteer, paid, and academic contributors. Due to a successful funding\ndrive but a lack of contributors willing to work directly for PyPy, we find\nourselves in the enviable situation of being able to offer this position.\n\n\nBackground\nPyPy's developers make all PyPy software available to the public\nwithout charge, under PyPy's Open Source copyright license, the\npermissive MIT License. PyPy's license assures that PyPy is equally\navailable to everyone freely on terms that allow both non-commercial\nand commercial activity. This license allows for academics, for-profit\nsoftware developers, volunteers and enthusiasts alike to collaborate\ntogether to make a better Python implementation for everyone.\nNumPy support for PyPy is licensed similarly, and therefore NumPy in\nPyPy support can directly help researchers and developers who seek to\ndo numeric computing but want an easier programming language to use\nthan Fortan or C, which is typically used for these\napplications. Being licensed freely to the general public means that\nopportunities to use, improve and learn about how NumPy in PyPy works\nitself will be generally available to everyone.\n\n\nThe Need for a Part-Time Developer\nNumPy project in PyPy has seen some slow, but steady progress since we started\nworking about a year ago.  On one hand,\nit's actually impressive what we could deliver with the effort undertaken,\non the other hand, we would like to see the development accelerated.\nPyPy has strict coding, testing, documentation, and review standards,\nwhich ensures excellent code quality, continually improving\ndocumentation and code test coverage, and minimal regressions. A\npart-time developer will be able to bring us closer to the goal of\nfull numpy-api implementation and speed improvements.\n\n\nWork Plan\nThe current proposal is split into two parts:\n\nCompatibility:\nThis part covers the core NumPy Python API. We'll implement most NumPy APIs\nthat are officially documented and we'll pass most of NumPy's tests that\ncover documented APIs and are not implementation details.\nSpecifically, we don't plan to:\n\nimplement NumPy's C API\nimplement other scientific libraries, like SciPy, matplotlib or biopython\nimplement details that are otherwise agreed by consensus to not have a place\nin PyPy's implementation of NumPy or agreed with NumPy community\nto be implementation details\n\n\nSpeed:\nThis part will cover significant speed improvements in the JIT that would\nmake numeric computations faster. This includes, but is not necesarilly\nlimited to:\n\nwrite a set of benchmarks covering various use cases\nteaching the JIT backend (or multiple backends) how to deal with vector\noperations, like SSE\nexperiments with automatic parallelization using multiple threads, akin\nto numexpr\nimproving the JIT register allocator that will make a difference, especially\nfor tight loops\n\nAs with all speed improvements, it's relatively hard to predict exactly\nhow it'll cope, however we expect the results to be withing an order\nof magnitude of handwritten C equivalent.\n\n\n\n\nPosition Candidate\nWe would like people who are proficient in NumPy and PyPy (but don't have to be\ncore developers of either) to step up. The developer selection will be done\nby consensus of PyPy core developers and consulted with the Software Freedom\nConservancy for lack of conflict of interest. The main criterium will be\npast contributions to the PyPy project, but they don't have to be significant\nin size.\nA candidate for the Developer position will demonstrate the following:\n\nThe ability to write clear, stable, suitable and tested code\nThe ability to understand and extend the JIT capabilities used in NumPyPy.\nA positive presence in PyPy's online community on IRC and the mailing\nlist.\n\nIdeally the Developer will also:\n\nHave familiarity with the infrastructure of the PyPy project (including\nbug tracker and buildbot).\nHave Worked to provide education or outreach on PyPy in other forums such as\nworkshops, conferences, and user groups.\n\nConservancy and PyPy are excited to announce the Developer Position.\nRenumeration for the position will be at the rate of 60 USD per hour, through\nthe Software Freedom Conservancy.\nPyPy community is promising to provide necessary guidance and help into\nthe current codebase, however we expect a successful candidate to be able\nto review code and incorporate external patches within two months of the\nstarting date of the contract.\nCandidates should submit their proposal (including their CV) to:\npypy-z@python.org\nThe deadline for this initial round of proposals is February 1, 2013.",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2013/01/numpypy-2013-developer-position-1547805593757893630.html"
    },
    {
      "title": "Py3k status update #9",
      "text": "This is the ninth status update about our work on the py3k branch, which\nwe can work on thanks to all of the people who donated to the py3k\nproposal.Just a very short update on December's work: we're now passing about 223 of\napproximately 355 modules of CPython's regression test suite, up from passing\n194 last month.Some brief highlights:More encoding related issues were addressed. e.g. now most if not all the\nmultibytecodec test modules pass.\nFixed some path handling issues (test_os, test_ntpath and\ntest_posixpath now pass)\nWe now pass test_class, test_descr and almost test_builtin (among\nother things): these are notable as they are fairly extensive test suites of\ncore aspects of the langauge.\nAmaury Forgeot d'Arc continued making progress on CPyExt (thanks again!)\ncheers,\nPhil",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2013/01/py3k-status-update-9-98332471264591773.html"
    },
    {
      "title": "PyPy related internship at NCAR",
      "text": "Hello everyone\nI would like to advertise a PyPy-related summer internship at\nthe National Center for Atmospheric Research, which is located in lovely\nBoulder, Colorado. As for the last year, the mentor will be Davide del Vento,\nwith my possible support on the PyPy side.\nThe full details of the application are to be found on\nthe internship description and make sure you read the requirements\nfirst. Important requirements:\n\nMust currently be enrolled in a United States university.\nOnly students authorized to work for any employer in the United\nStates will be considered for the SIParCS program.\nMust be a graduate or under graduate who has completed their sophomore year.\n\nIf you happen to fulfill the requirements, to me this sounds like\na great opportunity to spend a summer at NCAR in Boulder hacking on atmospheric\nmodels using PyPy.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/12/pypy-related-internship-at-ncar-7412729710421119926.html"
    },
    {
      "title": "Py3k status update #8",
      "text": "This is the eight status update about our work on the py3k branch, which\nwe can work on thanks to all of the people who donated to the py3k\nproposal.Just a short update on November's work: we're now passing about 194 of\napproximately 355 modules of CPython's regression test suite, up from passing\n160 last month. Many test modules only fail a small number of individual tests\nnow.We'd like to thank Amaury Forgeot d'Arc for his contributions, in particular he\nhas made significant progress on updating CPyExt for Python 3 this month.Some other highlights:test_marshal now passes, and there's been significant progress on\npickling (thanks Kenny Levinsen and Amaury for implementing\nint.{to,from}_bytes)\nWe now have a _posixsubprocess module\nMore encoding related fixes, which affects many failing tests\n_sre was updated and now test_re almost passes\nException behavior is almost complete per the Python 3 specs, what's mostly\nmissing now are the new __context__ and __traceback__ attributes (PEP\n3134)\nFixed some crashes and deadlocks occurring during the regression tests\nWe merged the unicode-strategies branch both to default and to py3k: now we\nhave versions of lists, dictionaries and sets specialized for unicode\nelements, as we already had for strings.\nHowever, for string-specialized containers are still faster in some cases\nbecause there are shortcuts which have not been implemented for unicode yet\n(e.g., constructing a set of strings from a list of strings). The plan is to\ncompletely kill the shortcuts and improve the JIT to produce the fast\nversion automatically for both the string and unicode versions, to have a\nmore maintainable codebase without sacrificing the speed. The autoreds\nbranch (already merged) was a first step in this direction.\ncheers,\nPhilip&Antonio",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/12/py3k-status-update-8-3932232806458251730.html"
    },
    {
      "title": "PyPy San Francisco Sprint Dec 1st - Dec 2nd 2012",
      "text": "The next PyPy sprint will be in San Francisco, California. It is a\npublic sprint, suitable for newcomers. It will run on Saturday December 1st and\nSunday December 2nd. The goals for the sprint are continued work towards the\n2.0 release as well as code cleanup, we of course welcome any topic which\ncontributors are interested in working on.Some other possible topics are:running your software on PyPy\nwork on PyPy's numpy (status)\nwork on STM (status)\nJIT improvements\nany exciting stuff you can think of\nIf there are newcomers, we'll run the usual introduction to hacking on\nPyPy.\nLocationThe sprint will be held at the Rackspace Office:620 Folsom St, Ste 100\nSan FranciscoThe doors will open at 10AM both days, and run until 6PM both days.Thanks to David Reid for helping get everything set up!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/11/pypy-san-francisco-sprint-dec-1st-dec-5133109101989613355.html"
    },
    {
      "title": "PyPy 2.0 beta 1",
      "text": "We're pleased to announce the 2.0 beta 1 release of PyPy. This release is\nnot a typical beta, in a sense the stability is the same or better than 1.9\nand can be used in production. It does however include a few performance\nregressions documented below that don't allow us to label is as 2.0 final.\n(It also contains many performance improvements.)\nThe main features of this release are support for ARM processor and\ncompatibility with CFFI. It also includes\nnumerous improvements to the numpy in pypy effort, cpyext and performance.\nYou can download the PyPy 2.0 beta 1 release here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.3. It's fast (pypy 2.0 beta 1 and cpython 2.7.3\nperformance comparison) due to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32. It also supports ARM machines running Linux.\nWindows 64 work is still stalling, we would welcome a volunteer\nto handle that.\n\n\nHow to use PyPy?\nWe suggest using PyPy from a virtualenv. Once you have a virtualenv\ninstalled, you can follow instructions from pypy documentation on how\nto proceed. This document also covers other installation schemes.\n\n\nRegressions\nReasons why this is not PyPy 2.0:\n\nthe ctypes fast path is now slower than it used to be. In PyPy\n1.9 ctypes was either incredibly faster or slower than CPython depending whether\nyou hit the fast path or not. Right now it's usually simply slower. We're\nprobably going to rewrite ctypes using cffi, which will make it\nuniversally faster.\ncffi (an alternative to interfacing with C code) is very fast, but\nit is missing one optimization that will make it as fast as a native\ncall from C.\nnumpypy lazy computation was disabled for the sake of simplicity.\nWe should reenable this for the final 2.0 release.\n\n\n\nHighlights\n\ncffi is officially supported by PyPy. You can install it normally by\nusing pip install cffi once you have installed PyPy and pip.\nThe corresponding 0.4 version of cffi has been released.\nARM is now an officially supported processor architecture.\nPyPy now work on soft-float ARM/Linux builds.  Currently ARM processors\nsupporting the ARMv7 and later ISA that include a floating-point unit are\nsupported.\nThis release contains the latest Python standard library 2.7.3 and is fully\ncompatible with Python 2.7.3.\nIt does not however contain hash randomization, since the solution present\nin CPython is not solving the problem anyway. The reason can be\nfound on the CPython issue tracker.\ngc.get_referrers() is now faster.\nVarious numpy improvements. The list includes:\naxis argument support in many places\nfull support for fancy indexing\ncomplex128 and complex64 dtypes\n\n\nJIT hooks are now a powerful tool to introspect the JITting process that\nPyPy performs.\n**kwds usage is much faster in the typical scenario\noperations on long objects are now as fast as in CPython (from\nroughly 2x slower)\nWe now have special strategies for dict/set/list which contain\nunicode strings, which means that now such collections will be both faster\nand more compact.\n\n\n\nThings we're working on\nThere are a few things that did not make it to the 2.0 beta 1, which\nare being actively worked on. Greenlets support in the JIT is one\nthat we would like to have before 2.0 final. Two important items that\nwill not make it to 2.0, but are being actively worked on, are:\n\nFaster JIT warmup time.\nSoftware Transactional Memory.\n\nCheers,\nMaciej Fijalkowski, Armin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/11/pypy-20-beta-1-2702952243260181341.html"
    },
    {
      "title": "Py3k status update #7",
      "text": "This is the seventh status update about our work on the py3k branch, which\nwe can work on thanks to all of the people who donated to the py3k\nproposal.The biggest news is that this month Philip started to work on py3k in parallel\nto Antonio. As such, there was an increased amount of activity.The py3k buildbots now fully translate the branch every night and run the\nPython standard library tests.We currently pass 160 out of approximately 355 modules of CPython's standard\ntest suite, fail 144 and skip approximately 51.Some highlights:dictviews (the objects returned by dict.keys/values/items) has been greatly\nimproved, and now they full support set operators\na lot of tests has been fixed wrt complex numbers (and in particular the\n__complex__ method)\n_csv has been fixed and now it correctly handles unicode instead of bytes\nmore parser fixes, py3k list comprehension semantics; now you can no longer\naccess the list comprehension variable after it finishes\n2to3'd most of the lib_pypy modules (pypy's custom standard lib\nreplacements/additions)\npy3-enabled pyrepl: this means that finally readline works at the command\nprompt, as well as builtins.input(). pdb seems to work, as well as\nfancycompleter to get colorful TAB completions :-)\npy3 round\nfurther tightening/cleanup of the unicode handling (more usage of\nsurrogateescape, surrogatepass among other things)\nas well as keeping up with some big changes happening on the default branch\nand of course various other fixes.\nFinally, we would like to thank Amaury Forgeot d'Arc for his significant\ncontributions.cheers,\nPhilip&Antonio",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/11/py3k-status-update-7-6182140595418083307.html"
    },
    {
      "title": "NumPy status update #5",
      "text": "Hello.\nI'm quite excited to inform that work on NumPy in PyPy has been restarted\nand there has been quite a bit of progress on the NumPy front in PyPy in the\npast two months. Things that happened:\n\ncomplex dtype support - thanks to matti picus, NumPy on PyPy now supports\ncomplex dtype (only complex128 so far, there is work on the other part)\nbig refactoring - probably the biggest issue we did was finishing\na big refactoring that disabled some speedups (notably lazy computation\nof arrays), but lowered the barrier of implementing cool new features.\nfancy indexing support - all fancy indexing tricks should now work,\nincluding a[b] where b is an array of integers.\nnewaxis support - now you can use newaxis features\nimprovements to ``intp``, ``uintp``, ``void``, ``string`` and record dtypes\n\nFeatures that have active branches, but hasn't been merged:\n\nfloat16 dtype support\nmissing ndarray attributes - this is a branch to finish all attributes\non ndarray, hence ending one chapter.\npickling support for numarray - hasn't started yet, but next on the list\n\nMore importantly, we're getting very close to able to import the python part\nof the original numpy with only import modifications and running it's tests.\nMost tests will fail at this point, however it'll be a good start for another\nchapter :-)\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/11/numpy-status-update-5-5489198414356844587.html"
    },
    {
      "title": "Cape Town 2012 sprint report",
      "text": "Hello.\nWe're about to finish a PyPy sprint in Cape Town, South Africa that was\none of the smallest done so far, only having Armin Rigo and Maciej Fijalkowski\nwith Alex Gaynor joining briefly at the beginning, however also one of the\nlongest, lasting almost 3 weeks. The sprint theme seems to be predominantly\n\"no new features\" and \"spring cleaning\". We overall removed about 20k lines\nof code in the PyPy source tree. The breakdown of things done and worked on:\n\nWe killed SomeObject support in annotation and rtyper. This is a modest\ncode saving, however, it reduces the complexity of RPython and also,\nhopefully, improves compile errors from RPython. We're far from done\non the path to have comprehensible compile-time errors, but the first\nstep is always the hardest :)\n\nWe killed some magic in specifying the interface between builtin functions\nand Python code. It used to be possible to write builtin functions like this:\n\ndef f(space, w_x='xyz'):\n\nwhich will magically wrap 'xyz' into a W_StringObject. Right now, instead,\nyou have to write:\n\n@unwrap_spec(w_x=WrappedDefault('xyz'))\ndef f(space, w_x):\n\nwhich is more verbose, but less magical.\n\nWe killed the CExtModuleBuilder which is the last remaining part of\ninfamous extension compiler that could in theory build C extensions\nfor CPython in RPython. This was never working very well and the main\npart was killed long ago.\n\nWe killed various code duplications in the C backend.\n\nWe killed microbench and a bunch of other small-to-medium unused\ndirectories.\n\nWe killed llgraph JIT backend and rewrote it from scratch. Now the llgraph\nbackend is not translatable, but this feature was rarely used and caused\na great deal of complexity.\n\nWe progressed on continulet-jit-3 branch, up to the point of merging\nit into result-in-resops branch, which also has seen a bit of progress.\nPurpose of those two branches:\n\ncontinulet-jit-3: enable stackless to interact with the JIT by killing\nglobal state while resuming from the JIT into the interpreter. This has\nmultiple benefits. For example it's one of the stones on the path to\nenable STM for PyPy. It also opens new possibilities for other optimizations\nincluding Python-Python calls and generators.\nresult-in-resops: the main goal is to speed up the tracing time of PyPy.\nWe found out the majority of time is spent in the optimizer chain,\nwhich faces an almost complete rewrite. It also simplifies the storage\nof the operations as well as the number of implicit invariants that have\nto be kept in mind while developing.\n\n\nWe finished and merged the excellent work by Ronan Lamy which makes the\nflow object space (used for abstract interpretation during RPython\ncompilation) independent from the Python interpreter. This means\nwe've achieved an important milestone on the path of separating the RPython\ntranslation toolchain from the PyPy Python interpreter.\n\n\nCheers,\nfijal & armin",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/10/cape-town-2012-sprint-report-1612771358321767072.html"
    },
    {
      "title": "Py3k status update #6",
      "text": "This is the sixth status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.The coolest news is not about what we did in the past weeks, but what we will\ndo in the next: I am pleased to announce that Philip Jenvey has been\nselected by the PyPy communitiy to be funded for his upcoming work on py3k,\nthanks to your generous donations. He will start to work on it shortly, and he\nwill surely help the branch to make faster progress.  I am also particularly\nhappy of this because Philip is the first non-core developer who is getting\npaid with donations: he demonstrated over the past months to be able to work\neffectively on PyPy, and so we were happy to approve his application for the\njob.  This means that anyone can potentially be selected in the future, the\nonly strict requirement is to have a deep interest in working on PyPy and to\nprove to be able to do so by contributing to the project.Back to the status of the branch. Most of the work since the last status\nupdate has been done in the area of, guess what? Unicode strings. As usual,\nthis is one of the most important changes between Python 2 and Python 3, so\nit's not surprising.  The biggest news is that now PyPy internally supports\nunicode identifiers (such as names of variables, functions, attributes, etc.),\nwhereas earlier it supported only ASCII bytes strings.  The changes is still\nbarely visible from the outside, because the parser still rejects non-ASCII\nidentifiers, however you can see it with a bit of creativity:>>>> def foo(x): pass\n>>>> foo(**{'\u00e0\u00e8\u00ec\u00f2\u00f9': 42})\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <module>\nTypeError: foo() got an unexpected keyword argument '\u00e0\u00e8\u00ec\u00f2\u00f9'\nBefore the latest changes, you used to get question marks instead of the\nproper name for the keyword argument.  Although this might seem like a small\ndetail, it is a big step towards a proper working Python 3 interpreter and it\nrequired a couple of days of headaches.  A spin-off of this work is that now\nRPython has better built-in support for unicode (also in the default branch):\nfor example, it now supports unicode string formatting (using the percent\noperator) and the methods .encode/.decode('utf-8').Other than that there is the usual list of smaller issues and bugs that got\nfixed, including (but not limited to):teach the compiler when to emit the new opcode DELETE_DEREF (and\nimplement it!)\ndetect when we use spaces and TABs inconsistently in the source code, as\nCPython does\nfix yet another bug related to the new lexically scoped exceptions (this\nis the last one, hopefully)\nport some of the changes that we did to the standard CPython 2.7 tests to\n3.2, to mark those which are implementation details and should not be run on\nPyPy\nFinally, I would like to thank Amaury Forgeot d'Arc and Ariel Ben-Yehuda for\ntheir work on the branch; among other things, Amaury recently worked on\ncpyext and on the PyPy _cffi_backend, while Ariel submitted a patch to\nimplement PEP 3138.",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/09/py3k-status-update-6-4049281716377789914.html"
    },
    {
      "title": "PyPy Cape Town Sprint Oct 7th - Oct 21st 2012",
      "text": "Hello everyone!\nThe next PyPy sprint will be in Cape Town, South Africa. It is a\npublic sprint, suitable for newcomers. It starts a couple of days\nafter PyCon South Africa, which is on the 4th and 5th of October.\nThis is a relatively unusual sprint in that it is hosted halfway\nacross the world from where most contributors live, so we plan to\nspend some time during those two weeks doing sprinting and some time\ndoing touristy stuff. The goals for the sprint are general progress\nand whatever people are interested in.\nPossible topics:\n\nPyPy release 2.0\nrunning your software on PyPy\nwork on PyPy's numpy (status)\nwork on STM (status)\nJIT improvements\nany exciting stuff you can think of\n\nIf there are newcomers, we'll run the usual introduction to hacking on\nPyPy.\n\nLocation\nThe sprint will be held either in the apartment of fijal, which is in\nTamboerskloof, Cape Town, or in the offices of the Praekelt\nFoundation, located in Woodstock, Cape Town. The Praekelt Foundation\nhas offered to host us, if needed.\nCape Town, as a very touristy place, has tons of accomodation ranging\nin quality from good to amazing. Depending on the sprint location you\nmight need a car.\n\n\nGood to Know\nYou probably don't need visa for South Africa -- consult Wikipedia.\nSouth Africa is a lovely place with lots of stuff to do. You can see\npenguins, elephants, lions and sharks all on one day (or better yet,\non multiple days).\nThere is a wide selection of good restaurants within a reasonable\ndistance of the sprint venue (depending on the venue, either walking\nor driving).\nThe power plug is some weird derivative of an old-english standard,\nbut adapters are easily acquired.\n\n\nWho's Coming?\nIf you'd like to come, please let us know when you will be arriving\nand leaving, as well as what your interests are. We'll keep a list of\npeople which we'll update (or you can do so yourself if you have\nbitbucket pypy commit rights).\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/09/pypy-cape-town-sprint-oct-7th-oct-21st-5757682347636918027.html"
    },
    {
      "title": "NumPy on PyPy status update",
      "text": "Hello everyone.\nIt's been a while since we posted a numpy work update, but I'm pleased to\ninform you that work on it has been restarted. A lot of the work has been\ndone by Matti Picus, who is one of the newest contributors to the PyPy\nproject. None of the work below has been merged so far, it's work in progress:\n\nComplex dtype support.\nFixing incompatibilities between numpy and pypy's version.\nRefactoring numpypy to simplify the code and make it easier for new\ncontributors.\nReuse most of the numpy's pure python code without modifications.\n\nFinishing this is also the plan for the next month.\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/09/numpy-on-pypy-status-update-1605312600799448094.html"
    },
    {
      "title": "CFFI release 0.3",
      "text": "Hi everybody,\nWe released CFFI 0.3.  This is the first release that supports more\nthan CPython 2.x :-)\n\nCPython 2.6, 2.7, and 3.x are supported (3.3 definitely, but maybe 3.2 or earlier too)\nPyPy trunk is supported.\n\nIn more details, the main news are:\n\nsupport for PyPy.  You need to get a trunk version of PyPy, which\ncomes with the built-in module _cffi_backend to use with the CFFI\nrelease.  For testing, you can download the Linux 32/64 versions of\nPyPy trunk.  The OS/X and Windows versions of _cffi_backend\nare not tested at all so far, so probably don't work yet.\nsupport for Python 3.  It is unknown which exact version is\nrequired; probably 3.2 or even earlier, but we need 3.3 to run the\ntests.  The 3.x version is not a separate source; it runs out of the same sources.  Thanks Amaury for starting this port.\nthe main change in the API is that you need to use ffi.string(cdata)\ninstead of str(cdata) or unicode(cdata).  The motivation for this\nchange was the Python 3 compatibility.  If your Python 2 code used to\ncontain str(<cdata 'char *'>), it would interpret the memory content\nas a null-terminated string; but on Python 3 it would just return a\ndifferent string, namely \"<cdata 'char *'>\", and proceed without even\na crash, which is bad.  So ffi.string() solves it by always returning\nthe memory content as an 8-bit string (which is a str in Python 2 and\na bytes in Python 3).\nother minor API changes are documented at\nhttps://cffi.readthedocs.org/ (grep for version 0.3).\n\nUpcoming work, to be done before release 1.0:\n\nexpose to the user the module cffi.model in a possibly refactored\nway, for people that don't like (or for some reason can't easily use)\nstrings containing snippets of C declarations.  We are thinking about\nrefactoring it in such a way that it has a ctypes-compatible\ninterface, to ease porting existing code from ctypes to cffi.  Note\nthat this would concern only the C type and function declarations, not\nall the rest of ctypes.\nCFFI 1.0 will also have a corresponding PyPy release.  We are thinking\nabout calling it PyPy 2.0 and including the whole of CFFI (instead of\njust the _cffi_backend module like now).  In other words it will\nsupport CFFI out of the box --- we want to push forward usage of CFFI\nin PyPy :-)\n\nCheers,\nArmin Rigo and Maciej Fija\u0142kowski",
      "tags": "releasecffi",
      "url": "https://www.pypy.org/posts/2012/08/cffi-release-03-4740491796308953732.html"
    },
    {
      "title": "C++ objects in cppyy, part 1: Data Members",
      "text": "The cppyy module makes it possible to call into C++ from PyPy through the\nReflex package.\nDocumentation and setup instructions are\navailable here.\nRecent work has focused on STL, low-level buffers, and code quality, but also\na lot on pythonizations for the\nCINT backend, which is\nmostly for High Energy Physics (HEP) use only.\nA\nprevious posting walked\nthrough the high-level structure and organization of the module, where it was\nargued why it is necessary to write cppyy in RPython and generate bindings at\nrun-time for the best performance.\nThis posting details how access to C++ data structures is provided and is part\nof a series of 3 postings on C++ object representation in Python: the second\nposting will be about method dispatching, the third will tie up several odds\nand ends by showing how the choices presented here and in part 2 work together\nto make features such as auto-casting possible.\n\n\nWrapping Choices\n\nSay we have a plain old data type (POD), which is the simplest possible\ndata structure in C++.\nLike for example:\n\n    struct A {\n        int    m_i;\n        double m_d;\n    };\n\nWhat should such a POD look like when represented in Python?\nLet's start by looking at a Python data structure that is functionally\nsimilar, in that it also carries two public data members of the desired\ntypes.\nSomething like this:\n\n    class A(object):\n        def __init__(self):\n            self.m_i = 0\n            self.m_d = 0.\n\nAlright, now how to go about connecting this Python class with the former\nC++ POD?\nOr rather, how to connect instances of either.\nThe exact memory layout of a Python\nA\ninstance is up to Python, and likewise the layout of a C++\nA instance is up\nto C++.\nBoth layouts are implementation details of the underlying language, language\nimplementation, language version, and the platform used.\nIt should be no surprise then, that for example an\nint in C++ looks\nnothing like a\nPyIntObject, even\nthough it is perfectly possible, in both cases, to point out in memory where\nthe integer value is.\nThe two representations can thus not make use of the same block of memory\ninternally.\nHowever, the requirement is that the access to C++ from Python looks and feels\nnatural in its use, not that the mapping is exact.\nAnother requirement is that we want access to the actual object from both\nPython and C++.\nIn practice, it is easier to provide natural access to C++ from Python than\nthe other way around, because the choices of memory layout in C++ are far more\nrestrictive: the memory layout defines the access, as the actual class\ndefinition is gone at run-time.\nThe best choice then, is that the Python object will act as a proxy to the C++\nobject, with the actual data always being in C++.\n\nFrom here it follows that if the\nm_i data member\nlives in C++, then Python needs some kind of helper to access it.\nConveniently, since version 2.2, Python has a\nproperty construct\nthat can take a getter and setter function that are called when the property\nis used in Python code, and present it to the programmer as if it were a data\nmember.\nSo we arrive at this (note how the\nproperty instance\nis a variable at the class level):\n\n    class A(object):\n        def __init__(self):\n            self._cppthis = construct_new_A()\n        m_i = property(get_m_i, set_m_i)\n        m_d = property(get_m_d, set_m_d)\n\nThe\nconstruct_new_A\nhelper is not very interesting (the reflection layer can provide for it\ndirectly), and methods are a subject for part 2 of this posting, so focus on\nget_m_i\nand set_m_i.\nIn order for the getter to work, the method needs to have access to the C++\ninstance for which the Python object is a proxy.\nOn access, Python will call the getter function with the proxy instance for\nwhich it is called.\nThe proxy has a\n_cppthis data\nmember from which the C++ instance can be accessed (think of it as a pointer)\nand all is good, at least for\nm_i.\nThe second data member\nm_d, however,\nrequires some more work: it is located at some offset into\n_cppthis.\nThis offset can be obtained from the reflection information, which lets the\nC++ compiler calculate it, so details such as\nbyte padding\nare fully accounted for.\nSince the setter also needs the offset, and since both share some more details\nsuch as the containing class and type information of the data member, it is\nnatural to create a custom property class.\nThe getter and setter methods then become bound methods of an instance of that\ncustom property,\nCPPDataMember, and\nthere is one such instance per data member.\nThink of something along these lines:\n\n    def make_datamember(cppclass, name):\n        cppdm = cppyy.CPPDataMember(cppclass, name)\n        return property(cppdm.get, cppdm.set)\n\nwhere the\nmake_datamember\nfunction replaces the call to\nproperty in the\nclass definition above.\n\nNow hold on a minute!\nBefore it was argued that Python and C++ can not share the same underlying\nmemory structure, because of choices internal to the language.\nBut if on the Python side choices are being made by the developer of the\nlanguage bindings, that is no longer a limitation.\nIn other words, why not go through e.g. the Python extension API, and do\nthis:\n\n    struct A_pyproxy {\n        PyObject_HEAD\n        int    m_i;\n        double m_d;\n    };\n\nDoing so would save on\nmalloc overhead and remove\na pointer indirection.\nThere are some technical issues specific to PyPy for such a choice: there is\nno such thing as\nPyPyObject_HEAD\nand the layout of objects is not a given as that is decided only at\ntranslation time.\nBut assume that those issues can be solved, and also accept that there is no\nproblem in creating structure definitions like this at run-time, since the\nreflection layer can provide both the required size and access to the\nplacement\nnew operator\n(compare e.g. CPython's\nstruct module).\nThere is then still a more fundamental problem: it must be possible to take\nover ownership in Python from instances created in C++ and vice-versa.\nWith a proxy scheme, that is trivial: just pass the pointer and do the\nnecessary bookkeeping.\nWith an embedded object, however, not every use case can be implemented: e.g.\nif an object is created in Python, passed to C++, and deleted in C++, it\nmust have been allocated independently.\nThe proxy approach is therefore still the best choice, although embedding\nobjects may provide for optimizations in some use cases.\n\n\nInheritance\n\nThe next step, is to take a more complicated C++ class, one with inheritance\n(I'm leaving out details such as constructors etc., for brevity):\n\n    class A {\n    public:\n        virtual ~A() {}\n        int    m_i;\n        double m_d;\n    };\n\n    class B : public A {\n    public:\n        virtual ~B() {}\n        int    m_j;\n    };\n\nFrom the previous discussion, it should already be clear what this will look\nlike in Python:\n\n    class A(object):\n        def __init__(self):\n            self._cppthis = construct_new_A()\n        m_i = make_datamember('A', 'm_i')\n        m_d = make_datamember('A', 'm_d')\n\n    class B(A):\n        def __init__(self):\n            self._cppthis = construct_new_B()\n        m_j = make_datamember('B', 'm_j')\n\nThere are some minor adjustments needed, however.\nFor one, the offset of the\nm_i data member\nmay be no longer zero: it is possible that a virtual function dispatch table\n(vtable)\npointer is added at the beginning of\nA (an alternative\nis to have the vtable pointer at the end of the object).\nBut if\nm_i is handled the\nsame way as\nm_d, with the\noffset provided by the compiler, then the compiler will add the bits, if any,\nfor the vtable pointer and all is still fine.\nA real problem could come in however, with a call of the\nm_i property on\nan instance of\nB: in that case,\nthe _cppthis\npoints to a B\ninstance, whereas the getter/setter pair expect an\nA instance.\nIn practice, this is usually not a problem: compilers will align\nA and\nB and calculate\nan offset for\nm_j from the start\nof A.\nStill, that is an implementation detail (even though it is one that can be\ndetermined at run-time and thus taken advantage of by the JIT), so it can not\nbe relied upon.\nThe m_i getter\nthus needs to take into account that it can be called with a derived type,\nand so it needs to add an additional offset.\nWith that modification, the code looks something like this (as you would have\nguessed, this is getting more and more into pseudo-code territory, although it\nis conceptually close to the actual implementation in cppyy):\n\n    def get_m_i(self):\n        return int(self._cppthis + offset(A, m_i) + offset(self.__class__, A))\n\nWhich is a shame, really, because the offset between\nB and\nA is going\nto be zero most of the time in practice, and the JIT can not completely\nelide\nthe offset calculation (as we will see later; it is easy enough to elide if\nself.__class__ is\nA, though).\nOne possible solution is to repeat the properties for each derived class, i.e.\nto have a\nget_B_m_i etc., but\nthat looks ugly on the Python side and anyway\ndoes not work in all cases: e.g. with multiple inheritance where there are\ndata members with the same name in both bases, or if\nB itself has a\npublic data member called\nm_i that shadows\nthe one from A.\nThe optimization then, is achieved by making\nB in charge of the\noffset calculations, by making\noffset a method of\nB, like so:\n\n    def get_m_i(self):\n        return int(self._cppthis + offset(A, m_i) + self.offset(A))\n\nThe insight is that by scanning the inheritance hierarchy of a derived\nclass like B, you\ncan know statically whether it may sometimes need offsets, or whether the\noffsets are always going to be zero.\nHence, if the offsets are always zero, the method\noffset on\nB will\nsimply return the literal\n0 as its\nimplementation, with the JIT taking care of the rest through inlining and\nconstant folding.\nIf the offset could be non-zero, then the method will perform an actual\ncalculation, and it will let the JIT elide the call only if possible.\n\n\nMultiple Virtual Inheritance\n\nNext up would be multiple inheritance, but that is not very interesting: we\nalready have the offset calculation between the actual and base class, which\nis all that is needed to resolve any multiple inheritance hierarchy.\nSo, skip that and move on to multiple virtual inheritance.\nThat that is going to be a tad more complicated will be clear if you show the\nfollowing code snippet to any old C++ hand and see how they respond.\nMost likely you will be told: \"Don't ever do that.\"\nBut if code can be written, it will be written, and so for the sake of the\nargument, what would this look like in Python:\n\n    class A {\n    public:\n        virtual ~A() {}\n        int m_a;\n    };\n\n    class B : public virtual A {\n    public:\n        virtual ~B() {}\n        int m_b;\n    };\n\n    class C : public virtual A {\n    public:\n        virtual ~C() {}\n        int m_c;\n    };\n\n    class D : public virtual B, public virtual C {\n    public:\n        virtual ~D() {}\n        int m_d;\n    };\n\nActually, nothing changes from what we have seen so far: the scheme as laid\nout above is fully sufficient.\nFor example, D\nwould simply look like:\n\n    class D(B, C):\n        def __init__(self):\n            self._cppthis = construct_new_D()\n        m_d = make_datamember('D', 'm_d')\n\nPoint being, the only complication added by the multiple virtual\ninheritance, is that navigation of the C++ instance happens with pointers\ninternal to the instance rather than with offsets.\nHowever, it is still a fixed offset from any location to any other location\nwithin the instance as its parts are laid out consecutively in memory (this is\nnot a requirement, but it is the most efficient, so it is what is used in\npractice).\nBut what you can not do, is determine the offset statically: you need a live\n(i.e. constructed) object for any offset calculations.\nIn Python, everything is always done dynamically, so that is of itself not a\nlimitation.\nFurthermore,\nself is already\npassed to the offset calculation (remember that this was done to put the\ncalculation in the derived class, to optimize the common case of zero\noffset), thus a live C++ instance is there precisely when it is needed.\nThe call to the offset calculation is hard to elide, since the instance will\nbe passed to a C++ helper and so the most the JIT can do is guard on the\ninstance's memory address, which is likely to change between traces.\nInstead, explicit caching is needed on the base and derived types, allowing\nthe JIT to elide the lookup in the explicit cache.\n\n\nStatic Data Members and Global Variables\n\nThat, so far, covers all access to instance data members.\nNext up are static data members and global variables.\nA complication here is that a Python\nproperty needs to\nlive on the class in order to work its magic.\nOtherwise, if you get the property, it will simply return the getter function,\nand if you set it, it will dissappear.\nThe logical conclusion then, is that a\nproperty\nrepresenting a static or global variable, needs to live on the class of the\nclass, or the metaclass.\nIf done directly though, that would mean that every static data member is\navailable from every class, since all Python classes have the same metaclass,\nwhich is class\ntype (and which is\nits own metaclass).\nTo prevent that from happening and because\ntype is actually\nimmutable, each proxy class needs to have its own custom metaclass.\nFurthermore, since static data can also be accessed on the instance, the\nclass, too, gets a\nproperty object\nfor each static data member.\nExpressed in code, for a basic C++ class, this looks as follows:\n\n    class A {\n    public:\n        static int s_i;\n    };\n\nPaired with some Python code such as this, needed to expose the static\nvariable both on the class and the instance level:\n\n    meta_A = type(CppClassMeta, 'meta_A', [CPPMetaBase], {})\n    meta_A.s_i = make_datamember('A', 's_i')\n\n    class A(object):\n        __metaclass__ = meta_A\n        s_i = make_datamember('A', 's_i')\n\nInheritance adds no complications for the access of static data per se, but\nthere is the issue that the metaclasses must follow the same hierarchy as the\nproxy classes, for the Python method resolution order (MRO) to work.\nIn other words, there are two complete, parallel class hierarchies that map\none-to-one: a hierarchy for the proxy classes and one for their metaclasses.\n\nA parallel class hierarchy is used also in other highly dynamic,\nobject-oriented environments, such as for example\nSmalltalk.\nIn Smalltalk as well, class-level constructs, such as class methods and data\nmembers, are defined for the class in the metaclass.\nA metaclass hierarchy has further uses, such as lazy loading of nested\nclasses and member templates (this would be coded up in the base class of all\nmetaclasses:\nCPPMetaBase), and\nmakes it possible to distribute these over different reflection libraries.\nWith this in place, you can write Python codes like so:\n\n    >>>> from cppyy.gbl import A\n    >>>> a = A()\n    >>>> a.s_i = 42\n    >>>> print A.s_i == a.s_i\n    True\n    >>>> # etc.\n\nThe implementation of the getter for\ns_i is a lot\neasier than for instance data: the static data lives at a fixed, global,\naddress, so no offset calculations are needed.\nThe same is done for global data or global data living in namespaces:\nnamespaces are represented as Python classes, and global data are implemented\nas properties on them.\nThe need for a metaclass is one of the reasons why it is easier for namespaces\nto be classes: module objects are too restrictive.\nAnd even though namespaces are not modules, you still can, with\nsome limitations,\nimport from\nthem anyway.\n\nIt is common that global objects themselves are pointers, and therefore it\nis allowed that the stored\n_cppthis is not a\npointer to a C++ object, but rather a pointer to a pointer to a C++ object.\nA double pointer, as it were.\nThis way, if the C++ code updates the global pointer, it will automatically\nreflect on the Python side in the proxy.\nLikewise, if on the Python side the pointer gets set to a different variable,\nit is the pointer that gets updated, and this will be visible on the C++ side.\nIn general, however, the same caveat as for normal Python code applies: in\norder to set a global object, it needs to be set within the scope of that\nglobal object.\nAs an example, consider the following code for a C++ namespace\nNS with\nglobal variable\ng_a, which behaves\nthe same as Python code for what concerns the visibility of changes to the\nglobal variable:\n\n    >>>> from cppyy.gbl import NS, A\n    >>>> from NS import g_a\n    >>>> g_a = A(42)                     # does NOT update C++ side\n    >>>> print NS.g_a.m_i\n    13                                   # the old value happens to be 13\n    >>>> NS.g_a = A(42)                  # does update C++ side\n    >>>> print NS.g_a.m_i\n    42\n    >>>> # etc.\n\n\nConclusion\n\nThat covers all there is to know about data member access of C++ classes in\nPython through a reflection layer!\nA few final notes: RPython does not support metaclasses, and so the\nconstruction of proxy classes (code like\nmake_datamember\nabove) happens in Python code instead.\nThere is an overhead penalty of about 2x over pure RPython code associated\nwith that, due to extra guards that get inserted by the JIT.\nA factor of 2 sounds like a lot, but the overhead is tiny to begin with, and\n2x of tiny is still tiny and it's not easy to measure.\nThe class definition of the custom property,\nCPPDataMember, is\nin RPython code, to be transparent to the JIT.\nThe actual offset calculations are in the reflection layer.\nHaving the proxy class creation in Python, with structural code in RPython,\ncomplicates matters if proxy classes need to be constructed on-demand.\nFor example, if an instance of an as-of-yet unseen type is returned by a\nmethod.\nExplaining how that is solved is a topic of part 2, method calls, so stay\ntuned.\n\nThis posting laid out the reasoning behind the object representation of C++\nobjects in Python by cppyy for the purpose of data member access.\nIt explained how the chosen representation of offsets gives rise to a very\npythonic representation, which allows Python introspection tools to work as\nexpected.\nIt also explained some of the optimizations done for the benefit of the JIT.\nNext up are method calls, which will be described in part 2.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/08/c-objects-in-cppyy-part-1-data-members-1105848719513737614.html"
    },
    {
      "title": "Multicore Programming in PyPy and CPython",
      "text": "Hi all,\nThis is a short \"position paper\" kind of post about my view (Armin\nRigo's) on the future of multicore programming in high-level languages.\nIt is a summary of the\nkeynote presentation at EuroPython.  As I learned by talking with people\nafterwards, I am not a good enough speaker to manage to convey a deeper\nmessage in a 20-minutes talk.  I will try instead to convey it in a\n250-lines post...\nThis is about three points:\n\nWe often hear about people wanting a version of Python running without\nthe Global Interpreter Lock (GIL): a \"GIL-less Python\".  But what we\nprogrammers really need is not just a GIL-less Python --- we need a\nhigher-level way to write multithreaded programs than using directly\nthreads and locks.  One way is Automatic Mutual Exclusion (AME), which\nwould give us an \"AME Python\".\nA good enough Software Transactional Memory (STM) system can be used\nas an internal tool to do that.\nThis is what we are building into an \"AME PyPy\".\nThe picture is darker for CPython, though there is a way too.  The\nproblem is that when we say STM, we think about either GCC 4.7's STM\nsupport, or Hardware Transactional Memory (HTM).  However, both\nsolutions are enough for a \"GIL-less CPython\", but not\nfor \"AME CPython\", due to capacity limitations.  For the latter, we\nneed somehow to add some large-scale STM into the compiler.\n\nLet me explain these points in more details.\n\nGIL-less versus AME\nThe first point is in favor of the so-called Automatic Mutual Exclusion\napproach.  The issue with using threads (in any language with or without\na GIL) is that threads are fundamentally non-deterministic.  In other\nwords, the programs' behaviors are not reproductible at all, and worse,\nwe cannot even reason about it --- it becomes quickly messy.  We would\nhave to consider all possible combinations of code paths and timings,\nand we cannot hope to write tests that cover all combinations.  This\nfact is often documented as one of the main blockers towards writing\nsuccessful multithreaded applications.\nWe need to solve this issue with a higher-level solution.  Such\nsolutions exist theoretically, and Automatic Mutual Exclusion (AME) is\none of them.  The idea of AME is that we divide the execution of each\nthread into a number of \"atomic blocks\".  Each block is well-delimited\nand typically large.  Each block runs atomically, as if it acquired a\nGIL for its whole duration.  The trick is that internally we use\nTransactional Memory, which is a technique that lets the system run the\natomic blocks from each thread in parallel, while giving the programmer\nthe illusion that the blocks have been run in some global serialized\norder.\nThis doesn't magically solve all possible issues, but it helps a lot: it\nis far easier to reason in terms of a random ordering of large atomic\nblocks than in terms of a random ordering of lines of code --- not to\nmention the mess that multithreaded C is, where even a random ordering\nof instructions is not a sufficient model any more.\nHow do such atomic blocks look like?  For example, a program might\ncontain a loop over all keys of a dictionary, performing some\n\"mostly-independent\" work on each value.  This is a typical example:\neach atomic block is one iteration through the loop.  By using the\ntechnique described here, we can run the iterations in parallel\n(e.g. using a thread pool) but using AME to ensure that they appear to\nrun serially.\nIn Python, we don't care about the order in which the loop iterations\nare done, because we are anyway iterating over the keys of a dictionary.\nSo we get exactly the same effect as before: the iterations still run in\nsome random order, but --- and that's the important point --- they\nappear to run in a\nglobal serialized order.  In other words, we introduced parallelism, but\nonly under the hood: from the programmer's point of view, his program\nstill appears to run completely serially.  Parallelisation as a\ntheoretically invisible optimization...  more about the \"theoretically\"\nin the next paragraph.\nNote that randomness of order is not fundamental: they are techniques\nbuilding on top of AME that can be used to force the order of the\natomic blocks, if needed.\n\n\nPyPy and STM/AME\nTalking more precisely about PyPy: the current prototype pypy-stm is\ndoing precisely this.  In pypy-stm, the length of the atomic blocks is\nselected in one of two ways: either explicitly or automatically.\nThe automatic selection gives blocks corresponding to some small number\nof bytecodes, in which case we have merely a GIL-less Python: multiple\nthreads will appear to run serially, with the execution randomly\nswitching from one thread to another at bytecode boundaries, just like\nin CPython.\nThe explicit selection is closer to what was described in the previous\nsection: someone --- the programmer or the author of some library that\nthe programmer uses --- will explicitly put with thread.atomic: in\nthe source, which delimitates an atomic block.  For example, we can use\nit to build a library that can be used to iterate over the keys of a\ndictionary: instead of iterating over the dictionary directly, we would\nuse some custom utility which gives the elements \"in parallel\".  It\nwould give them by using internally a pool of threads, but enclosing\nevery handling of an element into such a with thread.atomic block.\nThis gives the nice illusion of a global serialized order, and thus\ngives us a well-behaving model of the program's behavior.\nRestating this differently,\nthe only semantical difference between pypy-stm and\na regular PyPy or CPython is that it has thread.atomic, which is a\ncontext manager that gives the illusion of forcing the GIL to not be\nreleased during the execution of the corresponding block of code.  Apart\nfrom this addition, they are apparently identical.\nOf course they are only semantically identical if we ignore performance:\npypy-stm uses multiple threads and can potentially benefit from that\non multicore machines.  The drawback is: when does it benefit, and how\nmuch?  The answer to this question is not immediate.  The programmer\nwill usually have to detect and locate places that cause too many\n\"conflicts\" in the Transactional Memory sense.  A conflict occurs when\ntwo atomic blocks write to the same location, or when A reads it,\nB writes it, but B finishes first and commits.  A conflict\ncauses the execution of one atomic block to be aborted and restarted,\ndue to another block committing.  Although the process is transparent,\nif it occurs more than occasionally, then it has a negative impact on\nperformance.\nThere is no out-of-the-box perfect solution for solving all conflicts.\nWhat we will need is more tools to detect them and deal with them, data\nstructures that are made aware of the risks of \"internal\" conflicts when\nexternally there shouldn't be one, and so on.  There is some work ahead.\nThe point here is that from the point of view of the final programmer,\nwe gets conflicts that we should resolve --- but at any point, our\nprogram is correct, even if it may not be yet as efficient as it could\nbe.  This is the opposite of regular multithreading, where programs are\nefficient but not as correct as they could be.  In other words, as we\nall know, we only have resources to do the easy 80% of the work and not\nthe remaining hard 20%.  So in this model we get a program that has 80%\nof the theoretical maximum of performance and it's fine.  In the regular\nmultithreading model we would instead only manage to remove 80% of the\nbugs, and we are left with obscure rare crashes.\n\n\nCPython and HTM\nCouldn't we do the same for CPython?  The problem here is that\npypy-stm is implemented as a transformation step during translation,\nwhich is not directly possible in CPython.  Here are our options:\n\nWe could review and change the C code everywhere in CPython.\nWe use GCC 4.7, which supports some form of STM.\nWe wait until Intel's next generation of CPUs comes out (\"Haswell\")\nand use HTM.\nWe write our own C code transformation within a compiler (e.g. LLVM).\n\nI will personally file the first solution in the \"thanks but no thanks\"\ncategory.  If anything, it will give us another fork of CPython that\nwill painfully struggle to keep not more than 3-4 versions behind, and\nthen eventually die.  It is very unlikely to be ever merged into the\nCPython trunk, because it would need changes everywhere.  Not to\nmention that these changes would be very experimental: tomorrow we might\nfigure out that different changes would have been better, and have to\nstart from scratch again.\nLet us turn instead to the next two solutions.  Both of these solutions\nare geared toward small-scale transactions, but not long-running ones.\nFor example, I have no clue how to give GCC rules about performing I/O\nin a transaction --- this seems not supported at all; and moreover\nlooking at the STM library that is available so far to be linked with\nthe compiled program, it assumes short transactions only.  By contrast,\nwhen I say \"long transaction\" I mean transactions that can run for 0.1\nseconds or more.  To give you an idea, in 0.1 seconds a PyPy program\nallocates and frees on the order of ~50MB of memory.\nIntel's Hardware Transactional Memory solution is both more flexible and\ncomes with a stricter limit.  In one word, the transaction boundaries\nare given by a pair of special CPU instructions that make the CPU enter\nor leave \"transactional\" mode.  If the transaction aborts, the CPU\ncancels any change, rolls back to the \"enter\" instruction and causes\nthis instruction to return an error code instead of re-entering\ntransactional mode (a bit like a fork()).  The software then detects\nthe error code.  Typically, if transactions are rarely cancelled, it is\nfine to fall back to a GIL-like solution just to redo these cancelled\ntransactions.\nAbout the implementation: this is done by recording all the changes that\na transaction wants to do to the main memory, and keeping them invisible\nto other CPUs.  This is \"easily\" achieved by keeping them inside this\nCPU's local cache; rolling back is then just a matter of discarding a\npart of this cache without committing it to memory.  From this point of\nview, there is a lot to bet that we are actually talking about the\nregular per-core Level 1 and Level 2 caches --- so any transaction that\ncannot fully store its read and written data in the 64+256KB of the L1+L2\ncaches will abort.\nSo what does it mean?  A Python interpreter overflows the L1 cache of\nthe CPU very quickly: just creating new Python function frames takes a\nlot of memory (on the order of magnitude of 1/100 of the whole L1\ncache).  Adding a 256KB L2 cache into the picture helps, particularly\nbecause it is highly associative and thus avoids a lot of fake conflicts.\nHowever, as long as the HTM support is limited to L1+L2 caches,\nit is not going to be enough to run an \"AME Python\" with any sort of\nmedium-to-long transaction.  It can\nrun a \"GIL-less Python\", though: just running a few hundred or even\nthousand bytecodes at a time should fit in the L1+L2 caches, for most\nbytecodes.\nI would vaguely guess that it will take on the order of 10 years until\nCPU cache sizes grow enough for a CPU in HTM mode to actually be able to\nrun 0.1-second transactions.  (Of course in 10 years' time a lot of other\nthings may occur too, including the whole Transactional Memory model\nbeing displaced by something else.)\n\n\nWrite your own STM for C\nLet's discuss now the last option: if neither GCC 4.7 nor HTM are\nsufficient for an \"AME CPython\", then we might want to\nwrite our own C compiler patch (as either extra work on GCC 4.7, or an\nextra pass to LLVM, for example).\nWe would have to deal with the fact that we get low-level information,\nand somehow need to preserve interesting high-level bits through the\ncompiler up to the point at which our pass runs: for example, whether\nthe field we read is immutable or not.  (This is important because some\ncommon objects are immutable, e.g. PyIntObject.  Immutable reads don't\nneed to be recorded, whereas reads of mutable data must be protected\nagainst other threads modifying them.)  We can also have custom code to\nhandle the reference counters: e.g. not consider it a conflict if\nmultiple transactions have changed the same reference counter, but just\nresolve it automatically at commit time.  We are also free to handle I/O\nin the way we want.\nMore generally, the advantage of this approach over both the current GCC\n4.7 and over HTM is that we control the whole process.  While this still\nlooks like a lot of work, it looks doable.  It would be possible to come\nup with a minimal patch of CPython that can be accepted into core\nwithout too much troubles (e.g. to mark immutable fields and tweak the\nrefcounting macros), and keep all the cleverness inside the compiler\nextension.\n\n\nConclusion\nI would assume that a programming model specific to PyPy and not\napplicable to CPython has little chances to catch on, as long as PyPy is\nnot the main Python interpreter (which looks unlikely to change anytime\nsoon).  Thus as long as only PyPy has AME, it looks like it will not\nbecome the main model of multicore usage in Python.  However, I can\nconclude with a more positive note than during the EuroPython\nconference: it is a lot of work, but there is a more-or-less reasonable\nway forward to have an AME version of CPython too.\nIn the meantime, pypy-stm is around the corner, and together with\ntools developed on top of it, it might become really useful and used.  I\nhope that in the next few years this work will trigger enough motivation\nfor CPython to follow the ideas.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/08/multicore-programming-in-pypy-and-6595343388141556320.html"
    },
    {
      "title": "NumPyPy non-progress report",
      "text": "Hello everyone.\nNot much has happened in the past few months with numpypy development. A part\nof the reason was doing other stuff for me, a part of the reason was\nvarious unexpected visa-related admin, a part of the reason was EuroPython\nand a part was long-awaited holiday.\nThe thing that's maybe worth mentioning is that it does not mean the donations\ndisappeared in the mist. PyPy developers are being paid to work on NumPyPy on\nan hourly basis - that means if I decide to take holidays or work on something\nelse, the money is simply staying in the account until later.\nThanks again for all the donations, I hope to get back to this topic soon!\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/08/hello-everyone-5492331040603503642.html"
    },
    {
      "title": "CFFI release 0.2.1",
      "text": "Hi everybody,We released CFFI 0.2.1 (expected to be 1.0 soon).  CFFI is a way to call C from Python.EDIT: Win32 was broken in 0.2.  Fixed.This release is only for CPython 2.6 or 2.7.  PyPy support is coming in\nthe ffi-backend branch, but not finished yet.  CPython 3.x would be\neasy but requires the help of someone.The package is available on bitbucket as well as documented. You\ncan also install it straight from the python package index: pip install cffiContains numerous small changes and support for more C-isms.\nThe biggest news is the support for installing packages that use\nffi.verify() on machines without a C compiler.  Arguably, this\nlifts the last serious restriction for people to use CFFI.\nPartial list of smaller changes:mappings between 'wchar_t' and Python unicodes\nthe introduction of ffi.NULL\na possibly clearer API for ffi.new(): e.g. to allocate a single int and obtain a pointer to it, use ffi.new(\"int *\") instead of the old\nffi.new(\"int\")\nand of course a plethora of smaller bug fixes\n\nCFFI uses pkg-config to install itself if available.  This helps\nlocate libffi on modern Linuxes.  Mac OS/X support is available too\n(see the detailed installation instructions).  Win32 should work out\nof the box.  Win64 has not been really tested yet.\nCheers,\nArmin Rigo and Maciej Fija\u0142kowski",
      "tags": "releasecffi",
      "url": "https://www.pypy.org/posts/2012/07/cffi-release-02-4800000428934604295.html"
    },
    {
      "title": "Prototype PHP interpreter using the PyPy toolchain - Hippy VM",
      "text": "Hello everyone.\nI'm proud to release the result of a Facebook-sponsored study on the feasibility of\nusing the RPython toolchain to produce a PHP interpreter. The rules were\nsimple: two months; one person; get as close to PHP as possible, implementing\nenough warts and corner cases to be reasonably sure that it answers hard\nproblems in the PHP language. The outcome is called Hippy VM and implements\nmost of the PHP 1.0 language (functions, arrays, ints, floats and strings).\nThis should be considered an alpha release.\nThe resulting interpreter is obviously incomplete \u2013 it does not support all\nmodern PHP constructs (classes are completely unimplemented), builtin functions,\ngrammar productions, web server integration, builtin libraries\netc., etc.. It's just complete enough for me to reasonably be able to\nsay that \u2013 given some engineering effort \u2013 it's possible to provide a rock-solid\nand fast PHP VM using PyPy technologies.\nThe result is available in a Bitbucket repo and is released under the MIT\nlicense.\n\nPerformance\nThe table below shows a few benchmarks comparing Hippy VM to Zend (a standard\nPHP interpreter available in Linux distributions) and HipHop VM (a PHP-to-C++\noptimizing compiler developed by Facebook).  The versions used were Zend 5.3.2\n(Zend Engine v2.3.0) and HipHop VM heads/vm-0-ga4fbb08028493df0f5e44f2bf7c042e859e245ab\n(note that you need to check out the vm branch to get the newest version).\nThe run was performed on 64-bit Linux running on a Xeon W3580 with 8M of\nL2 cache, which was otherwise unoccupied.\nUnfortunately, I was not able to run it on the JITted version of HHVM, the new effort by Facebook,\nbut people involved with the project told me it's usually slower or comparable with the compiled HipHop.\nTheir JITted VM is still alpha software, so I'll update it as soon as I have the info.\n\n\n\n\n\n\n\n\n\n\n\nbenchmark\nZend\nHipHop VM\nHippy VM\nHippy / Zend\nHippy / HipHop\n\narr\n2.771\n0.508+-0%\n0.274+-0%\n10.1x\n1.8x\n\nfannkuch\n21.239\n7.248+-0%\n1.377+-0%\n15.4x\n5.3x\n\nheapsort\n1.739\n0.507+-0%\n0.192+-0%\n9.1x\n2.6x\n\nbinary_trees\n3.223\n0.641+-0%\n0.460+-0%\n7.0x\n1.4x\n\ncache_get_scb\n3.350\n0.614+-0%\n0.267+-2%\n12.6x\n2.3x\n\nfib\n2.357\n0.497+-0%\n0.021+-0%\n111.6x\n23.5x\n\nfasta\n1.499\n0.233+-4%\n0.177+-0%\n8.5x\n1.3x\n\n\n\n\nThe PyPy compiler toolchain provides a way to implement a dynamic\nlanguage interpreter in a high-level language called RPython. This is\na language which is lower-level than Python, but still higher-level than\nC or C++: for example, RPython is a garbage-collected language. The killer\nfeature is that the toolchain will generate a JIT for your interpreter which\nwill be able to leverage most of the work that has been done on speeding up Python\nin the PyPy project.  The resulting JIT is generated for your interpreter, and is not Python-specific.\nThis was one of the toolchain's original design decisions \u2013 in contrast to e.g. the JVM,\nwhich was initially only used to interpret Java and later adjusted to serve as a platform for\ndynamic languages.\nAnother important difference is that there is no common bytecode to which you compile both your\nlanguage and Python, so you don't inherit problems presented when implementing language X on top of,\nsay, Parrot VM or the JVM.  The PyPy toolchain does not impose constraints on the semantics of\nyour language, whereas the benefits of the JVM only apply to languages that map well onto Java concepts.\nTo read more about creating your own interpreters using the PyPy toolchain,\nread more blog posts or an excellent article by Laurence Tratt.\n\n\nPHP deviations\nThe project's biggest deviation from the PHP specification is probably\nthat GC is no longer reference counting. That means that the object finalizer, when\nimplemented, will not be called directly at the moment of object death, but\nat some later point. There are possible future developments to alleviate that\nproblem, by providing \"refcounted\" objects when leaving the current scope.\nResearch has to be done in order to achieve that.\n\n\nAssessment\nThe RPython toolchain seems to be a cost-effective choice for writing\ndynamic language VMs.  It both provides a fast JIT and gives you\naccess to low-level primitives when you need them. A good example is\nin the directory hippy/rpython which contains the implementation\nof an ordered dictionary. An ordered dictionary is not a primitive\nthat RPython provides \u2013 it's not necessary for the goal of\nimplementing Python.  Now, implementing it on top of a normal dictionary\nis possible, but inefficient. RPython provides a way to work\ndirectly at a lower level, if you desire to do so.\nThings that require improvements in RPython:\n\nLack of mutable strings on the RPython level ended up being a problem.\nI ended up using lists of characters; which are efficient, but inconvenient,\nsince they don't support any string methods.\nFrame handling is too conservative and too Python-specific, especially around\nthe calls. It's possible to implement less general, but simpler and faster\nframe handling implementation in RPython.\n\n\n\nStatus of the implementation\nDon't use it! It's a research prototype intended to assess the feasibility\nof using RPython to create dynamic language VMs. The most notable\nfeature that's missing is reasonable error reporting. That said, I'm\nconfident it implements enough of the PHP language to prove that the full\nimplementation will present the same performance characteristics.\n\n\nBenchmarks\nThe benchmarks are a selection of computer language shootout benchmarks, as well\nas cache_get_scb, which is a part of old Facebook code. All benchmarks other\nthan this one (which is not open source, but definitely the most interesting :( ) are\navailable in the bench directory. The Python program to run them is called\nrunner.py and is in the same directory. It runs them 10 times, cutting off the first\n3 runs (to ignore the JIT warm-up time) and averaging the rest. As you can see\nthe standard deviation is fairly minimal for all interpreters and runs; if\nit's omitted it means it's below 0.5%.\nThe benchmarks were not selected for their ease of optimization \u2013 the optimizations\nin the interpreter were written specifically for this set of benchmarks. No special JIT\noptimizations were added, and barring what's mentioned below a vanilla PyPy 1.9 checkout\nwas used for compilation.\n\n\nSo, how fast will my website run if this is completed?\nThe truth is that I lack the benchmarks to be able to answer that right now. The core\nof the PHP language is implemented up to the point where I'm confident\nthat the performance will not change as we get more of the PHP going.\n\n\nHow do I run it?\nGet a PyPy checkout, apply the diff if you want to squeeze out the last\nbits of performance and run pypy-checkout/pypy/bin/rpython targethippy.py to\nget an executable that resembles a PHP interpreter. You can also directly run\npython targethippy.py file.php, but this will be about 2000x slower.\n\n\nRPython modifications\nThere was a modification that I did to the PyPy source code; the diff\nis available. It's trivial, and should simply be made optional in the\nRPython JIT generator, but it was easier just to do it, given the very constrained time\nframe.\n\ngen_store_back_in_virtualizable was disabled. This feature is\nnecessary for Python frames but not for PHP frames. PHP frames\ndo not have to be kept alive after we exit a function.\n\n\n\nFuture\nHippy is a cool prototype that presents a very interesting path towards a fast\nPHP VM.  However, at the moment I have too many other open source commitments\nto take on the task of completing it in my spare time.  I do think that this project\nhas a lot of potential, but I will not commit to any further development at\nthis time.  If you send pull requests I'll try to review them.  I'm also open\nto having further development on this project funded, so if you're interested\nin this project and the potential of a fast PHP interpreter, please get in\ntouch.\n\nCheers,\nfijal\nEDIT: Fixed the path to the rpython binary",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/07/hello-everyone-6869934374873967346.html"
    },
    {
      "title": "Py3k status update #5",
      "text": "This is the fifth status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.Apart from the usual \"fix shallow py3k-related bugs\" part, most of my work in\nthis iteration has been to fix the bootstrap logic of the interpreter, in\nparticular to setup the initial sys.path.Until few weeks ago, the logic to determine sys.path was written entirely\nat app-level in pypy/translator/goal/app_main.py, which is automatically\nincluded inside the executable during translation.  The algorithm is more or\nless like this:find the absolute path of the executable by looking at sys.argv[0]\nand cycling through all the directories in PATH\nstarting from there, go up in the directory hierarchy until we find a\ndirectory which contains lib-python and lib_pypy\nThis works fine for Python 2 where the paths and filenames are represented as\n8-bit strings, but it is a problem for Python 3 where we want to use unicode\ninstead.  In particular, whenever we try to encode a 8-bit string into an\nunicode, PyPy asks the _codecs built-in module to find the suitable\ncodec. Then, _codecs tries to import the encodings package, to list\nall the available encodings. encodings is a package of the standard\nlibrary written in pure Python, so it is located inside\nlib-python/3.2. But at this point in time we yet have to add\nlib-python/3.2 to sys.path, so the import fails.  Bootstrap problem!The hard part was to find the problem: since it is an error which happens so\nearly, the interpreter is not even able to display a traceback, because it\ncannot yet import traceback.py. The only way to debug it was through some\ncarefully placed print statement and the help of gdb. Once found the\nproblem, the solution was as easy as moving part of the logic to RPython,\nwhere we don't have bootstrap problems.Once the problem was fixed, I was able to finally run all the CPython test\nagainst the compiled PyPy.  As expected there are lots of failures, and fixing\nthem will be the topic of my next months.",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/07/py3k-status-update-5-359698189825543897.html"
    },
    {
      "title": "EuroPython sprint",
      "text": "Hi all,\n\nEuroPython is next week.  We will actually be giving a presentation on Monday, in one of the plenary talks: PyPy: current status and GIL-less future.  This is the first international PyPy keynote we give, as far as I know, but not the first keynote about PyPy [David Beazley's video] :-)\n\nThe other talks are PyPy JIT under the hood and to some extent Performance analysis tools for JITted VMs.  This year we are also trying out a help desk.  Finally, we will have the usual sprint after EuroPython on Saturday and Sunday.\n\nSee you soon!\n\nArmin.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/06/europython-sprint-5668923199392472912.html"
    },
    {
      "title": "Architecture of Cppyy",
      "text": "The cppyy module makes it possible to call into C++ from PyPy through the\nReflex package.\nWork started about two years ago, with a follow-up sprint a year later.\nThe module has now reached an acceptable level of maturity and initial\ndocumentation with setup instructions, as well as a list of the currently\nsupported language features, are now available here.\nThere is a sizable (non-PyPy) set of unit and application tests that is still\nbeing worked through, not all of them of general applicability, so development\ncontinues its current somewhat random walk towards full language coverage.\nHowever, if you find that cppyy by and large works for you except for certain\nspecific features, feel free to ask for them to be given higher priority.\nCppyy handles bindings differently than what is typically found in other\ntools with a similar objective, so this update walks through some of these\ndifferences, and explains why choices were made as they are.\nThe most visible difference, is from the viewpoint of the Python programmer\ninteracting with the module.\nThe two canonical ways of making Python part of a larger environment, are to\neither embed or extend it.\nThe latter is done with so-called extension modules, which are explicitly\nconstructed to be very similar in their presentation to the Python programmer\nas normal Python modules.\nIn cppyy, however, the external C++ world is presented from a single entrance\npoint, the global C++ namespace (in the form of the variable cppyy.gbl).\nThus, instead of importing a package that contains your C++ classes, usage\nlooks like this (assuming class MyClass in the global namespace):\n\n>>>> import cppyy\n>>>> m = cppyy.gbl.MyClass()\n>>>> # etc.\n\nThis is more natural than it appears at first: C++ classes and functions are,\nonce compiled, represented by unique linker symbols, so it makes sense to give\nthem their own unique place on the Python side as well.\nThis organization allows pythonizations of C++ classes to propagate from one\ncode to another, ensures that all normal Python introspection (such as\nissubclass and isinstance) works as expected in all cases, and that it\nis possible to represent C++ constructs such as typedefs simply by Python\nreferences.\nAchieving this unified presentation would clearly require a lot of internal\nadministration to track all C++ entities if they each lived in their own,\npre-built extension modules.\nSo instead, cppyy generates the C++ bindings at run-time, which brings us to\nthe next difference.\nThen again, that is not really a difference: when writing or generating a\nPython extension module, the result is some C code that consists of calls into\nPython, which then gets compiled.\nHowever, it is not the bindings themselves that are compiled; it is the code\nthat creates the bindings that gets compiled.\nIn other words, any generated or hand-written extension module does exactly\nwhat cppyy does, except that they are much more specific in that the bound\ncode is hard-wired with e.g. fixed strings and external function calls.\nThe upshot is that in Python, where all objects are first-class and run-time\nconstructs, there is no difference whatsoever between bindings generated at\nrun-time, and bindings generated at ... well, run-time really.\nThere is a difference in organization, though, which goes back to the first\npoint of structuring the C++ class proxies in Python: given that a class will\nsettle in a unique place once bound, instead of inside a module that has no\nmeaning in the C++ world, it follows that it can also be uniquely located in\nthe first place.\nIn other words, cppyy can, and does, make use of a class loader to\nauto-load classes on-demand.\nIf at this point, this all reminds you of a bit ctypes, just with some extra\nbells and whistles, you would be quite right.\nIn fact, internally cppyy makes heavy use of the RPython modules that form the\nguts of ctypes.\nThe difficult part of ctypes, however, is the requirement to annotate\nfunctions and structures.\nThat is not very pleasant in C, but in C++ there is a whole other level of\ncomplexity in that the C++ standard specifies many low-level details, that are\nrequired for dispatching calls and understanding object layout, as\n\"implementation defined.\"\nOf course, in the case of Open Source compilers, getting at those details is\ndoable, but having to reverse engineer closed-source compilers gets old rather\nquickly in more ways than one.\nMore generally, these implementation defined details prevent a clean interface,\ni.e. without a further dependency on the compiler, into C++ like the one that\nthe CFFI module provides for C.\nStill, once internal pointers have been followed, offsets have been calculated,\nthis objects have been provided, etc., etc., the final dispatch into binary\nC++ is no different than that into C, and cppyy will therefore be able to make\nuse of CFFI internally, like it does with ctypes today.\nThis is especially relevant in the CLang/LLVM world, where stub functions\nare done away with.\nTo get the required low-level details then, cppyy relies on a back-end, rather\nthan getting it from the programmer, and this is where Reflex (together with\nthe relevant C++ compiler) comes in, largely automating this tedious process.\nThere is nothing special about Reflex per se, other than that it is relatively\nlightweight, available, and has proven to be able to handle huge code bases.\nIt was a known quantity when work on cppyy started, and given the number\nof moving parts in learning PyPy, that was a welcome relief.\nReflex is based on gccxml, and can therefore handle pretty much any C or\nC++ code that you care to throw at it.\nIt is also technically speaking obsolete as it will not support C++11, since\ngccxml won't, but its expected replacement, based on CLang/LLVM, is not\nquite there yet (we are looking at Q3 of this year).\nIn cppyy, access to Reflex, or any back-end for that matter, is through a\nthin C API (see the schematic below): cppyy asks high level questions to the\nback-end, and receives low-level results, some of which are in the form of\nopaque handles.\nThis ensures that cppyy is not tied to any specific back-end.\nIn fact, currently it already supports another, CINT, but that back-end is\nof little interest outside of High Energy Physics (HEP).\nThe Python side is always the same, however, so any Python code based on cppyy\ndoes not have to change if the back-end changes.\nTo use the system,  a back-end specific tool (genreflex for Reflex) is\nfirst run on a set of header files with a selection file for choosing the\nrequired classes.\nThis produces a C++ file that must be compiled into a shared library, and a\ncorresponding map file for the class loader.\nThese shared libraries, with their map files alongside, can be put anywhere\nas long as they can be located through the standard paths for the dynamic\nloader.\nWith that in place, the setup is ready, and the C++ classes are available to\nbe used from cppyy.\n\nSo far, nothing that has been described is specific to PyPy.\nIn fact, most of the technologies described have been used for a long time\non CPython already, so why the need for a new, PyPy-specific, module?\nTo get to that, it is important to first understand how a call is mediated\nbetween Python and C++.\nIn Python, there is the concept of a PyObject, which has a reference count, a\npointer to a type object, and some payload.\nThere are APIs to extract the low-level information from the payload for use\nin the C++ call, and to repackage any results from the call.\nThis marshalling is where the bulk of the time is spent when dispatching.\nTo be absolutely precise, most C++ extension module generators produce slow\ndispatches because they don't handle overloads efficiently, but even in there,\nthey still spend most of their time in the marshalling code, albeit in calls\nthat fail before trying the next overload.\nIn PyPy, speed is gained by having the JIT unbox objects into the payload only,\nallowing it to become part of compiled traces.\nIf the same marshalling APIs were used, the JIT is forced to rebox the payload,\nhand it over through the API, only to have it unboxed again by the binding.\nDoing so is dreadfully inefficient.\nThe objective of cppyy, then, is to keep all code transparent to the JIT until\nthe absolute last possible moment, i.e. the call into C++ itself, therefore\nallowing it to (more or less) directly pass the payload it already has, with\nan absolute minimal amount of extra work.\nIn the extreme case when the binding is not to a call, but to a data member of\nan object (or to a global variable), the memory address is delivered to the\nJIT and this results in direct access with no overhead.\nNote the interplay: cppyy in PyPy does not work like a binding in the CPython\nsense that is a back-and-forth between the interpreter and the extension.\nInstead, it does its work by being transparent to the JIT, allowing the JIT to\ndissolve the binding.\nAnd with that, we have made a full circle: if to work well with the JIT, and\nin so doing achieve the best performance, you can not have marshalling or do\nany other API-based driving, then the concept of compiled extension modules is\nout, and the better solution is in run-time generated bindings.\nThat leaves one final point.\nWhat if you do want to present an extension module-like interface to\nprogrammers that use your code?\nBut of course, this is Python: everything consists of first-class objects,\nwhose behavior can be changed on the fly.\nIn CPython, you might hesitate to make such changes, as every overlay or\nindirection results in quite a bit of overhead.\nWith PyPy, however, these layers are all optimized out of existences, making\nthat a non-issue.\nThis posting laid out the reasoning behind the organization of cppyy.\nA follow-up is planned, to explain how C++ objects are handled and\nrepresented internally.\nWim Lavrijsen",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/06/architecture-of-cppyy-9077100041707701102.html"
    },
    {
      "title": "Release 0.1 of CFFI",
      "text": "Hi.We're pleased to announce the first public release, 0.1 of CFFI, a way to call C from Python.\n(This release does not support PyPy yet --- but we announce it here as it is planned for the\nnext release :-)The package is available on bitbucket as well as documented. You can also install it\nstraight from the python package index (pip).The aim of this project is to provide a convenient and reliable way of calling C code from Python.\nThe interface is based on LuaJIT's FFI and follows a few principles:The goal is to call C code from Python.  You should be able to do so\nwithout learning a 3rd language: every alternative requires you to learn\ntheir own language (Cython, SWIG) or API (ctypes).  So we tried to\nassume that you know Python and C and minimize the extra bits of API that\nyou need to learn.\nKeep all the Python-related logic in Python so that you don't need to\nwrite much C code (unlike CPython native C extensions).\nWork either at the level of the ABI (Application Binary Interface)\nor the API (Application Programming Interface).  Usually, C\nlibraries have a specified C API but often not an ABI (e.g. they may\ndocument a \"struct\" as having at least these fields, but maybe more).\n(ctypes works at the ABI level, whereas Cython or native C extensions\nwork at the API level.)\nWe try to be complete.  For now some C99 constructs are not supported,\nbut all C89 should be, including macros (and including macro \"abuses\",\nwhich you can manually wrap in saner-looking C functions).\nWe attempt to support both PyPy and CPython (although PyPy support is not\ncomplete yet) with a reasonable path for other Python implementations like\nIronPython and Jython.\nNote that this project is not about embedding executable C code in\nPython, unlike Weave.  This is about calling existing C libraries\nfrom Python.\nStatus of the projectConsider this as a beta release. Creating CPython extensions is fully supported and the API should\nbe relatively stable; however, minor adjustements of the API are possible.PyPy support is not yet done and this is a goal for the next release. There are vague plans to make this the\npreferred way to call C from Python that can reliably work between PyPy and CPython.Right now CFFI's verify() requires a C compiler and header files to be available at run-time.\nThis limitation will be lifted in the near future and it'll contain a way to cache the resulting binary.Cheers,\n\nArmin Rigo and Maciej Fija\u0142kowski",
      "tags": "releasecffi",
      "url": "https://www.pypy.org/posts/2012/06/release-01-of-cffi-4760622823232463868.html"
    },
    {
      "title": "STM with threads",
      "text": "Hi all,A quick update.  The first version of pypy-stm based on regular\nthreads is ready.  Still having no JIT and a 4-or-5-times performance\nhit, it is not particularly fast, but I am happy that it turns out not\nto be much slower than the previous thread-less attempts.  It is at\nleast fast enough to run faster (in real time) than an equivalent no-STM\nPyPy, if fed with an eight-threaded program on an eight-core machine\n(provided, of course, you don't mind it eating all 8 cores' CPU power\ninstead of just one :-).You can download and play around with this binary for Linux 64.  It\nwas made from the stm-thread branch of the PyPy repository (translate.py --stm -O2 targetpypystandalone.py).  (Be sure\nto put it where it can find its stdlib, e.g. by putting it inside the\ndirectory from the official 1.9 release.)This binary supports the thread module and runs without the GIL.\nSo, despite the factor-of-4 slow-down issue, it should be the fourth\ncomplete Python interpreter in which we can reasonably claim to have\nresolved the problem of the GIL.  (The first one was Greg Stein's Python\n1.4, re-explored here; the second one is Jython; the third one is\nIronPython.)  Unlike the previous three, it is also the first one to\noffer full GIL semantics to the programmer, and additionally\nthread.atomic (see below).  I should also add that we're likely to\nsee in the next year a 5th such interpreter, too, based on Hardware\nTransactional Memory (same approach as with STM, but using e.g.\nIntel's HTM).The binary I linked to above supports all built-in modules from PyPy,\napart from signal, still being worked on (which can be a bit\nannoying because standard library modules like subprocess depend on\nit).  The sys.get/setcheckinterval() functions can be used to tweak\nthe frequency of the automatic commits.  Additionally, it offers\nthread.atomic, described in the previous blog post as a way to\ncreate longer atomic sections (with the observable effect of preventing\nthe \"GIL\" to be released during that time).  A complete\ntransaction.py module based on it is available from the sources.The main missing features are:the signal module;\nthe Garbage Collector, which does not do major collections so far, only\nminor ones;\nand finally, the JIT, which needs some amount of integration to generate\nthe correctly-tweaked assembler.\nHave fun!Armin.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/06/stm-with-threads-7818875111634541910.html"
    },
    {
      "title": "PyPy 1.9 - Yard Wolf",
      "text": "We're pleased to announce the 1.9 release of PyPy. This release brings mostly\nbugfixes, performance improvements, other small improvements and overall\nprogress on the numpypy effort.\nIt also brings an improved situation on Windows and OS X.You can download the PyPy 1.9 release here:https://pypy.org/download.htmlWhat is PyPy?PyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 1.9 and cpython 2.7.2 performance comparison)\ndue to its integrated tracing JIT compiler.This release supports x86 machines running Linux 32/64, Mac OS X 64 or\nWindows 32.  Windows 64 work is still stalling, we would welcome a volunteer\nto handle that.Thanks to our donorsBut first of all, we would like to say thank you to all people who\ndonated some money to one of our four calls:NumPy in PyPy (got so far $44502 out of $60000, 74%)\nPy3k (Python 3) (got so far $43563 out of $105000, 41%)\nSoftware Transactional Memory (got so far $21791 of $50400, 43%)\nas well as our general PyPy pot.\nThank you all for proving that it is indeed possible for a small team of\nprogrammers to get funded like that, at least for some\ntime.  We want to include this thank you in the present release\nannouncement even though most of the work is not finished yet.  More\nprecisely, neither Py3k nor STM are ready to make it in an official release\nyet: people interested in them need to grab and (attempt to) translate\nPyPy from the corresponding branches (respectively py3k and\nstm-thread).HighlightsThis release still implements Python 2.7.2.\nMany bugs were corrected for Windows 32 bit.  This includes new\nfunctionality to test the validity of file descriptors; and\ncorrect handling of the calling convensions for ctypes.  (Still not\nmuch progress on Win64.) A lot of work on this has been done by Matti Picus\nand Amaury Forgeot d'Arc.\nImprovements in cpyext, our emulator for CPython C extension modules.\nFor example PyOpenSSL should now work.  We thank various people for help.\nSets now have strategies just like dictionaries. This means for example\nthat a set containing only ints will be more compact (and faster).\nA lot of progress on various aspects of numpypy. See the numpy-status\npage for the automatic report.\nIt is now possible to create and manipulate C-like structures using the\nPyPy-only _ffi module.  The advantage over using e.g. ctypes is that\n_ffi is very JIT-friendly, and getting/setting of fields is translated\nto few assembler instructions by the JIT. However, this is mostly intended\nas a low-level backend to be used by more user-friendly FFI packages, and\nthe API might change in the future. Use it at your own risk.\nThe non-x86 backends for the JIT are progressing but are still not\nmerged (ARMv7 and PPC64).\nJIT hooks for inspecting the created assembler code have been improved.\nSee JIT hooks documentation for details.\nselect.kqueue has been added (BSD).\nHandling of keyword arguments has been drastically improved in the best-case\nscenario: proxy functions which simply forwards *args and **kwargs\nto another function now performs much better with the JIT.\nList comprehension has been improved.\nJitViewerThere will be a corresponding 1.9 release of JitViewer which is guaranteed to work\nwith PyPy 1.9. See the JitViewer docs for details.Cheers,\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/06/pypy-19-yard-wolf-7006180436602667005.html"
    },
    {
      "title": "Py3k status update #4",
      "text": "This is the fourth status update about our work on the py3k branch, which we\ncan work on thanks to all of the people who donated to the py3k proposal.For various reasons, less work than usual has been done since the last status\nupdate. However, some interesting things happened anyway.As readers know, so far we spent most of the effort in fixing all PyPy's own\ntests which started to fail for various py2/py3 differences.  Most of them\nfailed for shallow reasons, e.g. syntactic changes or the int/long\nunifications. Others failed for subtle differences and needed a bit more care,\nfor example the fact that unbound methods are gone in Py3k.The good news is that finally we are seeing the light at the end of the\ntunnel. Most of them have been fixed. For sine other tests, we introduced the\nconcept of \"py3k-skipping\": some optimizations and modules are indeed failing,\nbut right now we are concentrating on completing the core language and so we\nare not interested in those.  When the core language will be done, we will be\nable to easily find and work on the py3k-skipped tests.  In particular, for\nnow we disabled the Int and String dict strategies, which are broken\nbecause of the usual int/long unification and str vs bytes.  As for modules,\nfor now _continuation (needed for stackless) and _multiprocessing do\nnot work yet.Another non-trivial feature we implemented is the proper cleaning of exception\nvariables when we exit except blocks.  This is a feature which touches\nlots of levels of PyPy, starting from astcompiler, down to the bytecode\ninterpreter. It tooks two days of headache, but at the end we made it :-).Additionally, Amaury did a lot of improvements to cpyext, which had been\nbroken since forever on this branch.As for the next plans, now that things are starting to work and PyPy's own\ntests mostly pass, we can finally start to run the compiled PyPy against\nCPython's test suite.  It is very likely that we will have tons of failures at\nthe beginning, but once we start to fix them one by one, a Py3k-compatible\nPyPy will be closer and closer.",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/06/py3k-status-update-4-4834053219477515637.html"
    },
    {
      "title": "STM update: back to threads?",
      "text": "Hi again,\n\nHere is another update on the status of Software Transactional Memory on PyPy.\n\nThose of you who have been closely following this blog since last year know that, from the very first post about STM, I explored various design ideas about the API that we should get when programming in Python.\n\nI went a full circle, and now I am back to where I started (with, important difference, a very roughly working implementation of pypy-stm).\n\nWhat I realized is that the \"thread\" module is not that bad after all --- I mean, yes, it is a horribly low-level interface, but it is general enough to build various interesting things on top of it.  What the \"stm-thread\" branch of PyPy contains is, basically, the regular \"thread\" module in which the GIL was replaced with STM.  It gives multicore capabilities to any program based on multiple threads.  (This is so far exactly the idea same than the one being investigated for Hardware Transactional Memory.  It is roughly also what you would get if you managed to convince GCC 4.7 to compile CPython using STM.)\n\nNow while this might already be quite interesting to some people, here is how it relates to all I said previously: namely, threads are bad, and some new \"transaction\" module would be a better idea.\n\nThere is one new core functionality in the \"stm-thread\" branch: it is \"thread.atomic\", a context manager that can be used in a \"with\" statement (exact name subject to change).  In terms of the GIL, it prevents the GIL from being released in the \"with\" block.  In terms of STM, it prevents a \"transaction break\", which means that the whole \"with\" statement runs in one single transaction.  (From the Python programmer's point of view, the net effect is the same.)\n\nSo far, no ground-breaking news.  But what I missed previously is that this is enough to give multicore capabilities even to a program that is not using threads so far.  It is possible to rewrite an equivalent of the old transaction module in a few pages of pure Python, using \"thread.atomic\".  Something along the following lines: start N threads that each reads from a Queue.Queue() the next job to do, and does it in a \"with thread.atomic\" block.  The STM version of PyPy is then able to run these atomic blocks concurrently.  The key point is that the slightly delicate handling of threads should be nicely hidden inside the new \"transaction\" module, and from outside the observed behavior would be exactly as if the transactions that we schedule are run serially.\n\nThe point I kept missing was that, yes, this sounds like nonsense, because it seems that we create N threads just to serialize their work again in \"thread.atomic\" sections.  In fact this would be nonsense in any model that would \"just\" remove the GIL to let multiple threads run concurrently without crashing.  Indeed, you have multiple threads, but their atomic blocks would be again a sort of GIL: only one of them would run at a time.  And this is indeed the simple model of execution that you get even with STM --- but not the model of performance.  The performance with STM scales with the number of cores, as long as there is enough non-conflicting work to do.\n\nSo in summary the complete circle back to the starting point is that threads might be a good low-level model.  It mends itself naturally to, say, a kind of program in which the main thread polls file descriptors using select() or the Linux epoll(), and the work received is split along N other threads --- which is the kind of program you would naturally write in other languages that don't have a GIL, say Java.  The other threads can then use \"thread.atomic\" blocks to protect sections of their work.  The traditional Transactional Memory point of view is that you use such blocks to guard the short sections of code that communicate with other threads or modify global state, but nothing prevents you from using much larger sections: you should be able to scale them up to the size of a native \"unit of work\", so that every unit is naturally atomic.  And then it's only a matter of design: you can tweak an existing module that does the thread pooling to add one \"with thread.atomic\"; or do it yourself from scratch; or (if the design is compatible enough) just plug in the proposed pure-Python \"transaction\" module.  Or if you feel like it you can even use threads directly (but keep in mind that using threads too explicitly is not a composable abstraction, whereas higher-level designs typically are).\n\nAt the end of the day, you can write or reuse programs whose global structure you are already familiar with, for example with a thread pool (that can be hidden in a library if you prefer), or any other structure with or without explicit threads.  But you can do so without all the mess that comes with threads like locks and deadlocks.  From that angle it is really similar to Garbage Collection: e.g. the Boehm GC (now used by GCC itself) lets you write C code like you are used to, but forgeting all you had to learn about careful explicit memory management.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/05/stm-update-back-to-threads-6622746581767639355.html"
    },
    {
      "title": "STM update (and thanks everybody)",
      "text": "A short update on the Software Transactional Memory (STM) side.  Let me remind you that the work is to add STM internally into PyPy, with the goal of letting the user's programs run on multiple cores after a minor adaptation.  (The goal is not to expose STM to the user's program.)  I will soon write some official documentation that explains in more details exactly what you get.  For now you can read the previous blog posts, and you can also find technical details in the call for donation itself; or directly look at how I adapted the examples linked to later in this post.I have now reached the point where the basics seem to work.  There is no integration with the JIT so far; moreover the integration with the Garbage Collection subsystem is not finished right now, but at least it is \"not crashing in my simple tests and not leaking memory too quickly\". (It means that it is never calling __del__ so far, although it releases memory; and when entering transactional mode or when going to the next transaction, all live objects become immortal.  This should still let most not-too-long-running programs work.)If you want to play with it, you can download this binary (you need to put it in a place with the paths lib-python and lib_pypy, for example inside the main directory from a regular nightly tarball or from a full checkout). This version was compiled for Linux x86 32-bit from the stm-gc branch on the 25th of April.  It runs e.g. the modified version of richards. This branch could also be translated for Linux x86-64, but not for other OSes nor other CPUs for now.The resulting pypy-stm exposes the same interface as the pure Python transaction module, which is an emulator (running on CPython or any version of PyPy) which can be used to play around and prepare your programs.  See the comments in there.  A difference is that the real pypy-stm doesn't support epoll right now, so it cannot be used yet to play with a branch of Twisted that was already adapted (thanks Jean-Paul Calderone); but that's coming soon.  For now you can use it to get multi-core usage on purely computational programs.I did for example adapt PyPy's own translate.py: see the tweak in rpython/rtyper.py.  Lines 273-281 are all that I needed to add, and they are mostly a \"simplification and parallelization\" of the lines above.  There are a few more places in the whole translate.py that could be similarly modified, but overall it is just that: a few places. I did not measure performance, but I checked that it is capable of using multiple cores in the RTyping step of translation, with --- as expected --- some still-reasonable number of conflicts, particularly at the beginning when shared data structures are still being built.On a few smaller, more regular examples like richards, I did measure the performance.  It is not great, even taking into account that it has no JIT so far.  Running pypy-stm with one thread is roughly 5 times slower than running a regular PyPy with no JIT (it used to be better in previous versions, but they didn't have any GC; nevertheless, I need to investigate).  However, it does seem to scale.  At least, it scales roughly as expected on my 2-real-cores, 4-hyperthreaded-cores laptop (i.e. for N between 1 and 4, the N-threaded pypy-stm performs similarly to N independent pypy-stm's running one thread each).And finally......a big thank you to everyone who contributed some money to support this!  As you see on the PyPy site, we got more than 6700$ so far in only 5 or 6 weeks.  Thanks to that, my contract started last Monday, and I am now paid a small salary via the Software Freedom Conservancy (thanks Bradley M. Kuhn for organizational support from the SFC). Again, thank you everybody!UPDATE: The performance regression was due to disabling an optimization, the method cache, which caused non-deterministic results --- the performance could vary from simple to double.  Today, as a workaround, I made the method cache transaction-local for now; it is only effective for transactions that run for long enough (maybe 0.1ms or 1ms), but at least it is there in this situation.  In the version of richards presented above, the transactions are too short to make a difference (around 0.015ms).",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/04/stm-update-and-thanks-everybody-6071745734932940294.html"
    },
    {
      "title": "NumPy on PyPy progress report",
      "text": "Hello.\nA lot of things happened in March, like pycon. I was also busy doing other\nthings (pictured), so apologies for the late numpy status update.\nHowever, a lot of things have happened and numpy continues to be one of the\nmain points of entry for hacking on PyPy. Apologies to all the people whose\npatches I don't review in timely manner, but seriously, you do a lot of\nwork.\nThis list of changes is definitely not exhaustive, and I might be forgetting\nimportant contributions. In a loose order:\n\nMatti Picus made out parameter work for a lot of (but not all)\nfunctions.\n\nWe merged record dtypes support. The only missing dtypes left are complex\n(important), datetime (less important) and object (which will probably\nnever be implemented because it makes very little sense and is a mess with moving GCs).\n\nTaavi Burns and others implemented lots of details, including lots of ufuncs.\nOn the completely unscientific measure of \"implemented functions\" on\nnumpypy status page, we're close to 50% of numpy working. In reality\nit might be more or less, but after complex dtypes we're getting very close\nto running real programs.\n\nBool indexing of arrays of the same size should work, leaving only\narrays-of-ints indexing as the last missing element of fancy indexing.\n\nI did some very early experiments on SSE. This work is seriously\npreliminary - in fact the only implemented operation is addition of\nfloat single-dimension numpy arrays. However, results are encouraging,\ngiven that our assembler generator is far from ideal:\n\n\n\n\n\n\n\n\n\n\n\u00a0\nNumpy\n\nPyPy SSE\n\nPyPy\n\nGCC non-looped\n\nGCC looped\n\n\na+b\n\n0.6s\n\n0.3s\n\n0.4s\n\n0.3s\n\n0.25s\n\n\na+b+c\n\n1.9s\n\n0.35s\n\n0.5s\n\n0.7s\n\n0.32s\n\n\na+b+c+d+e\n\n3.2s\n\n0.36s\n\n0.8s\n\n1.7s\n\n0.51s\n\n\n\n\nThe benchmark repo is available. GCC was run with -O3, no further\noptions specified. PyPy was run with default options, the SSE branch is under\nbackend-vector-ops, but it's not working completely yet.\nOne might argue that C and Python is not the same code - indeed it is not.\nIt just shows some possible approach to writing numeric code.\n\n\nNext step would be to just continue implementing missing features such as\n\nspecialised arrays i.e. masked arrays and matrixes\ncore modules such as fft, linalg, random.\nnumpy's testing framework\n\nThe future is hard to predict, but we're not far off!\nCheers,fijal\n\nUPDATE:Indeed, string and unicode dtypes are not supported yet. They're as important as complex dtype",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/04/numpy-on-pypy-progress-report-6048076549081013253.html"
    },
    {
      "title": "PyCon 2012 wrap up",
      "text": "So, PyCon happened. This was the biggest PyCon ever and probably the biggest\ngathering of Python hackers ever.\nFrom the PyPy perspective, a lot at PyCon was about PyPy. Listing things:\n\nDavid Beazley presented an excellent keynote describing his experience\ndiving head-first into PyPy and at least partly failing. He, however, did\nnot fail to explain bits and pieces about PyPy's architecture.\nVideo is available.\nWe gave tons of talks, including the tutorial, why pypy by example\nand pypy's JIT architecture\nWe had a giant influx of new commiters, easily doubling the amount of pull\nrequests ever created for PyPy. The main topics for newcomers were numpy and\npy3k, disproving what David said about PyPy being too hard to dive into ;)\nGuido argued in his keynote that Python is not too slow. In the meantime,\nwe're trying to prove him correct :-)\n\nWe would like to thank everyone who talked to us, shared ideas and especially\nthose who participated in sprints - we're always happy to welcome newcomers!\nI'm sure there are tons of things I forgot, but thank you all!\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/04/pycon-2012-wrap-up-559575896040055505.html"
    },
    {
      "title": "Py3k status update #3",
      "text": "This is the third status update about my work on the py3k branch, which I can work on thanks to all of the people who donated to the py3k proposal.\n\nA lot of work has been done during the last month: as usual, the list of changes is too big to be reported in a detalied way, so this is just a summary of what happened.\n\nOne of the most active areas was killing old and deprecated features. In particular, we killed support for the __cmp__ special method and its counsins, the cmp builtin function and keyword argument for list.sort() and sorted().  Killing is easy, but then you have to fix all the places which breaks because of this, including all the types which relied on __cmp__ to be comparable,, fixing all the tests which tried to order objects which are no longer ordeable now, or implementing new behavior like forbidding calling hash() on objects which implement __eq__ but not __hash__.\n\nAmong the other features, we killed lots of now-gone functions in the operator module, the builtins apply(), reduce() and buffer, and the os.* functions to deal with temporary files, which has been deprecated in favour of the new tempfile module.\n\nThe other topic which can't miss in a py3k status update is, as usual, string-vs-unicode. At this round, we fixed bugs in string formatting (in particular to teach format() to always use unicode strings) and various corner cases about when calling the (possibly overridden) __str__ method on subclasses of str. Believe me, you don't want to know the precise rules :-).\n\nOther features which we worked on and fixed tests include, but are not limited to, marshal, hashlib, zipimport, _socket and itertools, plus the habitual endless lists of tests which fail for shallow reasons such as the syntactic differences, int vs long, range() vs list(range()) etc. As a result, the number of failing tests dropped from 650 to 235: we are beginning to see the light at the end of the tunnel :-)\n\nBenjamin finished implementing Python 3 syntax. Most of it was small cleanups and tweaks to be compatible with CPython such as making True and False keywords and preventing . . . (note spaces between dots) from being parsed as Ellipsis. Larger syntax additions included keyword only arguments and function annotations.\n\nFinally, we did some RPython fixes, so that it is possible again to translate PyPy in the py3k branch. However, the resuling binary is a strange beast which mixes python 2 and python 3 semantics, so it is unusable for anything but showing friends how cool it is.\n\nI would like to underline that I was not alone in doing all this work. In particular, a lot of people joined the PyPy sprint at Pycon and worked on the branch, as you can clearly see in this activity graph. I would like to thank all who helped!\n\ncheers,\nAntonio and Benjamin",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/04/py3k-status-update-3-6975588144646689872.html"
    },
    {
      "title": "PyPy sprint in Leipzig, Germany (June 22-27)",
      "text": "The next PyPy sprint will be held --- for the first time in a while ---\nin a place where we haven't been so far: Leipzig, Germany, at the\nPython Academy's Teaching Center.  It will take place from the 22nd\nto the 27th of June 2012, before EuroPython.  Thanks to Mike M\u00fcller for\norganizing it!\nThis is a fully public sprint, everyone is welcome to join us.  All days are\nfull sprint days, so it is recommended to arrive the 21st and leave the 28th.\nTopics and goals\nOpen.  Here are some goals:\n\nnumpy: progress towards completing the numpypy module; try to\nuse it in real code\nstm: progress on Transactional Memory; try out the transaction module on real code.\njit optimizations: there are a number of optimizations we can still\ntry out or refactor.\nwork on various, more efficient data structures for Python language.\nA good example would be lazy string slicing/concatenation or more efficient\nobjects.\nany other PyPy-related topic is fine too.\n\nGrants\nFor students, we have the possibility to support some costs via PyPy\nfunds.  Additionally, we can support you applying for grants from the\nPSF and other sources.\nRegistration\nIf you'd like to come, please sign up either by announcing yourself on\npypy-dev, or by directly adding yourself to the list of people.\n(We need to have a head count for the organization.)  If you are new to\nthe project please drop a note about your interests and post any\nquestions.\nMore...\nFor more information, please see the sprint announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/04/pypy-sprint-in-leipzig-june-22-27-6450601012927549960.html"
    },
    {
      "title": "Call for donations for Software Transactional Memory",
      "text": "Hi all,\n\nThe Software Transactional Memory\ncall for donations is up.  From the proposal:\n\n\nPrevious attempts on Hardware Transactional Memory focused on parallelizing existing programs written using the thread or threading modules. However, as argued here, this may not be the most practical way to achieve real multithreading; it seems that better alternatives would offer good scalability too. Notably, Transactional Memory could benefit any event-based system that is written to dispatch events serially (Twisted-based, most GUI toolkit, Stackless, gevent, and so on). The events would internally be processed in parallel, while maintaining the illusion of serial execution, with all the corresponding benefits of safety. This should be possible with minimal changes to the event dispatchers. This approach has been described by the Automatic Mutual Exclusion work at Microsoft Research, but not been implemented anywhere (to the best of our knowledge).\n\nNote that, yes, this gives you both sides of the coin: you keep using your non-thread-based program (without worrying about locks and their drawbacks like deadlocks, races, and friends), and your programs benefit from all your cores.\n\nIn more details, a low-level built-in module will provide the basics to start transactions in parallel; but this module will be only used internally in a tweaked version of, say, a Twisted reactor. Using this reactor will be enough for your existing Twisted-based programs to actually run on multiple cores. You, as a developer of the Twisted-based program, have only to care about improving the parallelizability of your program (e.g. by splitting time-consuming transactions into several parts; the exact rules will be published in detail once they are known).\n\n\nThe point is that your program is always correct, and can be tweaked to improve performance.  This is the opposite from what explicit threads and locks give you, which is a performant program which you need to tweak to remove bugs.  Arguably, this approach is the reason for why you use Python in the first place :-)\n\nArmin",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/03/call-for-donations-for-software-8853699867109654713.html"
    },
    {
      "title": "Py3k status update #2",
      "text": "This is the second status update about my work on the py3k branch, which I can work on thanks to all of the people who donated to the py3k proposal.Since my previous status update, things have improved a lot: first of all, I fixed the syntax of many more tests, which were failing on the branch because they used constructs which are no longer valid in Python 3, such as u'' strings, the print statement or the old except Exception, e syntax.  I have to say that this work is tedious and not very rewarding, but it has to be done anyway, so that the real failures can stand up.Then, I spent most of the rest of the time by killing features which are present in Python 2 and are gone in Python 3.Some of them were easy and mechnical: for example, I removed all the function attributes such as func_code and func_closure, which has been renamed to __code__ and __closure__, and then I had to find and fix all the places which still expected the old ones.Some were trickier: I removed support for the cmp function and the __cmp__ special method, but this also meant that I had to fix a few types which relied on it to be comparable (for example, did you know that the cells contained in __closure__ are comparable?). At the same time, I also removed the old behavior which in Python 2 allows us to compare arbitrary objects with <, > & co.: in Python 3 the only comparisons allowed between incompatible types are == and !=.Speaking of old special methods, __hex__ and __oct__ are gone as well (and I didn't even know about their existence before removing them :-))But the most important breakthrough was the removal of the _file module, containing the implementation of the file type in Python 2, which is now gone since in Python 3 files are handled by the _io module.  Killing the module was not straightforward, because some of the importing logic was tightly tied to the internal implementation of files, so it needed some refactoring. Finally, I had to fix the marshal module to correctly detect text files vs. byte files.Among these things, I fixed tons of smaller issues here and there. As a result, there are many fewer failing tests than a few weeks ago.  Obviously the number itself does not mean much, because sometimes fixing a single test takes hours, and some other times by changing one line one fixes tens of tests. But at the end, seeing it dropping from 999 to 650 always is nice and rewarding :-).The road for having a pypy3k is still long, but everything is going fine so far. Stay tuned for more updates!cheers, Antonio",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/03/py3k-status-update-2-4018939509128176130.html"
    },
    {
      "title": "Py3k status update",
      "text": "Thank to all the people who donated to the py3k proposal, we managed to collect enough money to start to work on the first step.  This is a quick summary of what I did since I began working on this.\nFirst of all, many thanks to Amaury Forgeot d'Arc, who started the py3k branch months ago, and already implemented lots of features including e.g. switching to \"unicode everywhere\" and the int/long unification, making my job considerably easier :-)\nI started to work on the branch at the last Leysin sprint together with Romain Guillebert, where we worked on various syntactical changes such as extended tuple unpacking and keyword-only arguments.  Working on such features is a good way to learn about a lot of the layers which the PyPy Python interpreter is composed of, because often you have to touch the tokenizer, the parser, the ast builder, the compiler and finally the interpreter.\nThen I worked on improving our test machinery in various way, e.g. by optimizing the initialization phase of the object space created by tests, which considerably speeds up small test runs, and adding the possibility to automatically run our tests against CPython 3, to ensure that what we are not trying to fix a test which is meant to fail :-). I also setup our buildbot to run the py3k tests nightly, so that we can have an up to date overview of what is left to do.\nFinally I started to look at all the tests in the interpreter/ directory, trying to unmangle the mess of failing tests. Lots of tests were failing because of simple syntax errors (e.g., by using the no longer valid except Exception, e syntax or the old print statement), others for slightly more complex reasons like unicode vs bytes or the now gone int/long distinction.  Others were failing simply because they relied on new features, such as the new lexical exception handlers.\nTo give some numbers, at some point in january we had 1621 failing tests in the branch, while today we are under 1000 (to be exact: 999, and this is why I've waited until today to post the status update :-)).\nBefore ending this blog post, I would like to thank once again all the people who donated to PyPy, who let me to do this wonderful job.  That's all for now, I'll post more updates soon.\ncheers, Antonio",
      "tags": "pypy3",
      "url": "https://www.pypy.org/posts/2012/02/py3k-status-update-8840622949715145821.html"
    },
    {
      "title": "A Larger Example for the Flow Graph Language",
      "text": "Part 4 of Comparing Partial Evaluation to Tracing\nThis is the fourth and final blog post in a series about comparing partial evaluation and\ntracing. We've come a long way: In the first post of the series I showed an interpreter for a small flow-graph\nlanguage together with a partial evaluator it. In the second post I showed how a tracer for\nthe same language works and how it relates to both execution and to partial\nevaluation. The third post described an optimizer for traces.\nIn this final post we can compare and contrast the two different approaches of\ntracing and partial evaluation by means of an example. The programs in the flow\nchart language seen so far have been rather small, so I want to give an example\nof a larger program: an interpreter for an extremely simple bytecode\ninstruction set. I will look at how the partial evaluator deals with that\ninterpreter, and\nwhat the tracer does with it. The code for\nthat, as well as all the code of the series can be found here: https://paste.pocoo.org/show/550282/ (some small\nadditions have been made, such as a nicer way to print traces).\nA Bytecode Interpreter\nWriting programs in the flow graph language is painful, but I still want to give\nan example that is a bit more interesting than the tiny ones that we've seen so\nfar. The example is an interpreter for the bytecode of a very trivial\nregister-based language. The language has four registers, one of which is an\naccumulator on which all the actual operations are performed.\nThe opcodes of the language are:\n\njump_if_a, jumps to a target address when the accumulator is non-zero\nmov_a_r0, mov_a_r1, mov_a_r2 move the value of the accumulator to\nthe respective register\nmov_r0_a, mov_r1_a, mov_r2_a move the value of a register to\nthe accumulator\nadd_r0_to_a, add_r1_to_a, add_r2_to_a add the value of the\nregister to the accumulator\ndecr_a decrement the accumulator\nreturn_a stop the program and print the accumulator\n\nThe interpreter has a main loop that reads the opcode at the current program\ncounter, does a (lengthy) dispatch to the right bytecode via a series of if\nstatements and then executes the right opcode. Afterwards the next opcode is\ntreated equivalently.\nHere is a part of the source code in the flow graph language. As pseudocode:\n\nbytecode_loop:\n    opcode = bytecode[pc]\n    pc = pc + 1\n    c = opcode == 'jump_if_a'\n    if c goto op_jump_if_a else goto not_jump_if_a\n\n# select the right bytecode via a long series of if statements\nnot_jump_if_a:\n    c = opcode == 'mov_a_r0'\n    if y goto op_mov_a_r0 else goto not_mov_a_r0\nnot_mov_a_r0:\n    c = opcode == 'mov_a_r0'\n    if y goto op_mov_a_r1 else goto not_mov_a_r1\n...\n\n# bytecode implementations\nop_mov_a_r0:\n    r0 = a\n    goto bytecode_loop\n\nop_jump_if_a:\n    c = a == 0\n    target = bytecode[pc]\n    pc += 1\n    if c goto bytecode_loop else goto op_jump_if_a_jump\n\nop_jump_if_a_jump:\n    pc = target\n    goto bytecode_loop\n...\n\nAnd actually working, as Prolog facts (the full implementation can be found at\nthe link above):\n% bytecode dispatch loop\nblock(bytecode_loop,\n      op2(opcode, readlist, var(bytecode), var(pc),\n      op2(pc, add, var(pc), const(1),\n      op2(c, eq, var(opcode), const(jump_if_a),\n      if(c, op_jump_if_a, not_jump_if_a))))).\n\n% select the right bytecode via a long series of if statements\nblock(not_jump_if_a,\n      op2(c, eq, var(opcode), const(mov_a_r0),\n      if(c, op_mov_a_r0, not_mov_a_r0))).\nblock(not_mov_a_r0,\n      op2(c, eq, var(opcode), const(mov_a_r1),\n      if(c, op_mov_a_r1, not_mov_a_r1))).\n...\n\n% bytecode implementations\nblock(op_jump_if_a,\n      op2(c, eq, var(a), const(0),\n      op2(target, readlist, var(bytecode), var(pc),\n      op2(pc, add, var(pc), const(1),\n      if(c, bytecode_loop, op_jump_if_a_jump))))).\nblock(op_jump_if_a_jump,\n      op1(pc, same, var(target),\n      promote(bytecode, bytecode_loop))).\nblock(op_mov_a_r0,\n      op1(r0, same, var(a), jump(bytecode_loop))).\n...\n\nThe bytecode_loop block is the main dispatch loop. It reads an opcode out of the\nbytecode list at the program counter position, then has a long series of if\nstatements that compares the current opcode to the various existing opcodes.\nThe full code of the interpreter can be found under the link above.\nThe bytecodes of the interpreter don't really permit hugely complex\nprograms, but it can be used to write a program that computes the square of a\nnumber with the following program:\n\nmov_a_r0     # r0 = a\nmov_a_r1     # r1 = a\n# 2:\nmov_r0_a     # r0--\ndecr_a\nmov_a_r0\nmov_r2_a     # r2 += a\nadd_r1_to_a\nmov_a_r2\nmov_r0_a     # if r0!=0: goto 2\njump_if_a 2\nmov_r2_a     # return r2\nreturn_a\n\nPartially Evaluating the Bytecode Interpreter\nThe partial evaluator from the first blog post can be easily used to partially\nevaluate the bytecode interpreter. The static input is the bytecode for\ncomputing the square and the initial program counter value, as given above. The\ndynamic input are the content of the accumulator (the number to be squared).\nThis can be done as follows:\n?- bytecode_square(B),\nEnv = [bytecode/B, pc/0],\ndo_pe(bytecode_loop, Env, Label),\nREnv = [a/16, r0/0, r1/0, r2/0],\ninterp(jump(Label), REnv), listing(block).\n256\n:- dynamic block/2.\n\n<lots of generated code>\n\nThe code that is generated by the partial evaluation process is somewhat hard to\nread. It contains a lot of passages like this:\n...\nblock(op_return_a1, print_and_stop(var(a))).\nblock(not_decr_a1, jump(op_return_a1)).\nblock(not_add_r2_to_a2, jump(not_decr_a1)).\nblock(not_add_r1_to_a2, jump(not_add_r2_to_a2)).\nblock(not_add_r0_to_a3, jump(not_add_r1_to_a2)).\nblock(not_mov_r2_a3, jump(not_add_r0_to_a3)).\nblock(not_mov_r1_a5, jump(not_mov_r2_a3)).\nblock(not_mov_r0_a5, jump(not_mov_r1_a5)).\nblock(not_mov_a_r27, jump(not_mov_r0_a5)).\nblock(not_mov_a_r18, jump(not_mov_a_r27)).\nblock(not_mov_a_r09, jump(not_mov_a_r18)).\nblock(not_jump_if_a11, jump(not_mov_a_r09)).\nblock(bytecode_loop12, jump(not_jump_if_a11)).\nblock(op_mov_r2_a2, op1(a, same, var(r2), jump(bytecode_loop12))).\n...\n\nI.e. lots of blocks that do nothing but jump to another block, interspersed with\nsome blocks that contain an actual operation. I cleaned the output up manually\nand got something like the following (this sort of cleanup is something a good\npartial evaluation system would do itself, after partial evaluation has\noccurred):\nblock(bytecode_loop1,\n    op1(r0, same, var(a),\n    op1(r1, same, var(a),\n    op1(a, same, var(r0),\n    op2(a, sub, var(a), const(1),\n    op1(r0, same, var(a),\n    op1(a, same, var(r2),\n    op2(a, add, var(a), var(r1),\n    op1(r2, same, var(a),\n    op1(a, same, var(r0),\n    op2(c, eq, var(a), const(0),\n    if(c, bytecode_loop11, op_jump_if_a_jump1)))))))))))).\n\nblock(bytecode_loop11,\n    op1(a, same, var(r2),\n    print_and_stop(var(a))).\n\nblock(op_jump_if_a_jump1,\n    op1(a, same, var(r0),\n    op2(a, sub, var(a), const(1),\n    op1(r0, same, var(a),\n    op1(a, same, var(r2),\n    op2(a, add, var(a), var(r1),\n    op1(r2, same, var(a),\n    op1(a, same, var(r0),\n    op2(c, eq, var(a), const(0),\n    if(c, bytecode_loop11, op_jump_if_a_jump1)))))))))).\n\nWhat do we see here? The partial evaluator has generated a block bytecode_loop1,\nwhich corresponds to the initialization opcodes mov_a_r0 and mov_a_r1 together\nwith one iteration of the loop. Then it either jumps to a copy of the main loop\n(label op_jump_if_a_jump1) or to block bytecode_loop11, which prints the result\nand then stops. The residual code does exactly what the bytecode did: It\nsquares the accumulator then prints that. All the uses of the bytecode and\npc variable are gone.\nWhy did the partial evaluator produce two copies of the main loop that\nlook the same? The reason for that is that in the second copy, the additional\nstatic information target = 2 is known, where target is a variable in\nthe interpreter source that stores the jump target, for very brief periods of\ntime. This additional static information does not have any effect on the\nresidual code, so the same code is uselessly generated twice. This is an\nexample of overspecialization.\nTracing the Interpreter\nIn this section we will look at what happens if we try to trace the interpreter.\nThe naive way of doing that yields traces that are not very useful, because they\nabort after one iteration. We will look at a way of avoiding this problem. The\nproblems described in this section are at the core of the paper Tracing the\nmeta-level: PyPy's tracing JIT compiler (that paper uses a slightly more\nadvanced version of the bytecode interpreter as an example).\nTo trace the interpreter, it is useful to change the bytecode_loop block from above\nto always promote the bytecode and the pc variables, because without\nknowing them the trace produced is not really interesting. This is similar to\nmaking these variables static in the partial evaluation example above:\nblock(bytecode_loop,\n      promote(bytecode, bytecode_loop_promote_bytecode)).\nblock(bytecode_loop_promote_bytecode,\n      promote(pc, bytecode_loop_promote_pc)).\nblock(bytecode_loop_promote_pc,\n      op2(opcode, readlist, var(bytecode), var(pc),\n      op2(pc, add, var(pc), const(1),\n      op2(c, eq, var(opcode), const(0),\n      if(c, op_jump_if_a, not_jump_if_a))))).\n...\n\nThe rest of the interpreter stays unchanged.\nTo trace the interpreter we would start naively at the bytecode_loop label, because\nthat's the label in the interpreter that is jumped to most often (which a\nprofiler could establish easily). The following command can be used for that\n(this output prints traces in a slightly more readable way than in previous blog\nposts):\n?- bytecode_square(B),\n        A = 16, Env = [bytecode/B, pc/2, a/A, r0/A, r1/A, r2/0],\n        do_trace(bytecode_loop, Env).\ntrace\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,2,[],bytecode_loop_promote_pc)\n  op2(opcode,readlist,var(bytecode),var(pc))\n  op2(pc,add,var(pc),const(1))\n  op2(c,eq,var(opcode),const(jump_if_a))\n  guard_false(c,[],op_jump_if_a)\n  op2(c,eq,var(opcode),const(mov_a_r0))\n  guard_false(c,[],op_mov_a_r0)\n  op2(c,eq,var(opcode),const(mov_a_r1))\n  guard_false(c,[],op_mov_a_r1)\n  op2(c,eq,var(opcode),const(mov_a_r2))\n  guard_false(c,[],op_mov_a_r2)\n  op2(c,eq,var(opcode),const(mov_r0_a))\n  guard_true(c,[],not_mov_r0_a)\n  op1(a,same,var(r0))\n  loop\n\nopttrace\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,2,[bytecode/[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a]],bytecode_loop_promote_pc)\n  op1(a,same,var(r0))\n  op1(bytecode,same,const([mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a]))\n  op1(pc,same,const(3))\n  op1(opcode,same,const(mov_r0_a))\n  op1(c,same,const(1))\n  loop\n\n256\nB = [mov_a_r0, mov_a_r1, mov_r0_a, decr_a, mov_a_r0, mov_r2_a, add_r1_to_a, mov_a_r2, mov_r0_a|...],\nA = 16,\nEnv = [bytecode/[mov_a_r0, mov_a_r1, mov_r0_a, decr_a, mov_a_r0, mov_r2_a, add_r1_to_a|...], pc/2, a/16, r0/16, r1/16, r2/0]\n\nThese traces are very short. They start with promoting the bytecode and the\npc, followed by the execution of the opcode mov_r0_a, which is the\none at position 2 in the given bytecode. Then they increment the pc and\nloop back to the beginning. Looking at the optimized trace, it is clear that the\ntrace is essentially useless. It will run only for one iteration, because in the\nsecond iteration the pc is 3, thus the guard_value at the beginning\nwill fail.\nThis problem can be solved by tracing more than just one iteration of the\nbytecode dispatch loop, which is called meta-tracing. To get this behaviour, in\nthis simple example it is enough to start (and thus end) tracing at a different\nlabel, op_jump_if_a_jump. This label is hit when the interpreter executes a\njump_if_a bytecode and the jump is taken. In a loop on the level of the\nexecuted bytecode program there is one such jump. Thus tracing from this label,\na full loop in the bytecode program is traced, containing potentially many\niterations of the bytecode dispatch loop in the control flow graph language.\nDoing that yields the following:\n?- bytecode_square(B),\n        A = 16, Env = [bytecode/B, pc/11, a/A, r0/A, r1/A, r2/0, target/2],\n        do_trace(op_jump_if_a_jump, Env).\ntrace\n  op1(pc,same,var(target))\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop)\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,2,[],bytecode_loop_promote_pc)\n  op2(opcode,readlist,var(bytecode),var(pc))\n  op2(pc,add,var(pc),const(1))\n  op2(c,eq,var(opcode),const(jump_if_a))\n  guard_false(c,[],op_jump_if_a)\n  op2(c,eq,var(opcode),const(mov_a_r0))\n  guard_false(c,[],op_mov_a_r0)\n  op2(c,eq,var(opcode),const(mov_a_r1))\n  guard_false(c,[],op_mov_a_r1)\n  op2(c,eq,var(opcode),const(mov_a_r2))\n  guard_false(c,[],op_mov_a_r2)\n  op2(c,eq,var(opcode),const(mov_r0_a))\n  guard_true(c,[],not_mov_r0_a)\n  op1(a,same,var(r0))\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,3,[],bytecode_loop_promote_pc)\n  op2(opcode,readlist,var(bytecode),var(pc))\n  ...\n  lots of operations ommitted\n  ...\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop_promote_bytecode)\n  guard_value(pc,9,[],bytecode_loop_promote_pc)\n  op2(opcode,readlist,var(bytecode),var(pc))\n  op2(pc,add,var(pc),const(1))\n  op2(c,eq,var(opcode),const(jump_if_a))\n  guard_true(c,[],not_jump_if_a)\n  op2(c,eq,var(a),const(0))\n  op2(target,readlist,var(bytecode),var(pc))\n  op2(pc,add,var(pc),const(1))\n  guard_false(c,[],bytecode_loop)\n  loop\n\nopttrace\n  op1(pc,same,var(target))\n  guard_value(bytecode,[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],[],bytecode_loop)\n  guard_value(pc,2,[bytecode/[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a]],bytecode_loop_promote_pc)\n  op1(a,same,var(r0))\n  op2(a,sub,var(a),const(1))\n  op1(r0,same,var(a))\n  op1(a,same,var(r2))\n  op2(a,add,var(a),var(r1))\n  op1(r2,same,var(a))\n  op1(a,same,var(r0))\n  op2(c,eq,var(a),const(0))\n  guard_false(c,[bytecode/[mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a],pc/11,opcode/jump_if_a,target/2],bytecode_loop)\n  op1(bytecode,same,const([mov_a_r0,mov_a_r1,mov_r0_a,decr_a,mov_a_r0,mov_r2_a,add_r1_to_a,mov_a_r2,mov_r0_a,jump_if_a,2,mov_r2_a,return_a]))\n  op1(pc,same,const(11))\n  op1(opcode,same,const(jump_if_a))\n  op1(target,same,const(2))\n  op1(c,same,const(0))\n  loop\n\n256\nB = [mov_a_r0, mov_a_r1, mov_r0_a, decr_a, mov_a_r0, mov_r2_a, add_r1_to_a, mov_a_r2, mov_r0_a|...],\nA = 16,\nEnv = [bytecode/[mov_a_r0, mov_a_r1, mov_r0_a, decr_a, mov_a_r0, mov_r2_a, add_r1_to_a|...], pc/11, a/16, r0/16, r1/16, r2/0, target/2] .\n\nThat looks better. The trace corresponds to the interpreter running all the\nbytecodes in the loop of the squaring function in the example bytecode above.\nThe optimized code starts with\ntwo guards (checking that the bytecode is still the one for the squaring\nfunction, checking that the pc is 2) and then only does the operations\nthat actually do the computation. No bytecode dispatching is performed, thus the\ninterpretation overhead is fully removed, apart from the two guard_value\noperations at the beginning.\nMany of the assignments in the trace are superfluous, e.g. all the copying back\nand forth between registers r1, r1, r2 and accumulator a. This\ncould be easily solved by an even more intelligent optimization utilizing SSA\nform.\nConclusion About the Interpreter\nBoth partial evaluation and meta-tracing can be used to transform the example\nbytecode computing a square into a form that shows the essential computation\nthat is going on, without the interpretation overhead. The naive partial evaluator\nproduces lots of extra blocks that just jump around, which could be solved with\na post-processing step. The tracer by itself produces uselessly short traces,\nbut with a simple trick of starting the trace at a different point the results\nbecome a lot better.\nIn a real meta-tracing system, the meta-tracer would need a way for the author\nof the interpreter\nto mark which bytecode corresponds to a backward jump. It would also need better\nintegration with the interpreter to start tracing automatically, as well as\ncache the traces. Additionally, it would have to deal better with guards that fail a\nlot, attaching new traces to the failing guards. However, all that is \"just\"\nengineering on top of the ideas presented in this series of blog posts.\nHigh-Level Conclusion\nSome concluding high-level thoughts about the similarities of tracing and\npartial evaluation: Tracing and partial evaluation try to tackle a similar\nproblem, that of automatically reducing the interpreter overhead, their\napproaches are slightly different though.\nTracing is very close to normal evaluation, only keeping some extra information\nin the process. But then, the optimizer that is used in a tracer\nis again very similar in structure to a partial evaluator. The task of the\noptimizer is much simpler though, because it does not need to deal with control\nflow at all, just a linear list of operations.\nSo in a sense tracing is taking those parts of partial evaluation that work (the\n\"just evaluate those things that you can, and leave the others\") and replacing\nthe parts that don't (controlling unfolding) by a much more pragmatic mechanism.\nThat mechanism observes actual execution runs of the program to choose control\nflow paths that are typical. At the same time, the tracer's focus is on loops,\nbecause they are where most programs spend significant amounts of time.\nAnother point of view of tracing is that it is a form of partial evaluation that\nreplaces the control components of a partial evaluator with an oracle (the\nactual execution runs) that provide the information which paths to look at.\nAlready in the quite trivial interpreter here the effects of this are visible.\nThe simple partial evaluator over-specializes the loop and produces two\nidentical versions of it, that aren't different. The tracer doesn't, and it\nalso generates only code for the loop itself, not for the initialization\nopcodes.\nThat's it for this series. To those that made it, thanks for following along.\nAlso thanks to Samuele and Sven, who consistently gave me good feedback on the\nposts before I put them here.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/02/larger-example-for-flow-graph-language-6139699450091061040.html"
    },
    {
      "title": "PyPy 1.8 - business as usual",
      "text": "We're pleased to announce the 1.8 release of PyPy. As habitual this\nrelease brings a lot of bugfixes, together with performance and memory\nimprovements over the 1.7 release. The main highlight of the release\nis the introduction of list strategies which makes homogenous lists\nmore efficient both in terms of performance and memory. This release\nalso upgrades us from Python 2.7.1 compatibility to 2.7.2. Otherwise\nit's \"business as usual\" in the sense that performance improved\nroughly 10% on average since the previous release.\nyou can download the PyPy 1.8 release here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 1.8 and cpython 2.7.1 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 32/64 or\nWindows 32. Windows 64 work has been stalled, we would welcome a volunteer\nto handle that.\n\n\nHighlights\n\nList strategies. Now lists that contain only ints or only floats should\nbe as efficient as storing them in a binary-packed array. It also improves\nthe JIT performance in places that use such lists. There are also special\nstrategies for unicode and string lists.\n\nAs usual, numerous performance improvements. There are many examples\nof python constructs that now should be faster; too many to list them.\n\nBugfixes and compatibility fixes with CPython.\n\nWindows fixes.\n\nNumPy effort progress; for the exact list of things that have been done,\nconsult the numpy status page. A tentative list of things that has\nbeen done:\n\nmulti dimensional arrays\nvarious sizes of dtypes\na lot of ufuncs\na lot of other minor changes\n\nRight now the numpy module is available under both numpy and numpypy\nnames. However, because it's incomplete, you have to import numpypy first\nbefore doing any imports from numpy.\n\nNew JIT hooks that allow you to hook into the JIT process from your python\nprogram. There is a brief overview of what they offer.\n\nStandard library upgrade from 2.7.1 to 2.7.2.\n\n\n\n\nOngoing work\nAs usual, there is quite a bit of ongoing work that either didn't make it to\nthe release or is not ready yet. Highlights include:\n\nNon-x86 backends for the JIT: ARMv7 (almost ready) and PPC64 (in progress)\nSpecialized type instances - allocate instances as efficient as C structs,\nincluding type specialization\nMore numpy work\nSince the last release there was a significant breakthrough in PyPy's\nfundraising. We now have enough funds to work on first stages of numpypy\nand py3k. We would like to thank again to everyone who donated.\nIt's also probably worth noting, we're considering donations for the\nSoftware Transactional Memory project. You can read more about our plans\n\nCheers,\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/02/pypy-18-business-as-usual-7266036404915945090.html"
    },
    {
      "title": "Introductory Article About RPython",
      "text": "Laurence Tratt from King's College London has written a long and detailed introduction to the goals and significance of RPython over on his blog. Laurie has been implementing his Converge Language in RPython in the last months. He is one of the first people external to the PyPy team who have pushed a sizeable RPython-based VM quite far, adding and tuning JIT hints. The post describes some of that work and his impressions of RPython and PyPy.\n\n\n\"RPython, to my mind, is an astonishing project. It has, almost single-handedly, opened up an entirely new approach to VM implementation. As my experience shows, creating a decent RPython VM is not a huge amount of work (despite some frustrations). In short: never again do new languages need come with unusably slow VMs. That the the PyPy / RPython team have shown that these ideas scale up to a fast implementation of a large, real-world language (Python) is another feather in their cap.\"",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/02/introductionary-article-about-rpython-5386281283454207551.html"
    },
    {
      "title": "Optimizing Traces of the Flow Graph Language",
      "text": "Part 3 of Comparing Partial Evaluation to Tracing\nThis is the third blog post in a series about comparing partial evaluation and\ntracing. In the first post of the series I introduced a small flow-graph\nlanguage together with an interpreter for it. Then I showed a partial evaluator\nfor the language. In the second post of the series I showed how a tracer for\nthe same language works and how it relates to both execution and to partial\nevaluation. Then I added support for promotion to that tracer.\nIn this post I will show how to optimize the traces that are produced by the\ntracer and compare the structure of the optimizer to that of partial\nevaluation.\nThe code from this post can be found here: https://paste.pocoo.org/show/547304/\nOptimizing Traces\nIn the last post we saw how to produce a linear trace with guards by\ninterpreting a control flow graph program in a special mode. A trace always end with\na loop statement, which jumps to the beginning. The tracer is just logging\nthe operations that are done while interpreting, so the trace can contain\nsuperfluous operations. On the other hand, the trace also contains some of the\nruntime values through promotions and some decisions made on them which can be\nexploited by optimization. An example for this is the trace produced by the\npromotion example from the last post:\nop2(c,ge,var(i),const(0),\nguard_true(c,[],l_done,\nguard_value(x,5,[],b2,\nop2(x2,mul,var(x),const(2),\nop2(x3,add,var(x2),const(1),\nop2(i,sub,var(i),var(x3),\nloop))))))\n\nAfter the guard_value(x, 5, ...) operation, x is know to be 5: If\nit isn't 5, execution falls back to the interpreter. Therefore, operations\non x after the guard can be constant-folded. To do that sort of\nconstant-folding,\nan extra optimization step is needed. That optimization step walks along the\ntrace, remembers which variables are constants and what their values are using a\npartial environment. The opimizer removes operations that have only constant\narguments and leaves the others in the trace. This process is actually\nremarkably similar to partial evaluation: Some variables are known to be\nconstants, operations on only constant arguments are optimized away, the rest\nremains.\nThe code for optimizing operations looks as follows:\noptimize(op1(ResultVar, Op, Arg, Rest), PEnv, NewOp) :-\n    presolve(Arg, PEnv, RArg),\n    (RArg = const(C) ->\n        do_op(Op, C, Res),\n        write_env(PEnv, ResultVar, Res, NEnv),\n        NewOp = RestResidual\n    ;\n        remove_env(PEnv, ResultVar, NEnv),\n        NewOp = op1(ResultVar, Op, RArg, RestResidual)\n    ),\n    optimize(Rest, NEnv, RestResidual).\n\noptimize(op2(ResultVar, Op, Arg1, Arg2, Rest), PEnv, NewOp) :-\n    presolve(Arg1, PEnv, RArg1),\n    presolve(Arg2, PEnv, RArg2),\n    (RArg1 = const(C1), RArg2 = const(C2) ->\n        do_op(Op, C1, C2, Res),\n        write_env(PEnv, ResultVar, Res, NEnv),\n        NewOp = RestResidual\n    ;\n        remove_env(PEnv, ResultVar, NEnv),\n        NewOp = op2(ResultVar, Op, RArg1, RArg2, RestResidual)\n    ),\n    optimize(Rest, NEnv, RestResidual).\n\nJust like partial evaluation! It even reuses the helper functions presolve\nfrom the partial evaluator and a partial environment PEnv. When the\narguments of the operation are known constants in the partial environment, the\noperation can be executed at optimization time and removed from the trace.\nOtherwise, the operation has to stay in the output trace. The result variable\n(as in the partial evaluator) needs to be removed from the partial environment,\nbecause it was just overwritten by an unknown result.\nNow we need to deal with guards in the trace.\noptimize(guard_true(V, [], L, Rest), PEnv, NewOp) :-\n    plookup(V, PEnv, Val),\n    (Val = const(C) ->\n        NewOp = RestResidual\n    ;\n        NewOp = guard_true(V, PEnv, L, RestResidual)\n    ),\n    optimize(Rest, PEnv, RestResidual).\n\noptimize(guard_false(V, [], L, Rest), PEnv, NewOp) :-\n    plookup(V, PEnv, Val),\n    (Val = const(C) ->\n        NewOp = RestResidual,\n        NEnv = PEnv\n    ;\n        write_env(PEnv, V, 0, NEnv),\n        NewOp = guard_false(V, PEnv, L, RestResidual)\n    ),\n    optimize(Rest, NEnv, RestResidual).\n\nWhen the variable that is being guarded is actually known to be a constant, we\ncan remove the guard. Note that it is not possible that the guard of that\nconstant fails: The tracer recorded the operation while running with real\nvalues, therefore the guards have to succeed for values the optimizer\ndiscovers to be constant.\nguard_false is slightly different from guard_true: after the former we\nknow that the argument is actually 0. After guard_true we only know that\nit is not equal to zero, but not which precise value it has.\nAnother point to note in the optimization of guards is that the second argument\nof the guard operation, which was so far always just an empty list, is now\nreplaced by the partial environment PEnv. I will discuss further down why\nthis is needed.\nOptimizing guard_value is very similar, except that it really gives precise\ninformation about the variable involved:\noptimize(guard_value(V, C, [], L, Rest), PEnv, NewOp) :-\n    plookup(V, PEnv, Val),\n    (Val = const(C1) ->\n        NewOp = RestResidual,\n        NEnv = PEnv\n    ;\n        write_env(PEnv, V, C, NEnv),\n        NewOp = guard_value(V, C, PEnv, L, RestResidual)\n    ),\n    optimize(Rest, NEnv, RestResidual).\n\nThis operation is the main way how the optimizer gains constant variables that\nit then exploits to do constant-folding on later operations. This is a chief\ndifference from partial evaluation: There the optimizer knows the value of some\nvariables from the start. When optimizing traces, at the beginning the value of\nno variable is known. Knowledge about some variables is only later gained\nthrough guards.\nNow we are missing what happens with the loop statement. In principle, it is\nturned into a loop statement again. However, at the loop statement a few\nadditional operations need to be emitted. The reason is that we optimized away\noperations and thus assignments when the result value of the variable was a\nconstant. That means the involved variable still potentially has some older\nvalue. The next iteration of the loop would continue with this older value,\nwhich is obviously wrong. Therefore we need to emit some assignments before the\nloop statement, one per entry in the partial environment:\noptimize(loop, PEnv, T) :-\n    generate_assignments(PEnv, T).\n\ngenerate_assignments([], loop).\ngenerate_assignments([Var/Val | Tail], op1(Var, same, const(Val), T)) :-\n    generate_assignments(Tail, T).\n\nAs an example of how generate_assignments assignments works, let's look at\nthe following example. When the partial environment is, [x/5, y/10] the\nfollowing assignments are generated:\n?- generate_assignments([x/5, y/10], Out).\nOut = op1(x, same, const(5), op1(y, same, const(10), loop)).\n\nThat's all the code of the optimizer. While the basic structure is quite similar to partial evaluation,\nit's a lot less complex as well. What made the partial evaluator hard was that\nit needs to deal with control flow statements and with making sure that code is\nreused if the same block is partially evaluated with the same constants. Here,\nall these complexities go away. The tracer has already removed all control flow\nand replaced it with guards and one loop operation at the end. Thus, the\noptimizer can simply do one pass over the operations, removing some (with some\nextra care around the loop statement).\nWith this machinery in place, we can optimize the trace from the promotion\nexample of the last post:\n?- optimize(\n    guard_value(x,3,[],b2,\n    op2(x2,mul,var(x),const(2),\n    op2(x3,add,var(x2),const(1),\n    op2(i,sub,var(i),var(x3),\n    op2(c,ge,var(i),const(0),\n    guard_true(c,[],l_done, loop)))))),\n    [],\n    LoopOut).\nLoopOut = guard_value(x, 3, [], b2, op2(i, sub, var(i), const(7), op2(c, ge, var(i), const(0), guard_true(c, [x/3, x2/6, x3/7], l_done, op1(x, same, const(3), op1(x2, same, const(6), op1(x3, same, const(7), loop)))))))\n\nMore readably, the optimized version is:\nguard_value(x, 3, [], b2,\nop2(i, sub, var(i), const(7),\nop2(c, ge, var(i), const(0),\nguard_true(c, [x/3, x2/6, x3/7], l_done,\nop1(x, same, const(3),\nop1(x2, same, const(6),\nop1(x3, same, const(7),\nloop)))))))\n\nAs intended, the operations on x after the guard_value have all been\nremoved. However, some additional assignments (to x, x2, x3) at the end have been generated as\nwell. The assignments look superfluous, but the optimizer does not have\nenough information to easily recognize this. That can be fixed, but only at the\ncost of additional complexity. (A real system would transform the trace into\nstatic single assignment form to answer such questions.)\nResuming to the Interpreter\nWhy does the code above need to add the partial environment to\nthe guards that cannot be optimized away? The reason is related to why we needed\nto generate assignments before the loop statement. The problem is that the optimizer\nremoves assignments to variables when it knows the values of these variables.\nThat means that when switching back from running the optimized trace to the\ninterpreter, a number of variables are not updated in the environment, making\nthe execution in the interpreter incorrect.\nIn the example above, this applies to the variables x2 and x3. When the\nsecond guard fails, they have not been assigned in the optimized case.\nTherefore, the guard lists them and their (always constant) values.\nWhen switching back these assignments need to be made. Thus we need to adapt the\nresume_interp function from the last blog post as follows:\nwrite_resumevars([], Env, Env).\nwrite_resumevars([Key / Value | Rest], Env, NEnv) :-\n    write_env(Env, Key, Value, Env1),\n    write_resumevars(Rest, Env1, NEnv).\n\nresume_interp(Env, ResumeVars, L) :-\n    write_resumevars(ResumeVars, Env, NEnv),\n    block(L, Block),\n    interp(Block, NEnv).\n\nOn resuming, the ResumeVars (a former partial environment) are simply added\nback to the normal environment before going back to the interpreter.\nThe data attached to guards about what needs to be done to resume to the\ninterpreter when the guard fails is often a very complex part of a tracing\nsystem. The data can become big, yet most guards never fail. Therefore, most\nreal systems try hard to compress the attached data or try to share it between\nsubsequent guards.\nSummary\nIn this post we have shown how to optimize traces by applying a variant of the\npartial evaluation principle: Perform all the operations that have only constant\narguments, leave the others alone. However, optimizing traces is much simpler,\nbecause no control flow is involved. All the questions about control flow have\nalready been solved by the tracing component.\nIn the next and final post of the series I will show a larger example of how\ntracing and partial evaluation can be used to optimize a small bytecode\ninterpreter.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/02/optimizing-traces-of-flow-graph-4169388883059419385.html"
    },
    {
      "title": "Almost There - PyPy's ARM Backend",
      "text": "In this post I want to give an update on the status of the ARM backend for PyPy's JIT and describe some of the issues and details of the backend.\n\n\n\n\n\n\n\nCurrent Status\nIt has been a more than a year that I have been working on the ARM backend. Now it is in a shape, that we can measure meaningful numbers and also ask for some feedback.\u00a0Since the last post about the backend we have added support floating point operations as well as for PyPy's framework GC's. Another area of work was to keep up with the constant improvements done in the main development branch, such as out-of-line guards, labels, etc.\u00a0It has been possible for about a year to cross-translate the PyPy Python interpreter and other interpreters such as Pyrolog, with a JIT, to run benchmarks on ARM. Up until now there remained some hard to track bugs that would cause the interpreter to crash with a segmentation fault in certain cases when running with the JIT on ARM. Lately it was possible to run all benchmarks without problems, but when running the translation toolchain itself it would crash.\u00a0During the last PyPy sprint in Leysin Armin and I managed to fix several of these hard to track bugs in the ARM backend with the result that, it is now possible to run the PyPy translator on ARM itself (at least unless until it runs out of memory), which is a kind of litmus test for the backend itself and used to crash before. Just to point it out, we are not able to complete a PyPy translation on ARM, because on the hardware we have currently available there is not enough memory. But up to the point we run out of memory the JIT does not hit any issues.\n\n\n\n\n\n\n\nImplementation Details\nThe hardware requirements to run the JIT on ARM follow those for Ubuntu on ARM which targets ARMv7 with a VFP unit running in little endian mode. The JIT can be translated without floating point support, but there might be a few places that need to be fixed to fully work in this setting.\u00a0We are targeting the ARM instruction set, because at least at the time we decided to use it seemed to be the best choice in terms of speed while having some size overhead compared to the Thumb2 instruction set. It appears that the Thumb2 instruction set should give comparable speed with better code density but has a few restriction on the number of registers available and the use of conditional execution. Also the implementation is a bit easier using a fixed width instruction set and we can use the full set of registers in the generated code when using the ARM instruction set.\n\n\n\n\n\n\n\nThe calling convention on ARM\nThe calling convention on ARM uses 4 of the general purpose registers to pass arguments to functions, further arguments are passed on the stack. The presence of a floating point unit is not required for ARM cores, for this reason there are different ways of handling floats with relation to the calling convention. There is a so called soft-float calling convention that is independent of the presence of a floating point unit. For this calling convention floating point arguments to functions are stored in the general purpose registers and on the stack. Passing floats around this way works with software and hardware floating point implementations. But in presence of a floating point unit it produces some overhead, because floating point numbers need to be moved from the floating point unit to the core registers to do a call and moved back to the floating point registers by the callee. The alternative calling convention is the so-called hard-float calling convention which requires the presence of a floating point unit but has the advantage of getting rid of the overhead of moving floating point values around when performing a call. Although it would be better in the long term to support the hard-float calling convention, we need to be able to interoperate with external code compiled for the operating system we are running on. For this reason at the moment we only support the soft-float to interoperate with external code.\u00a0We implemented and tested the backend on a BeagleBoard-xM with a Cortex-A8 processor running Ubuntu 11.04 for ARM.\n\n\n\n\n\n\n\nTranslating for ARM\nThe toolchain used to translate PyPy currently is based on a Scratchbox2. Scratchbox2 is a cross-compiling environment. Development had stopped for a while, but it seems to have revived again. We run a 32-bit Python interpreter on the host system and perform all calls to the compiler using a Scratchbox2 based environment. A description on how to setup the cross translation toolchain can be found here.\n\n\n\n\n\n\n\nResults\nThe current results on ARM, as shown in the graph below, show that the JIT currently gives a speedup of about 3.5 times compared to CPython on ARM. The benchmarks were run on the before mentioned BeagleBoard-xM with a 1GHz ARM Cortex-A8 processor and 512MB of memory. The operating system on the board is Ubuntu 11.04 for ARM. We measured the PyPy interpreter with the JIT enabled and disabled comparing each to CPython Python 2.7.1+ (r271:86832) for ARM. The graph shows the speedup or slowdown of both PyPy versions for the different benchmarks from our benchmark suite normalized to the runtime of CPython. The data used for the graph can be seen below.\n\n\n\nThe speedup is less than the speedup of 5.2 times we currently  get on x86 on our own benchmark suite (see https://speed.pypy.org for details). There are several possible reasons for this. Comparing the results for the interpreter without the JIT on ARM and x86 suggests that the interpreter generated by PyPy, without the JIT, has a worse performance when compared to CPython that it does on x86. Also it is quite possible that the code we are generating with the JIT is not yet optimal. Also there are some architectural constraints produce some overhead. One of these differences is the handling of constants, most ARM instructions only support 8 bit (that can be shifted) immediate values, larger constants need to be loaded into a register, something that is not necessary on x86.\n\n\nBenchmarkPyPy JITPyPy no JIT\nai0.4844397800473.72756749625\nchaos0.08072916919342.2908692212\ncrypto_pyaes0.07111148322453.30112318509\ndjango0.09777432455192.56779947601\nfannkuch0.2104237356982.49163632938\nfloat0.1542753346752.12053281495\ngo0.3304830342025.84628320479\nhtml5lib0.6292643898623.60333138526\nmeteor-contest0.9847474269122.93838610037\nnbody_modified0.2369695930821.40027234936\npyflate-fast0.3674471918072.72472422146\nraytrace-simple0.02905274614371.97270054339\nrichards0.0345755735533.29767342015\nslowspitfire0.7866425519083.7397367403\nspambayes0.6603243794563.29059863111\nspectral-norm0.0636107837314.01788986233\nspitfire0.436171311652.72050579076\nspitfire_cstringio0.2555387021341.7418593111\ntelco0.1029189304133.86388866047\ntwisted_iteration0.1227239868054.33632475491\ntwisted_names2.423677971352.99878698076\ntwisted_pb1.309918374314.48877805486\ntwisted_tcp0.9270333540552.8161624665\nwaf1.020598119321.03793427321\n\n\n\n\n\n\n\n\n\n\nThe next steps and call for help\nAlthough there probably still are some remaining issues which have not surfaced yet, the JIT backend for ARM is working. Before we can merge the backend into the main development line there are some things that we would like to do first, in particular it we are looking for a way to run the all PyPy tests to verify that things work on ARM before we can merge.\u00a0Additionally\u00a0there are some other longterm ideas. To do this we are looking for people willing to help, either by contributing to implement the open features or that can help us with hardware to test.\n\nThe incomplete list of open topics:\n\nWe are looking for a better way to translate PyPy for ARM, than the one describe above. I am not sure if there currently is hardware with enough memory to directly translate PyPy on an ARM based system, this would require between 1.5 or 2 Gig of memory. A fully QEMU based approach could also work, instead of Scratchbox2 that uses QEMU under the hood.\nTest the JIT on different hardware.\nExperiment with the JIT settings to find the optimal thresholds for ARM.\nContinuous integration: We are looking for a way to run the PyPy test suite to make sure everything works as expected on ARM, here QEMU also might provide an alternative.\nA long term plan would be to port the backend to ARMv5 ISA and improve the support for systems without a floating point unit. This would require to implement the ISA and create different code paths and improve the instruction selection depending on the target architecture.\nReview of the generated machine code the JIT generates on ARM to see if the instruction selection makes sense for ARM.\nBuild a version that runs on Android.\nImprove the tools, i.e. integrate with jitviewer.\n\nSo if you are interested or willing to help in any way contact us.",
      "tags": "arm,jit,pypy",
      "url": "https://www.pypy.org/posts/2012/02/almost-there-pypys-arm-backend_01-3216759488618774525.html"
    },
    {
      "title": "A Simple Tracer for the Flow Graph Language",
      "text": "Part 2 of Comparing Partial Evaluation to Tracing\nThis is the second blog post in a series about comparing partial evaluation and\ntracing. In the first post of the series I introduced a small flow-graph\nlanguage together with an interpreter for it. Then I showed a partial evaluator\nfor the language. In this post I will show how a tracer for the same language\nworks and how it relates to both execution and to partial evaluation.\nThe code from this post can be found here: https://paste.pocoo.org/show/543542/\nTracing Execution\nThe idea of a tracer (for the described language and also in general) is to do completely normal\ninterpretation but at the same time keep a log of all the normal operations\n(i.e. non-control-flow operations) that were performed. This continues until the\ntracer executes the code block where it started at, in which case the trace\ncorresponds to a closed loop. Then tracing stops and the last operation is\nreplaced by a jump to the start. After tracing has ended, the trace can be\nexecuted, optionally optimizing it before that.\nTo write a tracer, we start from the rules of the interpreter, rename the\npredicate to trace and add some extra arguments. Thus, the following rules\nin the interpreter:\ninterp(op1(ResultVar, Op, Arg, Rest), Env) :-\n    resolve(Arg, Env, RArg),\n    do_op(Op, RArg, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    interp(Rest, NEnv).\n\ninterp(op2(ResultVar, Op, Arg1, Arg2, Rest), Env) :-\n    resolve(Arg1, Env, RArg1),\n    resolve(Arg2, Env, RArg2),\n    do_op(Op, RArg1, RArg2, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    interp(Rest, NEnv).\n\nbecome the following rules in the tracer:\ntrace(op1(ResultVar, Op, Arg, Rest), Env, op1(ResultVar, Op, Arg, T), TraceAnchor) :-\n    resolve(Arg, Env, RArg),\n    do_op(Op, RArg, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    trace(Rest, NEnv, T, TraceAnchor).\n\ntrace(op2(ResultVar, Op, Arg1, Arg2, Rest), Env, op2(ResultVar, Op, Arg1, Arg2, T), TraceAnchor) :-\n    resolve(Arg1, Env, RArg1),\n    resolve(Arg2, Env, RArg2),\n    do_op(Op, RArg1, RArg2, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    trace(Rest, NEnv, T, TraceAnchor).\n\nNote how the bodies of the trace rules correspond exactly to the bodies of\nthe interp rules, the only difference is the recursive call to trace.\nThe meaning of the arguments of trace is as follows: The first and second argument are\nthe operation currently executed and the environment,\nlike in the interpreter. The argument\nafter that is an output argument that collects the currently traced operation,\nin the example above it is exactly like the operation that was executed.\nTraceAnchor is additional information about the trace that is being built\nright now, most of the time it is just handed on to the recursive call of\ntrace. We will see later what it contains.\nThe rule for print_and_stop is very simple, as execution (and therefore also\ntracing) simply stops there:\ntrace(print_and_stop(V), Env, print_and_stop(V), _) :-\n    resolve(V, Env, Val),\n    print(Val), nl.\n\nLeft are the rules for the control operations jump and if. A trace\nlinearizes one execution path, it contains no jumps. However, when a jump to the\nstarting label is reached, tracing should stop. Therefore, the implementation of\njump contains two cases:\ntrace(jump(L), Env, T, TraceAnchor) :-\n    (TraceAnchor = traceanchor(L, FullTrace) ->\n        T = loop,\n        write(trace), nl, write(FullTrace), nl,\n        do_optimize(FullTrace, OptTrace),\n        write(opttrace), nl, write(OptTrace), nl,\n        runtrace(OptTrace, Env, OptTrace)\n    ;\n        block(L, Block),\n        trace(Block, Env, T, TraceAnchor)\n    ).\n\nLet's disect this code in small increments. First, we see what TraceAnchor\nis. It is a term of the form\ntraceanchor(StartLabel, FullTrace). StartLabel is a label in the program\nwhere tracing started (and where it should end as well, when the loop is\nclosed). The argument FullTrace is an accumulator which contains the full\ntrace that is being built right now.\nThe condition at the start of the rule checks whether the taget-label L is\nthe same as the one stored in the trace anchor. If that is the case, we can stop\ntracing. The rest of the trace T is assigned the operation loop, which\njumps back to the beginning of the trace. Afterwards we print and optimize the\ntrace, then run it, using the FullTrace part of the traceanchor.\nIf the label we jump to is not the StartLabel we simply continue tracing\nwithout recording any operation. This part of the rule is again extremely\nsimilar to the interpretation of jump.\nFor now, we will not use any interesting optimizations, just return the\nunoptimized trace unchanged:\ndo_optimize(FullTrace, FullTrace).\n\nThe missing operation now is if. An if statement needs special treatment,\nbecause it is a way where control flow can diverge from the trace. The trace is\nlinear, therefore it can only record one of the two possible paths. When\nexecuting the trace it is possible for the other path to be taken. Therefore\nwe need to make sure that the same conditions that were true or false during\ntracing are still true or false during the execution of the trace. This is done\nwith a guard operation, which checks for this condition. The following rule\nimplements it:\ntrace(if(V, L1, L2), Env, T, TraceAnchor) :-\n    lookup(V, Env, Val),\n    (Val == 0 ->\n        L = L2, T = guard_false(V, [], L1, NT)\n    ;\n        L = L1, T = guard_true(V, [], L2, NT)\n    ),\n    trace(jump(L), Env, NT, TraceAnchor).\n\nIt is very similar to the interp rule of if. The rule inserts a\nguard_true into the case, if the condition is true, and a guard_false if\nthe condition is false. The arguments of the guard are: The variable that is\nbeing guarded, an empty list (the reason for that will be explained in a later\npost), the label where execution needs to continue when the guard fails and the\nrest of the trace.\nLet's also add a small helper predicate that can be used to conveniently start\ntracing:\ndo_trace(L, Env) :-\n    block(L, StartBlock),\n    trace(StartBlock, Env, ProducedTrace, traceanchor(L, ProducedTrace)).\n\nThe predicate takes a label and an environment and executes the label with the\ngiven environment by first producing a trace, then executing the trace and\neventually jumping back to interpretation, if a guard fails. It does this by\nreading the code at label L with the block statement, and then calling\ntrace with an unbound variable ProducedTrace to hold the trace and a trace\nanchor that contains the label where tracing started and the produced trace\nvariable.\nWith that predicate and the trace so far we can already trace the power\nimplementation from the last blog post, just not execute the trace (which we\nwill do in the next section):\n?- do_trace(power_rec, [res/1, x/10, y/20]).\ntrace\nop2(res,mul,var(res),var(x),op2(y,sub,var(y),const(1),guard_true(y,[],power_done,loop)))\nopttrace\nop2(res,mul,var(res),var(x),op2(y,sub,var(y),const(1),guard_true(y,[],power_done,loop)))\n...\n\nThe computed trace is:\n\nop2(res,mul,var(res),var(x),\nop2(y,sub,var(y),const(1),\nguard_true(y,[],power_done,\nloop)))\n\nwhich is exactly the content of the loop from power_rec. Note how the if\nis turned into a guard_true which jumps to power_done if the guard\nfails.\nA real tracing system would need a way for the tracer to get started, e.g. by\ndoing profiling in an interpreter and starting the tracer for labels that are\njumped to often. Also, traces for the same label are usually cached in some way.\nThese details are left out in this simple model.\nExecuting Traces\nIn a real tracing system, the traces would be turned into machine code and\nexecuted by the CPU. In our small model, we will simply write another\ninterpreter for them. This interpreter is very simple and looks again very\nsimilar to interp.\nruntrace(op1(ResultVar, Op, Arg, Rest), Env, TraceFromStart) :-\n    resolve(Arg, Env, RArg),\n    do_op(Op, RArg, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    runtrace(Rest, NEnv, TraceFromStart).\n\nruntrace(op2(ResultVar, Op, Arg1, Arg2, Rest), Env, TraceFromStart) :-\n    resolve(Arg1, Env, RArg1),\n    resolve(Arg2, Env, RArg2),\n    do_op(Op, RArg1, RArg2, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    runtrace(Rest, NEnv, TraceFromStart).\n\nThese rules are completely equivalent to the interp rules for op1 and\nop2. runtrace needs an extra argument, TraceFromStart, which is\nalways just handed over to the recursive call of runtrace.\nWhen the end of the trace is reached and the loop statement is encountered,\nwe simply start from the beginning:\nruntrace(loop, Env, TraceFromStart) :-\n    runtrace(TraceFromStart, Env, TraceFromStart).\n\nThe remaining question is what to do when encountering guards. In that case the\nguard condition needs to be checked. If the guard succeeds, executing the trace can\ncontinue. Otherwise the trace is aborted and the interpreter resumes execution:\nruntrace(guard_true(V, ResumeVars, L, Rest), Env, TraceFromStart) :-\n    lookup(V, Env, Val),\n    (Val == 0 ->\n        resume_interp(Env, ResumeVars, L)\n    ;\n        runtrace(Rest, Env, TraceFromStart)\n    ).\n\nruntrace(guard_false(V, ResumeVars, L, Rest), Env, TraceFromStart) :-\n    lookup(V, Env, Val),\n    (Val == 0 ->\n        runtrace(Rest, Env, TraceFromStart)\n    ;\n        resume_interp(Env, ResumeVars, L)\n    ).\n\n\nresume_interp(Env, [], L) :-\n    block(L, Block),\n    interp(Block, Env).\n\nNote how the execution is handed over to the interpreter at the label that was\nencoded as the third argument in the guard operation.\nWhat the ResumeVars are for we will see in a later post. For now we assume\nthat it is always an empty list.\nWith this interpreter for traces we can now trace and then execute the example:\n:- do_trace(power_rec, [res/1, x/10, y/20]).\ntrace\nop2(res,mul,var(res),var(x),op2(y,sub,var(y),const(1),guard_true(y,[],power_done,loop)))\nopttrace\nop2(res,mul,var(res),var(x),op2(y,sub,var(y),const(1),guard_true(y,[],power_done,loop)))\n100000000000000000000\n\nOf course this is example is not very exciting, because the trace looks more or less exactly\nlike the original code as well. There will be more exciting examples in a later\npost.\nExtension: Promotion\nAs it is, the tracer does not actually add much to the interpreter. It\nlinearizes control flow, but nothing deeply advanced happens. In this section I\nwill add a crucial but simple to implement extension to the control flow language that allows the tracer\nto do more interesting things. This extension is called promotion.\nPromotion is basically a hint that the programmer can add to her control flow\ngraph program. A promotion is an operation promote(V, L) that takes a\nvariable V and a label L. When the interpreter runs this statement, it\nsimply jumps to the label L and ignores the variable:\ninterp(promote(_, L), Env) :-\n    interp(jump(L), Env).\n\nHowever, the tracer does something much more interesting. For the tracer, the\npromote statement is a hint that it would be very useful to know the value\nof V and that the rest of the trace should keep that value as a constant.\nTherefore, when the tracer encounters a promotion, it inserts a special kind of\nguard called guard_value\ntrace(promote(V, L), Env, guard_value(V, Val, [], L, T), TraceAnchor) :-\n    lookup(V, Env, Val),\n    trace(jump(L), Env, T, TraceAnchor).\n\nThe guard_value is an interesting operation, because it freezes the current\nvalue FVal of variable V into the trace. When the trace is executed, the\nguard checks that the current value of the variable and the frozen value are the\nsame. If yes, execution continues, if not, the trace is aborted:\nruntrace(guard_value(V, FVal, ResumeVars, L, Rest), Env, TraceFromStart) :-\n    lookup(V, Env, Val),\n    (Val == FVal ->\n        runtrace(Rest, Env, TraceFromStart)\n    ;\n        resume_interp(Env, ResumeVars, L)\n    ).\n\nWhat can this operation be used for? It's a way to communicate to the tracer\nthat variable V is not changing very often and that it is therefore useful\nto freeze the current value into the trace. This can be done even without\nknowing the value of V in advance.\nLet's look at a (slightly contrived) example:\n\nl:\n    c = i >= 0\n    if c goto b else goto l_done\n\nl_done:\n    print_and_stop(var(i))\n\nb:\n    promote(x, b2)\n\nb2:\n    x2 = x * 2\n    x3 = x2 + 1\n    i = i - x3\n    goto l\n\nEncoded in Prolog syntax:\nblock(l, op2(c, ge, var(i), const(0),\n         if(c, b, l_done))).\nblock(l_done, print_and_stop(var(i))).\n\nblock(b, promote(x, b2)).\nblock(b2, op2(x2, mul, var(x), const(2),\n          op2(x3, add, var(x2), const(1),\n          op2(i, sub, var(i), var(x3),\n          jump(l))))).\n\nThis is a simple loop that counts down in steps of x * 2 + 1, whatever x\nmight be, until i >= 0 is no longer true. Assuming that x doesn't change\noften, it is worth to promote it to be able to constant-fold x * 2 + 1 to\nnot have to redo it every iteration. This is done with the promotion of x\n(of course optimizing this loop with loop invariant code motion would work as\nwell, because x doesn't actually change during the loop).\nTo trace this, we can run the following query:\n?- do_trace(b, [i/100, x/5]).\ntrace\nguard_value(x,5,[],b2,op2(x2,mul,var(x),const(2),op2(x3,add,var(x2),const(1),op2(i,sub,var(i),var(x3),op2(c,ge,var(i),const(0),guard_true(c,[],l_done,loop))))))\nopttrace\nguard_value(x,5,[],b2,op2(x2,mul,var(x),const(2),op2(x3,add,var(x2),const(1),op2(i,sub,var(i),var(x3),op2(c,ge,var(i),const(0),guard_true(c,[],l_done,loop))))))\n-10\n\nWriting the trace in a more readable way:\nguard_value(x,3,[],b2,\nop2(x2,mul,var(x),const(2),\nop2(x3,add,var(x2),const(1),\nop2(i,sub,var(i),var(x3),\nop2(c,ge,var(i),const(0),\nguard_true(c,[],l_done,\nloop))))))\n\nAfter the guard_value the operations performed on x could be\nconstant-folded away, because the guard ensures that x is 5 before\nexecution continues. To actually do the constant-folding we would need some\noptimization component that optimizes traces. This will be done in the next blog\npost.\nIn this section I mostly talked about how promotion is realized in the tracer,\nnot what and how to use to use it for. Promotion is one of the most important\ningredients that's responsible for the success of PyPy's tracing approach. How\nthis works is discussed in detail in the paper \"Runtime feedback in a\nmeta-tracing JIT for efficient dynamic languages\".\nConclusion\nIn this blog post we have seen a very minimalistic tracer and an interpreter for\nthe produced traces. The tracer is very much like the original interpreter, it\njust also keeps track of which operations were executed, in addition to\nexecuting the program. Tracing stops when a loop is closed, then the trace can\nbe optimized and run. Running a trace continues until a failing guard is hit. At\nthat point, execution goes back to the normal interpreter (and stays there, in\nthis very simple implementation).\nI also presented an extension of tracing that makes it possible to add a hint\ncalled promote to the original program that tells the tracer to feed back a\nruntime value into the trace and freeze it there. This extension would be\nimpossible to do in the partial evaluator from the last post, because partial\nevaluation is done strictly before run time, so if a variable isn't already\nknown, its likely runtime value cannot be found out.\nIn the next post I will show how to optimize traces before executing them and\nhow the optimizer for traces is related to partial evaluation.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/01/simple-tracer-for-flow-graph-language-6930951890987229484.html"
    },
    {
      "title": "NumPyPy status update",
      "text": "Hello.\nThis is just a quick status update on the NumPy in PyPy project that very\nrecently became my day job. I should give my thanks once again to Getco,\nNate Lawson and other contributors who donated above $40000 towards the goal.\nRecently we (Alex Gaynor, Matti Picus and me) implemented a few interesting things\nthat a lot of people use:\n\nmore ufuncs\nmost ufuncs now accept the axis parameter (except all and any)\nfixed string representation of arrays, now it's identical to numpy (uses\npretty much the same code)\nndarray.flat should be working correctly\nndarray.flatten, ndarray.ravel, ndarray.take\nindexing arrays by boolean arrays of the same size\nand various bugfixes.\n\nWe would also like to introduce the nightly report of numpy status. This\nis an automated tool that does package introspection. While it gives some\nsort of idea how much of numpy is implemented, it's not by far the authority.\nYour tests should be the authority. It won't report whether functions\nsupport all kinds of parameters (for example masked arrays and out parameter\nare completely unsupported) or that functions work at all. We also\nreserve the right to incorporate jokes in that website, so don't treat it\nthat seriously overall :-)\nThanks, and stay tuned.  We hope to post here regular updates on the\nprogress.\nCheers,\nfijal & the PyPy team",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/01/numpypy-status-update-6434340612277938795.html"
    },
    {
      "title": "Py3k and Numpy First Stage: Thanks to all who Gave",
      "text": "Last year was quite successful for PyPy fundraising through the Software Freedom Conservancy, and Conservancy and PyPy are very excited to announce that enough was raised to begin the first stages on the Py3k and Numpy grant proposals.As of the end of 2011, 135 different individuals gave to the Py3k campaign, and 114 to the Numpy campaign.  We thank each of you who donated to help make this work possible.  Meanwhile, if you haven't given to support these projects, we do hope you'll give generously now to help fund their second stages later this year!We're also particularly excited that a few donors gave particularly large donations to support this work; those big donations really filled in the gap to help us get started!Specifically, we're pleased to announce that Google  donated $35000 towards implementing Python 3 in PyPy.   Google's general support of the Python community is well known, and their specific support of our grant proposal is much appreciated.Meanwhile, Numpy was supported in part by contributions from Nate Lawson, Cantab Capital Partners, and Getco, as well as more than a hundred other contributors.With these donations combined with many others, we're now starting work on both projects.  This week, the Conservancy signed contracts with Antonio Cuni and Benjamin Peterson to work towards the Stage 1.1 goals in Py3k proposal (and is negotiating for another contractor as well), and with Maciej Fija\u0142kowski to work towards the Stage 1 goals in the Numpy proposal.In 2012, PyPy will continue regular sprint meetings, at which Py3K and Numpy efforts will certainly have a place.  We have some limited funds to fund travels of contributors to those meetings.We're very thankful for all who donated so far to support these efforts, and we hope that now that work has begun, even more donors will come forward to help us finish the job.  In the meantime, watch for the commits showing up from these developers and other contributors in the PyPy repositories!Cheers, The PyPy Team",
      "tags": "numpy,pypy3",
      "url": "https://www.pypy.org/posts/2012/01/py3k-and-numpy-first-stage-thanks-to-3008917396290059758.html"
    },
    {
      "title": "Comparing Partial Evaluation and Tracing, Part 1",
      "text": "As part of writing my PhD I am currently thinking about the relationship\nbetween PyPy's meta-tracing approach with various previous ideas to\nautomatically get a (JIT-)compiler from only an interpreter of a language. One\nof the most-researched ideas along these lines is that of partial evaluation.\nPartial evaluation has basically the same goals as PyPy when it comes to\ncompilers: Write an interpreter, and get a compiler for free. The methods for\nreaching that goal are a bit different. In this series of blog posts, I am\ntrying to explore the similarities and differences of partial evaluation and\nPyPy's meta-tracing.\nA Flowgraph Language\nTo be able to clearly understand what \"partial evaluation\" is and what\n\"meta-tracing\" is I will show an \"executable model\" of both. To that end, I am\ndefining a small imperative language and will then show what a partial evaluator\nand a tracer for that language look like. All this code will be\nimplemented in Prolog. (Any pattern-matching functional language would do, but I\nhappen to know Prolog best. Backtracking is not used, so you can read things\nsimply as functional programs.) In this post I will start with\nthe definition of the language, and a partial evaluator for it. The code\nwritten in this blog post can be found fully here: https://paste.pocoo.org/show/541004/\nThe language is conceptionally similar to PyPy's flow graphs, but a bit more\nrestricted. It does not have function calls, only labelled basic blocks\nthat consist of a series of linearly executed operations, followed by a\nconditional or an unconditional jump. Every operation is assigning a value to a\nvariable, which is computed by applying some operation to some arguments.\nA simple program to raise x to the yth power in that language looks like\nthis:\n\npower:\n    res = 1\n    if y goto power_rec else goto power_done\n\npower_rec:\n    res = res * x\n    y = y - 1\n    if y goto power_rec else goto power_done\n\npower_done:\n    print_and_stop(res)\n\nTo represent the same program as Prolog data structures, we use the\nfollowing Prolog code:\nblock(power, op1(res, same, const(1),\n             if(y, power_rec, power_done))).\nblock(power_rec, op2(res, mul, var(res), var(x),\n                 op2(y, sub, var(y), const(1),\n                 if(y, power_rec, power_done)))).\nblock(power_done, print_and_stop(var(res))).\n\nEvery rule of block declares one block by first giving the label of the\nblock, followed by the code. Code is a series of op1 or op2 statements\nterminated by a jump, an if or a print_and_stop. op1 statements\nare operations with one argument of the form op1(res_variable,\noperation_name, argument, next_statement). Arguments can be either variables\nin the form var(name) or constants in the form const(value).\nTo run programs in this flowgraph language, we first need some helper\nfunctionality. The first few helper functions are concerned with the handling of\nenvironments, the data structures the interpreter uses to map variable\nnames occuring in the program to the variables' current values. In Python\ndictionaries would be used for this purpose, but in Prolog we have to emulate\nthese by lists of key/value pairs (not very efficient, but good enough):\nlookup(X, [], _) :- throw(key_not_found(X)).\nlookup(Key, [Key/Value | _], Value) :- !.\nlookup(Key, [_ | Rest], Value) :- lookup(Key, Rest, Value).\n\nwrite_env([], X, V, [X/V]).\nwrite_env([Key/_ | Rest], Key, Value, [Key/Value | Rest]) :- !.\nwrite_env([Pair | Rest], Key, Value, [Pair | NewRest]) :- write_env(Rest, Key, Value, NewRest).\n\nremove_env([], _, []).\nremove_env([Key/_ | Rest], Key, Rest) :- !.\nremove_env([Pair | Rest], Key, [Pair | NewRest]) :- remove_env(Rest, Key, NewRest).\n\nresolve(const(X), _, X).\nresolve(var(X), Env, Y) :- lookup(X, Env, Y).\n\nThe implementation of these functions is not too important. The lookup\nfunction finds a key in an environment list, the write_env function adds a\nnew key/value pair to an environment, remove_env removes a key. The\nresolve function is used to take either a constant or a variable and return\na value. If it's a constant, the value of that constant is returned, if it's a\nvariable it is looked up in the environment. Note how the last argument of\nlookup and resolve is actually a return value, which is the typical\napproach in Prolog.\nSo far we have not specified what the primitive operations that can occur in the\nprogram actually mean. For that we define a do_op function which\nexecutes primitive operations:\ndo_op(same, X, X).\ndo_op(mul, X, Y, Z) :- Z is X * Y.\ndo_op(add, X, Y, Z) :- Z is X + Y.\ndo_op(sub, X, Y, Z) :- Z is X - Y.\ndo_op(eq, X, Y, Z) :- X == Y -> Z = 1; Z = 0.\ndo_op(ge, X, Y, Z) :- X >= Y -> Z = 1; Z = 0.\ndo_op(readlist, L, I, X) :- nth0(I, L, X).\ndo_op(Op, _, _, _) :- throw(missing_op(Op)).\n\nAgain the last argument is an output variable.\nNow we can start executing simple operations. For that an interp predicate\nis defined. It takes as its first argument the current environment and as the\nsecond argument the operation to execute. E.g. to execute primitive operations\nwith one or two arguments:\ninterp(op1(ResultVar, Op, Arg, Rest), Env) :-\n    resolve(Arg, Env, RArg),\n    do_op(Op, RArg, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    interp(Rest, NEnv).\n\ninterp(op2(ResultVar, Op, Arg1, Arg2, Rest), Env) :-\n    resolve(Arg1, Env, RArg1),\n    resolve(Arg2, Env, RArg2),\n    do_op(Op, RArg1, RArg2, Res),\n    write_env(Env, ResultVar, Res, NEnv),\n    interp(Rest, NEnv).\n\nFirst the arguments are resolved into values. Afterwards the operation is executed,\nand the result is written back into the environment. Then interp is called on\nthe rest of the program. Similarly easy are the unconditional jump and\nprint_and_stop:\ninterp(jump(L), Env) :-\n    block(L, Block),\n    interp(Block, Env).\n\n\ninterp(print_and_stop(Arg), Env) :-\n    resolve(Arg, Env, Val),\n    print(Val), nl.\n\nIn the unconditional jump we simply get the target block and continue executing\nthat. To execute print_and_stop we resolve the argument, print the value and\nthen are done.\nThe conditional jump is only slightly more difficult:\ninterp(if(V, L1, L2), Env) :-\n    lookup(V, Env, Val),\n    (Val == 0 ->\n        block(L2, Block)\n    ;\n        block(L1, Block)\n    ),\n    interp(Block, Env).\n\nFirst the variable is looked up in the environment. If the variable is zero,\nexecution continues at the second block, otherwise it continues at the first\nblock.\nGiven this interpreter, we can execute the above example program like this, on a\nProlog console:\n$ swipl -s cfglang.pl\n?- block(power, Block), interp(Block, [x/10, y/10]).\n10000000000\n\nPartial Evaluation of the Flowgraph Language\nLet's look at what a partial evaluator for this simple flowgraph language would\nlook like. Partial evaluation (PE), also called specialization, is a program\nmanipuation technique. PE takes an input program and transforms it into a\n(hopefully) simpler and faster output program. It does this by assuming that\nsome variables in the input program are constants. All operations that act only\non such constants can be folded away. All other operations need to remain in the\noutput program (called residual program). Thus the partial evaluator proceeds\nmuch like an interpreter, just that it cannot actually execute some operations.\nAlso, its output is not just a value, but also list of remaining operations that\ncould not be optimized away.\nThe partial evaluator cannot use normal environments, because unlike the\ninterpreter not all variables' values are known to it. It will therefore work on\npartial environments, which store just the know variables. For these partial\nenvironments, some new helper functions are needed:\nplookup(Key, [], var(Key)).\nplookup(Key, [Key/Value | _], const(Value)) :- !.\nplookup(Key, [_ | Rest], Value) :- plookup(Key, Rest, Value).\n\npresolve(const(X), _, const(X)).\npresolve(var(V), PEnv, X) :- plookup(V, PEnv, X).\n\nThe function plookup takes a variable and a partial environment and returns\neither const(Value) if the variable is found in the partial environment or\nvar(Key) if it is not. Equivalently, presolve is like resolve,\nexcept that it uses plookup instead of lookup.\nWith these helpers we can start writing a partial evaluator. The following two\nrules are where the main optimization in the form of constant folding happens.\nThe idea is that when the partial evaluator sees an operation that involves\nonly constant arguments, it can constant-fold the operation, otherwise it\ncan't:\npe(op1(ResultVar, Op, Arg, Rest), PEnv, NewOp) :-\n    presolve(Arg, PEnv, RArg),\n    (RArg = const(C) ->\n        do_op(Op, C, Res),\n        write_env(PEnv, ResultVar, Res, NEnv),\n        RestResidual = NewOp\n    ;\n        remove_env(PEnv, ResultVar, NEnv),\n        NewOp = op1(ResultVar, Op, RArg, RestResidual)\n    ),\n    pe(Rest, NEnv, RestResidual).\n\npe(op2(ResultVar, Op, Arg1, Arg2, Rest), PEnv, NewOp) :-\n    presolve(Arg1, PEnv, RArg1),\n    presolve(Arg2, PEnv, RArg2),\n    (RArg1 = const(C1), RArg2 = const(C2) ->\n        do_op(Op, C1, C2, Res),\n        write_env(PEnv, ResultVar, Res, NEnv),\n        RestResidual = NewOp\n\n    ;\n        remove_env(PEnv, ResultVar, NEnv),\n        NewOp = op2(ResultVar, Op, RArg1, RArg2, RestResidual)\n    ),\n    pe(Rest, NEnv, RestResidual).\n\nThe pe predicate takes a partial environment, the current operations and\npotentially returns a new operation. To partially evaluate a simple operation, its arguments are\nlooked up in the partial environment. If all the arguments are constants, the\noperation can be executed, and no new operation is produced. Otherwise, we need\nto produce a new residual operation which is exactly like the one currently\nlooked at. Also, the result variable needs to be removed from the partial\nenvironment, because it was just overwritten by an unknown value.\nThe potentially generated residual operation is stored into the output argument\nNewOp. The output argument of the recursive call is the last argument of\nthe newly created residual operation, which will then be filled by the\nrecursive call. This is a typical approach in Prolog, but may look strange if\nyou are not familiar with it.\nNote how the first case of these two rules is just like interpretation. The\nsecond case doesn't really do anything, it just produces a residual operation.\nThis relationship between normal evaluation and partial evaluation is very\ntypical.\nThe unconditional jump and print_and_stop are not much more complex:\npe(jump(L), PEnv, jump(LR)) :-\n    do_pe(L, PEnv, LR).\n\npe(print_and_stop(Arg), Env, print_and_stop(RArg)) :-\n    presolve(Arg, Env, RArg).\n\nTo partially evaluate an unconditional jump we again produce a jump. The target\nlabel of that residual jump is computed by asking the partial evaluator to\nproduce residual code for the label L with the given partial environment.\nprint_and_stop is simply turned into a print_and_stop. We will see the\ncode for do_pe soon.\nConditional jumps are more interesting:\npe(if(V, L1, L2), PEnv, NewOp) :-\n    plookup(V, PEnv, Val),\n    (Val = const(C) ->\n        (C = 0 ->\n            L = L2\n        ;\n            L = L1\n        ),\n        do_pe(L, PEnv, LR),\n        NewOp = jump(LR)\n    ;\n        do_pe(L1, PEnv, L1R),\n        do_pe(L2, PEnv, L2R),\n        NewOp = if(V, L1R, L2R)\n    ).\n\nFirst we look up the value of the condition variable. If it is a constant, we\ncan produce better code, because we know statically that only one path is\nreachable. Thus we produce code for that path, and then emit an unconditional\njump there. If the condition variable is not known at partial evaluation time,\nwe need to partially evaluate both paths and produce a conditional jump in the\nresidual code.\nThis rule is the one that causes the partial evaluator to potentially do much\nmore work than the interpreter, because after an if sometimes both paths\nneed to be explored. In the worst case this process never stops, so a real\npartial evaluator would need to ensure somehow that it terminates. There are\nmany algorithms for doing that, but I will ignore this problem here.\nNow we need to understand what the do_pe predicate is doing. Its most\nimportant task is to make sure that we don't do the same work twice by\nmemoizing code that was already partially evaluated in the past. For that it\nkeeps a mapping of Label, Partial Environment to Label of the residual\ncode:\ndo_pe(L, PEnv, LR) :-\n    (code_cache(L, PEnv, LR) ->\n        true\n    ;\n        gensym(L, LR),\n        assert(code_cache(L, PEnv, LR)),\n        block(L, Code),\n        pe(Code, PEnv, Residual),\n        assert(block(LR, Residual))\n    ).\n\nIf the code cache indicates that label L was already partially evaluated\nwith partial environment PEnv, then the previous residual code label\nLPrevious\nis returned. Otherwise, a new label is generated with gensym, the code cache\nis informed of that new label with assert, then the block is partially\nevaluated and the residual code is added to the database.\nFor those who know partial evaluation terminology: This partial evaluator is a\npolyvariant online partial evaluator. \"Polyvariant\" means that for every label,\nseveral specialized version of the block can be generated. \"Online\" means that\nno preprocessing is done before the partial evaluator runs.\n\nPartial Evaluation Example\nWith this code we can look at the classical example of partial evaluation (it's\nprobably the \"Hello World\" of partial evaluation). We\ncan ask the partial evaluator to compute a power function, where the exponent\ny is a fixed number, e.g. 5, and the base x is unknown:\n?- do_pe(power, [y/5], LR).\nLR = power1.\n\nTo find out which code was produced, we can use listing:\n?- listing(code_cache)\ncode_cache(power, [y/5], power1).\ncode_cache(power_rec, [y/5, res/1], power_rec1).\ncode_cache(power_rec, [y/4], power_rec2).\ncode_cache(power_rec, [y/3], power_rec3).\ncode_cache(power_rec, [y/2], power_rec4).\ncode_cache(power_rec, [y/1], power_rec5).\ncode_cache(power_done, [y/0], power_done1).\n\n?- listing(block)\n.... the block definition of the user program ....\nblock(power_done1, print_and_stop(var(res))).\nblock(power_rec5, op2(res, mul, var(res), var(x), jump(power_done1))).\nblock(power_rec4, op2(res, mul, var(res), var(x), jump(power_rec5))).\nblock(power_rec3, op2(res, mul, var(res), var(x), jump(power_rec4))).\nblock(power_rec2, op2(res, mul, var(res), var(x), jump(power_rec3))).\nblock(power_rec1, op2(res, mul, const(1), var(x), jump(power_rec2))).\nblock(power1, jump(power_rec1)).\n\nThe code_cache tells which residual labels correspond to which original\nlabels under which partial environments. Thus, power1 contains the code of\npower under the assumption that y is 5. Looking at the block listing,\nthe label power1 corresponds to code that simply multiplies res by x\nfive times without using the variable x at all. The loop that was present\nin the original program has been fully unrolled, the loop variable y has\ndisappeared. Hopefully this is faster than the original program.\n\nConclusion\nIn this blog post we saw an interpreter for a simple flow graph language in\nProlog, together with a partial evaluator for it. The partial evaluator\nessentially duplicates every rule of the interpreter. If all the arguments of\nthe current operation are known, it acts like the interpreter, otherwise it\nsimply copies the operation into the residual code.\nPartial evaluation can be used for a variety of applications, but the most\ncommonly cited one is that of applying it to an interpreter. To do that, the\nprogram that the interpreter runs is assumed to be constant by the partial\nevaluator. Thus a specialized version of the interpreter is produced that does\nnot use the input program at all. That residual code can be seen as a compiled\nversion of the input program.\nIn the next blog post in this series we will look at writing a simple tracer for\nthe same flowgraph language.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/01/comparing-partial-evaluation-and-7255412724168990164.html"
    },
    {
      "title": "PyPy internship at NCAR",
      "text": "Hello, everyone\nI would like to inform you that there is a very interesting opportunity\nfor doing an internship at NCAR in the lovely town of Boulder, situated\non the foothils of Rocky Mountains. Before you read on, make sure you:\n\nare a student of a US University, who is legally eligible to work in the US\nare at least finishing second year this year\napply before February 3rd.\n\nThe internship itself will focus on using PyPy (in some way) to provide\na high performance numeric kernel for an atmospheric model, and measuring how\nfast we can go. This is very much in line with what the current effort on\nNumPy in PyPy is about. The internship will be mentored by Davide del Vento\nand I hope to have some influence over where it goes myself :-)\nA few interesting links:\n\nprogram website\ninternship proposal - note that the actual roadmap is very flexible, as\nlong as it's a numeric kernel of an atmospheric model using PyPy.\n\nFeel free to contact Davide for details about the proposal and pypy-dev or\nme directly for details about PyPy.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2012/01/pypy-internship-at-ncar-2244162842744077724.html"
    },
    {
      "title": "Transactional Memory (II)",
      "text": "Here is an update about the previous blog post about the\nGlobal Interpreter Lock (GIL).  In 5 months, the point of view\nchanged quite a bit.\nLet me remind you that the GIL is the technique used in both CPython and\nPyPy to safely run multi-threaded programs: it is a global lock that\nprevents multiple threads from actually running at the same time.  The\nreason to do that is that it would have disastrous effects in the\ninterpreter if several threads access the same object concurrently --- to\nthe point that in CPython even just manipulating the object's reference\ncounter needs to be protected by the lock.\nSo far, the ultimate goal to enable true multi-CPU usage has been to\nremove the infamous GIL from the interpreter, so that multiple threads\ncould actually run in parallel.  It's a lot of work, but this has been\ndone in Jython.  The reason that it has not been done in CPython so far\nis that it's even more work: we would need to care not only about\ncarefully adding fine-grained locks everywhere, but also about reference\ncounting; and there are a lot more C extension modules that would need\ncare, too.  And we don't have locking primitives as performant as\nJava's, which have been hand-tuned since ages (e.g. to use help from the\nJIT compiler).\nBut we think we have a plan to implement a different model for using\nmultiple cores.  Believe it or not, this is better than just removing\nthe GIL from PyPy.  You might get to use all your cores without ever\nwriting threads.\nYou would instead just use some event dispatcher, say from Twisted, from\nStackless, or from your favorite GUI; or just write your own.  From\nthere, you (or someone else) would add some minimal extra code to the\nevent dispatcher's source code, to exploit the new transactional features\noffered by PyPy.  Then you would run your program on a\nspecial version of PyPy, and voil\u00e0: you get some form of automatic parallelization.\nSounds magic, but the basic idea is simple: start handling multiple\nevents in parallel, giving each one its own transaction.  More about\nit later.\n\nThreads or Events?\nFirst, why would this be better than \"just\" removing the GIL?  Because\nusing threads can be a mess in any complex program.  Some authors (e.g.\nLee) have argued that the reason is that threads are fundamentally\nnon-deterministic.  This makes it very hard to reason about them.\nBasically the programmer needs to \"trim\" down the non-determinism (e.g.\nby adding locks, semaphores, etc.), and it's hard to be sure when he's\ngot a sufficiently deterministic result, if only because he can't write\nexhaustive tests for it.\nBy contrast, consider a Twisted program.  It's not a multi-threaded\nprogram, which means that it handles the \"events\" one after the other.\nThe exact ordering of the events is not really deterministic, because\nthey often correspond to external events; but that's the only source of\nnon-determinism.  The actual handling of each event occurs in a nicely\ndeterministic way, and most importantly, not in parallel with the\nhandling of other events.  The same is true about other libraries like\nGUI toolkits, gevent, or Stackless.\n(Of course the Twisted and the Stackless models, to cite only these two,\nare quite different from each other; but they have in common the fact\nthat they are not multi-threaded, and based instead on \"events\" ---\nwhich in the Stackless case means running a tasklet from one switch()\npoint to the next one.)\nThese two models --- threads or events --- are the two main models we\nhave right now.  The latter is more used in Python, because it is much\nsimpler to use than the former, and the former doesn't give any benefit\nbecause of the GIL.  A third model, which is the only one that gives\nmulti-core benefits, is to use multiple processes, and do inter-process\ncommunication.\n\nThe problem\nConsider the case of a big program that has arbitrary complicated\ndependencies.  Even assuming a GIL-less Python, this is likely enough to\nprevent the programmer from even starting a multi-threaded rewrite,\nbecause it would require a huge mess of locks.  He could also consider\nusing multiple processes instead, but the result is annoying as well:\nthe complicated dependencies translate into a huge mess of inter-process\nsynchronization.\nThe problem can also be down-sized to very small programs, like the kind\nof hacks that you do and forget about.  In this case, the dependencies\nmight be simpler, but you still have to learn and use subtle locking\npatterns or a complex inter-process library, which is overkill for the\npurpose.\n(This is similar to how explicit memory management is not very hard for\nsmall programs --- but still, nowadays a lot of people agree that\nautomatic memory management is easier for programs of all sizes.  I\nthink the same will eventually be true for using multiple CPUs, but the\ncorrect solution will take time to mature, like garbage collectors did.\nThis post is a step in hopefully the right direction :-))\n\nEvents in Transactions\nLet me introduce the notion of independent events: two events are\nindependent if they don't touch the same set of objects. In a multi-threaded\nworld, it means that they can be executed in parallel without needing any lock\nto ensure correctness.\nEvents might also be mostly independent, i.e. they rarely access the same\nobject concurrently.  Of course, in a multi-threaded world we would still need\nlocks to ensure correctness, but the point is that the locks are rarely causing\npauses: lock contention is low.\nConsider again the Twisted example I gave above.  There are often several\nevents pending in the dispatch queue (assuming the program is using 100%\nof our single usable CPU, otherwise the whole discussion is moot).  The case I am\ninterested in is the case in which these events are generally mostly\nindependent, i.e. we expect few conflicts between them.  However\nthey don't have to be proved independent.  In fact it is fine if\nthey have arbitrary complicated dependencies as described above.  The\npoint is the expected common case.  Imagine that you have a GIL-less\nPython and that you can, by a wave of your hand, have all the careful\nlocking mess magically done.  Then what I mean here is the case in which\nsuch a theoretical program would run mostly in parallel on multiple\ncore, without waiting too often on the locks.\nIn this case, the solution I'm proposing is that with minimal tweaks\nin the event dispatch loop, we can\nhandle multiple events on multiple threads, each in its own transaction.\nA transaction is basically a tentative execution of the corresponding\npiece of code: if we detect conflicts with other concurrently executing\ntransactions, we abort the whole transaction and restart it from\nscratch.\nBy now, the fact that it can basically work should be clear: multiple\ntransactions will only get into conflict when modifying the same data\nstructures, which is the case where the magical wand above would have\nput locks.  If the magical program could progress without too many\nlocks, then the transactional program can progress without too many\nconflicts.  In a way, you get even more than what the magical program\ncan give you: each event is dispatched in its own transaction, which\nmeans that from each event's point of view, we have the illusion that\nnobody else is running concurrently.  This is exactly what all existing\nTwisted-/Stackless-/etc.-based programs are assuming.\nNote that this solution, without transactions, already exists in some\nother languages: for example, Erlang is all about independent events.\nThis is the simple case where we can just run them on multiple cores,\nknowing by construction of the language that you can't get conflicts.\nOf course, it doesn't work for Python or for a lot of other languages.\nFrom that point of view, what I'm suggesting is merely that\ntransactional memory could be a good model to cope with the risks of\nconflicts that come from not having a special-made language.\n\nNot a perfect solution\nOf course, transactional memory\n(TM) is not a perfect solution either.  Right now, the biggest issue is\nthe performance hit that comes from the software implementation (STM).\nIn time, hardware support (HTM) is likely to show up and help\nmitigate the problem; but I won't deny the fact that in some cases,\nbecause it's simple enough and/or because you really need the top\nperformance, TM is not the best solution.\nAlso, the explanations above are silent on what is a hard point for TM,\nnamely system calls.  The basic general solution is to suspend other\ntransactions as soon as a transaction does its first system call, so\nthat we are sure that the transaction will succeed.  Of course this\nsolution is far from optimal.  Interestingly, it's possible to do better\non a case-by-case basis: for example, by adding in-process buffers, we\ncan improve the situation for sockets, by having recv() store in a\nbuffer what is received so that it can be re-recv()-ed later if the\ntransaction is aborted; similarly, send() or writes to log files can be\ndelayed until we are sure that the transaction will commit.\nFrom my point of view, the most important point is that the TM solution\ncomes from the correct side of the \"determinism\" scale.  With threads,\nyou have to prune down non-determinism.  With TM, you start from a\nmostly deterministic point, and if needed, you add non-determinism.  The\nreason you would want to do so is to make the transactions shorter:\nshorter transactions have less risks of conflicts, and when there are\nconflicts, less things to redo.  So making transactions shorter\nincreases the parallelism that your program can achieve, while at the\nsame time requiring more care.\nIn terms of an event-driven model, the equivalent would be to divide the\nresponse of a big processing event into several events that are handled\none after the other: for example, the first event sets things up and fires the second\nevent, which does the actual computation; and afterwards a third event\nwrites the results back.  As a result, the second event's transaction\nhas little risks of getting aborted.  On the other hand, the writing\nback needs to be aware of the fact that it's not in the same transaction\nas the original setting up, which means that other unrelated\ntransactions may have run in-between.\n\nOne step towards the future?\nThese, and others, are the problems of the TM approach.  They are \"new\"\nproblems, too, in the sense that the existing ways of programming don't\nhave these problems.\nStill, as you have guessed, I think that it is overall a win, and\npossibly a big win --- a win that might be on the same scale for the age\nof multiple CPUs as automatic garbage collection was 20 years ago for\nthe age of RAM size explosion.\nStay tuned for more!\n--- Armin (and reviews by Antonio and Fijal)\n\n\nUPDATE: please look at the tiny transaction module I wrote as an example.  The idea is to have the same interface as this module, but implemented differently.  By making use of transactional memory internally, it should be possible to safely run on multiple CPUs while keeping the very same programmer interface.",
      "tags": "stm",
      "url": "https://www.pypy.org/posts/2012/01/transactional-memory-ii-7225309560970774590.html"
    },
    {
      "title": "NumPyPy progress report - running benchmarks",
      "text": "Hello.\nWe're excited to let you know about some of the great progress we've made on\nNumPyPy: both completeness and performance. In this blog entry we mostly\nwill talk about performance and how much progress we have made so far.\nWord of warning: this\nwork is in progress -- we're maybe half way to where we want to be and there are\nmany trivial and not so trivial optimizations to be written. (For example, we\nhaven't even started to implement important optimizations, like vectorization.)\n\nBenchmark\nWe chose a laplace equation solver, based on SciPy's PerformancePython wiki.\nUnfortunately, the different implementations on the wiki page accidentally use\ntwo different algorithms, which have different convergences, and very different\nperformance characteristics on modern computers. As a result, we implemented\nour own versions in both C and Python (with and without NumPy). The full source\ncan be found in fijal's hack repo, all these benchmarks were performed at\nrevision 18502dbbcdb3.\nFirst, let me describe various algorithms used. Note that some of them contain\nPyPy-specific hacks to work around limitations in the current implementation.\nThese hacks will go away eventually and the performance will improve.\nNumerically the algorithms used are identical, however exact data layout in\nmemory differs between them.\nA note about all the benchmarks: they each were run once, but the\nperformance is very stable across runs.\nStarting with the C version, it implements a trivial laplace transform\nusing two loops and double-reference memory (array of int*). The double\nreference does not matter for performance and the two algorithms are\nimplemented in inline-laplace.c and laplace.c. They were both compiled\nwith gcc 4.4.5 at -O3. The inline version modifies array in-place while the non-inline version stores results in a copy. That makes them converge at different rate, hence different number of iterations\nA straightforward version of those in Python is implemented in laplace.py\nusing, respectively, inline_slow_time_step and slow_time_step.\nslow_2_time_step does the same thing, except it copies arrays in-place\ninstead of creating new copies. Table below compares running PyPy against C:\n\n\n\n\n\n\n\nbench\nnumber of iterations\ntime per iteration\n\nlaplace C\n219\n6.3ms\n\ninline-laplace C\n278\n20ms\n\nslow python\n219\n17ms\n\nslow 2 python\n219\n14ms\n\ninline_slow python\n278\n23.7ms\n\n\n\nAn important thing to notice is the data dependency of the inline\nversion causes a huge slowdown for the C versions. This is not a severe\ndisadvantage for us though -- the brain-dead Python version takes longer\nand PyPy is not able to take advantage of the knowledge that the data is\nindependent. The results are in the same ballpark as the C versions --\n15% - 170% slower, but the algorithm\none chooses matters more than the language. By comparison, the slow versions\ntake about 5.75s each on CPython 2.6 per iteration and, by estimation,\nare about 200x slower than the PyPy equivalent, if I had the patience to\nmeasure the full run.\nThe next step is to use NumPy expressions. The first problem we run into is\nthat computing the error requires walking the entire array a second time. This\nis fairly inefficient in terms of cache access, so I took the liberty of\ncomputing the errors every 15 steps. This results in the convergence being\nrounded to the nearest 15 iterations, but speeds things up considerably.\nnumeric_time_step takes the most braindead approach of replacing the array\nwith itself, like this:\n\nu[1:-1, 1:-1] = ((u[0:-2, 1:-1] + u[2:, 1:-1])*dy2 +\n                       (u[1:-1,0:-2] + u[1:-1, 2:])*dx2)*dnr_inv\n\nWe need 3 arrays here -- one is an intermediate (PyPy only needs one, for all of\nthose subexpressions), one is a copy for computing the error, and one is the\nresult. This works automatically because in NumPy + or * creates an\nintermediate, while NumPyPy avoids allocating the intermediate if possible.\nnumeric_2_time_step works in pretty much the same way:\n\nsrc = self.u\nself.u = src.copy()\nself.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +\n                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv\n\nexcept the copy is now explicit rather than implicit.\nnumeric_3_time_step does the same thing, but notice one doesn't have to copy\nthe entire array, it's enough to copy the border pieces and fill rest with\nzeros:\n\nsrc = self.u\nself.u = numpy.zeros((self.nx, self.ny), 'd')\nself.u[0] = src[0]\nself.u[-1] = src[-1]\nself.u[:, 0] = src[:, 0]\nself.u[:, -1] = src[:, -1]\nself.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +\n                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv\n\nnumeric_4_time_step is the one that tries hardest to resemble the C version.\nInstead of doing an array copy, it actually notices that one can alternate\nbetween two arrays. This is exactly what the C version does. The\nremove_invalidates call is a PyPy specific hack - we hope to remove this\ncall in the near future, but, in short, it promises \"I don't have any unbuilt\nintermediates that depend on the value of the argument\", which means one doesn't\nhave to compute sub-expressions one is not actually using:\n\nremove_invalidates(self.old_u)\nremove_invalidates(self.u)\nself.old_u[:,:] = self.u\nsrc = self.old_u\nself.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +\n                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv\n\nThis one is the most comparable to the C version.\nnumeric_5_time_step does the same thing, but notices one doesn't have to copy\nthe entire array, it's enough to just copy the edges. This is an optimization\nthat was not done in the C version:\n\nremove_invalidates(self.old_u)\nremove_invalidates(self.u)\nsrc = self.u\nself.old_u, self.u = self.u, self.old_u\nself.u[0] = src[0]\nself.u[-1] = src[-1]\nself.u[:, 0] = src[:, 0]\nself.u[:, -1] = src[:, -1]\nself.u[1:-1, 1:-1] = ((src[0:-2, 1:-1] + src[2:, 1:-1])*dy2 +\n                      (src[1:-1,0:-2] + src[1:-1, 2:])*dx2)*dnr_inv\n\nLet's look at the table of runs. As before, gcc 4.4.5, compiled at -O3,\nand PyPy nightly 7bb8b38d8563, on an x86-64 machine. All of the numeric methods\nrun for 226 steps, slightly more than the 219, rounding to the next 15 when the\nerror is computed.\n\n\n\n\n\n\n\nbenchmark\nPyPy\nCPython\n\nnumeric\n21ms\n35ms\n\nnumeric 2\n14ms\n37ms\n\nnumeric 3\n13ms\n29ms\n\nnumeric 4\n11ms\n31ms\n\nnumeric 5\n9.3ms\n21ms\n\n\n\nWe think that these preliminary results are pretty good. They're not as fast as\nthe C version (or as fast as we'd like them to be), but we're already much\nfaster than NumPy on CPython -- almost always by more than 2x on this relatively\nreal-world example. This is not the end, though. In fact, it's hardly the\nbeginning! As we continue work, we hope to make even more use of the\nhigh level information that we have. Looking at the assembler generated by\ngcc for this example, it's pretty clear we can outperform it thanks to better\naliasing information and hence better possibilities for vectorization.\nStay tuned.\nEDIT: fixed the benchmark name\n\nEDIT2: added info that first table is about PyPy\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2012/01/numpypy-progress-report-running-3336055571122066974.html"
    },
    {
      "title": "Leysin Winter Sprint",
      "text": "PyPy Leysin Winter Sprint: 15-22nd January 2012\n\n\nThe next PyPy sprint will be in Leysin, Switzerland, for the\neighth time.  This is a fully public sprint: newcomers and topics\nother than those proposed below are welcome.\n\nGoals and topics of the sprint\n\n\nPy3k: work towards supporting Python 3 in PyPy\n\nNumPyPy: work towards supporting the numpy module in PyPy\n\nJIT backends: integrate tests for ARM; look at the PowerPC 64;\n  maybe try again to write an LLVM- or GCC-based one\n\nSTM and STM-related topics; or the Concurrent Mark-n-Sweep GC\n\nAnd as usual, the main side goal is to have fun in winter sports :-)\n  We can take a day off for ski.\n\n\nExact times\n\nThe work days should be 15-21 January 2011 (Sunday-Saturday).  The\nofficial plans are for people to arrive on the 14th or the 15th, and to\nleave on the 22nd.\n\nInterested? Read more...",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/12/leysin-winter-sprint-6862532189897876336.html"
    },
    {
      "title": "Come see us at PyCon 2012",
      "text": "PyCon 2012 is coming up in just a few short months, and PyPy will be well\nrepresented there.  We'll be delivering a tutorial, two talks, plus we'll be\naround for the sprints.Here are the abstracts for the tutorials and talks:How to get the most out of your PyPy, by Maciej Fijalkowski, Alex Gaynor\nand Armin Rigo: For many applications PyPy can provide performance benefits\nright out of the box. However, little details can push your application to\nperform much better. In this tutorial we'll give you insights on how to push\nPyPy to its limits. We'll focus on understanding the performance\ncharacteristics of PyPy, and learning the analysis tools in order to maximize\nyour applications' performance. This is the tutorial.\nWhy PyPy by example, by Maciej Fijalkowski, Alex Gaynor and Armin Rigo:\nOne of the goals of PyPy is to make existing Python code faster; however an\neven broader goal was to make it possible to write things in Python that\npreviously would needed to be written in C or other low-level language. This\ntalk will show examples of this, and describe how they represent the\ntremendous progress PyPy has made, and what it means for people looking at\nusing PyPy.\nHow the PyPy JIT works, by Benjamin Peterson: The Python community is\nabuzz about the major speed gains PyPy can offer for pure Python code. But how\ndoes the PyPy JIT actually work? This talk will discuss how the PyPy JIT is\nimplemented. It will include descriptions of the tracing, optimization, and\nassembly generation phases. I will demonstrate each step with an example loop.\nIf you have any questions let us know!  We look forward to seeing people at\nPyCon and chatting about PyPy and the entire Python ecosystem.See you there,\nMaciej Fijalkowski, Alex Gaynor, Benjamin Peterson, Armin Rigo, and the entire PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/12/come-see-us-at-pycon-2012-610420698450130659.html"
    },
    {
      "title": "Plotting using matplotlib from PyPy",
      "text": "Big fat warning This is just a proof of concept. It barely works. There are\nmissing pieces left and right, which were replaced with hacks so I can get this\nto run and prove it's possible. Don't try this at home, especially your home.\nYou have been warned.\nThere has been a lot of talking about PyPy not integrating well with the\ncurrent scientific Python ecosystem, and numpypy (a NumPy reimplementation\non top of pypy) was dubbed \"a fancy array library\". I'm going to show that\nintegration with this ecosystem is possible with our design.\nFirst, the demo:\n\n#!/usr/bin/env pypy\n\n# numpy, pypy version\nimport numpypy as numpy\n# DRAGONS LIVE THERE (fortunately hidden)\nfrom embed.emb import import_mod\n\npylab = import_mod('matplotlib.pylab')\n\nif __name__ == '__main__':\n    a = numpy.arange(100, dtype=int)\n    b = numpy.sin(a)\n    pylab.plot(a, b)\n    pylab.show()\n\nAnd you get:\n\n\n\nNow, how to reproduce it:\n\nYou need a PyPy without cpyext, I did not find a linker that would support\noverriding symbols. Right now there are no nightlies like this, so you have\nto compile it yourself, like:\n\n./translate.py -Ojit targetpypystandalone.py --withoutmod-cpyext\n\nThat would give you a PyPy that's unable to load some libraries like PIL, but\nperfectly working otherwise.\n\nSpeaking of which, you need a reasonably recent PyPy.\n\nThe approach is generally portable, however the implementation has been\ntested only on 64bit linux. Few tweaks might be required.\n\nYou need to install python2.6, the python2.6 development headers, and have\nnumpy and matplotlib installed on that python.\n\nYou need a checkout of my hacks directory and put embedded on your\nPYTHONPATH, your pypy checkout also has to be on the PYTHONPATH.\n\n\n\nEr wait, what happened?\nWhat didn't happen is we did not reimplement matplotlib on top of PyPy. What\ndid happen is we embed CPython inside of PyPy using ctypes. We instantiate it.\nand follow the embedding tutorial for CPython. Since numpy arrays are not\nmovable, we're able to pass around an integer that's represents the memory\naddress of the array data and reconstruct it in the embedded interpreter. Hence\nwith a relatively little effort we managed to reuse the same array data on both\nsides to plot at array. Easy, no?\nThis approach can be extended to support anything that's not too tied with\npython objects. SciPy and matplotlib both fall into the same category\nbut probably the same strategy can be applied to anything, like GTK or QT.\nIt's just a matter of extending a hack into a working library.\nTo summarize, while we're busy making numpypy better and faster, it seems\nthat all external libraries on the C side can be done using an embedded Python\ninterpreter with relatively little effort. To get to that point, I spent\na day and a half to learn how to embed CPython, with very little prior\nexperience in the CPython APIs. Of course you should still keep as much as\npossible in PyPy to make it nice and fast :)\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/12/plotting-using-matplotlib-from-pypy-6389240123679375092.html"
    },
    {
      "title": "PyPy 1.7 on Win32",
      "text": "Hi all,\n\nWe have fixed _continuation on Win32 (thanks Stakkars), and so we have now a Win32 version of PyPy 1.7.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/11/pypy-17-on-win32-4962523601794245248.html"
    },
    {
      "title": "PyPy 1.7 - widening the sweet spot",
      "text": "We're pleased to announce the 1.7 release of PyPy. As became a habit, this\nrelease brings a lot of bugfixes and performance improvements over the 1.6\nrelease. However, unlike the previous releases, the focus has been on widening\nthe \"sweet spot\" of PyPy. That is, classes of Python code that PyPy can greatly\nspeed up should be vastly improved with this release. You can download the 1.7\nrelease here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7. It's fast (pypy 1.7 and cpython 2.7.1 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64, Mac OS X 32/64 or\nWindows 32. Windows 64 work is ongoing, but not yet natively supported.\nThe main topic of this release is widening the range of code which PyPy\ncan greatly speed up. On average on\nour benchmark suite, PyPy 1.7 is around 30% faster than PyPy 1.6 and up\nto 20 times faster on some benchmarks.\n\n\nHighlights\n\nNumerous performance improvements. There are too many examples which python\nconstructs now should behave faster to list them.\n\nBugfixes and compatibility fixes with CPython.\n\nWindows fixes.\n\nPyPy now comes with stackless features enabled by default. However,\nany loop using stackless features will interrupt the JIT for now, so no real\nperformance improvement for stackless-based programs. Contact pypy-dev for\ninfo how to help on removing this restriction.\n\nNumPy effort in PyPy was renamed numpypy. In order to try using it, simply\nwrite:\n\nimport numpypy as numpy\n\nat the beginning of your program. There is a huge progress on numpy in PyPy\nsince 1.6, the main feature being implementation of dtypes.\n\nJSON encoder (but not decoder) has been replaced with a new one. This one\nis written in pure Python, but is known to outperform CPython's C extension\nup to 2 times in some cases. It's about 20 times faster than\nthe one that we had in 1.6.\n\nThe memory footprint of some of our RPython modules has been drastically\nimproved. This should impact any applications using for example cryptography,\nlike tornado.\n\nThere was some progress in exposing even more CPython C API via cpyext.\n\n\n\n\nThings that didn't make it, expect in 1.8 soon\nThere is an ongoing work, which while didn't make it to the release, is\nprobably worth mentioning here. This is what you should probably expect in\n1.8 some time soon:\n\nSpecialized list implementation. There is a branch that implements lists of\nintegers/floats/strings as compactly as array.array. This should drastically\nimprove performance/memory impact of some applications\nNumPy effort is progressing forward, with multi-dimensional arrays coming\nsoon.\nThere are two brand new JIT assembler backends, notably for the PowerPC and\nARM processors.\n\n\n\nFundraising\nIt's maybe worth mentioning that we're running fundraising campaigns for\nNumPy effort in PyPy and for Python 3 in PyPy. In case you want to see any\nof those happen faster, we urge you to donate to numpy proposal or\npy3k proposal. In case you want PyPy to progress, but you trust us with\nthe general direction, you can always donate to the general pot.\n\nCheers,Maciej Fija\u0142kowki, Armin Rigo and the entire PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/11/pypy-17-widening-sweet-spot-4260962828394182017.html"
    },
    {
      "title": "Gothenburg sprint report",
      "text": "In the past week, we have been busy hacking on PyPy at the Gothenburg sprint, the second of this 2011.  The sprint was hold at Laura's and Jacob's place, and here is a brief report of what happened.\n\n\nIn the first day we welcomed Mark Pearse, who was new to PyPy and at his first sprint.  Mark worked the whole sprint in the new SpecialisedTuple branch, whose aim is to have a special implementation for small 2-items and 3-items tuples of primitive types (e.g., ints or floats) to save memory.  Mark paired with Antonio for a couple of days, then he continued alone and did an amazing job.  He even learned how to properly do Test Driven Development :-).\nAntonio spent a couple of days investigating whether it is possible to use application checkpoint libraries such as BLCR and DMTCP to save the state of the PyPy interpreter between subsequent runs, thus saving also the JIT-compiled code to reduce the warmup time.  The conclusion is that these are interesting technologies, but more work would be needed (either on the PyPy side or on the checkpoint library side) before it can have a practical usage for PyPy users.\nThen, Antonio spent most of the rest of the sprint working on his ffistruct branch, whose aim is to provide a very JIT-friendly way to interact with C structures, and eventually implement ctypes.Structure on top of that.  The \"cool part\" of the branch is already done, and the JIT already can compile set/get of fields into a single fast assembly instruction, about 400 times faster than the corresponding ctypes code.  What is still left to do is to add a nicer syntax (which is easy) and to implement all the ctypes peculiarities (which is tedious, at best :-)).\nAs usual, Armin did tons of different stuff, including fixing a JIT bug, improving the performance of file.readlines() and working on the STM branch (for Software Transactional Memory), which is now able to run RPython multithreaded programs using software transaction (as long as they don't fill up all the memory, because support for the GC is still missing :-)).  Finally, he worked on improving the Windows version of PyPy. While doing so he discovered together with Anto a terrible bug which lead to a continuous leak of stack space because the JIT called some functions using the wrong calling convention.\nH\u00e5kan, with some help from Armin, worked on the jit-targets branch, whose goal is to heavily refactor the way the traces are internally represented by the JIT, so that in the end we can produce (even :-)) better code than what we do nowadays.  More details in this mail.\nAndrew Dalke worked on a way to integrate PyPy with FORTRAN libraries, and in particular the ones which are wrapped by Numpy and Scipy: in doing so, he wrote f2pypy, which is similar to the existing f2py but instead of producing a CPython extension module it produces a pure python modules based on ctypes.  More work is needed before it can be considered complete, but f2pypy is already able to produce a wrapper for BLAS which passes most of the tests under CPython, although there's still work left to get it working for PyPy.\n\n\nArmin and H\u00e5kan with Laura's \"5x faster\" cake\nChristian Tismer worked the whole sprint on the branch to make PyPy compatible with Windows 64 bit.  This needs a lot of work because a lot of PyPy is written under the assumption that the long type in C has the same bit size than void*, which is not true on Win64.  Christian says that in the past Genova-Pegli sprint he completed 90% of the work, and in this sprint he did the other 90% of the work.  Obviously, what is left to complete the task is the third 90% :-).  More seriously, he estimated a total of 2-4 person-weeks of work to finish it.\nBut, all in all, the best part of the sprint has been the cake that Laura baked to celebrate the \"5x faster than CPython\" achievement. Well, actually our speed page reports \"only\" 4.7x, but that's because in the meantime we switched from comparing against CPython 2.6 to comparing against CPython 2.7, which is slightly faster.  We are confident that we will reach the 5x goal again, and that will be the perfect excuse to eat another cake :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/11/gothenburg-sprint-report-8371395613874909242.html"
    },
    {
      "title": "Speeding up JSON encoding in PyPy",
      "text": "Hi\nRecently I spent a bit of effort into speeding up JSON in PyPy. I started with\nwriting a benchmark, which is admittedly not a very good one, but it's\nbetter than nothing (suggestions on how to improve it are welcome!).\nFor this particular benchmark, the numbers are as follow. Note that CPython by\ndefault uses the optimized C extension, while PyPy uses the pure Python one.\nPyPy trunk contains another pure Python version which has been optimized\nspecifically for the PyPy JIT. Detailed optimizations are described later in\nthis post.\nThe number reported is the time taken for the third run, when things are\nwarmed up. Full session here.\n\n\n\n\n\n\nCPython 2.6\n22s\n\nCPython 2.7\n3.7s\n\nCPython 2.7 no C extension\n44s\n\nPyPy 1.5\n34s\n\nPyPy 1.6\n22s\n\nPyPy trunk\n3.3s\n\n\n\nLessons learned:\n\nExpectations are high\nA lot of performance critical stuff in Python world is already written in a hand\noptimized C. Writing C (especially when you interface with CPython C API) is\nugly and takes significant effort. This approach does not scale well when\nthere is a lot of code to be written or when there is a very tight coupling\nbetween the part to be rewritten and the rest of the code. Still, people would\nexpect PyPy to be better at \"tasks\" and not precisely at running equivalent\ncode, hence a comparison between the C extension and the pure python version\nis sound. Fortunately it's possible to outperform the C extension, but requires\na bit of effort on the programmer side as well.\n\n\nOften interface between the C and Python part is ugly\nThis is very clear if you look at json module as implemented in CPython's\nstandard library. Not everything is in C (it would probably be just too\nmuch effort) and the interface to what is in C is guided via profiling not\nby what kind of interface makes sense. This especially is evident comparing CPython 2.6 to 2.7.\nJust adapting the code to an interface with C made the Python version slower.\nRemoving this clutter improves the readability a lot and improves PyPy's version\na bit, although I don't have hard numbers.\n\n\nJitViewer is crucial\nIn case you're fighting with PyPy's performance, jitviewer is worth a shot.\nWhile it's not completely trivial to understand what's going on, it'll\ndefinitely show you what kind of loops got compiled and how.\n\n\nNo nice and fast way to build strings in Python\nPyPy has a custom thing called __pypy__.builders.StringBuilder. It has\na few a features that make it much easier to optimize than other ways like\nstr.join() or cStringIO.\n\nYou can specify the start size, which helps a lot if you can even provide\na rough estimate on the size of the string (less copying)\nOnly append and build are allowed. While  the string is being built you\ncan't seek or do anything else. After it's built you can never append any more.\nUnicode version available as well as __pypy__.builders.UnicodeBuilder.\n\n\n\nMethod calls are ok, immutable globals are ok\nPyPy's JIT seems to be good enough for at least the simple cases. Calling\nmethods for common infrastructure or loading globals (instead of rebinding as\nlocals) is fast enough and improves code readability.\n\n\nString copying is expensive\nEdit: see the comment at the end\nIf you use re.sub, the current implementation will always create a copy\nof the string even if there was no match to replace.\nIf you know your regexp is simple, first try to check if there is\nanything to replace. This is a pretty hard optimization to\ndo automatically -- simply matching the regular expression can be too costly\nfor it to make sense. In our particular example however, the regexp is really\nsimple, checking ranges of characters. It also seems that this is by far the\nfastest way to escape characters as of now.\n\n\nGenerators are slower than they should be\nI changed the entire thing to simply call builder.append instead of\nyielding to the main loop where it would be gathered. This is kind of a PyPy\nbug that using generators extensively is slower, but a bit hard to fix.\nEspecially in cases where there is relatively little data being passed around\n(few bytes), it makes sense to gather it first. If I were to implement an\nefficient version of iterencode, I would probably handle chunks of\npredetermined size, about 1000 bytes instead of yielding data every few bytes.\n\n\nI must admit I worked around PyPy's performance bug\nFor obscure (although eventually fixable) reasons, this:\n\nfor c in s: # s is string\n  del c\n\nis faster than:\n\nfor c in s:\n  pass\n\nThis is a PyPy performance bug and should be fixed, but on a different branch ;-)\n\n\nPyPy's JIT is good\nI was pretty surprised, but the JIT actually did make stuff work nicely.\nThe changes that were done were relatively minor and straightforward, once\nthe module was cleaned to the normal \"pythonic\" state.\nIt is worth noting that it's possible to write code in Python and make it\nrun really fast, but you have to be a bit careful. Again, jitviewer is your\nfriend when determining why things are slow. I hope we can write more tools\nin the future that would more automatically guide people through potential\nperformance pitfals.\nCheers,\nfijal\nEdit: I was wrong about re.sub. It just seems to be that the JIT is figuring match better than sub, will be fixed soon",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/10/speeding-up-json-encoding-in-pypy-8937643890263223898.html"
    },
    {
      "title": "PyPy G\u00f6teborg Post-Hallowe'en Sprint Nov 2nd - Nov 9th",
      "text": "The next PyPy sprint will be in Gothenburg, Sweden. It is a public sprint,\nsuitable for newcomers.  We'll focus on making a public kickoff for\nboth the numpy/pypy integration project\nand the Py3k support project,\nas well as whatever interests the Sprint attendees.  Since both of these\nprojects are very new, there will be plenty of work suitable for newcomers\nto PyPy.\nOther topics might include:\n\nHelping people get their code running with PyPy\nwork on a FSCons talk?\nstate of the STM Vinnova project (We most likely, but not for certain will\nknow whether or not we are approved by this date.)\n\n\nOther Useful dates\nGothPyCon - Saturday Oct 29.\nFSCONS Friday Nov 11 - Sunday Nov 12.\n\n\nLocation\nThe sprint will be held in the apartment of Laura Creighton and Jacob Hall\u00e9n\nwhich is at G\u00f6tabergsgatan 22 in Gothenburg, Sweden.  Here is a map.  This is\nin central Gothenburg.  It is between the tram stops of Vasaplatsen and\nValand, (a distance of 4 blocks) where many lines call -- the 2, 3, 4, 5,\n7, 10 and 13.\nProbably cheapest and not too far away is to book accomodation at SGS\nVeckobostader. The  Elite Park Avenyn Hotel is a luxury hotel just a\nfew blocks away. There are scores of hotels a short walk away from the\nsprint location, suitable for every budget, desire for luxury, and desire\nfor the unusual.  You could, for instance, stay on a boat.  Options are\ntoo numerous to go into here. Just ask in the mailing list or on the blog.\nHours will be\nfrom 10:00 until people have had enough.  It's a good idea to arrive a\nday before the sprint starts and leave a day later.  In the middle of\nthe sprint there usually is a break day and it's usually ok to take\nhalf-days off if you feel like it.  Of course, many of you may be interested\nin sticking around for FSCons, held the weekend after the sprint.\n\n\nGood to Know\nSweden is not part of the Euro zone. One SEK (krona in singular, kronor\nin plural) is roughly 1/10th of a Euro (9.36 SEK to 1 Euro).\nThe venue is central in Gothenburg.  There is a large selection of\nplaces to get food nearby, from edible-and-cheap to outstanding.  We\noften cook meals together, so let us know if you have any food allergies,\ndislikes, or special requirements.\nSweden uses the same kind of plugs as Germany. 230V AC.\n\n\nGetting Here\nIf are coming train, you will arrive at the Central Station.  It is\nabout 12 blocks to the site from there, or you can take a tram.\nThere are two airports which are local to G\u00f6teborg, Landvetter (the main\none) and Gothenburg City Airport (where some budget airlines fly).\nIf you arrive at Landvetter  the airport bus stops right downtown at\nElite Park Avenyn Hotel which is the second stop, 4 blocks from the\nSprint site, as well as the end of the line, which is the Central Station.\nIf you arrive at Gothenburg City Airport take the bus to the end of the\nline.  You will be at the  Central Station.\nYou can also arrive by ferry, from either Kiel in Germany or Frederikshavn\nin Denmark.\n\n\nWho's Coming?\nIf you'd like to come, please let us know when you will be arriving and\nleaving, as well as letting us know your interests  We'll keep a list\nof people which we'll update (which you can do so yourself if you\nhave bitbucket pypy commit rights).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/10/pypy-goteborg-post-halloween-sprint-nov-7335004338996313725.html"
    },
    {
      "title": "Numpy funding and status update",
      "text": "Hi everyone,\nIt's been a little while since we wrote about NumPy on PyPy, so we wanted to\ngive everyone an update on what we've been up to, and what's up next for us.\nWe would also like to note that we're launching a funding campaign\nfor NumPy support in PyPy. Details can be found on the donation page.\nSome of the things that have happened since last we wrote are:\n\nWe added dtype support, meaning you can now create arrays of a bunch of\ndifferent types, including bools, ints of a various sizes, and floats.\nMore array methods and ufuncs, including things like comparison methods\n(==, >, etc.)\nSupport for more and more argument types, for example you can index by a\ntuple now (only works with tuples of length one, since we only have\nsingle-dimension arrays thus far).\n\nSome of the things we're working on at the moment:\n\nMore dtypes, including complex values and user-defined dtypes.\nSubscripting arrays by other array as indices, and by bool arrays as masks.\nStarting to reuse Python code from the original numpy.\n\nSome of the things on the near horizon are:\n\nBetter support for scalar data, for example did you know that\nnumpy.array([True], dtype=bool)[0] doesn't return a bool object?\nInstead it returns a numpy.bool_.\nMulti-dimensional array support.\n\nIf you're interested in helping out, we always love more contributors,\nAlex, Maciej, Justin, and the whole PyPy team",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2011/10/numpy-funding-and-status-update-2380711174693638392.html"
    },
    {
      "title": "More Compact Lists with List Strategies",
      "text": "Since we come closer to merging the list-strategy branch I want to try to explain this memory optimization today. Datatypes in PyPy are stored as W_<type>Objects (e.g. W_StringObject to represent strings, W_IntObject to represent ints). This is necessary due to the dynamic nature of Python. So the actual value (e.g. string, integer) is stored inside that box, resulting in an indirection. When having a large amount of such boxed objects, for example in a list, the wasted memory can become quite large.   If you have a closer look at such lists, you will see that in many of them only one type of data is stored and only few (and smaller) lists store mixed types. Another thing to observe is that those lists often won't change the types of the objects they contain at runtime very often. For instance a list of a million integers is very unlikely to suddenly get a string appended to it. List StrategiesThe goal of this work is to write an optimization that exploits this behaviour. Instead of wrapping all items in a list, we implement lists in a way that they are optimized for storing certain (primitive) datatypes. These implementations store the content of the list in unwrapped form, getting rid of the extra indirection and wrapper objects. One approach would be to add a level of indirection, making each W_ListObject instance point to another object that stores the actual content. For this other object, several implementations would exist, for every datatype we want to store without wrapping it (as well as a general one that deals with arbitrary content). The data layout would look something like this:  This approach has the problem that we need two indirections to get to the data and that the implementation instances need memory themselves.What we would like to do is to make the W_ListObject point to an RPython list directly, that contains either wrapped or unwrapped data. This plan has the problem that storing different unwrapped data is not directly possible in RPython.  To solve the problem, we use the rerased RPython library module. It allows us to erase the type of an object, in this case lists, and returns something similar to void-star in C, or Object in Java. This object is then stored on the W_ListObject in the field storage. If we want to work with the list, for example to append or delete items, we need to unerase the storage again.Example for rerase: storage = erase([1 ,2 ,3 ,4])\n# storage is an opaque object that you can do nothing with\n....\nl = unerase(storage)\nl.clear()\nNow that we know how to make the W_ListObject point directly to wrapped or unwrapped data, we need to find out how to actually do any operations on this data. This can be accomplished by adding another field to our W_ListObject. This field points to a ListStrategy object. The actual implementation of W_ListObject is now deferred to those ListStrategy classes. For instance, a W_ListObject which holds only integers will use the IntegerListStrategy.When the type of content is being changed, we need to change the used strategy as well as the storage in compatible ways. For example when we add a string to the list of integers we need to switch to the ObjectListStrategy and change the storage to be a list of wrapped objects. Thus the currently used strategy always knows what to do with what is currently in the storage.  As you can see, we now save one level of indirections by storing some of the data unwrapped. Of course each operation on a list needs to go via the strategy, but since we save one indirection for each element stored in that list and the Strategy classes are singletons, the benefits outweigh the costs.Currently there are only strategies for integers and strings since many lists seem to have these datatypes. Other strategies i.e for floats and unicode strings are planned. We also implemented two special strategies for empty lists and range-lists. The EmptyListStrategy's storage is None. If objects are added to the list we just switch to the appropriate strategy (determined by the item's type). RangeListsStrategies do not store any items at all. Instead they only store values describing the range of the list, i.e. start, step and length. On any operations that changes the data of the list we switch to the IntegerStrategy.A nice side-effect of storing unwrapped datatypes is that we can implement optimized methods for certain cases. For instance, since comparison of unwrapped integers is now much faster than comparison between arbitrary objects, we can rewrite the sorting methods for lists containing integers.MicrobenchmarksFinally here is an early overview of the memory consumption of different Python implementations: CPython, PyPy and PyPy-list which uses list-strategies. To demonstrate how powerful list-strategies can be in the best case, we wrote benchmarks that create a list of integers, a list of strings and a range-list each with one million elements each and then reads out the heap size of the process as reported by the OS. The results are as follows:    The savings on integers and strings in this ideal case are quite big.The benchmark for range-lists is a little unfair, since in CPython one could accomplish the same memory behaviour using xrange. However, in PyPy users won't notice that internally the list does not store all items, making it still possible to use all list methods, such as append or delete.ConclusionWe hope that list strategies bring memory savings for applications that use homogeneous lists of primitive types. Furthermore, operations on such lists tend to be somewhat faster as well. This also integrates well with the JIT. The list strategies optimizations will be merged to the PyPy's default branch at some point in the next months. An equivalent optimization for dictionaries has already been merged (and is part of PyPy 1.6), one for sets is coming in the future.Lukas Diekmann and Carl Friedrich Bolz",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/10/more-compact-lists-with-list-strategies-8229304944653956829.html"
    },
    {
      "title": "Py3k for PyPy fundraiser",
      "text": "Hi,We would like to announce a donation campaign for implementing Python 3 in PyPy.\nPlease read our detailed plan for all the details and donate using the\nbutton on that page!Thanks,\nThe PyPy Team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/09/py3k-for-pypy-fundraiser-8139653689520709617.html"
    },
    {
      "title": "Wrapping C++ Libraries with Reflection \u2014 Status Report One Year Later",
      "text": "Well over a year ago, work was started on the cppyy module which lives in the\nreflex-support branch.\nSince then, work has progressed at a varying pace and has included a recent\nsprint in D\u00fcsseldorf, last July.\nLet's first take a step back and recap why we're interested in doing this,\ngiven that it is perfectly possible to use C++ through generated bindings and\ncpyext.\ncppyy makes use of reflection information generated for the C++ classes of\ninterest, and has that reflection information available at run time.\nTherefore, it is able to open up complex C++ types to the JIT in a\nconceptually similar manner as simple types are open to it.\nThis means that it is possible to get rid of a lot of the marshalling layers\nwhen making cross-language calls, resulting in much lower call overhead than\nis possible when going through the CPython API, or other methods of wrapping.\nThere are two problems that need to be solved: C++ language constructs need to\nbe presented on the Python side in a natural way; and cross-language impedance\nmismatches need to be minimized, with some hints of the user if need be.\nFor the former, the list of mapped features has grown to a set that is\nsufficient to do real work.\nThere is now support for:\n\n\nbuiltin, pointer, and array types\nnamespaces, classes, and inner classes\nglobal functions, global data\nstatic/instance data members and methods\ndefault variables, object return by value\nsingle and multiple (virtual) inheritance\ntemplated classes\nbasic STL support and pythonizations\nbasic (non-global) operator mapping\n\n\nThe second problem is harder and will always be an on-going process.\nBut one of the more important issues has been solved at the recent D\u00fcsseldorf\nsprint, namely, that of reclaiming C++ objects instantiated from the Python\nside by the garbage collector.\nPerformance has also improved, especially that of the nicer \"pythonized\"\ninterface that the user actually sees, although it still misses out on\nabout a factor of 2.5 in comparison to the lower-level interface (which has\ngotten uglier, so you really don't want to use that).\nMost of this improvement is due to restructuring so that it plays nicer with\nthe JIT and libffi, both of which themselves have seen improvements.\nWork is currently concentrated on the back-ends: a CINT back-end is underway\nand a LLVM/CLang pre-compiled headers (PCH) back-end is planned.\nThe latter is needed for this code to be released in the wild, rather than\njust used in high energy physics (HEP), as that would be easier to support.\nAlso, within HEP, CLang's PCH are foreseen to be the future format of\nreflection information.\nAt the end of the D\u00fcsseldorf sprint, we tried a little code that did something\nactually \"useful,\" namely the filling of a histogram with some random values.\nWe did get it to work, but trying cppyy on a large class library showed\nthat a good warning system for such things like missing classes was sorely\nneeded.\nThat has been added since, and revisiting the histogram example later, here is\nan interesting note: the pypy-c run takes 1.5x the amount of time of that\nof the compiled, optimized, C++ code.\nThe run was timed start to finish, including the reflection library loading\nand JIT warm-up that is needed in the case of Python, but not for the compiled\nC++ code.\nHowever, in HEP, scientists run many short jobs while developing their\nanalysis codes, before submitting larger jobs on the GRID to run during lunch\ntime or overnight.\nThus, a more realistic comparison is to include the compilation time needed\nfor the C++ code and with that, the Python code needs only 55% of the time\nrequired by C++.\nThe choice of a programming language is often a personal one, and such\narguments like the idea that C++ is hard to use typically do not carry much\nweight with the in-crowd that studies quantum field dynamics for fun.\nHowever, getting the prompt with your analysis results back faster is a sure\nwinner. We hope that cppyy will soon have progressed far enough to make it\nuseful first to particle physicists and then other uses for wrapping C++\nlibraries.\n\nWim Lavrijsen, Carl Friedrich Bolz, Armin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/wrapping-c-libraries-with-reflection-3916959558080483711.html"
    },
    {
      "title": "We need Software Transactional Memory",
      "text": "Hi all.  Here is (an extract of) a short summary paper about my current position on\nSoftware Transactional Memory as a general tool in the implementation\nof Python or Python-like languages.  Thanks to people on IRC for discussion on making\nthis blog post better (lucian, Alex Gaynor, rguillebert, timonator, Da_Blitz).\nFor the purpose of the present discussion, we are comparing Java with Python\nwhen it comes to multi-threading.\n\nThe problem in complex high-level languages\nLike Java, the Python language gives guarantees: it is not acceptable\nfor the Python virtual machine to crash due to incorrect usage of\nthreads.  A primitive operation in Java is something like reading or\nwriting a field of an object; the corresponding guarantees are along the\nlines of: if the program reads a field of an object, and another thread\nwrites to the same field of the same object, then the program will see\neither the old value, or the new value, but not something else entirely,\nand the virtual machine will not crash.\nHigher-level languages like Python differ from Java by the fact that a\n\"primitive operation\" is far more complex.  It may for example involve\nlooking in several hash maps, perhaps doing updates.  In general, it is\ncompletely impossible to map every operation that must be atomic to a\nsingle processor instruction.\n\nJython: fine-grained locking\nThis problem has been solved \"explicitly\" in the Jython interpreter that\nruns on top of Java.  The solution is explicit in the following sense:\nthroughout the Jython interpreter, every single operation makes careful\nuse of Java-level locking mechanisms.  This is an application of\n\"fine-grained locking\".  For example, operations like attribute lookup,\nwhich need to perform look-ups in a number of hash maps, are protected\nby acquiring and releasing locks (in __getattribute__).\nA draw-back of this solution is the attention to detail required.\nIf even one place misses a lock, then there is either a\nbug --- and such bugs occur in cases that are increasingly rare and hard\nto debug as the previous bugs are fixed --- or we just file it under \"differences\nfrom CPython\".  There is however the risk of\ndeadlock, if two threads attempt to lock the same objects in different\norder.\n\nIn practice, the situation is actually not as bad as\nI may paint it: the number of locks in Jython is reasonable, and allows for\nall the \"common cases\" to work as expected.\n(For the uncommon cases, see below.)\n\nPerformance-wise, the Java virtual machine itself comes with locks that\nhave been heavily optimized over a long period of time, so the\nperformance is acceptable.  However if this solution were coded in C, it\nwould need a lot of extra work to optimize the locks manually (possibly\nintroducing more of the subtle bugs).\n\nCPython: coarse-grained locking\nCPython, the standard implementation of Python in C, took a different\nand simpler approach: it has a single global lock, called the Global\nInterpreter Lock (GIL).  It uses \"coarse-grained locking\": the lock is\nacquired and released around the whole execution of one bytecode (or\nactually a small number of bytecodes, like 100).  This solution is\nenough to ensure that no two operations can conflict with each other,\nbecause the two bytecodes that invoke them are themselves\nserialized by the GIL.  It is a solution which avoids --- unlike Jython\n--- writing careful lock-acquiring code all over the interpreter.  It\nalso offers even stronger guarantees: every bytecode runs entirely\natomically.\nNowadays, the draw-back of the GIL approach is obvious on multi-core\nmachines: by serializing the execution of bytecodes, starting multiple\nthreads does not actually let the interpreter use of more than one core.\nPyPy, the Python implementation in Python, takes the same approach so\nfar.\n\nExisting usage\nAs we have seen, we have the following situation: the existing Python\nlanguage, as CPython implements it, offers very strong guarantees about\nmulti-threaded usage.  It is important to emphasize that most existing\nmulti-threaded Python programs actually rely on such strong guarantees.\nThis can be seen for example in a problem that takes a populated list\nand does in several threads:\n\nnext_item = global_list.pop()\n\nThis implicitly relies on the fact that pop() will perform atomic\nremoval from the list.  If two threads try to pop() from the same list\nat the same time, then the two operations will occur in one order or the\nother; but they will not e.g. return the same object to both threads or\nmess up the internal state of the list object.\nWith such an example in mind, it should be clear that we do not want a\nsolution to the multi-core issue that involves dropping these strong\nguarantees.  It is ok however to lower the barrier, as Jython does; but\nany Python implementation must offer some guarantees, or not offer\nmulti-threading at all.  This includes the fact that a lot of methods on\nbuilt-in types are supposed to be atomic.\n\n(It should be noted that not offering multi-threading at all is actually\nalso a (partial) solution to the problem.  Recently, several \"hacks\"\nhave appeared that give a programmer more-or-less transparent access to\nmultiple independent processes (e.g. multiprocessing).  While these provide appropriate\nsolutions in some context, they are not as widely applicable as\nmulti-threading.  As a typical example, they fail to apply when the\nmutiple cores need to process information that cannot be serialized at\nall --- a requirement for any data exchange between several processes.)\n\nHere is an example of how Jython's consistency is weaker than CPython's GIL.\nIt takes uncommon examples to show it, and the fact that it does not work\nlike a CPython programmer expect them to is generally considered as an\nimplementation detail.  Consider:\nThread 1:  set1.update(set2)\nThread 2:  set2.update(set3)\nThread 3:  set3.update(set1)\nEach operation is atomic in the case of CPython, but decomposed in two steps\n(which can each be considered atomic) in the case of Jython: reading from the\nargument, and then updating the target set.  Suppose that initially\nset1 = {1}, set2 = {2}, set3 = {3}.  On CPython, independently on\nthe order in which the threads run, we will end up with at least one of the\nsets being {1, 2, 3}.  On Jython, it is possible that all\nthree sets end up as containing two items only.  The example is a bit\nfar-fetched but should show that CPython's consistency is strictly stronger\nthan Jython's.\n\nPyPy\nPyPy is a Python interpreter much like CPython or Jython, but the way it\nis produced is particular.  It is an interpreter written in RPython, a\nsubset of Python, which gets turned into a complete virtual machine (as\ngenerated C code) automatically by a step called the \"translation\".  In\nthis context, the trade-offs are different from the ones in CPython and\nin Jython: it is possible in PyPy, and even easy, to apply arbitrary\nwhole-program transformations to the interpreter at \"translation-time\".\nWith this in mind, it is possible to imagine a whole-program\ntransformation that would add locking on every object manipulated in\nRPython by the interpreter.  This would end up in a situation similar to\nJython.  However, it would not automatically solve the issue of\ndeadlocks, which is avoided in the case of Jython by careful manual\nplacement of the locks.  (In fact, being deadlock-free is a global\nprogram property that cannot be automatically ensured or verified; any\nchange to Jython can in theory break this property, and thus introduce\nsubtle deadlocks.  The same applies to non-atomicity.)\nIn fact, we can easily check that if the interpreter accesses (for\nboth reading and writing)\nobjects A and B in a bytecode of thread 1, and objects B and A (in the\nopposite order) in a bytecode of thread 2 --- and moreover if you need to\nhave accessed the first object before you can decide that you will need\nto access the second object --- then there is no way (apart from the GIL) to avoid\na deadlock while keeping the strong guarantee of atomicity.  Indeed, if\nboth threads have progressed to the middle of the execution of their\nbytecode, then A has already been mutated by thread 1 and similarly B\nhas already been mutated by thread 2.  It is not possible to\nsuccessfully continue running the threads in that case.\n\nUsing Software Transactional Memory\nSoftware Transactional Memory (STM) is an approach that gives a solution\nto precisely the above problem.  If a thread ended up in a situation\nwhere continuing to run it would be wrong, then we can abort and\nrollback.  This is similar to the notion of transaction on databases.\nIn the above example, one or both threads would notice that they are\nabout to run into troubles and abort.  This means more concretely that\nthey need to have a way to restart execution at the start of the\nbytecode, with all the side-effects of what they did so far being either\ncancelled or just not committed yet.\nWe think that this capacity to abort and rollback is the missing piece\nof the puzzle of multi-threaded implementations of Python.\nActually, according to the presentation of the problem given\nabove, it is unavoidable that any solution that wants to offer the\nsame level of consistency and atomicity as CPython would involve\nthe capacity of aborting and rolling back --- which means precisely\nthat STM cannot be avoided.\n\nOk, but why not settle down with Jython's\napproach and put careful locks left and right throughout the interpreter?\nBecause (1) we would have to consider every operation's atomicity and make decisions\n(or steal Jython's) and document them\nhere;\n(2) it would also be really a lot of work, to optimize these locks e.g. with the\nJIT as well as the JVM does; and (3) it is not the PyPy way to require manually\ntweaking your code everywhere for a feature that should be orthogonal.  Point\n(3) is probably the most important here: you need to redo the work for every\nlanguage you implement in PyPy.\nIt also implies my own point (4): it is not fun :-)\n\nIn more details, the process would work as follows.  (This gives an\noverview of one possible model; it is possible that a different model\nwill end up being better.)  In every thread:\n\nAt the start of a bytecode, we start a \"transaction\".  This means\nsetting up a thread-local data structure to record a log of what\noccurs in the transaction.\nWe record in the log all objects that are read, as well as the\nmodifications that we would like to make.\nDuring this time, we detect \"read\" inconsistencies, shown by the\nobject's \"last-modified\" timestamp being later than the start time\nof the current transaction, and abort.  This prevents the rest of\nthe code from running with inconsistent values.\nIf we reach the end of the bytecode without a \"read\" inconsistency,\nthen we atomically check for \"write\" inconsistencies.  These are\ninconsistencies which arise from concurrent updates to objects\nin the other threads --- either our \"write\" objects, or our \"read\"\nobjects.\nIf no inconsistency is found, we \"commit\" the transaction by copying\nthe delayed writes from the log into main memory.\n\n\nThe points at which a transaction starts or ends are exactly the\npoints at which, in CPython, the Global Interpreter Lock is\nrespectively acquired and released.  If we ignore the fact that (purely for\nperformance) CPython acquires and releases the GIL only every N bytecodes,\nthen this means:\n\nBefore any bytecode we acquire the GIL (start a transaction), and after\nthe bytecode we release it (ends the transaction); and\nBefore doing an external call to the C library or the OS we release the GIL\n(ends the transaction) and afterwards re-acquire it (start the next transaction).\n\nSo in particular this model is well suited to the STM condition that we cannot\ndo anything in a transaction that cannot be rolled back, like --- precisely ---\nsystem calls.  Indeed, by construction, these system calls occur outside a\ntransaction, because in CPython they occur with the GIL released.\n\nPerformance\nA large number of implementation details are still open for now.\nFrom a user's point of view (i.e. the programmer using Python),\nthe most relevant one is the overall performance impact.  We\ncannot give precise numbers so far, and we expect the initial\nperformance to be abysmally bad (maybe 10x slower); however, with\nsuccessive improvements to the locking mechanism, to the global\nprogram transformation inserting the locks, to the garbage \ncollector (GC), and to the Just-in-Time (JIT) compiler, we\nbelieve that it should be possible to get a roughly reasonable\nperformance (up to maybe 2x slower).  For example, the GC can\nmaintain flags on the objects to know that they did not escape\ntheir creation thread, and do not need any logging; and the JIT\ncompiler can aggregate several reads or writes to an object into\none.  We believe that these are the kind of optimizations that\ncan give back a lot of the performance lost.\n\nThe state of STM\nTransactional Memory is itself a relatively old idea, originating\nfrom a 1986 paper by Tom Knight.  At first based on hardware\nsupport, the idea of software-only transactional memory (STM) was\npopularized in 1995 and has recently been the focus of intense \nresearch.\nThe approach outlined above --- using STM to form the core of the\nimplementation of a language --- is new, as far as we know.  So\nfar, most implementations provide STM as a library feature.  It\nrequires explicit usage, often in the form of explicitly\ndeclaring which objects must be protected by STM (object-based\nSTMs).  It is only recently that native STM support has started\nto appear, notably in the Clojure language.\nSTM is described on Wikipedia as an approach that \"greatly\nsimplifies conceptual understanding of multithreaded programs and\nhelps make programs more maintainable by working in harmony with\nexisting high-level abstractions such as objects and modules.\"\nWe actually think that these benefits are important enough to\nwarrant being exposed to the Python programmer as well, instead\nof being used only internally.  This would give the Python\nprogrammer a very simple interface:\n\nwith atomic:\n    <these operations are executed atomically>\n\n(This is an old idea.  Funny how back in 2003 people, including me, thought that this was a hack.  Now I'm writing a blog post to say \"it was not a hack; it's explicitly using locks that is a hack.\"  I'm buying the idea of composability.)\n\nFrom a practical point of view, I started looking seriously at\nthe University of Rochester STM (RSTM), a C++ library that has\nbeen a focus of --- and a collection of results from --- recent\nresearch.  One particularly representative paper is\nA\nComprehensive Strategy for Contention Management in Software\nTransactional Memory by Michael F. Spear, Luke Dalessandro,\nVirendra J. Marathe and Michael L. Scott.\n\nConclusion\nTaking these ideas and applying them in the context of an\nimplementation of a complex high-level language like Python comes\nwith its own challanges.  In this context, using PyPy makes sense\nas both an experimentation platform and as a platform that is\nrecently gaining attention for its performance.  The alternatives\nare unattractive: doing it in CPython for example would mean\nglobally rewriting the interpreter.  In PyPy instead, we write it\nas a transformation that is applied systematically at translation-time.\nAlso, PyPy is a general platform for generating fast interpreters\nfor dynamic languages; the STM implementation in PyPy would work\nout of the box for other language implementations as well, instead\nof just for Python.\n\nUpdate:\n\nThis is mostly me (Armin Rigo) ranting aloud and trying experiments;\nthis post should not be confused as meaning that the whole PyPy team\nwill now spend the next years working on it full-time.\nAs I said it is orthogonal to the actual Python interpreter, and it is in\nany case a feature that can be turned on or off during translation; I know\nthat in many or most use cases, people are more interested in getting a\nfast PyPy rather than one which is twice as slow but scales well.\nNothing I said is really new.  For proof, see\nRiley and Zilles (2006)\nas well as Tabba (2010) who both experimented with Hardware Transactional Memory, turning CPython or PyPy interpreter's GIL into start/end transactions, as I describe here.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/we-need-software-transactional-memory-6513983438425039230.html"
    },
    {
      "title": "PyPy 1.6 - kickass panda",
      "text": "We're pleased to announce the 1.6 release of PyPy. This release brings a lot\nof bugfixes and performance improvements over 1.5, and improves support for\nWindows 32bit and OS X 64bit. This version fully implements Python 2.7.1 and\nhas beta level support for loading CPython C extensions.  You can download it\nhere:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.1. It's fast (pypy 1.6 and cpython 2.6.2 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release supports x86 machines running Linux 32/64 or Mac OS X.  Windows 32\nis beta (it roughly works but a lot of small issues have not been fixed so\nfar).  Windows 64 is not yet supported.\nThe main topics of this release are speed and stability: on average on\nour benchmark suite, PyPy 1.6 is between 20% and 30% faster than PyPy 1.5,\nwhich was already much faster than CPython on our set of benchmarks.\nThe speed improvements have been made possible by optimizing many of the\nlayers which compose PyPy.  In particular, we improved: the Garbage Collector,\nthe JIT warmup time, the optimizations performed by the JIT, the quality of\nthe generated machine code and the implementation of our Python interpreter.\n\n\nHighlights\n\nNumerous performance improvements, overall giving considerable speedups:\nbetter GC behavior when dealing with very large objects and arrays\nfast ctypes: now calls to ctypes functions are seen and optimized\nby the JIT, and they are up to 60 times faster than PyPy 1.5 and 10 times\nfaster than CPython\nimproved generators(1): simple generators now are inlined into the caller\nloop, making performance up to 3.5 times faster than PyPy 1.5.\nimproved generators(2): thanks to other optimizations, even generators\nthat are not inlined are between 10% and 20% faster than PyPy 1.5.\nfaster warmup time for the JIT\nJIT support for single floats (e.g., for array('f'))\noptimized dictionaries: the internal representation of dictionaries is now\ndynamically selected depending on the type of stored objects, resulting in\nfaster code and smaller memory footprint.  For example, dictionaries whose\nkeys are all strings, or all integers. Other dictionaries are also smaller\ndue to bugfixes.\n\n\nJitViewer: this is the first official release which includes the JitViewer,\na web-based tool which helps you to see which parts of your Python code have\nbeen compiled by the JIT, down until the assembler. The jitviewer 0.1 has\nalready been release and works well with PyPy 1.6.\nThe CPython extension module API has been improved and now supports many\nmore extensions. For information on which one are supported, please refer to\nour compatibility wiki.\nMultibyte encoding support: this was of of the last areas in which we were\nstill behind CPython, but now we fully support them.\nPreliminary support for NumPy: this release includes a preview of a very\nfast NumPy module integrated with the PyPy JIT.  Unfortunately, this does\nnot mean that you can expect to take an existing NumPy program and run it on\nPyPy, because the module is still unfinished and supports only some of the\nnumpy API. However, barring some details, what works should be\nblazingly fast :-)\nBugfixes: since the 1.5 release we fixed 53 bugs in our bug tracker, not\ncounting the numerous bugs that were found and reported through other\nchannels than the bug tracker.\n\nCheers,\nHakan Ardo, Carl Friedrich Bolz, Laura Creighton, Antonio Cuni,\nMaciej Fijalkowski, Amaury Forgeot d'Arc, Alex Gaynor,\nArmin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/pypy-16-kickass-panda-559424594592497545.html"
    },
    {
      "title": "Visualization of JITted code",
      "text": "Hello.\nWe're proud to announce the first public release of the jitviewer. As of now,\njitviewer is a slightly internal tool that helps understanding how your Python\nsource code is compiled by the PyPy's JIT all the way down to machine code.\nTo install it, you need a very recent version of PyPy\n(newer than 9th of August), for example one of the nightly builds:\n\n\ninstall pip and distribute either by creating a PyPy virtualenv\nor by following the installation instructions.\nmake sure to have a source code checkout of PyPy and put it in your\nPYTHONPATH.\npip install jitviewer.  Note that you need to run the pip\nexecutable which belongs to PyPy, not the globally installed one.\n\n\nHave a look at the README for how to start it, or try the online demo if\nyou just want to play with it.\nThe jitviewer is a web application written with flask and jinja2.  If\nyou have experience with web development and you want to help PyPy, don't\nhesitate to contact us, there are plenty of things to improve in it :-).\n\nWhat does the jitviewer really do?\nAt the top of the page, you will see the list of pieces of code which has been\ncompiled by the JIT.  You will see entries for both normal loops and for\n\"entry bridges\".  This is not the right place to discuss the difference\nbetween those, but you most probably want to look at loops, because usually\nit's where most of the time is spent.\nNote that for each loop, you will see the name of the function which contains\nthe first instruction of the loop.  However, thanks to the inlining done\nby the JIT, it will contain also the code for other functions.\nOnce you select a loop, the jitviewer shows how the JIT has compiled the\nPython source code into assembler in a hierarchical way. It displays four\nlevels:\n\nPython source code: only the lines shown in azure have been compiled for\nthis particular loop, the ones in gray have not.\n\nPython bytecode, the one you would get by doing:\n\ndef f(a, b):\n   return a + b\n\nimport dis\ndis.dis(f)\n\nThe opcodes are e.g. LOAD_FAST, LOAD_GLOBAL etc.  The opcodes\nwhich are not in bold have been completely optimized aways by the JIT.\n\nIntermediate representation of jit code (IR). This is a combination of\noperations (like integer addition, reading fields out of structures) and\nguards (which check that the assumptions we made are actually true). Guards\nare in red.  These operations are \"at the same level as C\": so, for example,\n+ takes two unboxed integers which can be stored into the register\nof the CPU.\n\nAssembler: you can see it by clicking on \"Show assembler\" in the menu on the\nright.\n\n\nSometimes you'll find that a guard fails often enough that a new piece of\nassembler is required to be compiled. This is an alternative path through the\ncode and it's called a bridge. You can see bridges in the jitviewer when\nthere is a link next to a guard. For more information about purpose look up\nthe jit documentation.\n\n\nI'm still confused\nJitviewer is not perfect when it comes to explaining what's going on. Feel free\nto pop up on IRC or send us a mail to the mailing list, we'll try to explain\nand/or improve the situation. Consult the contact page for details.\nCheers,\nfijal & antocuni",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/visualization-of-jitted-code-6202490807361942120.html"
    },
    {
      "title": "PyPy is faster than C, again: string formatting",
      "text": "String formatting is probably something you do just about every day in Python,\nand never think about.  It's so easy, just \"%d %d\" % (i, i) and you're\ndone.  No thinking about how to size your result buffer, whether your output\nhas an appropriate NULL byte at the end, or any other details.  A C\nequivalent might be:\n\nchar x[44];\nsprintf(x, \"%d %d\", i, i);\n\nNote that we had to stop for a second and consider how big numbers might get\nand overestimate the size (44 = length of the biggest number on 64bit (20) +\n1 for the sign * 2 + 1 (for the space) + 1 (NUL byte)), it took the authors of\nthis post, fijal and alex, 3 tries to get the math right on this :-)\nThis is fine, except you can't even return x from this function, a more\nfair comparison might be:\n\nchar *x = malloc(44 * sizeof(char));\nsprintf(x, \"%d %d\", i, i);\n\nx is slightly overallocated in some situations, but that's fine.\nBut we're not here to just discuss the implementation of string\nformatting, we're here to discuss how blazing fast PyPy is at it, with\nthe new unroll-if-alt branch.  Given the Python code:\n\ndef main():\n    for i in xrange(10000000):\n        \"%d %d\" % (i, i)\n\nmain()\n\nand the C code:\n\n#include <stdio.h>\n#include <stdlib.h>\n\n\nint main() {\n    int i = 0;\n    char x[44];\n    for (i = 0; i < 10000000; i++) {\n        sprintf(x, \"%d %d\", i, i);\n    }\n}\n\nRun under PyPy, at the head of the unroll-if-alt branch, and\ncompiled with GCC 4.5.2 at -O4 (other optimization levels were tested,\nthis produced the best performance). It took 0.85 seconds to\nexecute under PyPy, and 1.63 seconds with the compiled binary. We\nthink this demonstrates the incredible potential of dynamic\ncompilation, GCC is unable to inline or unroll the sprintf call,\nbecause it sits inside of libc.\nBenchmarking the C code:\n\n#include <stdio.h>\n#include <stdlib.h>\n\n\nint main() {\n    int i = 0;\n    for (i = 0; i < 10000000; i++) {\n        char *x = malloc(44 * sizeof(char));\n        sprintf(x, \"%d %d\", i, i);\n        free(x);\n    }\n}\n\nWhich as discussed above, is more comperable to the Python, gives a\nresult of 1.96 seconds.\nSummary of performance:\n\n\n\n\n\n\n\n\n\nPlatform\nGCC (stack)\nGCC (malloc)\nCPython\nPyPy (unroll-if-alt)\n\nTime\n1.63s\n1.96s\n10.2s\n0.85s\n\nrelative to C\n1x\n0.83x\n0.16x\n1.9x\n\n\n\nOverall PyPy is almost 2x faster. This is clearly win for dynamic\ncompilation over static - the sprintf function lives in libc and so\ncannot be specializing over the constant string, which has to be parsed\nevery time it's executed. In the case of PyPy, we specialize\nthe assembler if we detect the left hand string of the modulo operator\nto be constant.\nCheers,\nalex & fijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/08/pypy-is-faster-than-c-again-string-6756589731691762127.html"
    },
    {
      "title": "Realtime image processing in Python",
      "text": "Image processing is notoriously a CPU intensive task.  To do it in realtime,\nyou need to implement your algorithm in a fast language, hence trying to do it\nin Python is foolish: Python is clearly not fast enough for this task. Is it?\n:-)\nActually, it turns out that the PyPy JIT compiler produces code which is fast\nenough to do realtime video processing using two simple algorithms implemented\nby H\u00e5kan Ard\u00f6.\nsobel.py implements a classical way of locating edges in images, the\nSobel operator. It is an approximation of the magnitude of the image\ngradient. The processing time is spend on two convolutions between the\nimage and 3x3-kernels.\nmagnify.py implements a pixel coordinate transformation that rearranges\nthe pixels in the image to form a magnifying effect in the center.\nIt consists of a single loop over the pixels in the output image copying\npixels from the input image.\nYou can try by yourself by downloading the appropriate demo:\n\n\npypy-image-demo.tar.bz2: this archive contains only the source code,\nuse this is you have PyPy already installed\npypy-image-demo-full.tar.bz2: this archive contains both the source\ncode and prebuilt PyPy binaries for linux 32 and 64 bits\n\n\nTo run the demo, you need to have mplayer installed on your system.  The\ndemo has been tested only on linux, it might (or not) work also on other\nsystems:\n$ pypy pypy-image-demo/sobel.py\n\n$ pypy pypy-image-demo/magnify.py\n\nBy default, the two demos uses an example AVI file.  To have more fun, you can\nuse your webcam by passing the appropriate mplayer parameters to the scripts,\ne.g:\n$ pypy demo/sobel.py tv://\n\nBy default magnify.py uses nearest-neighbor interpolation.  By adding the\noption -b, bilinear interpolation will be used instead, which gives\nsmoother result:\n$ pypy demo/magnify.py -b\n\nThere is only a single implementation of the algorithm in\nmagnify.py. The two different interpolation methods are implemented by\nsubclassing the class used to represent images and embed the\ninterpolation within the pixel access method. PyPy is able to achieve good\nperformance with this kind of abstractions because it can inline\nthe pixel access method and specialize the implementation of the algorithm.\nIn C++ that kind of pixel access method would be virtual and you'll need to use\ntemplates to get the same effect without incurring in runtime overhead.\n\n\n\n\n\n\nThe video above shows PyPy and CPython running sobel.py side by\nside (PyPy taking input from the webcam, CPython from the test\nfile). Alternatively, to have a feeling on how much PyPy is faster than\nCPython, try to run the demo with the latter.  These are the the average fps\n(frames per second) that I get on my machine (Ubuntu 64 bit, Intel i7 920, 4GB\nRAM) when processing the default test.avi video and using the prebuilt\nPyPy binary found in the full tarball alinked above.  For sobel.py:\n\n\nPyPy: ~47.23 fps\nCPython: ~0.08 fps\n\n\nFor magnify.py:\n\n\nPyPy: ~26.92 fps\nCPython: ~1.78 fps\n\n\nThis means that on sobel.py, PyPy is 590 times faster.  On\nmagnify.py the difference is much less evident and the speedup is \"only\"\n15x.\nIt must be noted that this is an extreme example of what PyPy can do.  In\nparticular, you cannot expect (yet :-)) PyPy to be fast enough to run an\narbitrary video processing algorithm in real time, but the demo still proves\nthat PyPy has the potential to get there.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/07/realtime-image-processing-in-python-6985924592886873374.html"
    },
    {
      "title": "Global Interpreter Lock, or how to kill it",
      "text": "People that listened to my (Armin Rigo) lightning talk at EuroPython know that\nsuddenly, we have a plan to remove the Global Interpreter Lock --- the\ninfamous GIL, the thing in CPython that prevents multiple threads from\nactually running in your Python code in parallel.\nThat's not actually new, because Jython has been doing it all along.\nJython works by very carefully adding locks to\nall the mutable built-in types, and by relying on the underlying Java\nplatform to be efficient about them (so that the result is faster than,\nsay, very carefully adding similar locks in CPython).  By \"very\ncarefully\", I mean really really carefully; for example,\n'dict1.update(dict2)' needs to lock both dict1 and dict2, but if you do\nit naively, then a parallel 'dict2.update(dict1)' might cause a\ndeadlock.\nAll of PyPy, CPython and IronPython have a GIL.  But for PyPy we are considering\na quite different approach than Jython's, based on Software\nTransactional Memory.  This is a recent development in computer\nscience, and it gives a nicer solution than locking.  Here is a short\nintroduction to it.\nSay you want to atomically pop an item from 'list1' and append it to\n'list2':\n\ndef f(list1, list2):\n    x = list1.pop()\n    list2.append(x)\n\nThis is not safe in multithreaded cases (even with the GIL).  Say that\nyou call f(l1, l2) in thread 1 and f(l2, l1) in thread 2.  What\nyou want is that it has no effect at all (x is moved from one list to\nthe other, then back).  But what can occur is that instead the top of\nthe two lists are swapped, depending on timing issues.\nOne way to fix it is with a global lock:\n\ndef f(list1, list2):\n    global_lock.acquire()\n    x = list1.pop()\n    list2.append(x)\n    global_lock.release()\n\nA finer way to fix it is with locks that come with the lists:\n\ndef f(list1, list2):\n    acquire_all_locks(list1.lock, list2.lock)\n    x = list1.pop()\n    list2.append(x)\n    release_all_locks(list1.lock, list2.lock)\n\nThe second solution is a model for Jython's, while the first is a model\nfor CPython's.  Indeed, in CPython's interpreter, we acquire the GIL,\nthen we do one bytecode (or actually a number of them, like 100), then\nwe release the GIL; and then we proceed to the next bunch of 100.\nSoftware Transactional Memory (STM) gives a third solution:\n\ndef f(list1, list2):\n    while True:\n        t = transaction()\n        x = list1.pop(t)\n        list2.append(t, x)\n        if t.commit():\n            break\n\nIn this solution, we make a transaction object and use it in all\nreads and writes we do to the lists.  There are actually several\ndifferent models, but let's focus on one of them.  During a transaction,\nwe don't actually change the global memory at all.  Instead, we use the\nthread-local transaction object.  We store in it which objects we\nread from, which objects we write to, and what values we write.  It is\nonly when the transaction reaches its end that we attempt to \"commit\"\nit.  Committing might fail if other commits have occurred in between,\ncreating inconsistencies; in that case, the transaction aborts and\nmust restart from the beginning.\nIn the same way as the previous two solutions are models for CPython and\nJython, the STM solution looks like it could be a model for PyPy in the\nfuture.  In such a PyPy, the interpreter would start a transaction, do\none or several bytecodes, and then end the transaction; and repeat.\nThis is very similar to what is going on in CPython with the GIL.  In\nparticular, it means that it gives programmers all the same guarantees\nas the GIL does.  The only difference is that it can actually run\nmultiple threads in parallel, as long as their code does not interfere\nwith each other.  (In particular, if you need not just the GIL but actual\nlocks in your existing multi-threaded program, then this will not\nmagically remove the need for them.  You might get an additional built-in\nmodule that exposes STM to your Python programs, if you prefer it over\nlocks, but that's another question.)\nWhy not apply that idea to CPython?  Because we would need to change\neverything everywhere.  In the example above, you may have noted that I\nno longer call 'list1.pop()', but 'list1.pop(t)'; this is a way to tell\nthat the implementation of all the methods needs to be changed, in order\nto do their work \"transactionally\".  This means that instead of really\nchanging the global memory in which the list is stored, it must instead\nrecord the change in the transation object.  If our interpreter is\nwritten in C, as CPython is, then we need to write it explicitly\neverywhere.  If it is written instead in a higher-level language, as\nPyPy is, then we can add this behavior as as set of translation rules, and\napply them automatically wherever it is necessary.  Moreover, it can be\na translation-time option: you can either get the current \"pypy\" with a\nGIL, or a version with STM, which would be slower due to the extra\nbookkeeping.  (How much slower?  I have no clue, but as a wild guess,\nmaybe between 2 and 5 times slower.  That is fine if you have enough\ncores, as long as it scales nicely :-)\nA final note: as STM research is very recent (it started around 2003),\nthere are a number of variants around, and it's not clear yet which one\nis better in which cases.  As far as I can tell, the approach described\nin \"A Comprehensive Strategy for Contention Management in Software\nTransactional Memory\" seems to be one possible state-of-the-art; it also\nseems to be \"good enough for all cases\".\nSo, when will it be done?  I cannot say yet.  It is still at the idea\nstage, but I think that it can work.  How long would it take us to\nwrite it?  Again no clue, but we are looking at many months rather\nthan many days.  This is the sort of thing that I would\nlike to be able to work on full time after the Eurostars funding\nruns out on September 1.  We are currently looking at ways to use\ncrowdfunding to raise money so that I can do exactly that.  Expect\na blog post about that very soon.  But this looks like a perfect\ncandidate for crowdfunding -- there are at least thousands of you who\nwould be willing to pay 10s of Euros to Kill the GIL.  Now we only\nhave to make this happen.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/06/global-interpreter-lock-or-how-to-kill-8270246310848099963.html"
    },
    {
      "title": "Report back from our survey",
      "text": "Hi all,\nI'm here to report back the results of our survey. First, we're very pleased to\nreport that a number of you guys are happilly running PyPy in production! Most\n(97%) of the respondants using PyPy are using it because it's faster, but a\nfurther 26% (respondants could choose multiple answers) are using it because of\nlower memory usage. Of users who aren't using PyPy, the most common reason was\nC extensions, followed by \"Other\".\nFrom reading the extra comments section there are a few things we've learned:\n\nGoogle docs needs a better UI for this stuff\nA huge number of people want NumPy and SciPy, it was easily the most\nrequested C extension (25% of respondants said somthing about NumPy). We've\nalready blogged on the topic of our plans for NumPy.\nHaving packages in the various OS's repositories would be a big help in\ngetting users up and running.\n\nA huge thanks to everyone who responded! Finally, if you're using PyPy in\nproduction we'd love to get a testimonial from you, if you're willing to spare\na few minutes to give us a quote or two please get in contact with us via our\nmailing list.\nThanks,\nAlex",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/06/report-back-from-our-survey-2083371215707583264.html"
    },
    {
      "title": "PyPy Genova-Pegli Post-EuroPython Sprint June 27 - July 2 2011",
      "text": "The next PyPy sprint will be in Genova-Pegli, Italy, the week after EuroPython\n(which is in Florence, about 3h away by train). This is a fully public sprint:\nnewcomers and topics other than those proposed below are welcome.\n\n\n\nGoals and topics of the sprint\n\n\nNow that we have released 1.5, the sprint itself is going to be mainly\nworking on fixing issues reported by various users.  Possible topics\ninclude, but are not limited to:\n\n\nfixing issues in the bug tracker\nimprove cpyext, the C-API compatibility layer, to support more extension\nmodules\nfinish/improve/merge jitypes2, the branch which makes ctypes JIT friendly\ngeneral JIT improvements\nimprove our tools, like the jitviewer or the buildbot infrastructure\nmake your favorite module/application working on PyPy, if it doesn't yet\n\n\n\n\nOf course this does not prevent people from showing up with a more precise\ninterest in mind  If there are newcomers, we will gladly give introduction\ntalks.\n\n\nSince we are almost on the beach, we can take one day off for summer\nrelaxation and/or tourist visits nearby :-).\n\n\n\n\n\n\nExact times\nThe work days should be 27 June - 2 July 2011.  People may arrive on\nthe 26th already and/or leave on the 3rd.\n\n\n\nLocation & Accomodation\nBoth the sprint venue and the lodging will be at Albergo Puppo in\nGenova-Pegli, Italy.  Pegli is a nice and peaceful little quarter of Genova,\nand the hotel is directly on the beach, making it a perfect place for those\nwho want to enjoy the sea in the middle of the Italian summer, as a quick\nsearch on Google Images shows :-)\n\nThe place has a good ADSL Internet connexion with wireless installed.  You can\nof course arrange your own lodging anywhere but I definitely recommend lodging\nthere too.\nPlease confirm that you are coming so that we can adjust the reservations as\nappropriate.  The prices are as follows, and they include breakfast and a\nparking place for the car, in case you need it:\n\n\nsingle room:  70 \u20ac\ndouble room:  95 \u20ac\ntriple room: 105 \u20ac\n\n\nPlease register by hg:\n\nhttps://foss.heptapod.net/pypy/extradoc/-/blob/branch/default/extradoc/sprintinfo/genova-pegli-2011/people.txt\nor on the pypy-dev mailing list if you do not yet have check-in rights:\n\nhttps://mail.python.org/mailman/listinfo/pypy-dev\nIn case you want to share a room with someone else but you don't know who,\nplease let us know (either by writing it directly in people.txt or by writing\non the mailing list) and we will try to arrange it.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/05/pypy-genova-pegli-post-europython-4004229800858530064.html"
    },
    {
      "title": "PyPy Usage Survey",
      "text": "We've been working on PyPy for a long time. But readers of this blog will know\nthat in the past year something has changed: we think PyPy is production ready.\nAnd it's not just us, this week LWN.net wrote an article about how PyPy\nsped up one of their scripts by a factor of three, noting that, \"plans are to\nrun gitdm under PyPy from here on out\". All in all we think PyPy is pretty\ngreat, but not everyone is using it yet, and we want to know why. We want your\nfeedback on why PyPy isn't ready to be your only Python yet, and how we can\nimprove it to make that happen.\nTherefore, we've put together a quick survey, whether you're using PyPy or not\nif you could take a few minutes to fill it out and let us know how we're doing\nwe'd really appreciate it. You can find the form here.\nThanks,\nThe PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/05/pypy-usage-survey-1402303968715807009.html"
    },
    {
      "title": "Server migration in progress",
      "text": "Hi all,\n\nWe are in the process of migrating the hosting machine for PyPy, moving away from codespeak.net and towards a mixture of custom servers (e.g. for buildbot.pypy.org) and wide-scale services (e.g. for the docs, now at readthedocs.org).\n\nWhen this is done, a proper announce will be posted here.  In the meantime, we have already moved the mailing lists, now hosted on python.org.  The subscribers' list have been copied, so if you didn't notice anything special for the past week, then everything works fine :-)  This concerns pypy-dev, pypy-issue and pypy-commit.  Two notes:\nSome settings have not been copied, notably if you used to disable mail delivery.  Sorry about that; you have to re-enter such settings.\nFollowing the move, about 50 addresses have been dropped for being invalid.  I'm unsure why they were not dropped earlier, but in case sending mail to you from python.org instead of codespeak.net fails, then you have been dropped from the mailing lists, and you need to subscribe again.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/05/server-migration-in-progress-2113491786141182920.html"
    },
    {
      "title": "Playing with Linear Programming on PyPy",
      "text": "Fancy hi-level interfaces often come with a high runtime overhead\nmaking them slow. Here is an experiment with building such an\ninterface using constructions that PyPy should be good at\noptimizing. The idea is to allow the JIT in PyPy to remove the\noverhead introduced by using a fancy high-level python interface\non top of a low-level C interface. The application considered is\nLinear\nprogramming. It is a tool used to solve linear optimization\nproblems. It can for example be used to find the nonnegative values\nx, y and z that gives the maximum value of\n\n\n\n\n\nwithout violating the constraints\n\n\n\n\n\n\n\nThere exists general purpose solvers for these kind of problems that\nare very fast and can literally handle millions of variables. To use\nthem however the problem has to be transformed into some specific\nmatrix form, and the coefficients of all the matrices\nhas to be passed to the solver using some API. This transformation is\na tedious and error prone step that forces you to work with matrix\nindexes instead of readable variable names. Also it makes maintaining\nan implementation hard since any modification has to be transformed\ntoo.\n\n\nThe example above comes from the manual of\nthe glpk library. That\nmanual continues by describing how to convert this problem into the\nstandard form of glpk (which involves introducing three new variables)\nand then gives the c-code needed to call the\nlibrary. Relating that c-code to the problem above without the\nintermediate explanation of the manual is not easy. A common\nsolution here is to build a hi-level interface that allows a more\nnatural way of defining the matrices and/or allow the equations to be\nentered symbolically. Unfortunately, such interfaces often become\nslow. For the benchmark below for example, \ncvxopt\nrequires 20 minutes to setup a problem that takes 9.43 seconds to solve\n(this seems a bit extreme, am I doing something wrong?).\n\n\nThe high-level interface I constructed on top of the\nglpk library is \npplp and it allows\nthe equations to be entered symbolically. The above problem can be\nsolved using\n\n    lp = LinearProgram()\n    x, y, z = lp.IntVar(), lp.IntVar(), lp.IntVar()\n    lp.objective = 10*x + 6*y + 4*z\n    lp.add_constraint( x + y + z <= 100 )\n    lp.add_constraint( 10*x + 4*y + 5*z <= 600 )\n    lp.add_constraint( 2*x + 2*y + 6*z <= 300 )\n    lp.add_constraint( x >= 0 )\n    lp.add_constraint( y >= 0 )\n    lp.add_constraint( z >= 0 )\n\n    maxval = lp.maximize()\n    print maxval\n    print x.value, y.value, z.value\n\n\n\nTo benchmark the API I used it to solve a \nminimum-cost\n  flow problem with 154072 nodes and 390334 arcs. The C library\n  needs 9.43 s to solve this and the pplp interface adds another 5.89\n  s under PyPy and 28.17 s under CPython. A large amount of time is\n  still spend setting up the problem, but it's a significant\n  improvement over the 20 minutes required on CPython by\n  cvxopt. It is\n  probably not designed to be fast on this kind of benchmark. I have\n  not been able to get cvxopt to work under PyPy. The benchmark used is\n  available here",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/05/playing-with-linear-programming-on-pypy-4040572987275633047.html"
    },
    {
      "title": "NumPy Follow up",
      "text": "Hi everyone.  Since yesterday's blog post we got a ton of feedback, so we want\nto clarify a few things, as well as share some of the progress we've made, in\nonly the 24 hours since the post.\nReusing the original NumPy\nFirst, a lot of people have asked why we cannot just reuse the original NumPy\nthrough cpyext, our CPython C-API compatibility layer.  We believe this is\nnot the best approach, for a few reasons:\n\n\ncpyext is slow, and always will be slow. It has to emulate far too many\ndetails of the CPython object model that don't exist on PyPy (e.g.,\nreference counting). Since people are using NumPy primarily for speed this\nwould mean that even if we could have a working NumPy, no one would want to\nuse it.  Also, as soon as the execution crosses the cpyext boundary, it\nbecomes invisible to the JIT, which means the JIT has to assume the worst\nand deoptimize stuff away.\nNumPy uses many obscure documented and undocumented details of the CPython\nC-API. Emulating these is often difficult or impossible (e.g. we can't fix\naccessing a struct field, as there's no function call for us to intercept).\nIt's not much fun. Frankly, working on cpyext, debugging the crashes,\nand everything else that goes with it is not terribly fun, especially when\nyou know that the end result will be slow. We've demonstrated we can build\na much faster NumPy, in a way that's more fun, and given that the people\nworking on this are volunteers, it's important to keep us motivated.\n\n\nFinally, we are not proposing to rewrite the entirety of NumPy or, god\nforbid, BLAST, or any of the low level stuff that operates on C-level arrays,\nonly the parts that interface with Python code directly.\nC bindings vs. CPython C-API\nThere are two issues on C code, one has a very nice story, and the other not so\nmuch. First is the case of arbitrary C-code that isn't Python related, things\nlike libsqlite, libbz2, or any random C shared library on your system.\nPyPy will quite happily call into these, and bindings can be developed either\nat the RPython level (using rffi) or in pure Python, using ctypes.\nWriting bindings with ctypes has the advantage that they can run on every\nalternative Python implementation, such as Jython and IronPython.  Moreover,\nonce we merge the jittypes2 branch ctypes calls will even be smoking\nfast.\nOn the other hand there is the CPython C-extension API. This is a very specific\nAPI which CPython exposes, and PyPy tries to emulate. It will never be fast,\nbecause there is far too much overhead in all the emulation that needs to be\ndone.\nOne of the reasons people write C extensions is for speed.  Often, with PyPy\nyou can just forget about C, write everything in pure python and let the JIT to\ndo its magic.\nIn case the PyPy JIT alone isn't fast enough, or you just want to\nuse existing C code then it might make sense to split\nyour C-extension into 2 parts, one which doesn't touch the CPython C-API and\nthus can be loaded with ctypes and called from PyPy, and another which does\nthe interfacing with Python for CPython (where it will be faster).\nThere are also libraries written in C to interface with existing C codebases,\nbut for whom performance is not the largest goal, for these the right solution\nis to try using CPyExt, and if it works that's great, but if it fails the\nsolution will be to rewrite using ctypes, where it will work on all Python\nVMs, not just CPython.\nAnd finally there are rare cases where rewriting in RPython makes more sense,\nNumPy is one of the few examples of these because we need to be able to give\nthe JIT hints on how to appropriately vectorize all of the operations on an\narray.  In general writing in RPython is not necessary for almost any\nlibraries, NumPy is something of a special case because it is so ubiquitous\nthat every ounce of speed is valuable, and makes the way people use it leads to\ncode structure where the JIT benefits enormously from extra hints and the\nability to manipulate memory directly, which is not possible from Python.\nProgress\nOn a more positive note, after we published the last post, several new people\ncame and contributed improvements to the numpy-exp branch. We would like to\nthank all of them:\n\n\nnightless_night contributed: An implementation of __len__, fixed bounds\nchecks on __getitem__ and __setitem__.\nbrentp contributed: Subtraction and division on NumPy arrays.\nMostAwesomeDude contributed: Multiplication on NumPy arrays.\nhodgestar contributed: Binary operations between floats and NumPy arrays.\n\n\nThose last two were technically an outstanding branch we finally merged, but\nhopefully you get the picture. In addition there was some exciting work done by\nregular PyPy contributors. I hope it's clear that there's a place to jump in\nfor people with any level of PyPy familiarity. If you're interested in\ncontributing please stop by #pypy on irc.freenode.net, the pypy-dev mailing\nlist, or send us pull requests on bitbucket.\nAlex",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2011/05/numpy-follow-up-6928627691060102514.html"
    },
    {
      "title": "Numpy in PyPy - status and roadmap",
      "text": "Hello.\nNumPy integration is one of the single most requested features for PyPy. This\npost tries to describe where we are, what we plan (or what we don't plan), and\nhow you can help.\nShort version for the impatient: we are doing experiments, which show that\nPyPy+numpy can be faster and better than CPython+numpy.  We have a plan on how\nto move forward, but at the moment there is lack of dedicated people or money\nto tackle it.\n\nThe slightly longer version\nIntegrating numpy in PyPy has been my pet project on an on-and-off (mostly off)\nbasis over the past two years. There were some experiments, then a long\npause, and then some more experiments which are documented below.\nThe general idea is not to use the existing CPython module, but to\nreimplement numpy in RPython (i.e. the language PyPy is implemented in), thus\nletting our JIT achieve extra speedups. The really cool thing about this part\nis that numpy will automatically benefit of any general JIT improvements,\nwithout any need of extra tweaking.\nAt the moment, there is branch called numpy-exp which contains a\ntranslatable version of a very minimal version of numpy in the module called\nmicronumpy. Example benchmarks show the following:\n\n\n\n\n\n\n\n\u00a0\nadd\niterate\n\nCPython 2.6.5 with numpy 1.3.0\n0.260s (1x)\n4.2 (1x)\n\nPyPy numpy-exp @ 3a9d77b789e1\n0.120s (2.2x)\n0.087 (48x)\n\n\n\nThe add benchmark spends most of the time inside the + operator on\narrays (doing a + a + a + a + a), , which in CPython is implemented in C.\nAs you can see from the table above, the PyPy version is already ~2 times\nfaster. (Although numexpr is still faster than PyPy, but we're working on it).\nThe exact way array addition is implemented is worth another blog post, but in\nshort it lazily evaluates the expression and computes it at the end, avoiding\nintermediate results. This approach scales much better than numexpr\nand can lead to speeding up all the operations that you can perform on matrices.\nThe next obvious step to get even more speedups would be to extend the JIT to\nuse SSE operations on x86 CPUs, which should speed it up by about additional\n2x, as well as using multiple threads to do operations.\niterate is also interesting, but for entirely different reasons. On CPython\nit spends most of the time inside a Python loop; the PyPy version is ~48 times\nfaster, because the JIT can optimize across the python/numpy boundary, showing\nthe potential of this approach, users are not grossly penalized for writing\ntheir loops in Python.\nThe drawback of this approach is that we need to reimplement numpy in RPython,\nwhich takes time.  A very rough estimate is that it would be possible to\nimplement an useful subset of it (for some definition of useful) in a period\nof time comprised between one and three man-months.\nIt also seems that the result will be faster for most cases and the same speed\nas original numpy for other cases. The only problem is finding the dedicated\npersons willing to spend quite some time on this and however, I am willing to\nboth mentor such a person and encourage him or her.\nThe good starting point for helping would be to look at what's already\nimplemented in micronumpy modules and try extending it. Adding a - operator\nor adding integers would be an interesting start. Drop by on #pypy on\nirc.freenode.net or get in contact with developers via some other channel (such\nas the pypy-dev mailing list) if you want to help.\nAnother option would be to sponsor NumPy development. In case you're\ninterested, please get in touch with us or leave your email in comments.\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2011/05/numpy-in-pypy-status-and-roadmap-8332894230779779992.html"
    },
    {
      "title": "PyPy 1.5 Released: Catching Up",
      "text": "We're pleased to announce the 1.5 release of PyPy. This release updates\nPyPy with the features of CPython 2.7.1, including the standard library. Thus\nall the features of CPython 2.6 and CPython 2.7 are now supported. It\nalso contains additional performance improvements. You can download it here:\n\nhttps://pypy.org/download.html\n\nWhat is PyPy?\nPyPy is a very compliant Python interpreter, almost a drop-in replacement for\nCPython 2.7.1. It's fast (pypy 1.5 and cpython 2.6.2 performance comparison)\ndue to its integrated tracing JIT compiler.\nThis release includes the features of CPython 2.6 and 2.7. It also includes a\nlarge number of small improvements to the tracing JIT compiler. It supports\nIntel machines running Linux 32/64 or Mac OS X.  Windows is beta (it roughly\nworks but a lot of small issues have not been fixed so far).  Windows 64 is\nnot yet supported.\nNumerous speed achievements are described on our blog. Normalized speed\ncharts comparing pypy 1.5 and pypy 1.4 as well as pypy 1.5 and cpython\n2.6.2 are available on our benchmark website. The speed improvement over 1.4\nseems to be around 25% on average.\n\n\nMore highlights\n\nThe largest change in PyPy's tracing JIT is adding support for loop invariant\ncode motion, which was mostly done by H\u00e5kan Ard\u00f6. This feature improves the\nperformance of tight loops doing numerical calculations.\nThe CPython extension module API has been improved and now supports many more\nextensions. For information on which one are supported, please refer to our\ncompatibility wiki.\nThese changes make it possible to support Tkinter and IDLE.\nThe cProfile profiler is now working with the JIT. However, it skews the\nperformance in unstudied ways. Therefore it is not yet usable to analyze\nsubtle performance problems (the same is true for CPython of course).\nThere is an external fork which includes an RPython version of the\npostgresql.  However, there are no prebuilt binaries for this.\nOur developer documentation was moved to Sphinx and cleaned up.\nand many small things :-)\n\nCheers,\nCarl Friedrich Bolz, Laura Creighton, Antonio Cuni, Maciej Fijalkowski,\nAmaury Forgeot d'Arc, Alex Gaynor, Armin Rigo and the PyPy team",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2011/04/pypy-15-released-catching-up-302997959079576809.html"
    },
    {
      "title": "Using Tkinter and IDLE with PyPy",
      "text": "We are pleased to announce that Tkinter, the GUI library based on TCL/TK, now\nworks with PyPy.\nTkinter is composed of two parts:\n\n\n_tkinter, a module written in C which interfaces with the TCL world\nTkinter, a pure Python package which wraps _tkinter to expose the\npythonic API we are used to\n\n\n\n\n\n\nThe PyPy version of _tkinter reuses the C code of as found in CPython and\ncompile it through the PyPy C-API compatibility layer, cpyext.  To make it\nwork with PyPy, we had to modify it slightly, in order to remove the\ndependency on some API functions which are not supported by PyPy.  In particular, we\nremoved the dependency on the PyOS_InputHook variable, which allows a nice\nintegration of Tkinter and the Python interactive prompt: the result is that,\nunlike CPython, in PyPy Tk windows created at the interactive prompt are not\nshown until we manually call the mainloop method.  Apart from this\ninconvenience, all the rest works fine.\nAt the moment, _tkinter is not distributed with PyPy because our build\nsystem does not support automatic compilation of C extension.  Instead, it is\nnecessary to install it manually, either directly from source or by\neasy_installing/pip installing tkinter-pypy from PyPI.\nFor everything to work correctly, you need a recent build of PyPy: the\nfollowing is a step-by-step guide to install _tkinter in a PyPy nightly\nbuild for Linux 64 bit; for other architectures, look at the nightly build\npage:\n$ wget https://buildbot.pypy.org/nightly/trunk/pypy-c-jit-43485-1615dfd7d8f1-linux64.tar.bz2\n\n$ tar xfv pypy-c-jit-43485-1615dfd7d8f1-linux64.tar.bz2\n\n$ cd pypy-c-jit-43485-1615dfd7d8f1-linux64/\n\n$ wget https://peak.telecommunity.com/dist/ez_setup.py\n\n$ ./bin/pypy ez_setup.py    # install setuptools\n\n$ ./bin/easy_install tkinter-pypy\n\nOnce you complete the steps above, you can start using Tkinter from your\npython programs.  In particular, you can use IDLE, the IDE which is part of\nthe Python standard library.  To start IDLE, type:\n$ ./bin/pypy -m idlelib.idle\n\nHave fun :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/04/using-tkinter-and-idle-with-pypy-6156563216925585965.html"
    },
    {
      "title": "Tutorial Part 2: Adding a JIT",
      "text": "This is the second part of a tutorial written by Andrew Brown. The first\npart described how to write an interpreter with PyPy.\n\nAdding JIT\nTranslating RPython to C is pretty cool, but one of the best features of PyPy\nis its ability to generate just-in-time compilers for your interpreter.\nThat's right, from just a couple hints on how your interpreter is structured,\nPyPy will generate and include a JIT compiler that will, at runtime, translate\nthe interpreted code of our BF language to machine code!\nSo what do we need to tell PyPy to make this happen? First it needs to know\nwhere the start of your bytecode evaluation loop is. This lets it keep track of\ninstructions being executed in the target language (BF).\nWe also need to let it know what defines a particular execution frame. Since\nour language doesn't really have stack frames, this boils down to what's\nconstant for the execution of a particular instruction, and what's not. These\nare called \"green\" and \"red\" variables, respectively.\nRefer back to example2.py for the following.\nIn our main loop, there are four variables used: pc, program, bracket_map, and\ntape. Of those, pc, program, and bracket_map are all green variables. They\ndefine the execution of a particular instruction. If the JIT routines see the\nsame combination of green variables as before, it knows it's skipped back and\nmust be executing a loop.  The variable \"tape\" is our red variable, it's what's\nbeing manipulated by the execution.\nSo let's tell PyPy this info. Start by importing the JitDriver class and making\nan instance:\nfrom pypy.rlib.jit import JitDriver\njitdriver = JitDriver(greens=['pc', 'program', 'bracket_map'],\n        reds=['tape'])\n\nAnd we add this line to the very top of the while loop in the mainloop\nfunction:\njitdriver.jit_merge_point(pc=pc, tape=tape, program=program,\n        bracket_map=bracket_map)\n\nWe also need to define a JitPolicy. We're not doing anything fancy, so this is\nall we need somewhere in the file:\ndef jitpolicy(driver):\n    from pypy.jit.codewriter.policy import JitPolicy\n    return JitPolicy()\n\nSee this example at example3.py\nNow try translating again, but with the flag --opt=jit:\n\n$ python ./pypy/pypy/translator/goal/translate.py --opt=jit example3.py\n\nIt will take significantly longer to translate with JIT enabled, almost 8\nminutes on my machine, and the resulting binary will be much larger. When it's\ndone, try having it run the mandelbrot program again. A world of difference,\nfrom 12 seconds compared to 45 seconds before!\nInterestingly enough, you can see when the JIT compiler switches from\ninterpreted to machine code with the mandelbrot example. The first few lines of\noutput come out pretty fast, and then the program gets a boost of speed and\ngets even faster.\n\n\nA bit about Tracing JIT Compilers\nIt's worth it at this point to read up on how tracing JIT compilers work.\nHere's a brief explanation: The interpreter is usually running your interpreter\ncode as written. When it detects a loop of code in the target language (BF) is\nexecuted often, that loop is considered \"hot\" and marked to be traced. The next\ntime that loop is entered, the interpreter gets put in tracing mode where every\nexecuted instruction is logged.\nWhen the loop is finished, tracing stops. The trace of the loop is sent to an\noptimizer, and then to an assembler which outputs machine code. That machine\ncode is then used for subsequent loop iterations.\nThis machine code is often optimized for the most common case, and depends on\nseveral assumptions about the code. Therefore, the machine code will contain\nguards, to validate those assumptions. If a guard check fails, the runtime\nfalls back to regular interpreted mode.\nA good place to start for more information is\nhttps://en.wikipedia.org/wiki/Just-in-time_compilation\n\n\nDebugging and Trace Logs\nCan we do any better? How can we see what the JIT is doing? Let's do two\nthings.\nFirst, let's add a get_printable_location function, which is used during debug\ntrace logging:\ndef get_location(pc, program, bracket_map):\n    return \"%s_%s_%s\" % (\n            program[:pc], program[pc], program[pc+1:]\n            )\njitdriver = JitDriver(greens=['pc', 'program', 'bracket_map'], reds=['tape'],\n        get_printable_location=get_location)\n\nThis function is passed in the green variables, and should return a string.\nHere, we're printing out the BF code, surrounding the currently executing\ninstruction with underscores so we can see where it is.\nDownload this as example4.py and translate it the same as example3.py.\nNow let's run a test program (test.b, which just prints the letter \"A\" 15 or so\ntimes in a loop) with trace logging:\n\n$ PYPYLOG=jit-log-opt:logfile ./example4-c test.b\n\nNow take a look at the file \"logfile\". This file is quite hard to read, so\nhere's my best shot at explaining it.\nThe file contains a log of every trace that was performed, and is essentially a\nglimpse at what instructions it's compiling to machine code for you. It's\nuseful to see if there are unnecessary instructions or room for optimization.\nEach trace starts with a line that looks like this:\n\n[3c091099e7a4a7] {jit-log-opt-loop\n\nand ends with a line like this:\n\n[3c091099eae17d jit-log-opt-loop}\n\nThe next line tells you which loop number it is, and how many ops are in it.\nIn my case, the first trace looks like this:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29  [3c167c92b9118f] {jit-log-opt-loop\n  # Loop 0 : loop with 26 ops\n  [p0, p1, i2, i3]\n  debug_merge_point('+<[>[_>_+<-]>.[<+>-]<<-]++++++++++.', 0)\n  debug_merge_point('+<[>[>_+_<-]>.[<+>-]<<-]++++++++++.', 0)\n  i4 = getarrayitem_gc(p1, i2, descr=<SignedArrayDescr>)\n  i6 = int_add(i4, 1)\n  setarrayitem_gc(p1, i2, i6, descr=<SignedArrayDescr>)\n  debug_merge_point('+<[>[>+_<_-]>.[<+>-]<<-]++++++++++.', 0)\n  debug_merge_point('+<[>[>+<_-_]>.[<+>-]<<-]++++++++++.', 0)\n  i7 = getarrayitem_gc(p1, i3, descr=<SignedArrayDescr>)\n  i9 = int_sub(i7, 1)\n  setarrayitem_gc(p1, i3, i9, descr=<SignedArrayDescr>)\n  debug_merge_point('+<[>[>+<-_]_>.[<+>-]<<-]++++++++++.', 0)\n  i10 = int_is_true(i9)\n  guard_true(i10, descr=<Guard2>) [p0]\n  i14 = call(ConstClass(ll_dict_lookup__dicttablePtr_Signed_Signed), ConstPtr(ptr12), 90, 90, descr=<SignedCallDescr>)\n  guard_no_exception(, descr=<Guard3>) [i14, p0]\n  i16 = int_and(i14, -9223372036854775808)\n  i17 = int_is_true(i16)\n  guard_false(i17, descr=<Guard4>) [i14, p0]\n  i19 = call(ConstClass(ll_get_value__dicttablePtr_Signed), ConstPtr(ptr12), i14, descr=<SignedCallDescr>)\n  guard_no_exception(, descr=<Guard5>) [i19, p0]\n  i21 = int_add(i19, 1)\n  i23 = int_lt(i21, 114)\n  guard_true(i23, descr=<Guard6>) [i21, p0]\n  guard_value(i21, 86, descr=<Guard7>) [i21, p0]\n  debug_merge_point('+<[>[_>_+<-]>.[<+>-]<<-]++++++++++.', 0)\n  jump(p0, p1, i2, i3, descr=<Loop0>)\n  [3c167c92bc6a15] jit-log-opt-loop}\n\nI've trimmed the debug_merge_point lines a bit, they were really long.\nSo let's see what this does. This trace takes 4 parameters: 2 object pointers\n(p0 and p1) and 2 integers (i2 and i3). Looking at the debug lines, it seems to\nbe tracing one iteration of this loop: \"[>+<-]\"\nIt starts executing the first operation on line 4, a \">\", but immediately\nstarts executing the next operation. The \">\" had no instructions, and looks\nlike it was optimized out completely.  This loop must always act on the same\npart of the tape, the tape pointer is constant for this trace. An explicit\nadvance operation is unnecessary.\nLines 5 to 8 are the instructions for the \"+\" operation. First it gets the\narray item from the array in pointer p1 at index i2 (line 6), adds 1 to it and\nstores it in i6 (line 7), and stores it back in the array (line 8).\nLine 9 starts the \"<\" instruction, but it is another no-op. It seems that i2\nand i3 passed into this routine are the two tape pointers used in this loop\nalready calculated. Also deduced is that p1 is the tape array. It's not clear\nwhat p0 is.\nLines 10 through 13 perform the \"-\" operation: get the array value (line 11),\nsubtract (line 12) and set the array value (line 13).\nNext, on line 14, we come to the \"]\" operation. Lines 15 and 16 check whether\ni9 is true (non-zero). Looking up, i9 is the array value that we just\ndecremented and stored, now being checked as the loop condition, as expected\n(remember the definition of \"]\").  Line 16 is a guard, if the condition is not\nmet, execution jumps somewhere else, in this case to the routine called\n<Guard2> and is passed one parameter: p0.\nAssuming we pass the guard, lines 17 through 23 are doing the dictionary lookup\nto bracket_map to find where the program counter should jump to.  I'm not too\nfamiliar with what the instructions are actually doing, but it looks like there\nare two external calls and 3 guards. This seems quite expensive, especially\nsince we know bracket_map will never change (PyPy doesn't know that).  We'll\nsee below how to optimize this.\nLine 24 increments the newly acquired instruction pointer. Lines 25 and 26 make\nsure it's less than the program's length.\nAdditionally, line 27 guards that i21, the incremented instruction pointer, is\nexactly 86. This is because it's about to jump to the beginning (line 29) and\nthe instruction pointer being 86 is a precondition to this block.\nFinally, the loop closes up at line 28 so the JIT can jump to loop body <Loop0>\nto handle that case (line 29), which is the beginning of the loop again. It\npasses in parameters (p0, p1, i2, i3).\n\n\nOptimizing\nAs mentioned, every loop iteration does a dictionary lookup to find the\ncorresponding matching bracket for the final jump. This is terribly\ninefficient, the jump target is not going to change from one loop to the next.\nThis information is constant and should be compiled in as such.\nThe problem is that the lookups are coming from a dictionary, and PyPy is\ntreating it as opaque. It doesn't know the dictionary isn't being modified or\nisn't going to return something different on each query.\nWhat we need to do is provide another hint to the translation to say that the\ndictionary query is a pure function, that is, its output depends only on its\ninputs and the same inputs should always return the same output.\nTo do this, we use a provided function decorator pypy.rlib.jit.purefunction,\nand wrap the dictionary call in a decorated function:\n@purefunction\ndef get_matching_bracket(bracket_map, pc):\n    return bracket_map[pc]\n\nThis version can be found at example5.py\nTranslate again with the JIT option and observe the speedup. Mandelbrot now\nonly takes 6 seconds!  (from 12 seconds before this optimization)\nLet's take a look at the trace from the same function:\n[3c29fad7b792b0] {jit-log-opt-loop\n# Loop 0 : loop with 15 ops\n[p0, p1, i2, i3]\ndebug_merge_point('+<[>[_>_+<-]>.[<+>-]<<-]++++++++++.', 0)\ndebug_merge_point('+<[>[>_+_<-]>.[<+>-]<<-]++++++++++.', 0)\ni4 = getarrayitem_gc(p1, i2, descr=<SignedArrayDescr>)\ni6 = int_add(i4, 1)\nsetarrayitem_gc(p1, i2, i6, descr=<SignedArrayDescr>)\ndebug_merge_point('+<[>[>+_<_-]>.[<+>-]<<-]++++++++++.', 0)\ndebug_merge_point('+<[>[>+<_-_]>.[<+>-]<<-]++++++++++.', 0)\ni7 = getarrayitem_gc(p1, i3, descr=<SignedArrayDescr>)\ni9 = int_sub(i7, 1)\nsetarrayitem_gc(p1, i3, i9, descr=<SignedArrayDescr>)\ndebug_merge_point('+<[>[>+<-_]_>.[<+>-]<<-]++++++++++.', 0)\ni10 = int_is_true(i9)\nguard_true(i10, descr=<Guard2>) [p0]\ndebug_merge_point('+<[>[_>_+<-]>.[<+>-]<<-]++++++++++.', 0)\njump(p0, p1, i2, i3, descr=<Loop0>)\n[3c29fad7ba32ec] jit-log-opt-loop}\n\nMuch better! Each loop iteration is an add, a subtract, two array loads, two\narray stores, and a guard on the exit condition. That's it! This code doesn't\nrequire any program counter manipulation.\nI'm no expert on optimizations, this tip was suggested by Armin Rigo on the\npypy-dev list. Carl Friedrich has a series of posts on how to optimize your\ninterpreter that are also very useful: https://bit.ly/bundles/cfbolz/1\n\n\nFinal Words\nI hope this has shown some of you what PyPy is all about other than a faster\nimplementation of Python.\nFor those that would like to know more about how the process works, there are\nseveral academic papers explaining the process in detail that I recommend. In\nparticular: Tracing the Meta-Level: PyPy's Tracing JIT Compiler.\nSee https://readthedocs.org/docs/pypy/en/latest/extradoc.html",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/04/tutorial-part-2-adding-jit-8121732841568309472.html"
    },
    {
      "title": "Tutorial: Writing an Interpreter with PyPy, Part 1",
      "text": "This is a guest blog post written by Andrew Brown, with help from the PyPy developers\non the pypy-dev mailing list.\nThis tutorial's master copy and supporting files live at\nhttps://bitbucket.org/brownan/pypy-tutorial/\n\nWhen I first learned about the PyPy project, it took me a while to figure out\nexactly what it was about. For those that don't already know, it's two things:\n\nA set of tools for implementing interpreters for interpreted languages\nAn implementation of Python using this toolchain\n\nThe second part is probably what most people think PyPy is, but this tutorial\nis not about their Python interpreter.  It is about writing your own\ninterpreter for your own language.\nThis is the project I undertook to help myself better understand how PyPy works\nand what it's all about.\nThis tutorial assumes you know very little about PyPy, how it works, and even\nwhat it's all about. I'm starting from the very beginning here.\n\nWhat PyPy Does\nHere's a brief overview of what PyPy can do. Let's say you want to write an\ninterpreted language. This involves writing some kind of source code parser, a\nbytecode interpretation loop, and lots of standard library code.\nThat's quite a bit of work for moderately complicated languages, and there's a\nlot of low level work involved. Writing the parser and compiler code usually\nisn't fun, that's why there are tools out there to generate parsers and\ncompilers for you.\nEven then, you still must worry about memory management in your interpreter,\nand you're going to be re-implementing a lot if you want data types like\narbitrary precision integers, nice general hash tables, and such. It's enough\nto put someone off from implementing their idea for a language.\nWouldn't it be nice if you could write your language in an existing high level\nlanguage like, for example, Python? That sure would be ideal, you'd get all the\nadvantages of a high level language like automatic memory management and rich\ndata types at your disposal.  Oh, but an interpreted language interpreting\nanother language would be slow, right? That's twice as much interpreting going\non.\nAs you may have guessed, PyPy solves this problem. PyPy is a sophisticated\ntoolchain for analyzing and translating your interpreter code to C code (or JVM\nor CLI). This process is called \"translation\", and it knows how to translate\nquite a lot of Python's syntax and standard libraries, but not everything. All\nyou have to do is write your interpreter in RPython, a subset of the Python\nlanguage carefully defined to allow this kind of analysis and translation, and\nPyPy will produce for you a very efficient interpreter.\nBecause efficient interpreters should not be hard to write.\n\n\nThe Language\nThe language I've chosen to implement is dead simple. The language runtime\nconsists of a tape of integers, all initialized to zero, and a single pointer\nto one of the tape's cells. The language has 8 commands, described here:\n\n>\nMoves the tape pointer one cell to the right\n\n\n<\nMoves the tape pointer one cell to the left\n+\nIncrements the value of the cell underneath the pointer\n-\nDecrements the value of the cell underneath the pointer\n\n\n[\nIf the cell under the current pointer is 0, skip to the instruction after\nthe matching ]\n\n\n]\nSkip back to the matching [ (evaluating its condition)\n\n\n.\nPrint out a single byte to stdout from the cell under the pointer\n\n\n,\nRead in a single byte from stdin to the cell under the pointer\n\nAny unrecognized bytes are ignored.\nSome of you may recognize this language. I will be referring to it as BF.\nOne thing to notice is that the language is its own bytecode; there is no\ntranslation from source code to bytecode. This means that the language can be\ninterpreted directly: the main eval loop of our interpreter will operate right\non the source code. This simplifies the implementation quite a bit.\n\n\nFirst Steps\nLet's start out by writing a BF interpreter in plain old Python. The first step\nis sketching out an eval loop:\ndef mainloop(program):\n    tape = Tape()\n    pc = 0\n    while pc < len(program):\n        code = program[pc]\n\n        if code == \">\":\n            tape.advance()\n        elif code == \"<\":\n            tape.devance()\n        elif code == \"+\":\n            tape.inc()\n        elif code == \"-\":\n            tape.dec()\n        elif code == \".\":\n            sys.stdout.write(chr(tape.get()))\n        elif code == \",\":\n            tape.set(ord(sys.stdin.read(1)))\n        elif code == \"[\" and value() == 0:\n            # Skip forward to the matching ]\n        elif code == \"]\" and value() != 0:\n            # Skip back to the matching [\n\n        pc += 1\n\nAs you can see, a program counter (pc) holds the current instruction index. The\nfirst statement in the loop gets the instruction to execute, and then a\ncompound if statement decides how to execute that instruction.\nThe implementation of [ and ] are left out here, but they should change the\nprogram counter to the value of the matching bracket. (The pc then gets\nincremented, so the condition is evaluated once when entering a loop, and once\nat the end of each iteration)\nHere's the implementation of the Tape class, which holds the tape's values as\nwell as the tape pointer:\nclass Tape(object):\n    def __init__(self):\n        self.thetape = [0]\n        self.position = 0\n\n    def get(self):\n        return self.thetape[self.position]\n    def set(self, val):\n        self.thetape[self.position] = val\n    def inc(self):\n        self.thetape[self.position] += 1\n    def dec(self):\n        self.thetape[self.position] -= 1\n    def advance(self):\n        self.position += 1\n        if len(self.thetape) <= self.position:\n            self.thetape.append(0)\n    def devance(self):\n        self.position -= 1\n\nAs you can see, the tape expands as needed to the right, indefinitely. We\nshould really add some error checking to make sure the pointer doesn't go\nnegative, but I'm not worrying about that now.\nExcept for the omission of the \"[\" and \"]\" implementation, this code will work\nfine.  However, if the program has a lot of comments, it will have to skip over\nthem one byte at a time at runtime. So let's parse those out once and for all.\nAt the same time, we'll build a dictionary mapping between brackets, so that\nfinding a matching bracket is just a single dictionary lookup. Here's how:\ndef parse(program):\n    parsed = []\n    bracket_map = {}\n    leftstack = []\n\n    pc = 0\n    for char in program:\n        if char in ('[', ']', '<', '>', '+', '-', ',', '.'):\n            parsed.append(char)\n\n            if char == '[':\n                leftstack.append(pc)\n            elif char == ']':\n                left = leftstack.pop()\n                right = pc\n                bracket_map[left] = right\n                bracket_map[right] = left\n            pc += 1\n\n    return \"\".join(parsed), bracket_map\n\nThis returns a string with all invalid instructions removed, and a dictionary\nmapping bracket indexes to their matching bracket index.\nAll we need is some glue code and we have a working BF interpreter:\ndef run(input):\n    program, map = parse(input.read())\n    mainloop(program, map)\n\nif __name__ == \"__main__\":\n    import sys\n    run(open(sys.argv[1], 'r'))\n\nIf you're following along at home, you'll also need to change the signature of\nmainloop() and implement the bracket branches of the if statement. Here's the\ncomplete example: example1.py\nAt this point you can try it out to see that it works by running the\ninterpreter under python, but be warned, it will be very slow on the more\ncomplex examples:\n\n$ python example1.py 99bottles.b\n\nYou can find mandel.b and several other example programs (not written by me) in\nmy repository.\n\n\nPyPy Translation\nBut this is not about writing a BF interpreter, this is about PyPy. So what\ndoes it take to get PyPy to translate this into a super-fast executable?\nAs a side note, there are some simple examples in the pypy/translator/goal\ndirectory of the PyPy source tree that are helpful here. My starting point for\nlearning this was the example \"targetnopstandalone.py\", a simple hello world\nfor PyPy.\nFor our example, the module must define a name called \"target\" which returns the\nentry point. The translation process imports your module and looks for that\nname, calls it, and the function object returned is where it starts the\ntranslation.\ndef run(fp):\n    program_contents = \"\"\n    while True:\n        read = os.read(fp, 4096)\n        if len(read) == 0:\n            break\n        program_contents += read\n    os.close(fp)\n    program, bm = parse(program_contents)\n    mainloop(program, bm)\n\ndef entry_point(argv):\n    try:\n        filename = argv[1]\n    except IndexError:\n        print \"You must supply a filename\"\n        return 1\n\n    run(os.open(filename, os.O_RDONLY, 0777))\n    return 0\n\ndef target(*args):\n    return entry_point, None\n\nif __name__ == \"__main__\":\n    entry_point(sys.argv)\n\nThe entry_point function is passed the command line arguments when you run the\nresulting executable.\nA few other things have changed here too. See the next section...\n\n\nAbout RPython\nLet's talk a bit about RPython at this point. PyPy can't translate arbitrary\nPython code because Python is a bit too dynamic. There are restrictions on what\nstandard library functions and what syntax constructs one can use. I won't be\ngoing over all the restrictions, but for more information see\nhttps://readthedocs.org/docs/pypy/en/latest/coding-guide.html#restricted-python\nIn the example above, you'll see a few things have changed.  I'm now using low\nlevel file descriptors with os.open and os.read instead of file objects.\nThe implementation of \".\" and \",\" are similarly tweaked (not shown above).\nThose are the only changes to make to this code, the rest is simple enough for\nPyPy to digest.\nThat wasn't so hard, was it? I still get to use dictionaries, expandable lists,\nand even classes and objects! And if low level file descriptors are too low for\nyou, there are some helpful abstractions in the rlib.streamio module included\nwith PyPy's \"RPython standard library.\"\nFor the example thus far, see example2.py\n\n\nTranslating\nIf you haven't already, check yourself out the latest version of PyPy from\ntheir bitbucket.org repository:\n\n$ hg clone https://bitbucket.org/pypy/pypy\n\n(A recent revision is necessary because of a bugfix that makes my example\npossible)\nThe script to run is in \"pypy/translator/goal/translate.py\". Run this script,\npassing in our example module as an argument.\n[A note added much later: this script has been moved to \"rpython/bin/rpython\".]\n\n$ python ./pypy/pypy/translator/goal/translate.py example2.py\n\n(You can use PyPy's python interpreter for extra speed, but it's not necessary)\nPyPy will churn for a bit, drawing some nice looking fractals to your console\nwhile it works. It takes around 20 seconds on my machine.\nThe result from this is an executable binary that interprets BF programs.\nIncluded in my repository are some example BF programs, including a mandelbrot\nfractal generator, which takes about 45 seconds to run on my computer. Try it\nout:\n\n$ ./example2-c mandel.b\n\nCompare this to running the interpreter un-translated on top of python:\n\n$ python example2.py mandel.b\n\nTakes forever, doesn't it?\nSo there you have it. We've successfully written our own interpreter in RPython\nand translated it with the PyPy toolchain.\n\n(more in the next blog post...)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/04/tutorial-writing-interpreter-with-pypy-3785910476193156295.html"
    },
    {
      "title": "PyPy G\u00f6teborg Post-Easter Sprint April 25 - May 1 2011",
      "text": "The next PyPy sprint will be in Gothenburg, Sweden. It is a public sprint,\nvery suitable for newcomers.  We'll focus on making the 1.5 release (if\nit hasn't already happened) and whatever interests the Sprint attendees.\n\nTopics and goals\nThe main goal is to polish and release PyPy 1.5, supporting Python 2.7\nas well as the last few months' improvements in the JIT (provided that\nit hasn't already happened).  Other topics:\n\nGoing over our documentation, and classifying our docs in terms of\nmouldiness.  Deciding what needs writing, and maybe writing it.\nHelping people get their code running with PyPy\nmaybe work on EuroPython Training, and talks\nSummer of Code preparation\nspeed.pypy.org\nany other programming task is welcome too -- e.g. tweaking the\nPython or JavaScript interpreter, Stackless support, and so on.\n\n\n\nLocation\nThe sprint will be held in the apartment of Laura Creighton and Jacob Hall\u00e9n\nwhich is at G\u00f6tabergsgatan 22 in Gothenburg, Sweden.  Here is a map.  This is\nin central Gothenburg.  It is between the tram stops of Vasaplatsen and\nValand, (a distance of 4 blocks) where many lines call -- the 2, 3, 4, 5,\n7, 10 and 13.\nProbably cheapest and not too far away is to book accomodation at SGS\nVeckobostader. The  Elite Park Avenyn Hotel is a luxury hotel just a\nfew blocks away. There are scores of hotels a short walk away from the\nsprint location, suitable for every budget, desire for luxury, and desire\nfor the unusual.  You could, for instance, stay on a boat.  Options are\ntoo numerous to go into here. Just ask in the mailing list or on the blog.\nHours will be\nfrom 10:00 until people have had enough.  It's a good idea to arrive a\nday before the sprint starts and leave a day later.  In the middle of\nthe sprint there usually is a break day and it's usually ok to take\nhalf-days off if you feel like it.\n\n\nGood to Know\nSweden is not part of the Euro zone. One SEK (krona in singular, kronor\nin plural) is roughly 1/10th of a Euro (9.36 SEK to 1 Euro).\nThe venue is central in Gothenburg.  There is a large selection of\nplaces to get food nearby, from edible-and-cheap to outstanding.  We\noften cook meals together, so let us know if you have any food allergies,\ndislikes, or special requirements.\nSweden uses the same kind of plugs as Germany. 230V AC.\nThe Sprint will be held the week following Easter.  This means, as always,\nthat Gothcon will be taking place the weekend before (Easter weekend).\nGothcon, now in its 35 year, is the largest European game players conference.\nSome of you may be interested in arriving early for the board games.\nThe conference site is only in Swedish, alas.  You don't need to register\nin advance unless you are planning to host a tournament, (and it's too\nlate for that anyway).\n\n\nGetting Here\nIf are coming train, you will arrive at the Central Station.  It is\nabout 12 blocks to the site from there, or you can take a tram.\nThere are two airports which are local to G\u00f6teborg, Landvetter (the main\none) and Gothenburg City Airport (where some budget airlines fly).\nIf you arrive at Landvetter  the airport bus stops right downtown at\nElite Park Avenyn Hotel which is the second stop, 4 blocks from the\nSprint site, as well as the end of the line, which is the Central Station.\nIf you arrive at Gothenburg City Airport take the bus to the end of the\nline.  You will be at the  Central Station.\nYou can also arrive by ferry, from either Kiel in Germany or Frederikshavn\nin Denmark.\n\n\nWho's Coming?\nIf you'd like to come, please let us know when you will be arriving and\nleaving, as well as letting us know your interests  We'll keep a list\nof people which we'll update (which you can do so yourself if you\nhave bitbucket pypy commit rights).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/04/pypy-goteborg-post-easter-sprint-april-16274563331982977.html"
    },
    {
      "title": "Controlling the Tracing of an Interpreter With Hints, Part 4: Benchmarks",
      "text": "This is part 4 and the final part of the series on how to speed up an interpreter\nwritten with PyPy by adding JIT hints to the interpreter. Part 1 described how\nto control the extent of tracing. Part 2 described how to influence the\noptimizer with promotion and pure functions. Part 3 described a simple object\nmodel and how it can be optimized by doing small rewrites. In this (short) post\nI present some benchmarks.\n\nBenchmarks\nFor the benchmarks I ran a subset of the benchmarks on https://speed.pypy.org\nwith CPython and four different executables of PyPy's Python interpreter (all\nwith a JIT). The executables contain all combinations of enabling maps (which\nmake instance attributes fast) and type versions (which makes method lookup\nfast).\n\npypy-slow: contains neither maps nor type versions.\npypy-map: contains maps but not type versions.\npypy-version: contains type versions but not maps.\npypy-full: contains both maps and type versions\n\nThe results are as follows:\n\nThe graph shows the speedup over CPython's numbers. The results are quite\ninteresting. Maps by themselves do not speed up much over the bare JIT, whereas\ntyped versions alone improve on the JIT baseline in many cases. However, maps\nare not useless. In combination with type versions they add a nice improvement\nover just type versions in a number of benchmarks (most notably\nraytrace-simple and richards but also in crypto-pyaes, django\nand go).\nIt's clear that type versions can be arbitrarily effective. A method lookup on a\nclass can be arbitrarily slow, if the inheritance hierarchy becomes deeper and\ndeeper. The full lookup is replaced by one promotion if type versions are\nenabled.\nMaps on the other hand always replace one dict lookup with one promotion. Since\ndict lookups are already very fast, this by itself does not lead to a gigantic\nimprovement. Only in combination with type versions do they show their full\npotential.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/controlling-tracing-of-interpreter-with_26-3072929156700508140.html"
    },
    {
      "title": "A thank you to the PSF",
      "text": "This year's PyCon was an incredible time; several members of the PyPy team were\nthere, and we'll be blogging more about our experiences in the coming days.\nHowever, we quickly wanted to extend a thank you to the Python Software\nFoundation (PSF).\nAs you may have heard, on Friday morning at PyCon Jesse Noller handed the PyPy\nteam a check for $10,000, on behalf of the PSF.  This was in recognition of our\nsuccess over the past few years in bringing PyPy from a research project\nto a fast, compliant, production-ready Python implementation, and to allow us\nto continue our work on making it faster and more up-to-date with upstream\nversion changes.\nBeyond the large check, we're grateful for the endorsement this represents,\nnot only of our work on PyPy, but also of all alternatve Python VMs.\nThe PSF has shifted its focus from representing just CPython to representing\nthe Python Language, reguardless of its implementation, something we are very\nappreciative of.\n\nFrom left to right, PyPy people present at PyCon 2011: Maciej Fija\u0142kowski, Armin Rigo, Alex Gaynor, Laura Creighton and Jacob Hall\u00e9n\n\nThank you, PSF.",
      "tags": "sponsors",
      "url": "https://www.pypy.org/posts/2011/03/thank-you-to-psf-5934275567667314914.html"
    },
    {
      "title": "Controlling the Tracing of an Interpreter With Hints, Part 3: Putting it All Together",
      "text": "This is part 3 of the series on how to speed up an interpreter written with\nPyPy by adding JIT hints to the interpreter. Part 1 described how to control\nthe extent of tracing. Part 2 described how to influence the optimizer with\npromotion and pure functions. In this post I describe a worked-out example of\na small object model for a dynamic language and how to make it efficient using\nthe hints described in the previous posts.\n\nA Simple Object Model\nTo implement a dynamic language efficiently, the operations on its objects need\nto be fast. Most dynamic languages have object models that are made by using\ndictionaries everywhere. Let's look at an example of how the JIT can be made to\noptimize such operations.\nFor the purpose of this blog post we will use a very simple and bare-bones\nobject model that just supports very simple classes and instances, without any\ninheritance or any fancy features. The model has classes, which contain methods.\nInstances have a class. Instances have their own attributes. When looking up an\nattribute on an instance, the instances attributes are searched. If the\nattribute is not found there, the class' attributes are searched.\nTo implement this object model, we could use the following RPython code as part\nof the interpreter source code:\nclass Class(object):\n    def __init__(self, name):\n        self.name = name\n        self.methods = {}\n\n    def instantiate(self):\n        return Instance(self)\n\n    def find_method(self, name):\n        result = self.methods.get(name)\n        if result is not None:\n            return result\n        raise AttributeError(name)\n\n    def change_method(self, name, value):\n        self.methods[name] = value\n\n\nclass Instance(object):\n    def __init__(self, cls):\n        self.cls = cls\n        self.attributes = {}\n\n    def getfield(self, name):\n        result = self.attributes.get(name)\n        if result is not None:\n            return result\n        raise AttributeError(name)\n\n    def write_attribute(self, name, value):\n        self.attributes[name] = value\n\n    def getattr(self, name):\n        try:\n            return self.getfield(name)\n        except AttributeError:\n            return self.cls.find_method(name)\n\nIn this straightforward implementation the methods and attributes are just\nstored in dictionaries on the classes/instances. While this object model is very\nsimple it already contains all the hard parts of Python's object model. Both\ninstances and classes can have arbitrary fields, and they are changeable at\nany time.  Moreover, instances can change their class after they have been\ncreated.\nWhen using this object model in\nan interpreter, a huge amount of time will be spent doing lookups in these\ndictionaries. To make the language efficient using a tracing JIT, we need to\nfind a way to get rid of these dictionary lookups somehow.\nLet's assume we trace through code that sums three attributes, such as:\ninst.getattr(\"a\") + inst.getattr(\"b\") + inst.getattr(\"c\")\n\nThe trace could look like this:\n# inst.getattr(\"a\")\nattributes1 = inst.attributes\nresult1 = dict.get(attributes1, \"a\")\nguard(result1 is not None)\n\n# inst.getattr(\"b\")\nattributes2 = inst.attributes\nv1 = dict.get(attributes2, \"b\")\nguard(v1 is None)\ncls1 = inst.cls\nmethods1 = cls.methods\nresult2 = dict.get(methods1, \"b\")\nguard(result2 is not None)\nv2 = result1 + result2\n\n# inst.getattr(\"c\")\nattributes3 = inst.attributes\nv3 = dict.get(attributes3, \"c\")\nguard(v3 is None)\ncls1 = inst.cls\nmethods2 = cls.methods\nresult3 = dict.get(methods2, \"c\")\nguard(result3 is not None)\n\nv4 = v2 + result3\nreturn(v4)\n\nIn this example, the attribute a is found on the instance, but the\nattributes b and c are found on the class. The trace indeed contains\nfive calls to dict.get, which is slow.\n\n\nMaking Instance Attributes Faster Using Maps\nThe first step in making getattr faster in our object model is to optimize\naway the dictionary lookups on the instances. The hints we have looked at in the\ntwo earlier blog posts don't seem to help with the current object model. There is\nno pure function to be seen, and the instance is not a candidate for promotion,\nbecause there tend to be many instances.\nThis is a common problem when trying to apply hints. Often, the interpreter\nneeds a small rewrite to expose the pure functions and nearly-constant objects\nthat are implicitly there. In the case of instance fields this rewrite is not\nentirely obvious. The basic idea is as follows. In theory instances can have\narbitrary fields. In practice however many instances share their layout (i.e.\ntheir set of keys) with many other instances.\nTherefore it makes sense to factor the layout information out of the instance\nimplementation into a shared object. This shared layout object is called a\nmap. Maps are an old idea that comes originally from the SELF language. They are\nalso used by many JavaScript implementations such as V8. I've written about maps\nbefore, so I won't explain them fully again.\nThe rewritten Instance class using maps looks like this:\nclass Map(object):\n    def __init__(self):\n        self.attribute_indexes = {}\n        self.other_maps = {}\n\n    @purefunction\n    def getindex(self, name):\n        return self.attribute_indexes.get(name, -1)\n\n    @purefunction\n    def new_map_with_additional_attribute(self, name):\n        if name not in self.other_maps:\n            newmap = Map()\n            newmap.attribute_indexes.update(self.attribute_indexes)\n            newmap.attribute_indexes[name] = len(self.attribute_indexes)\n            self.other_maps[name] = newmap\n        return self.other_maps[name]\n\n\nEMPTY_MAP = Map()\n\nclass Instance(object):\n    def __init__(self, cls):\n        self.cls = cls\n        self.map = EMPTY_MAP\n        self.storage = []\n\n    def getfield(self, name):\n        map = hint(self.map, promote=True)\n        index = map.getindex(name)\n        if index != -1:\n            return self.storage[index]\n        raise AttributeError(name)\n\n    def write_attribute(self, name, value):\n        map = hint(self.map, promote=True)\n        index = map.getindex(name)\n        if index != -1:\n            self.storage[index] = value\n            return\n        self.map = map.new_map_with_additional_attribute(name)\n        self.storage.append(value)\n\n    def getattr(self, name):\n        try:\n            return self.getfield(name)\n        except AttributeError:\n            return self.cls.find_method(name)\n\nInstances no longer use dictionaries to store their fields. Instead, they have a\nreference to a map, which maps field names to indexes into a storage list. The\nstorage list contains the actual field values. The maps are shared between\nobjects with the same layout. Therefore they have to be immutable, which means\nthat their getindex method is a pure function. When a new attribute is added\nto an instance, a new map needs to be chosen, which is done with the\nnew_map_with_additional_attribute method on the previous map. Now that we have\nintroduced maps, it is safe to promote the map everywhere, because we assume\nthat the number of different instance layouts is small.\nWith this changed instance implementation, the trace we had above changes to the\nfollowing, where 0xb74af4a8 is the memory address of the Map instance that\nhas been promoted:\n# inst.getattr(\"a\")\nmap1 = inst.map\nguard(map1 == 0xb74af4a8)\nindex1 = Map.getindex(map1, \"a\")\nguard(index1 != -1)\nstorage1 = inst.storage\nresult1 = storage1[index1]\n\n# inst.getattr(\"b\")\nmap2 = inst.map\nguard(map2 == 0xb74af4a8)\nindex2 = Map.getindex(map2, \"b\")\nguard(index2 == -1)\ncls1 = inst.cls\nmethods1 = cls.methods\nresult2 = dict.get(methods1, \"b\")\nguard(result2 is not None)\nv2 = result1 + result2\n\n# inst.getattr(\"c\")\nmap3 = inst.map\nguard(map3 == 0xb74af4a8)\nindex3 = Map.getindex(map3, \"c\")\nguard(index3 == -1)\ncls1 = inst.cls\nmethods2 = cls.methods\nresult3 = dict.get(methods2, \"c\")\nguard(result3 is not None)\n\nv4 = v2 + result3\nreturn(v4)\n\nThe calls to Map.getindex can be optimized away, because they are calls to\na pure function and they have constant arguments. That means that index1/2/3\nare constant and the guards on them can be removed. All but the first guard on\nthe map will be optimized away too, because the map cannot have changed in\nbetween. The optimized trace looks like this:\n# inst.getattr(\"a\")\nmap1 = inst.map\nguard(map1 == 0xb74af4a8)\nstorage1 = inst.storage\nresult1 = storage1[0]\n\n# inst.getattr(\"b\")\ncls1 = inst.cls\nmethods1 = cls1.methods\nresult2 = dict.get(methods1, \"b\")\nguard(result2 is not None)\nv2 = result1 + result2\n\n# inst.getattr(\"c\")\ncls2 = inst.cls\nmethods2 = cls2.methods\nresult3 = dict.get(methods2, \"c\")\nguard(result3 is not None)\n\nv4 = v2 + result3\nreturn(v4)\n\nThe index 0 that is used to read out of the storage array is the result\nof the constant-folded getindex call. This trace is already much better than\nthe original one. Now we are down from five dictionary lookups to just two.\n\n\nVersioning of Classes\nInstances were optimized making the assumption that the total number of\nInstance layouts is small compared to the number of instances. For classes we\nwill make an even stronger assumption. We simply assume that it is rare for\nclasses to change at all. This is not totally reasonable (sometimes classes contain\ncounters or similar things) but for this simple example it is good enough.\nWhat we would really like is if the Class.find_method method were pure.\nBut it cannot be, because it is always possible to change the class itself.\nEvery time the class changes, find_method can potentially return a\nnew value.\nTherefore, we give every class a version number, which is increased every time a\nclass gets changed (i.e., the content of the methods dictionary changes).\nThis means that the result of methods.get() for a given (name,\nversion) pair will always be the same, i.e. it is a pure operation.  To help\nthe JIT to detect this case, we factor it out in a helper method which is\nexplicitly marked as @purefunction. The refactored Class looks like\nthis:\nclass VersionTag(object):\n    pass\n\nclass Class(object):\n    def __init__(self, name):\n        self.name = name\n        self.methods = {}\n        self.version = VersionTag()\n\n    def find_method(self, name):\n        self = hint(self, promote=True)\n        version = hint(self.version, promote=True)\n        result = self._find_method(name, version)\n        if result is not None:\n            return result\n        raise AttributeError(name)\n\n    @purefunction\n    def _find_method(self, name, version):\n        return self.methods.get(name)\n\n    def change_method(self, name, value):\n        self.methods[name] = value\n        self.version = VersionTag()\n\nWhat is interesting here is that _find_method takes the version\nargument but it does not use it at all. Its only purpose is to make the call\npure (because when the version number changes, the result of the call might be\ndifferent than the previous one).\nThe trace with this new class implementation looks like this:\n# inst.getattr(\"a\")\nmap1 = inst.map\nguard(map1 == 0xb74af4a8)\nindex1 = Map.getindex(map1, \"a\")\nguard(index1 != -1)\nstorage1 = inst.storage\nresult1 = storage1[index1]\n\n# inst.getattr(\"b\")\nmap2 = inst.map\nguard(map2 == 0xb74af4a8)\nindex2 = Map.getindex(map2, \"b\")\nguard(index2 == -1)\ncls1 = inst.cls\nguard(cls1 == 0xb7aaaaf8)\nversion1 = cls1.version\nguard(version1 == 0xb7bbbb18)\nresult2 = Class._find_method(cls, \"b\", version1)\nguard(result2 is not None)\nv2 = result1 + result2\n\n# inst.getattr(\"c\")\nmap3 = inst.map\nguard(map3 == 0xb74af4a8)\nindex3 = Map.getindex(map3, \"c\")\nguard(index3 == -1)\ncls2 = inst.cls\nguard(cls2 == 0xb7aaaaf8)\nversion2 = cls2.version\nguard(version2 == 0xb7bbbb18)\nresult3 = Class._find_method(cls, \"c\", version2)\nguard(result3 is not None)\n\nv4 = v2 + result3\nreturn(v4)\n\nThe calls to Class._find_method can now be optimized away, also the\npromotion of the class and the version, except for the first one. The final\noptimized trace looks like this:\n# inst.getattr(\"a\")\nmap1 = inst.map\nguard(map1 == 0xb74af4a8)\nstorage1 = inst.storage\nresult1 = storage1[0]\n\n# inst.getattr(\"b\")\ncls1 = inst.cls\nguard(cls1 == 0xb7aaaaf8)\nversion1 = cls1.version\nguard(version1 == 0xb7bbbb18)\nv2 = result1 + 41\n\n# inst.getattr(\"c\")\nv4 = v2 + 17\nreturn(v4)\n\nThe constants 41 and 17 are the results of the folding of the\n_find_method` calls. This final trace is now very good. It no longer performs any\ndictionary lookups. Instead it contains several guards. The first guard\nchecks that the map is still the same. This guard will fail if the same\ncode is executed with an instance that has another layout. The second guard\nchecks that the class of inst is still the same. It will fail if trace is\nexecuted with an instance of another class. The third guard checks that the\nclass did not change since the trace was produced. It will fail if somebody\ncalls the change_method method on the class.\n\n\nReal-World Considerations\nThe techniques used above for the simple object model are used for the object\nmodel of PyPy's Python interpreter too. Since Python's object model is\nconsiderably more complex, some additional work needs to be done.\nThe first problem that needs to be solved is that Python supports (multiple)\ninheritance. Therefore looking up a method in a class needs to consider the\nwhole method resolution order. This makes the versioning of classes more\ncomplex. If a class is changed its version changes. At the same time, the\nversions of all the classes inheriting from it need to be changed as well,\nrecursively. This makes class changes expensive, but they should be rare.  On the\nother hand, a method lookup in a complex class hierarchy is as optimized in the\ntrace as in our object model here.\nA downside of the versioning of classes that we haven't yet fixed in PyPy, is\nthat some classes do change a lot. An example would be a class that keeps a\ncounter of how many instances have been created so far. This is very slow right\nnow, but we have ideas about how to fix it in the future.\nAnother optimization is that in practice the shape of an instance is correlated\nwith its class. In our code above, we allow both to vary independently.\nIn PyPy's Python interpreter we act somewhat more cleverly. The class of\nan instance is not stored on the instance itself, but on the map. This means\nthat we get one fewer promotion (and thus one fewer guard) in the trace, because the class doesn't need to\nbe promoted after the map has been.\n\n\nMore General Patterns\nThe techniques we used above to make instance and class lookups faster are\napplicable in more general cases than the one we developed them for. A more\nabstract view of maps is that of splitting a data-structure into a part that\nchanges slowly, and a part that changes quickly. In the concrete example of maps\nwe split the original dictionary into the map (the slow-changing part) and the\nstorage array (the quick-changing part). All the computation on the\nslow-changing part can be constant-folded during tracing so that only the\nmanipulation of the quick-changing part remains.\nSimilarly, versions can be used to constant-fold arbitrary functions of large data\nstructures. The version needs to be updated carefully every time the result of\nthis function can change. Therefore this is useful only if the data structure is\nexpected to change slowly.\n\n\nConclusion\nIn this post I showed how to use purefunction and promote to make a\nsmall but still relevant dynamic object model no longer use any dictionary lookups\nafter tracing. Instead a number of guards are inserted into the\ntrace to check whether the assumptions about the objects are still true. This\nmakes operations on objects seriously faster. I plan to write another small post\nthat shows the speed benefits for PyPy's Python interpreter for exactly these\noperations.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/controlling-tracing-of-interpreter-with_21-6524148550848694588.html"
    },
    {
      "title": "Controlling the Tracing of an Interpreter With Hints, Part 2: Controlling Optimization",
      "text": "This is part 2 of a series on how to speed up an interpreter written with PyPy\nby adding JIT hints to the interpreter. Part 1 described how to control the\nextent of tracing. In this post I will describe how to add hints that\ninfluence the optimizer.  If applied correctly these techniques can give\nreally big speedups by pre-computing parts of what happens at runtime. On the other\nhand, if applied incorrectly they might lead to code bloat, thus making the\nresulting program actually slower.\n\nBackground\nBefore sending the trace to the backend to produce actual machine code, it is\noptimized.  The optimizer applies a number of techniques to remove or reduce\nthe number of operations: most of these are well known compiler optimization\ntechniques, with the difference that it is easier to apply them in a tracing\nJIT because it only has to deal with linear traces.  Among the techniques:\n\nconstant folding\ncommon subexpression elimination\nallocation removal, as described in the paper that I recently presented at\nPEPM\nstore/load propagation\nloop invariant code motion\n\nIn some places it turns out that if the interpreter author rewrites some parts\nof the interpreter with these optimizations in mind the traces that are produced\nby the optimizer can be vastly improved.\nIn this post I will describe two hints that allow the interpreter author to\nincrease the optimization opportunities for constant folding. For constant\nfolding to work, two conditions need\nto be met:\n\nthe arguments of an operation actually need to all be constant,\ni.e. statically known by the optimizer\nthe operation needs to be pure, i.e. always yield the same result given\nthe same arguments.\n\nThe PyPy JIT generator automatically detects the majority of these conditions.\nHowever, for the cases in which the automatic detection does not work, the\ninterpreter author can apply hints to improve the optimization\nopportunities. There is one kind of hint for both of the conditions above.\nNote: These hints are written by an interpreter developer and applied to the\nRPython source of the interpreter. Normal Python users will never see them.\n\n\nWhere Do All the Constants Come From\nIt is worth clarifying what is a \"constant\" in this context.  A variable of\nthe trace is said to be constant if its value is statically known by the\noptimizer.\nThe simplest example of constants are literal values.  For example, if in the\nRPython source code we have a line like y = x + 1, the second operand will\nbe a constant in the trace.\nHowever, the optimizer can statically know the value of a variable even if it\nis not a constant in the original source code. For example, consider the\nfollowing fragment of RPython code:\nif x == 4:\n    y = y + x\n\nIf the fragment is traced with x being 4, the following trace is\nproduced:\n\nguard(x == 4)\ny = y + x\n\nIn the trace above, the value of x is statically known thanks to the\nguard. Remember that a guard is a runtime check. The above trace will run to\ncompletion when x == 4. If the check fails, execution of the trace is\nstopped and the interpreter continues to run.\nThere are cases in which it is useful to turn an arbitrary variable\ninto a constant value. This process is called promotion and it is an old idea\nin partial evaluation (it's called \"the trick\" there). Promotion is also heavily\nused by Psyco and by all older versions of PyPy's JIT. Promotion is a technique\nthat only works well in JIT compilers, in\nstatic compilers it is significantly less applicable.\nPromotion is essentially a tool for trace specialization. In some places in the\ninterpreter it would be very useful if a variable were constant, even though it\ncould have different values in practice. In such a place, promotion is used. The\ntypical reason to do that is if there is\na lot of computation depending on the value of that variable.\nLet's make this more concrete. If we trace a call to the following function:\ndef f1(x, y):\n    z = x * 2 + 1\n    return z + y\n\nWe get a trace that looks like this:\n\nv1 = x * 2\nz = v1 + 1\nv2 = z + y\nreturn(v2)\n\nObserve how the first two operations could be constant-folded if the value of\nx were known. Let's assume that the value of x can vary, but does so\nrarely, i.e. only takes a few different values at runtime. If this is the\ncase, we can add a hint to promote x, like this:\ndef f2(x, y):\n    x = hint(x, promote=True)\n    z = x * 2 + 1\n    return z + y\n\nThe meaning of this hint is that the tracer should pretend that x is a\nconstant\nin the code that follows. When just running the code, the function has no\neffect, as it simply returns its first argument. When tracing, some extra work\nis done. Let's assume that this changed function is traced with\nthe arguments 4 and 8. The trace will be the same, except for one\noperation at the beginning:\n\nguard(x == 4)\nv1 = x * 2\nz = v1 + 1\nv2 = z + y\nreturn(v2)\n\nThe promotion is turned into a guard operation in the trace. The guard\ncaptures the value of x as it was at runtime. From the point of view of the\noptimizer, this guard is not any different than the one produced by the if\nstatement in the example above. After the guard, the rest of the trace can\nassume that x is equal to 4, meaning that the optimizer will turn this\ntrace into:\n\nguard(x == 4)\nv2 = 9 + y\nreturn(v2)\n\nNotice how the first two arithmetic operations were constant folded. The hope is\nthat the guard is executed quicker than the multiplication and the addition that\nwas now optimized away.\nIf this trace is executed with values of x other than 4, the guard will\nfail, and execution will continue in the interpreter. If the guard fails often\nenough, a new trace will be started from the guard. This other trace will\ncapture a different value of x. If it is e.g. 2, then the optimized\ntrace looks like this:\n\nguard(x == 2)\nv2 = 5 + y\nreturn(v2)\n\nThis new trace will be attached to the guard instruction of the first trace. If\nx takes on even more values, a new trace will eventually be made for all of them,\nlinking them into a chain. This is clearly not desirable, so we should promote\nonly variables that don't vary much. However, adding a promotion hint will never produce wrong\nresults. It might just lead to too much assembler code.\nPromoting integers, as in the examples above, is not used that often.\nHowever, the internals of dynamic language interpreters often\nhave values that are variable but vary little in the context of parts of a user\nprogram. An example would be the types of variables in a user function. Even\nthough in principle the argument to a Python function could be any Python type,\nin practise the argument types tend to not vary much. Therefore it is possible to\npromote the types. In the next blog post I will give a complete example for how\nthis works.\n\n\nDeclaring New Pure Operations\nIn the last section we saw a way to turn arbitrary variables into constants. All\npure operations on these constants can be constant-folded. This works great for\nconstant folding of simple types, e.g. integers. Unfortunately, in the context of an\ninterpreter for a dynamic\nlanguage, most operations actually manipulate objects, not simple types. The\noperations on objects are often not pure and might even have side-effects. If\none reads a field out of a constant reference to an object this cannot\nnecessarily be folded away because the object can be mutated. Therefore, another\nhint is needed.\nAs an example, take the following class:\nclass A(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def f(self, val):\n        self.y = self.compute() + val\n\n    def compute(self):\n        return self.x * 2 + 1\n\nTracing the call a.f(10) of some instance of A yields the following\ntrace (note how the call to compute is inlined):\n\nx = a.x\nv1 = x * 2\nv2 = v1 + 1\nv3 = v2 + val\na.y = v3\n\nIn this case, adding a promote of self in the f method to get rid of the\ncomputation of the first few operations does not help. Even if a is a\nconstant reference to an object, reading the x field does not necessarily\nalways yield the same value. To solve this problem, there is another annotation,\nwhich lets the interpreter author communicate invariants to the optimizer. In\nthis case, she could decide that the x field of instances of A is\nimmutable, and therefore compute\nis a pure function. To communicate this, there is a purefunction decorator.\nIf the code in compute should be constant-folded away, we would change the\nclass as follows:\nclass A(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def f(self, val):\n        self = hint(self, promote=True)\n        self.y = self.compute() + val\n\n    @purefunction\n    def compute(self):\n        return self.x * 2 + 1\n\nNow the trace will look like this:\n\nguard(a == 0xb73984a8)\nv1 = compute(a)\nv2 = v1 + val\na.y = v2\n\nHere, 0xb73984a8 is the address of the instance of A that was used\nduring tracing. The call to compute is not inlined, so that the optimizer\nhas a chance to see it. Since compute function is marked as pure, and its\nargument\nis a constant reference, the call will be removed by the optimizer. The final\ntrace looks like this:\n\nguard(a == 0xb73984a8)\nv2 = 9 + val\na.y = v2\n\n(assuming that the x field's value is 4).\nOn the one hand, the purefunction annotation is very powerful. It can be\nused to constant-fold arbitrary parts of the computation in the interpreter.\nHowever, the annotation also gives you ample opportunity to mess things up. If a\nfunction is annotated to be pure, but is not really, the optimizer can produce\nsubtly wrong code. Therefore, a lot of care has to be taken when using this\nannotation.\n\nObservably Pure Functions\nWhy can't we simply write an analysis to find out that the x fields of the\nA instances is immutable and deduce that compute is a pure function,\nsince it only reads the x field and does not have side effects? This might\nbe possible in this particular case, but in practice the functions that are\nannotate with the purefunction decorator are usually more complex.\nThe easiest example for this is that of a function that uses memoization to\ncache its results. If you analyze this function, it looks like the function has\nside effects, because it changes the memoizing dictionary. However, because this side\neffect is not externally visible, the function from the outside is pure. This is\na property that is not easily detectable by analysis. Therefore, the purity\nof this function needs to be annotated.\n\n\nImmutable Fields\nOne of the most common cases of pure functions is reading immutable\nvalues out of objects. Since this is so common, we have special syntactic sugar\nfor it. A RPython class can have a class attribute _immutable_fields_ set to\na list of strings, listing the fields that cannot be changed. This is equivalent\nto using getters and annotating them with purefunction.\n\n\n\nConclusion\nIn this blog post I explained two more hints that can be used in the source code\nof the interpreter. They are used to influence what the optimizer does with the\ntrace. I realize the examples given here are a bit too small, in the next\ninstallment I will give a worked-out example that puts all the pieces together.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/controlling-tracing-of-interpreter-with_15-3281215865169782921.html"
    },
    {
      "title": "Controlling the Tracing of an Interpreter With Hints, Part 1: Controlling the Extent of Tracing",
      "text": "The question I was asked most often during my recent US trip was how exactly\nthe hints work that interpreter authors can use to improve the execution speed\nof the programs running on their interpreters. Since those hints are not really\ndocumented all that well, I decided to write blog posts about them. This is the\nfirst one.\n\nBackground\nFirst, let's recap some basics: PyPy's approach to implementing dynamic\nlanguages is to write an interpreter for\nthe language in RPython. This interpreter can be translated to C and then\nfurther to machine code. The interpreter consists of code in the form of a\nlarge number of generated C functions and some data. Similarly, the user\nprogram consists of functions in the language the interpreter executes.\nAs was explained in a blog post and a paper two years ago, PyPy's JIT is a\nmeta-tracer. Since we want to re-use our tracer for a variety of languages, we\ndon't trace the execution of the user program, but instead trace the execution\nof the interpreter that is running the program. This means that the traces\ndon't contain the bytecodes of the language in question, but RPython-level\noperations that the interpreter did to execute the program.\nOn the other hand, the loops that are traced by the tracer are the loops in the\nuser program. This means that the tracer stops tracing after one iteration of\nthe loop in the user function that is being considered. At this point, it can\nhave traced many iterations of the interpreter main loop.\nHere's a diagram of this process:\n\n\n\nOn the left you see the levels of execution. The CPU executes the binary of\nPyPy's Python interpreter, which consists of RPython functions that have been\ncompiled first to C, then to machine code. Some of these functions contain\nloops, others don't. The interpreter runs a Python program written by a\nprogrammer (the user). If the tracer is used, it traces operations on the level\nof the interpreter. However, the extent of the trace is determined by the loops\nin the user program.\n\n\nHow Far Should Tracing Go\nWhen the tracer encounters a function call at the interpreter level, e.g. the\ninterpreter main loop calling a helper function, it can do one of two things:\n\nit can trace into the helper function, effectively inlining it into the trace.\nit can not trace into the function and instead record a call to that function\nas an operation in the trace. Such a call operation in the trace is sometimes\ncalled residual call.\n\nAs a default, the tracer will try to trace into the helper because that will\ngive more information to the optimizer, allowing it to do a better job. This is\nparticularly important for the allocation removal optimization, because if a\nfreshly allocated object is passed as an argument to a residual call, its\nallocation cannot be optimized away.\nThere is a problem however if the helper function itself contains a loop. The\ntracer records the linear sequence of operations that are being executed. Thus\nwhen it encounters a loop on the interpreter level it records all the\noperations of every iteration of the loop itself, with the net effect of\nunrolling it. The only places where the tracer stops and tries to close the\ntrace is in the main loop of the interpreter. When the tracer encounters the\nmain loop, it also checks whether the original user loop has been closed, and\nthus whether it can stop tracing.\nFor most helper functions in the interpreter that contain loops, fully\nunrolling does not make sense. If a loop is unrolled, the trace is specific to\nthe number of iteration that was seen during tracing. If the trace is later\nexecuted with a different number of iterations, the trace will be left via a\nguard failure, which is inefficient. Therefore the default behaviour of the\ntracer is to never trace into a function on the interpreter level that contains\na loop, but to trace into all non-looping helper functions.\nThis default behaviour is essentially a heuristic, but one that usually makes\nsense. We want to produce just enough traces to make the resulting code\nefficient, but not more. Therefore we trace as much as possible (everything by\ndefault) except the functions which loops where tracing would produce code that\nis less general than it could be.\nAs an example for a helper with a loop, take string concatenation. It loops over\nthe characters of both arguments and copies them over into the result string. It\ndoes not make sense to unroll the loops in this function. If we do that,\nthe resulting trace can only be used for strings of the length that was seen\nduring tracing. In practise, the string lengths are usually different each run,\nmeaning that the trace with unrolling is not run to completion in most cases.\n\n\nInfluencing the Default Behaviour\nSometimes the default behaviour is not actually what is wanted. This is\nsomething the interpreter author has to decide, usually by looking at the traces\nthat are produced and deciding that they should be improved. There are two ways\nin which the default is wrong:\n\nfalse negatives: if a helper function that does contain a loop should\nbe traced into, unrolling the loop.\nfalse positives: if a helper function that does not contain a loop is\ninlined into the trace, but the interpreter author decides that this is not\nhelpful.\n\nIf the interpreter author finds false negatives or false positives, she can fix\nthat by applying a hint to the tracer. These hints take the form of function\ndecorators (which both live in the pypy.rlib.jit module). In the next two\nsubsections I will describe these two function decorators and their use.\n\nUnrolling Functions With Loops\nThe first decorator, used to fix false negatives, is the unroll_safe\ndecorator. It is used to tell the tracer to always trace into a function that\nhas a loop, effectively unrolling the loop. This decorator should be used only\nif the loop in the helper function is expected to always run for the same number\nof iterations. This sounds like a strong restriction, in practise this is less\nsevere: The number of iterations needs to only be the same in the context where\nthe helper functions is traced from.\nIt is easiest to understand this condition via an example. Let's look at the\nBUILD_TUPLE bytecode in Python. It takes one argument, the length n of\nthe tuple being built. The bytecode pops n arguments from the stack, turns\nthem into a tuple and pushes that tuple on the stack. Thus the function that\nimplements BUILD_TUPLE in PyPy's Python interpreter calls a helper\npopvalues which pops n values from the stack and returns them in a list.\nThis helper is implemented with a loop and would thus not be traced into by\ndefault.  The loop in the helper can run for very different numbers of\niterations, because it is used in a variety of places. However, for every\nconcrete BUILD_TUPLE bytecode, the argument will be constant. Therefore it\nis safe (and even necessary) to annotate popvalues with the unroll_safe\ndecorator.\nA different example is the implementation of the isinstance builtin. It is\nused to check whether an object a is an instance of a class B like\nthis: isinstance(a, B). The second argument of the function can also be a\ntuple of classes to check whether an object is an instance of one of a number of\nclasses: isinstance(a, (A, B, C, D)). To implement this second case, the\nimplementation of isinstance contains a loop iterating over the elements of\nthe tuple. The number of loop iterations can vary, but is usually fixed for each\nindividual call site which typically just lists a few classes in the source\ncode. Therefore it is also safe to annotate the implementation of isinstance\nwith the unroll_safe decorator.\n\n\nPreventing the Tracing of Functions\nThe second decorator dont_look_inside is used to fix false positives. It\ntells the JIT to never trace into the decorated function and just always produce\na residual call instead. This decorator is in many ways less important than the\nunrolling one (except for a special situation that I will describe in a\nfollow-up post). It is used if tracing into a function is not expected to yield\nany speed benefits, because the optimizer will not be able to improve it much.\nThis is often the case if the called helper function does not contain any\n\"dynamic\" behaviour. In such a situation it is better to just leave the function\ncall in the trace, because that produces less code.\nAn example would be the import mechanism in Python. It's very unlikely that any\nperformance improvement can be had by turning part of it into assembler.\nTherefore we hide it from the tracer by annotating them with\ndont_look_inside.\n\n\n\nConclusion\nIn this post we discussed two hints that can be used to control precisely which\nparts of the interpreter should be meta-traced. If these hints are used\ncarefully, this can go a long way to making the interpreter produce traces that\ncontain exactly the interesting part of the execution, and will contain calls to\nthe functions that can not be optimized by tracing techniques.\nIn the next part of this series I will discuss a different set of hints that can\nbe used to strongly optimize traces.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/controlling-tracing-of-interpreter-with-871085470935630424.html"
    },
    {
      "title": "Bay Area 2011 Tour Summary",
      "text": "We spent the week in the San Francisco Bay Area showing off PyPy.\nHere are notes and photos of the tour.\n\nDay 1: Google SF\nGoogle has offices in downtown San Francisco.  They are at a beautiful\nplace and the views are spectacular.  We thank Wesley Chun and Guido van\nRossum for organizing this meeting.  Between 25 and 30 engineers showed\nup.  Some of them were Python programmers, but others were C++\nprogrammers; and they all seem to have real problems that they want to\nsolve with PyPy.  We didn't have prepared slides so far, so we mostly\nran demos and talked.  As predicted, Google would love SWIG support.\nThey suggested that we rename the translation toolchain (as we vaguely\nthought too) to separate it more from PyPy's Python interpreter; up\nuntil today, many had no idea that they could use PyPy for other\nlanguages.  All in all, it was very positive and people looked forward\nto meeting up at PyCon.\n\n\nDay 2: Stanford\n\n\n\n\nThis was the most academically-oriented talk.  You can find the\nabstract, the slides (PgUp/PgDown to navigate) and the video here.\nThere were around 35 people in the audience, and maybe 1000 real-time\nvideo watchers (who didn't get to ask questions).  The live audience\nseemed to be a mixture of students, professors, and people from the\nlocal industry.  We thank David Allison and Andy Freeman for organizing\nit.  It has been two or three years since they invited me (Armin) and I\nfinally managed to get here :-)\nThe slides are longer than the talk; we focused on the JIT because that\nwas what the audience was most interested in.  They were really\nimpressed at the stability, the tests, and that we don't have lots of\nbugs reported in the JIT of our latest public release.  We later found\nout that many who came to the talk believed that they were going to get\na talk about how we jitted a subset of python because real python is too\nhard -- impossible to do.  They came to heckle with examples of how\npython was impossible.  So they were amazed when the first slide of\nArmin's presentation was \"Python is complicated\", and the next slide\n\"Python is messy\".  It was a positive outcome.  We made new fans :-)\n\n\nDay 3: Yelp\n\n\n\n\n\nAs you can see in the image, tons of people showed up -- ~140.  Thanks\nto Grace Law, who is the coordinator for the SF Python Meet-up, and to\nJimmy Retzlaff and Ashley King-Bishof from Yelp.  Yelp is also located\nin downtown San Francisco.  This looks like the place to be if you are a\nstart-up in California (and not in Silicon Valley): lots of enthusiastic\nyoung people are here, and they are hiring.  Yelp has an enormous open\nspace, suitable for huge parties, and the coolest beer dispensers on the\nplanet, made as a hack-a-thon project by three Yelp engineers (pictured\nbelow):\n\n\n\n\n\n\n\n\n\nBy the way, their management structure seems to be flat.  There are\nalmost no line managers, i.e. managers for the engineering staff;\ninstead they self-organize into teams.  This is not what you expect\nfor the USA; things appear to have changed a lot.\nThe talk was in two sections, \"PyPy from the user's point of view\" and\n\"How the JIT works\".  Good feedback; impressed that we support all of\nPython 2.7 (including all the modules that are in C in the stdlib), and\nimpressed that the Python 3.0 conversion is not considered a big deal by\nus, although we have no precise date yet.  The plan is, of course, just\nto tweak the interpreter until it supports both (by adding the necessary\nconditions); the other aspects like GC and the JIT will not be affected\nat all.\n\n\nDay 4: Dropbox\n\n\n\n\n\n\n\nThis was another place full of excited, successful young people.  The\nCTO looks like he turned 30 last week, and he's been CTO for 4 years\nnow.  The three of us were quite obviously the oldest people there.  We\nfelt old.  They have another great big open barn complex. It's\nloud. Very loud.  Loud refrigerators, loud street noise, loud machinery\nin the walls doing who knows what, loudly.\nThis was the first tech talk at dropbox.  Thanks to Rian Hunter for\norganizing it.  They have a big kitchen, and we held the talk in there.\nThere was a skylight, which made the room too bright, so harder to read\nthe slides than would otherwise be the case.  They were jazzed about our\nvisit, and wanted copies of all the pictures Jacob took before he left.\nThey seemed familiar with Google V8, and thought that how long it took\nto build PyPy was a great incentive for us to make PyPy faster.  They\nare very interested in fast ctypes, fast SWIG, fast Cython.  They were\npleased and surprised that we don't have too much JIT bloat (typically\n~10% of the total RAM usage).\nThe mobile developers want a smaller Python more than a faster one.\nPython takes too much memory given the tiny amount available on a lot of\ncell phones.  Not that we have an answer to this problem now.\nThey were pleased to learn that we will soon be able to JIT ctypes code.\nAnd the fact that Armin knows many ways to segfault CPython was a bit of\na shock.  We talked for an hour after the presentation.  Again, a very\npositive outcome.\n\n\nDays 5 and 6: Noisebridge sprint\n\n\n\nAbout six people showed up for the sprint.  (Late.  Californians really\ndo start the day at 11.)  Noisebridge is a very eclectic place; people\nshow up to do pretty much everything from sewing to breaking apart\nequipment to making robots and beer.  It's donation-driven.  Thanks to\nJim Stockford for volunteering the space and arranging this and helping\nus set up for the sprint.\nDuring the sprint, we did a little bit of everything; there was no clear\npattern.  Ademan worked on sqlite, Greg Price looked to see if his\nsoftware could run on PyPy, Will worked on the documentation, and a few\nof us fixed some more 2.7 tests.  Alex Gaynor and Fijal joined us, too.\n\n\nDay 7: Google Mountain View and Mozilla\nWe gave two talks on the 7th day of our trip so we were already quite\nexhausted. Fortunately new people joined, so the talks were actually split\nbetween multiple people. We would like to thank Peter Norvig and Ben Bayer\nfor inviting us to Google and Andreas Gal, Brendan Eich and Dave Herman\nfor inviting us to Mozilla. Both talks should hopefully appear online\nat some point soon, but as of now we don't have a link.\nIt was pretty incredible to find ourselves at Mozilla talking with at\nleast 15 people who deeply understood the ideas of tracing JITs and\nalso understood why we undertook the decision to generate our JIT\ninstead of writing it. They suffered from having to write JavaScript\nJIT (even multiple ones) by hand, as Armin did with Psyco.  He deeply\nsympathizes. The discussion afterwards was very successful and we're\nlooking forward to cooperating with them.  Many exciting things were\ndiscussed as possibilities.\nNext day we went to Pycon, which is ongoing and a topic for yet another\nblog post.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/bay-area-2011-tour-summary-9117372109664978472.html"
    },
    {
      "title": "US Trip Report: POPL, Microsoft, IBM",
      "text": "Some notes from my recent trip (from 23rd of January to 17th of February) to the\nUS where, I presented PyPy at various scientifically oriented places. In\nsummary, there seems to be quite a bit of interest in PyPy within the research\ncommunity, details below.\n\nPEPM/POPL/STOP\nFrom the 24th to the 29th of January I was in Austin, Texas at the POPL\nconference, where I gave a talk at one of the workshops, PEPM (Partial\nEvaluation and Program Manipulation). The title of our paper is\n\"Allocation Removal by Partial Evaluation in a Tracing JIT\", the abstract is:\n\nThe performance of many dynamic language implementations suffers from high\nallocation rates and runtime type checks. This makes dynamic languages less\napplicable to purely algorithmic problems, despite their growing\npopularity. In this paper we present a simple compiler optimization based\non online partial evaluation to remove object allocations and runtime type\nchecks in the context of a tracing JIT. We evaluate the optimization using\na Python VM and find that it gives good results for all our (real-life)\nbenchmarks.\nThe talk (slides) seemed to be well-received and there was\na good discussion afterwards. PEPM in general was a very enjoyable workshop\nwith many interesting talks on partial evaluation (which I am very interested\nin) and a great keynote by Olivier Danvy about \"A Walk in the Semantic Park\".\nPOPL itself was a bit outside of the area I am most knowledgeable in, most of\nthe talks being on formal topics. Some of the talks that stuck to my mind:\n\n\"The Design of Kodu: A Tiny Visual Programming Language for Children on the\nXbox 360\", the keynote by Matthew MacLaurin from Microsoft Research. I didn't\nknow about Kodu before, and was very impressed by it.\n\n\n\"Automating String Processing in Spreadsheets using Input-Output Examples\"\n(paper) by Sumit Gulwani (also from MS Research) describes a plugin to Excel\nthat can automate many common string processing tasks by giving a couple of\nexamples, which are then abstracted into a generic string manipulation. Very\ncool.\n\n\n\"Dynamic Inference of Static Types for Ruby\" (paper) by   Michael Furr,\nJong-hoon (David) An, Jeffrey S. Foster and Michael Hicks describes an\napproach to type inference that works by observing the actual types seen\nduring unit-testing. Similar things have been done a few times before,\nhowever, the paper actually gives a correctness result.\n\n\n\"The Essence of Compiling with Traces\" (paper) by Shu-Yu Guo and Jens\nPalsberg describes a formalization of a simple imperative language and\nproves that executing it using trace compilation will do exactly the same\nthing than using an interpreter. It also looks at what conditions an\noptimization on traces must fulfill to still produce valid results.\n\nAfter the main conference, I took part in the STOP (Scripts to Programs)\nworkshop. It had a great keynote \"Scripting in a Concurrent World\" by John Field\nabout the Thorn language and a few interesting other talks.\n\n\nMicrosoft Research\nAfter POPL I went to Redmond to visit Microsoft Research for a week,\nspecifically the RiSE group. This is the group that did the SPUR project,\na meta-tracing JIT for C# applied to a JavaScript interpreter in C#. I compared\nPyPy to SPUR last year. I am very grateful for Microsoft for inviting me\nthere.\nAt Microsoft I gave a talk about \"PyPy's Approach to Implementing Dynamic\nLanguages Using a Tracing JIT Compiler\", the slides of which can be found\nhere. The talk was filmed and is online. People seemed to be impressed\nwith the \"product qualities\" of PyPy, e.g. the buildbot infrastructure and\nspeed tracking website.\nThe rest of the time I discussed with various researchers in the RiSE group,\nparticularly with Nikolai Tillmann. We talked a lot about similarities and\ndifferences between SPUR and PyPy and tried to understand our respective projects\nbetter. SPUR is a really great project and I learned a lot in the discussions,\nfor example about the optimizations and heuristics their trace compiler uses.\nAnother very cool project done by the RiSE group that I learned more about is\nPEX. PEX is a unit test generator for C# that tries to produce unit tests for\nso-far untested execution paths within methods. There is an online puzzle\nversion of it, if you want to get an impression of the technology (including a\nvery impressive C# IDE in the browser).\n\n\nIBM\nFor the last part of the trip I stayed in New York City for two weeks,\nmostly as a vacation. However, I also visited IBM Watson Research Center for\ntwo days, to which I had been invited by David Edelsohn.\nThe first day I gave the same presentation I had given at Microsoft (with some\nimprovements to the slides), again it was quite well received. The rest of\nthe time I spent in (very fruitful) discussions with various people and teams,\namong them the Liquid Metal team and the Thorn team.\nThe second day I met with members of the FIORANO group, who are working on\ndynamic compilation for dynamic languages and Java. They explored various ways\nto speed up Python, both by improving the CPython interpreter as well as with\nJIT compilation techniques.\nAnother of their projects is to add a trace compiler to IBM's J9 JVM, about\nwhich the paper \"A Trace-based Java JIT Compiler Retrofitted from a\nMethod-based Compiler\" is going to appear at CGO. I discussed tracing JITs with\nPeng Wu, one of the authors of that paper. Peng tries to systematically look at\nthe various heuristics found in the different VMs that use tracing JITs. This\nis a very different perspective from the one I usually have, focusing on how to\nimprove PyPy's specific heuristics. Therefore that discussion helped me thinking\nabout the issues more generally.\nAnother goal of the group is to try to find benchmarks that are representative\nfor typical Python workloads, which is something that has been done very\ncarefully for Java e.g. when developing the DaCapo benchmark suite. The\nbenchmarks that the Python community uses have not been selected in such a\ncareful and measured way, so I think that trying to be more systematic there is\na very worthwhile endeavour.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/03/us-trip-report-popl-microsoft-ibm-3874568000250679204.html"
    },
    {
      "title": "PyPy Winter Sprint Report",
      "text": "A few weeks ago I had the great fortune to attend the PyPy winter sprint in Leysin Switzerland. I've wanted to contribute to PyPy for a long time and I thought diving into a sprint might be a good way to get familiar with some of the code. What I wasn't expecting was to be using RPython to implement new methods on built-in Python objects on the first day. The main thing I took away from the sprint was just how easy it is to get involved in developing PyPy (well, some bits of it at least and being surrounded by core developers helps). I wrote up a very short description of how to get started here, but I'll do a longer blog post with examples on my own blog soon(ish).\n\nThe sprint was kicked off by Armin merging the \"fast-forward\" branch of PyPy onto trunk. \"fast-forward\" brings PyPy from Python 2.5 compatibility to Python 2.7. Along with this it brought a large number of test failures, as the sterling work done by Benjamin Peterson and Amaury Forgeot d'Arc was not complete. This immediately set the primary sprint goal to reduce the number of test failures.\n\nWe made a great deal of progress on this front, and you can see how close PyPy is now from the buildbots.\n\nJacob Hall\u00e9n and I started working through the list of tests with failures alphabetically. We made short work of test_asyncore and moved onto test_bytes where I was stuck for the rest of the sprint. I spent much of the remaining days working with Laura Creighton on the pypy bytearray implementation to make it more compatible with Python 2.7. This meant adding new methods, changing some of the Python protocol method implementations and even changing the way that bytearray is constructed. All in all great fun and a great introduction to working with RPython.\n\nA big part of the compatibility with Python 2.7 work was done by Laura and Armin who basically rewrote the math module from scratch. This was needed to incorporate all the improvements made (mostly by Mark Dickinson) in CPython in 2.7. That involved a lot of head-scratching about such subtleties as whether -0.0 should be considered almost equal to 0.0 and other fun problems.\n\n\n\n\nThe first meal together, before everyone had arrived\n\nIf you add on top of this the wonderful people, the beautiful scenery, the Swiss cheese fondues, managing to not kill myself with a days skiing and traditional pypy card games, I can heartily recommend pypy sprints as a close approximation of geek nirvana.\n\n\n\nView of the mountains from the sprint\n\n\nWorking on 2.7 compatibility wasn't the only work that happened during the sprint. Other activities included:\n\nAntonio Cuni worked on the \"jittypes\" branch. This is a reimplementation of the core of the PyPy ctypes code to make it jittable. The goal is that for common cases the jit should be able to turn ctypes calls from Python into direct C level calls. This work was not completed but very close and is great for the future of integrating C libraries with PyPy. As ctypes is also available in CPython and IronPython, and hopefully will be available in Jython soon, integrating C code with Python through ctypes is the most \"implementation portable\" technique.\nDavid Schneider continued his work on the JIT backend for ARM. PyPy has been cross-compilable to ARM for a long time, but bringing the JIT to ARM will provide a *fast* PyPy for ARM, which includes platforms like Android. Again David didn't complete this work but did complete the float support.\nH\u00e5kan Ardo was present for two days and continued his crazy-clever work on JIT optimisations, some of which are described in the Loop invariant code motion blog entry.\nHolger Krekel worked on updating the PyPy test suite to the latest version of py.test and also worked with me on the interminable bytearray changes for part of the sprint.\nNo one was sure what \u00a0Maciej Fija\u0142kowski worked on but he seemed to be quite busy.\n\nI think that was most of the work done during the actual sprint. There was also a great deal of healthy discussion about the future of PyPy. Expect lots more interesting and exciting developments over the coming year.",
      "tags": "sprint",
      "url": "https://www.pypy.org/posts/2011/02/pypy-winter-sprint-report-4155886720346408516.html"
    },
    {
      "title": "The PyPy San Franciso Bay Area Tour 2011",
      "text": "PyPy is coming to the San Francisco Bay Area in the beginning of March with\na series of talks and a mini sprint.\n\n\nWednesday March 2, 4:15 p.m.  Armin Rigo gives\na\ntalk at Stanford.  open to the public.\n\nThursday March 3, 6:00 p.m.  General talk at Yelp, 706 Mission St 9th Floor,\n  San Francisco CA 94103 open to the public.\n\nSaturday and Sunday March 5 and 6.\n  PyPy mini sprint at noisebridge.\n  2169 Mission street between 17th and 18th in San Francisco.  Open to the public.\n\nMonday March 7th, 11:30 a.m.  Google Tech talk in Mountain View at the\n  Googleplex.  Not open to the public (but the video should be available\n  later).\n\nMonday March 7th, 2:30 p.m.  Talk at Mozilla in Mountain View.  Not\n  open to the public (but Mozilla developers can videoconference).\n\n\nFrom the PyPy project team we will have Armin Rigo, Maciej Fija\u0142kowski\n(from 6th March), Laura Creighton and Jacob Hall\u00e9n and possibly\nChristian Tismer attending.\n\nMost of the talks will focus on (some of) the highlights and the\nstatus of pypy:\n\n\nmost Python benchmarks run much faster than with CPython or Psyco\nthe real-world PyPy compiler toolchain itself (200 KLocs) runs twice as fast\nsupports x86 32 and 64bit and is in the process of supporting ARM\nfull compatibility with CPython (more than Jython/IronPython)\nfull (and JIT-ed) ctypes support to call C libraries from Python\nsupports Stackless Python (in-progress)\nnew \"cpyext\" layer which integrates existing CPython C extensions\nan experimental super-fast JIT-compilation of calls to C++ libraries\n\n\nAs is usual for us, there is vastly more material that is available for\nus to cover than time, especially when it comes to possible future\ndirections for PyPy.  We want to reserve a certain amount of time at\neach talk purely to discuss things that are of interest to audience\nmembers.  However, if you already know what you wish we would discuss,\nand are attending a talk (or even if you aren't), please let us know.\nYou can either reply to this blog post, or mail Laura directly at\nlac at openend.se .\n\nApart from getting more technical and project insight, our travel is\nalso a good possibility for companies in the SF area to talk to us\nregarding contracting.  In September 2011 our current \"Eurostars\" research\nproject ends and some of us are looking for ways to continue working on\nPyPy through consulting, subcontracting or hiring.  The two companies,\nOpen End and merlinux, have successfully done a number of such contracts\nand projects in the past.  If you want to talk business or get together for\nlunch or dinner, let us know! If you would like us to come to your company\nand make a presentation, let us know!  If you have any ideas about what\nwe should discuss in a presentation so that you could use it to convince\nthe powers-that-be at your place of employment that investing time and\nmoney in PyPy would be a good idea, let us know!\n\nOn Tuesday March 8th we will be heading for Atlanta for the Python VM\nand Language Summits before attending PyCon.  Maciej Fija\u0142kowski and\nAlex Gaynor will be giving a talk entitled\nWhy is\nPython slow and how can PyPy help?\nMaciej will also be giving the talk\nRunning\nultra large telescopes in Python which is\npartially about his experiences using PyPy in the  Square Kilometer Array\nproject in South Africa.  There will be a PyPy Sprint March 14-17.\nAll are welcome.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/02/pypy-san-franciso-bay-area-tour-2011-6179180737090334330.html"
    },
    {
      "title": "PyPy faster than C on a carefully crafted example",
      "text": "Good day everyone.\nRecent round of optimizations, especially loop invariant code motion\nhas been very good for small to medium examples. There is work ongoing to\nmake them scale to larger ones, however there are few examples worth showing\nhow well they perform. This one following example, besides getting benefits\nfrom loop invariants, also shows a difference between static and dynamic\ncompilation. In fact, after applying all the optimizations C does, only a\nJIT can use the extra bit of runtime information to run even faster.\nThe example is as follows. First Python. I create two files, x.py:\n\ndef add(a, b):\n  return a + b\n\nAnd y.py:\n\nfrom x import add\n\ndef main():\n    i = 0\n    a = 0.0\n    while i < 1000000000:\n        a += 1.0\n        add(a, a)\n        i += 1\n\nmain()\n\nFor C, x.c:\n\ndouble add(double a, double b)\n{\n  return a + b;\n}\n\nand y.c:\n\ndouble add(double a, double b);\n\nint main()\n{\n  int i = 0;\n  double a = 0;\n  while (i < 1000000000) {\n    a += 1.0;\n    add(a, a);\n    i++;\n  }\n}\n\nResults?\n\n1.97s - PyPy\n3.07s - C\n\nCompilation options:\n\nPyPy trunk (386ed41eae0c), running pypy-c y.py\nC - gcc -O3 (GCC 4.4.5 shipped with Ubuntu Maverick)\n\nHence, PyPy 50% faster than C on this carefully crafted example. The reason\nis obvious - static compiler can't inline across file boundaries. In C,\nyou can somehow circumvent that, however, it wouldn't anyway work\nwith shared libraries. In Python however, even when the whole import system\nis completely dynamic, the JIT can dynamically find out what can be inlined.\nThat example would work equally well for Java and other decent JITs, it's\nhowever good to see we work in the same space :-)\nCheers,\nfijal\nEDIT: Updated GCC version",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/02/pypy-faster-than-c-on-carefully-crafted-5614784244310486765.html"
    },
    {
      "title": "A JIT Backend for ARM Processors",
      "text": "In the past few months, I have been developing as a part of my master thesis\nthe ARM backend for the the PyPy JIT, in the arm-backend branch. Currently, it is still work in progress: all integer and object operations are working and\nthe support for floating point is also under development.\nARM processors are very widely used, being deployed in servers, some netbooks\nand mainly mobile devices such as phones and tablets. One of our goals is to be\nable to run PyPy on phones, specially on Android. Currently is not yet possible\nto translate and compile PyPy for Android automatically, but there has been\nsome work  on using Android's NDK to compile PyPy's generated C code.\nThe JIT Backend targets the application profile of the ARMv7 instruction set\narchitecture which is found for example in the Cortex-A8 processors used in many Android powered devices and in Apple's A4 processors built into the latest iOS devices. To develop and\ntest the backend we are using a BeagleBoard-xM which has a 1 GHz ARM\nCortex-A8 and 512 MB of RAM running the ARM port of Ubuntu 10.10.\nCurrently on Linux it is possible to translate and cross-compile PyPy's Python\ninterpreter as well as other interpreters with the ARM JIT backend enabled\nusing Scratchbox 2 to provide a build environment and the GNU ARM cross\ncompilation toolchain. So far the backend only supports the Boehm garbage\ncollector which does not produce the best results combined with the JIT, but we\nplan to add support for the other GCs in the future, doing so should increase\nthe performance of PyPy on ARM.\nWhile still debugging the last issues with the backend we already can run some\nsimple benchmarks on Pyrolog, a prolog interpreter written in RPython.\nEven using Boehm as the GC the results look very promising. In the benchmarks\nwe compare Pyrolog to SWI-Prolog, a prolog interpreter written in C, which\nis available from the package repositories for Ubuntu's ARM port.\nThe benchmarks can be found in the pyrolog-bench repository.\n\nBenchmarkSWI-Prolog in ms.Pyrolog in ms.Speedup\n\niterate60.06.010.0\niterate_assert130.06.021.67\niterate_call3310.05.0662.0\niterate_cut60.0359.00.16713\niterate_exception4950.0346.014.306\niterate_failure400.0127.03.1496\niterate_findall740.0No res.\niterate_if140.06.023.333\n\nThe iterate_call benchmark, which constructs a predicate and calls it at\nruntime, with a speedup of 662 times over SWI-Prolog is an example where the\nJIT can show its strength. The Pyrolog interpreter and the JIT treat\ndynamically defined predicates as static ones and can generate optimezed code\nin both cases. Whereas SWI only compiles statically defined rules and has to\nfall back to interpretation on dynamic ones.\nFor simple benchmarks running on PyPy's Python intepreter we see some speedups\nover CPython, but we still need to debug the backend bit more before we can\nshow numbers on more complex benchmarks. So, stay tuned.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/01/jit-backend-for-arm-processors-5994810755839586463.html"
    },
    {
      "title": "PyPy wants you!",
      "text": "If you ever considered contributing to PyPy, but never did so far, this is a\ngood moment to start! :-)\nRecently, we merged the fast-forward branch which brings Python 2.7\ncompatibility, with the plan of releasing a new version of PyPy as soon as all\ntests pass.\nHowever, at the moment there are still quite a few of failing tests because\nof new 2.7 features that have not been implemented yet: many of them are easy\nto fix, and doing it represents a good way to get confidence with the code\nbase, for those who are interested in it. Michael Foord wrote a little howto\nexplaining the workflow for running lib-python tests.\nThus, if you are willing to join us in the effort of having a PyPy compatible\nwith Python 2.7, probably the most sensible option is to come on the #PyPy IRC\nchannel on Freenode, so we can coordinate each other not to fix the same test\ntwice.\nMoreover, if you are a student and are considering participating in the next\nGoogle Summer of Code this is a good time to get into pypy. You have the\nopportunity to get a good understanding of pypy for when you decide what you\nwould like to work on over the summer.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/01/pypy-wants-you-4543209863582915733.html"
    },
    {
      "title": "Loop invariant code motion",
      "text": "Recently, the jit-unroll-loops branch was merged. It implements the\nidea described in \nUsing Escape Analysis Across Loop Boundaries for Specialization.\nThat post does only talk about virtuals, but the idea turned out\nto be more far reaching. After the metainterpreter produces a trace,\nseveral optimizations are applied to the trace before it is turned\ninto binary code. Removing allocations is only one of them. There are also\nfor instance\n\n Heap optimizations that removes memory accesses by reusing results\n  previously read from or written to the same location.\n Reusing of the results of pure operations if the same pure\n  operation is executed twice.\n Removal of redundant guards.\n ...\n\nA lot of these optimizations are in one way or another removing\noperations form the trace and/or reusing previous results. All of these\noptimizations could benefit from being able to operate across loop\nboundaries. Not only in the sense that operations operating on loop\ninvariants could be moved out of the loop entirely. But also that\nresults produced at the end of an iteration could be reused at the\nbeginning of the next even if there are no loop invariants involved.\n\n\n\nThis is achieved by unrolling the trace into two iterations, and\nletting the optimizer work on this two-iteration-trace.\nThe optimizer will now be able to optimize the second iteration more than the\nfirst since it can reuse results from the first iteration. The\noptimized version of the first iteration we call the preamble and the\noptimized version of the second iteration we call the loop. The\npreamble will end with a jump to the loop, while the loop will end\nwith a jump to itself. This means that the preamble will be executed\nonce for the first iteration, the loop will be executed for all following\niterations.\n \n\nSqrt example\nHere is an example of a Python implementation of sqrt using a fairly\nsimple algorithm\n\n\n\n  \n\ndef sqrt(y, n=10000):\n    x = y / 2\n    while n > 0:\n        n -= 1\n        x = (x + y/x) / 2\n    return x\n\n\n\nIf it is called with sqrt(1234.0),  \na fairly long trace is produced. From this trace\nthe optimizer creates\nthe\nfollowing preamble (Loop 1) and loop (Loop 0) \n\n\n\n\n\n\n\nLooking at the preamble, it starts by making sure that it is not \ncurrently being profiled, the guard\non i5, and that the function object have not been changed\nsince the trace was made, the guard on p3. Somewhat\nintermixed with that, the\ninteger variable n is unboxed, by making sure p11\npoints to an integer object and reading out the integer value from\nthat object. \nThese operations are not needed in the\nloop (and have been removed from it) as emitting the same guards again\nwould be redundant and n becomes a virtual before the\nend of the preamble.\n\n        guard_value(i5, 0, descr=<Guard6>) \n        guard_nonnull_class(p11, ConstClass(W_IntObject), descr=<Guard7>) \n        guard_value(p3, ConstPtr(ptr15), descr=<Guard8>) \n        i16 = getfield_gc_pure(p11, descr=<W_IntObject.inst_intval>)\n\n\nNext comes a test and a guard implementing the while statement\nfollowed by the decrementing of n. These operation appear\nboth in the preamble and in the loop\n\n        i18 = int_gt(i16, 0)\n        guard_true(i18, descr=<Guard9>) \n        i20 = int_sub(i16, 1)\n\n\nAfter that the two floating point variables x and y\nare unboxed. Again this is only needed in the preamble. Note how the\nunboxed value of y, called f23, is passed unchanged\nfrom the preamble to the loop in arguments of the jump \nto allow it to be reused. It will not become a virtual\nsince it is never changed within the loop.\n\n        guard_nonnull_class(p12, 17652552, descr=<Guard10>) \n        guard_nonnull_class(p10, 17652552, descr=<Guard11>) \n        f23 = getfield_gc_pure(p10, descr=<W_FloatObject.inst_floatval>)\n        f24 = getfield_gc_pure(p12, descr=<W_FloatObject.inst_floatval>)\n\n\nFollowing that is the actual calculations performed in the loop in\nform of floating point operations (since the function was called with\na float argument). These appear in both the loop\nand the preamble.\n\n        i26 = float_eq(f24, 0.000000)\n        guard_false(i26, descr=<Guard12>) \n        f27 = float_truediv(f23, f24)\n        f28 = float_add(f24, f27)\n        f30 = float_truediv(f28, 2.000000)\n\n\nFinally there are some tests checking if a signal was received\n(such as when the user presses ctrl-C) and thus should execute some\nsignal handler or if we need to hand over to another thread. This is\nimplemented with a counter that is decreased once every iteration. It\nwill go below zero after some specific number of iterations, tunable by\nsys.setcheckinterval. The counter is read from and written to\nsome global location where it also can be made negative by a C-level\nsignal handler. \n\n        i32 = getfield_raw(32479328, descr=<pypysig_long_struct.c_value>)\n        i34 = int_sub(i32, 2)\n        setfield_raw(32479328, i34, descr=<pypysig_long_struct.c_value>)\n        i36 = int_lt(i34, 0)\n        guard_false(i36, descr=<Guard13>) \n        jump(p0, p1, p2, p4, p10, i20, f30, f23, descr=<Loop0>)\n\n\n\nBridges\n\nWhen a guard fails often enough, the meta-interpreter is started again\nto produce a new trace starting at the failing guard. The tracing is\ncontinued until a previously compiled loop is entered. This could\neither be the the same loop that contains the failing guard\nor some completely different loop. If it is the same loop, executing\nthe preamble again maybe be unnecessary.\nIt is preferable to end the bridge with a jump directly to \nthe loop. To achieve this the optimizer tries to produce short\n  preambles that are inlined at the end of bridges allowing\nthem to jump directly to the loop. Inlining is better than jumping to\na common preamble because most of the inlined short preamble can\ntypically be removed again by the optimizer.\nCreating such a short\npreamble is however not always possible. Bridges jumping to loops for which\nno short preamble can be generated have to end with a jump to the\nfull preamble instead.\n\n\n\nThe short preamble is created by comparing the operations in the\npreamble with the operations in the loop. The\noperations that are in the preamble but not in the loop \nare moved to the short preamble whenever it is safe to move them to\nthe front of the operations remaining. In other words, the full preamble\nis equivalent to the short preamble followed by one iteration of the\nloop. \n\n\n\nThis much has currently been implemented. To give the full picture\nhere, there are two more features that \nhopefully will be implemented in the near future.\nThe first is to replace the full preamble, used by the interpreter\nwhen it reaches a compiled loop, with the short preamble.\nThis is currently not done and is probably not as straight forward as\nit might first seem. The problem is where to resume interpreting on a\nguard failure. However, implementing that should save some\nmemory. Not only \nbecause the preamble will become smaller, but mainly because the\nguards will appear either in the loop or in the preamble, but not\nin both (as they do now). That means there will only be a single bridge and \nnot potentially two copies once the guards are traced.\n\n\n\nThe sqrt example above would with a short preamble result in a trace\nlike this\n\n\n\n\n\nIf it is executed long enough, the last guard will be traced to form a\nbridge. The trace will inherit the virtuals from its parent. This can\nbe used to optimize away the part of the inlined short preamble\nthat deals with virtuals. The resulting bridge should look\nsomething like\n\n\n    [p0, p1, p2, p3, p4, f5, i6]\n    i7 = force_token()\n    setfield_gc(p1, i7, descr=<PyFrame.vable_token>)\n    call_may_force(ConstClass(action_dispatcher), p0, p1, descr=<VoidCallDescr>)\n    guard_not_forced(, descr=<Guard19>) \n    guard_no_exception(, descr=<Guard20>) \n\n    guard_nonnull_class(p4, 17674024, descr=<Guard21>) \n    f52 = getfield_gc_pure(p4, descr=<W_FloatObject.inst_floatval>)\n    jump(p1, p0, p2, p3, p4, i38, f53, f52, descr=<Loop0>)\n\n\nHere the first paragraph comes from the traced bridge and the second\nis what remains of the short preamble after optimization. The\nbox p4 is \nnot a virtual (it contains a pointer to y which is never\nchanged), and it is only virtuals \nthat the bridge inherit from it's parents. This is why the last two\noperations currently cannot be removed.\n\n\n\n\nEach time the short preamble is inlined, a new copy of each of the\nguards in it is generated. Typically the short preamble is inlined in\nseveral places and thus there will be several copies of each of those\nguards. \nIf they fail often enough bridges\nfrom them will be traced (as with all guards). But since there\ntypically are several copies of each guard the same bridge\nwill be generated in \nseveral places. To prevent this, mini-bridges from the inlined guards\nare produced already during the inlining. These mini-bridges contain\nnothing but a jump to the preamble.\n\n\nThe mini-bridges needs the arguments of the preamble to be able\nto jump to it. These arguments contain among other things, boxed\nversions of the \nvariables x and y. Those variables are virtuals in\nthe loop, and have to be allocated. Currently those allocations\nare placed in front of the inlined guard. Moving those allocations into\nthe mini-bridges is the  second feature that \nhopefully will be implemented in the near future. \n\nAfter this feature is\nimplemented, the result should look something like\n\n\n\n\n\nMultiple specialized versions\n\nFloating point operations were generated in the trace above\nbecause sqrt was called with a float argument. If it is\ninstead called with an int argument, integer operations will be generated. The\nsomewhat more complex situations is when both int's and float's are\nused as arguments. Then the jit need to generate multiple versions of\nthe same loop, specialized in different ways. The details, given\nbelow, on how this is achieved is somewhat involved. For the casual\nreader it would make perfect sense to skip to the next section here.\n\n\n\nConsider the case when sqrt is first called with a float\nargument (but with n small enough not to generate the\nbridge). Then the trace shown above will be\ngenerated. If sqrt is now called with an int argument, the\nguard in the preamble testing that the type of the input object is float\nwill fail:\n\n        guard_nonnull_class(p12, 17652552, descr=<Guard10>) \n\nIt will fail every iteration, so soon enough a bridge will be\ngenerated from this guard in the preamble. This guard will end with a\njump to the same loop, and the optimizer will try to inline\nthe short preamble at the end of it. This will however fail\nsince now there are two guards on p12. One that makes sure it\nis an int and and one that makes sure it is a float. The optimizer\nwill detect that the second guard will always fail and mark the bridge\nas invalid. Invalid loops are not passed on to the backend for\ncompilation. \n\n\n\nIf a loop is detected to be invalid while inlining the short preamble,\nthe metainterpreter will continue to trace for yet another \niteration of the loop. This new trace can be compiled as above and\nwill produce a new loop with a new preamble that are now specialized\nfor int arguments instead of float arguments. The bridge that\npreviously became invalid will now be tried again. This time inlining\nthe short preamble of the new loop instead. This will produce a set of\ntraces connected like this\n\n\n\n\n(click for some hairy details)\n\n\nThe height of the boxes is this figure represents how many instructions\nthey contain (presuming the missing features from the previous section\nare implemented). Loop 0 is specialized for floats and it's preamble have\nbeen split into two boxes at the failing guard. Loop 2 is specialized\nfor ints and is larger than Loop 0. This is mainly because the integer\ndivision in python does not map to the integer division of the\nmachine, but have to be implemented with several instructions (integer\ndivision in python truncates its result towards minus\ninfinity, while the the machine integer division truncates towards\n0). Also the height of the bridge is about the same as the height of\nLoop 2. This is because it contains a full iteration of the loop.\n\n\n\nA More Advanced Example\n\nLet's conclude with an example that is a bit more advanced, where this unrolling\napproach actually outperforms the previous approach. Consider\nmaking a\nfixed-point\nimplementation of the square root using 16 bit's of decimals. This can be\ndone using the same implementation\nof sqrt but calling it with an object of a class representing\nsuch fixed-point real numbers:\n\n\nclass Fix16(object):\n    def __init__(self, val, scale=True):\n        if isinstance(val, Fix16):\n            self.val = val.val\n        else:\n            if scale:\n                self.val = int(val * 2**16)\n            else:\n                self.val = val\n\n    def __add__(self, other):\n        return  Fix16(self.val + Fix16(other).val, False)\n\n    def __sub__(self, other):\n        return  Fix16(self.val - Fix16(other).val, False)\n\n    def __mul__(self, other):\n        return  Fix16((self.val >> 8) * (Fix16(other).val >> 8), False)\n\n    def __div__(self, other):\n        return  Fix16((self.val << 16) / Fix16(other).val, False)\n\n\n\n\nBelow is a table comparing the runtime of the sqrt function above with\ndifferent argument types on different python interpreters. Pypy 1.4.1\nwas released before the optimizations described in this post were in place\nwhile they are in place in the \nnightly\n  build from January 5, \ndenoted pypy in the table. There are also the running time for the same\nalgorithms implemented in C and compiled with \"gcc -O3\n-march=native\". Tests were executed on a 2.53GHz Intel Core2\nprocessor with n=100000000 iterations.\nComparing the integer versions with C may be considered a\nbit unfair because of the more advanced integer division operator in\npython. The left part of this table shows runtimes of sqrt in\na program containing a single call to sqrt (i.e. only a single\nspecialized version of the loop is needed). The right part shows the\nruntime of sqrt when it has been called with a different\ntype of argument before.\n\n\n\n\n  First callSecond call\n  floatintFix16\u00a0\u00a0\n               floatintFix16\n  cpython\n     28.18 s\n     22.13 s\n     779.04 s\n    \n     28.07 s\n     22.21 s\n     767.03 s    \n  \n  pypy 1.4.1\n     1.20 s\n     6.49 s\n     11.31 s\n    \n     1.20 s\n     6.54 s\n     11.23 s\n  \n  pypy\n     1.20 s\n     6.44 s\n     6.78 s\n    \n     1.19 s\n     6.26 s\n     6.79 s\n  \n  gcc\n     1.15 s\n     1.82 s\n     1.89 s\n    \n     1.15 s\n     1.82 s\n     1.89 s\n  \n\n\n\n\nFor this to work in the last case, when Fix16 is the argument type in\nthe second type, \nthe trace_limit had to be increased from its default value to prevent\nthe metainterpreter from aborting while tracing the second version of\nthe loop. Also sys.setcheckinterval(1000000) were used to prevent the\nbridge from being generated. With the bridge the performance of the\nlast case is significantly worse. Maybe because the optimizer currently\nfails to generate a short preamble for it. But the slowdown\nseems too big for that to be the only explanation. Below are the runtimes\nnumbers with checkinterval set to its default value of 100:\n\n\n  First callSecond call\n  floatintFix16\u00a0\u00a0\n               floatintFix16\n  cpython\n     28.71 s\n     22.09 s\n     781.86 s\n    \n     28.28 s\n     21.92 s\n     761.59 s\n  \n  pypy 1.4.1\n     1.21 s\n     6.48 s\n     11.22 s\n    \n     1.72 s\n     7.58 s\n     12.18 s\n  \n  pypy\n     1.21 s\n     6.27 s\n     7.22 s\n    \n     1.20 s\n     6.29 s\n     90.47 s\n  \n\n\n\nConclusions\nEven though we are seeing speedups in a variety of different small\nbenchmarks, more complicated examples are not affected much by these\noptimizations. It might partly be because larger examples have longer\nand more complicated loops, and thus allowing optimizations to operate\nacross loop boundary will have a smaller relative effect. Another problem is\nthat with more complicated examples there will be more bridges, and bridges\nare currently not handled very well (most of the time all virtuals are\nforced at the end of the bridge as explained above). But moving those\nforcings into the mini bridges should fix that.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2011/01/loop-invariant-code-motion-1998392217676829154.html"
    },
    {
      "title": "PyPy 1.4.1",
      "text": "Here is PyPy 1.4.1 :-)\n\nUpdate: Win32 binaries available.\n\nEnjoy!\n\nRelease announcement\n\nWe're pleased to announce\nthe 1.4.1 release of PyPy.\nThis release consolidates all the bug fixes that occurred since the\nprevious release.  To everyone that took the trouble to report\nthem, we want to say thank you.\n\nWhat is PyPy\n\nPyPy is a very compliant Python interpreter, almost a drop-in\nreplacement for CPython.  Note that it still only emulates Python\n2.5 by default; the fast-forward branch with Python 2.7\nsupport is slowly getting ready but will only be integrated in\nthe next release.\n\nIn two words, the advantage of trying out PyPy instead of CPython\n(the default implementation of Python) is, for now, the\nperformance.  Not all programs are faster in PyPy, but we are\nconfident that any CPU-intensive task will be much faster, at\nleast if it runs for long enough (the JIT has a slow warm-up\nphase, which can take several seconds or even one minute on the\nlargest programs).\n\nNote again that we do support compiling and using C extension\nmodules from CPython (pypy setup.py install).  However, this\nis still an alpha feature, and the most complex modules typically\nfail for various reasons; others work (e.g. PIL) but take a\nserious performance hit.  Also, for Mac OS X see below.\n\nPlease note also that PyPy's performance was optimized almost\nexclusively on Linux.  It seems from some reports that on Windows\nas well as Mac OS X (probably for different reasons) the\nperformance might be lower.  We did not investigate much so far.\n\nMore highlights\n\n\n\nWe migrated to Mercurial (thanks to Ronny Pfannschmidt and\n  Antonio Cuni) for the effort) and moved to bitbucket.  The new\n  command to check out a copy of PyPy is:\n  hg clone https://bitbucket.org/pypy/pypy\n\nIn long-running processes, the assembler generated by old\n  JIT-compilations is now freed.  There should be no more leak,\n  however long the process runs.\n\nImprove a lot the performance of the binascii module, and\n  of hashlib.md5 and hashlib.sha.\n\nMade sys.setrecursionlimit() a no-op.  Instead, we rely purely\n  on the built-in stack overflow detection mechanism, which also\n  gives you a RuntimeError -- just not at some exact recursion\n  level.\n\nFix argument processing (now e.g. pypy -OScpass works like\n  it does on CPython --- if you have a clue what it does there\n  :-) )\n\ncpyext on Mac OS X: it still does not seem to work.  I get\n  systematically a segfault in dlopen().  Contributions welcome.\n\nFix two corner cases in the GC (one in minimark, one in\n  asmgcc+JIT).  This notably prevented pypy translate.py -Ojit\n  from working on Windows, leading to crashes.\n\nFixed a corner case in the JIT's optimizer, leading to Fatal\n  RPython error: AssertionError.\n\nAdded some missing built-in functions into the 'os' module.\n\nFix ctypes (it was not propagating keepalive information from\n  c_void_p).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/pypy-141-7283625923182122073.html"
    },
    {
      "title": "PyPy migrates to Mercurial",
      "text": "The assiduous readers of this blog surely remember that during the last\nD\u00fcsseldorf sprint in October, we started the process for migrating our main\ndevelopment repository from Subversion to Mercurial.  Today, after more than\ntwo months, the process has finally been completed :-).\nThe new official PyPy repository is hosted on BitBucket.\nThe migration has been painful because the SVN history of PyPy was a mess and\nnone of the existing conversion tools could handle it correctly.  This was\npartly because PyPy started when subversion was still at version 0.9 when some\nbest-practices were still to be established, and partly because we probably\nmanaged to invent all the possible ways to do branches (and even some of the\nimpossible ones: there is at least one commit which you cannot do with the\nplain SVN client but you have to speak to the server by yourself :-)).\nThe actual conversion was possible thanks to the enormous work done by Ronny\nPfannschmidt and his hackbeil tool. I would like to personally thank Ronny\nfor his patience to handle all the various requests we asked for.\nWe hope that PyPy development becomes even more approachable now, at least from\na version control point of view.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/pypy-migrates-to-mercurial-3308736161543832134.html"
    },
    {
      "title": "Oh, and btw: PyPy gets funding through \"Eurostars\"",
      "text": "There is a supporting reason why we made so many advances in the last year:\nfunding through Eurostars, a European research funding program.\nThe title of our proposal (accepted in 2009) is: \"PYJIT - a fast\nand flexible toolkit for dynamic programming languages based on PyPy\".\nAnd the participants are Open End AB, the Heinrich-Heine-Universit\u00e4t\nD\u00fcsseldorf (HHU), and merlinux GmbH.\nIt's not hard to guess what PYJIT is actually about, is it?\nQuoting: \"The PYJIT project will deliver a fast and flexible\nJust-In-Time Compiler toolkit based on PyPy to the market of dynamic\nlanguages.  Our main aim is to showcase our project's results for the\nOpen Source language Python, providing unprecedented levels of\nflexibility and with speed hitherto only available using statically\ntyped languages.\" (Details in German or in Swedish :-)\nA subgoal is to improve our development and testing infrastructure,\nmainly showcased by Holger's recent py.test releases, the testing tool\nused by PyPy for its 16K tests and the speed.pypy.org infrastructure\n(web app programmed by Miquel Torres on his own time).\nThe overall scope of this project is smaller than that of the previous EU project\nfrom 2004 to 2007.  The persons that are (or were) getting money to work\non PyPy are Samuele Pedroni (at Open End), Maciej Fijalkowski (as a\nsubcontractor), Carl Friedrich Bolz, Armin Rigo, Antonio Cuni (all at\nHHU), and Holger Krekel (at merlinux) as well as Ronny Pfannschmidt (as\na subcontractor).\nThe Eurostars funding lasts until August 2011.  What comes afterwards?\nWell, for one, many of the currently funded people have done work without\ngetting funding in previous years.  This will probably continue.\nWe also have non-funded people in the core group right now and we'll\nhope to enlarge it further.  But of course there are still large tasks\nahead which may greatly benefit from funding.  We have setup a\ndonation infrastructure and maybe we can win one or more larger\norganisations to provide higher or regular sums of money to fund future\ndevelopment work.   Another possibility for companies is to pay\nPyPy developers to help and improve PyPy for their particular use cases.\nAnd finally, your help, donations and suggestions are always\nwelcome and overall we hope to convince more and more people it's\nworthwhile to invest into PyPy's future.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/oh-and-btw-pypy-gets-funding-through-3568486750776147382.html"
    },
    {
      "title": "Leysin Winter sprint",
      "text": "Hi all,\n\n\n\n\n\nThe next sprint will be in Leysin, Switzerland, during the week of the 16th-22nd of January 2011.\n\nNow that we have released 1.4, and plan to release 1.4.1 soon, the sprint is going to be mainly working on fixing issues reported by various users.  Of course this does not prevent people from showing up with a more precise interest in mind.\n\nAs usual, the break day on the sprint will likely be a day of skiing :-)\n\nHoping to see you there.\n\n\n\n\n\n\n\n\nUpdate: there are actually a number of branches that we want to polish and merge into trunk: at least fast-forward, jit-unroll-loops, arm-backend and jitypes2.  For more details, see the announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/leysin-winter-sprint-8115212435349091722.html"
    },
    {
      "title": "PyPy 1.4 release aftermath",
      "text": "A couple days have passed since the announcement of the 1.4 release, and this\nis a short summary of what happened afterwards. Let's start with\nnumbers:\n\n16k visits to the release announcement on our blog\nwe don't have download statistics unfortunately\n10k visits to speed center\nmost traffic comes from referring sites, reddit alone creating above a third\nof our traffic\n\nNot too bad for a project that doesn't have a well-established user base.\nLessons learned:\n\nReleases are very important. They're still the major way projects communicate\nwith community, even if we have nightly builds that are mostly stable.\nNo segfaults were reported, no incompatibilities between JIT and normal\ninterpretation. We think that proves (or at least provides a lot of\nexperimental evidence) that our write-once-and-then-transform method is\neffective.\nA lot of people complained about their favorite module in C not working, we\nshould have made it clearer that CPyExt is in alpha state.  Indeed, we\nwould like to know which C extension modules do work :-).\nSome people reported massive speedups, other reported slowdowns compared\nto CPython. Most of those slowdowns relate to modules being inefficient\n(or doing happy nonsense), like ctypes. This is expected, given that\nnot all modules are even jitted (although having them jitted is usually\na matter of a couple of minutes).\nNobody complained about a lack of some stdlib module. We implemented the ones\nwhich are used more often, but this makes us wonder if less used stdlib modules\nhave any users at all.\n\nIn general feedback has been overwhelmingly positive and we would like to\nthank everyone trying (and especially those reporting problems)\nCheers,\nfijal",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2010/12/pypy-14-release-aftermath-2979780282210978576.html"
    },
    {
      "title": "We are not heroes, just very patient",
      "text": "Inspired by some of the comments to the release that said \"You are heroes\", I though a bit about the longish history of PyPy and hunted around for some of the mailing list posts that started the project. Then I put all this information together into the following timeline:\n\n   timeline \n\nThere is also a larger version of the timeline. Try to click on some of the events, the links usually go to the sprint descriptions. I also tried to find pictures for the sprints but succeeded for only half of them, if anybody still has some, I would be interested. It's kind of fun to browse around in some of the old sprint descriptions to see how PyPy evolved. Some of the current ideas have been around for a long time, some are new. In the description of the releases I put estimates for the speed of the release.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/12/we-are-not-heroes-just-very-patient-7114408885070101720.html"
    },
    {
      "title": "PyPy 1.4: Ouroboros in practice",
      "text": "We're pleased to announce the 1.4 release of PyPy. This is a major breakthrough\nin our long journey, as PyPy 1.4 is the first PyPy release that can translate\nitself faster than CPython.  Starting today, we are using PyPy more for\nour every-day development.  So may you :) You can download it here:\nhttps://pypy.org/download.html\n\nWhat is PyPy\nPyPy is a very compliant Python interpreter, almost a drop-in replacement\nfor CPython. It is fast (pypy 1.4 and cpython 2.6 comparison).\nNew Features\nAmong its new features, this release includes numerous performance improvements\n(which made fast self-hosting possible), a 64-bit JIT backend, as well\nas serious stabilization. As of now, we can consider the 32-bit and 64-bit\nlinux versions of PyPy stable enough to run in production.\nNumerous speed achievements are described on our blog. Normalized speed\ncharts comparing pypy 1.4 and pypy 1.3 as well as pypy 1.4 and cpython 2.6\nare available on the benchmark website. For the impatient: yes, we got a lot faster!\n\n\nMore highlights\n\nPyPy's built-in Just-in-Time compiler is fully transparent and\nautomatically generated; it now also has very reasonable memory\nrequirements.  The total memory used by a very complex and\nlong-running process (translating PyPy itself) is within 1.5x to\nat most 2x the memory needed by CPython, for a speed-up of 2x.\nMore compact instances.  All instances are as compact as if\nthey had __slots__.  This can give programs a big gain in\nmemory.  (In the example of translation above, we already have\ncarefully placed __slots__, so there is no extra win.)\nVirtualenv support: now PyPy is fully compatible with virtualenv: note that\nto use it, you need a recent version of virtualenv (>= 1.5).\nFaster (and JITted) regular expressions - huge boost in speeding up\nthe re module.\nOther speed improvements, like JITted calls to functions like map().\n\nCheers,\nCarl Friedrich Bolz, Antonio Cuni, Maciej Fijalkowski,\nAmaury Forgeot d'Arc, Armin Rigo and the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/pypy-14-ouroboros-in-practice-5437628000869417542.html"
    },
    {
      "title": "Improving Memory Behaviour to Make Self-Hosted PyPy Translations Practical",
      "text": "In our previous blog post, we talked about how fast PyPy can translate\nitself compared to CPython.  However, the price to pay for the 2x speedup was\nan huge amount of memory: actually, it was so huge that a standard -Ojit\ncompilation could not be completed on 32-bit because it required more than the\n4 GB of RAM that are addressable on that platform.  On 64-bit, it consumed\n8.3 GB of RAM instead of the 2.3 GB needed by CPython.\nThis behavior was mainly caused by the JIT, because at the time we wrote the\nblog post the generated assembler was kept alive forever, together with some\nbig data structure needed to execute it.\nIn the past two weeks Anto and Armin attacked the issue in the jit-free\nbranch, which has been recently merged to trunk.  The branch solves several\nissues. The main idea of the branch is that if a\nloop has not been executed for a certain amount of time (controlled by the new\nloop_longevity JIT parameter) we consider it \"old\" and no longer needed,\nthus we deallocate it.\n(In the process of doing this, we also discovered and fixed an\noversight in the implementation of generators, which led to generators being\nfreed only very slowly.)\nTo understand the freeing of loops some more, let's look at how many loops are\nactually created during a translation.\nThe purple line in the following graph shows how many loops (and bridges) are\nalive at any point in time with an infinite longevity, which is equivalent to\nthe situation we had before the jit-free branch.  By contrast, the blue\nline shows the number of loops that you get in the current trunk: the\ndifference is evident, as now we never have more than 10000 loops alive, while\npreviously we got up to about 37000 ones.  The time on the X axis is expressed\nin \"Giga Ticks\", where a tick is the value read out of the Time Stamp Counter\nof the CPU.\n\n\n\nThe grey vertical bars represent the beginning of each phase of the\ntranslation:\n\nannotate performs control flow graph construction and type inference.\nrtype lowers the abstraction level of the control flow graphs with types to that of C.\npyjitpl constructs the JIT.\nbackendopt optimizes the control flow graphs.\nstackcheckinsertion finds the places in the call graph that can overflow the C stack and inserts checks that raise an exception instead.\ndatabase_c produces a database of all the objects the C code will have to know about.\nsource_c produces the C source code.\ncompile_c calls the compiler to produce the executable.\n\nYou can nicely see, how the number of alive graphs drops shortly after the\nbeginning of a new phase.\nThose two fixes, freeing loops and generators, improve the memory usage greatly:\nnow, translating PyPy\non PyPy on 32-bit consumes 2 GB of RAM, while on CPython it consumes 1.1 GB.\nThis result can even be improved somewhat, because we are not actually freeing\nthe assembler code itself, but\nonly the large data structures around it; we can consider it as a residual\nmemory leak of around 150 MB in this case.  This will be fixed in the\njit-free-asm branch.\nThe following graph shows the memory usage in more detail:\n\n\nthe blue line (cpython-scaled) shows the total amount of RAM that the\nOS allocates for CPython.  Note that the X axis (the time) has been\nscaled down so that it spans as much as the PyPy one, to ease the\ncomparison. Actually, CPython took more than twice as much time as PyPy to\ncomplete the translation\nthe red line (VmRss) shows the total amount of RAM that the\nOS allocates for PyPy: it includes both the memory directly handled by\nour GC and the \"raw memory\" that we need to allocate for other tasks, such\nas the assembly code generated by the JIT\nthe brown line (gc-before) shows how much memory is used by the GC\nbefore each major collection\nthe yellow line (gc-after) shows how much memory is used by the GC\nafter each major collection: this represent the amount of memory which is\nactually needed to hold our Python objects.  The difference between\ngc-before and gc-after (the GC delta) is the amout of memory that the GC\nuses before triggering a new major collection\n\n\n\n\n\nBy comparing gc-after and cpython-scaled, we can see that PyPy\nuses mostly the same amount of memory as CPython for storing the application\nobjects (due to reference counting the memory usage in CPython is always very\nclose to the actually necessary memory).  The extra memory\nused by PyPy is due to the GC delta, to the machine code generated by the JIT\nand probably to some other external effect (such as e.g. Memory\nFragmentation).\nNote that the GC delta can be set arbitrarly low (another recent addition --\nthe default value depends on the actual RAM on your computer; it probably\nworks to translate if your computer has precisely 2 GB, because in this\ncase the GC delta and thus the total memory usage will be somewhat\nlower than reported here), but the cost is to have more\nfrequent major collections and thus a higher run-time overhead.  The same is\ntrue for the memory needed by the JIT, which can be reduced by telling the JIT\nto compile less often or to discard old loops more frequently.  As often\nhappens in computer science, there is a trade-off between space and time, and\ncurrently for this particular example PyPy runs twice as fast as CPython by\ndoubling the memory usage. We hope to improve even more on this trade-off.\nOn 64-bit, things are even better as shown by the the following graph:\n\n\n\nThe general shape of the lines is similar to the 32-bit graph. However, the\nrelative difference to CPython is much better: we need about 3 GB of RAM, just\n24% more than the 2.4 GB needed by CPython.  And we are still more than 2x\nfaster!\nThe memory saving is due (partly?) to the vtable ptr optimization, which is\nenabled by default on 64-bit because it has no speed penalty (see\nUnifying the vtable ptr with the GC header).\nThe net result of our work is that now translating PyPy on PyPy is practical\nand takes less than 30 minutes.  It's impressive how quickly you get used to\ntranslation taking half the time -- now we cannot use CPython any more for that\nbecause it feels too slow :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/improving-memory-behaviour-to-make-self-856966667913962461.html"
    },
    {
      "title": "Running large radio telescope software on top of PyPy and twisted",
      "text": "Hello.\nAs some of you already know, I've recently started working on a\nvery large radio telescope at SKA South Africa. This telescope's\noperating software runs almost exclusively on Python (several high throughput\npieces are in C or CUDA or directly executed by FPGAs). Some cool telescope pictures:\n\n\n\n\n\n\n\n(photos courtesy of SKA South Africa)\nMost of the operation software is using the KatCP protocol to talk between devices.\nThe currently used implementation is Open Source software with a custom home built\nserver and client. As part of the experiments, I've implemented a Twisted based\nversion and run in on top of CPython and PyPy for both the default\nimplementation and the one based on Twisted to see how those perform.\nThere are two testing scenarios: the first one is trying to saturate the connection\nby setting up multiple sensors that report state every 10ms, the second one\nis measuring a round-trip between sending a request and receiving the response.\nBoth numbers are measuring the number of requests per 0.2s, so the more the better. On X axis there is a number of simultanously connected clients.\nAll benchmark code is available in the KatCP repository.\nThe results are as follows:\n\n\n\n\n\n\nAs you can see, in general Twisted has larger overhead for a single client\nand scales better as the number of clients increases. That's I think expected,\nsince Twisted has extra layers of indirection. The round trip degradation of\nTwisted has to be investigated, but for us scenario1 is by far more important.\nAll across the board PyPy performs much better than CPython for both\nTwisted and a home-made solution, which I think is a pretty good result.\nNote: we didn't roll this set up into production yet, but there are high\nchances for both twisted and PyPy to be used in some near future.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/running-large-radio-telescope-software-7600337209616168504.html"
    },
    {
      "title": "Efficiently Implementing Python Objects With Maps",
      "text": "As could be foreseen by my Call for Memory Benchmarks post a while ago, I am\ncurrently working on improving the memory behaviour of PyPy's Python\ninterpreter. In this blog post I want to describe the various data a Python\ninstance can store. Then I want to describe how a branch that I did and that was\nrecently merged implements the various features of instances in a very\nmemory-efficient way.\n\nPython's Object Model\nAll \"normal\" new-style Python instances (i.e. instances of subclasses of object\nwithout added declarations) store two (or possibly three) kinds of information.\n\nStoring the Class\nEvery instance knows which class it belongs to. This information is accessible\nvia the .__class__ attribute. It can also be changed to other (compatible\nenough) classes by writing to that attribute.\n\n\nInstance Variables\nEvery instance also has an arbitrary number of attributes stored (also called\ninstance variables). The instance variables used can vary per instance, which is\nnot the case in most other class-based languages: traditionally (e.g. in\nSmalltalk or Java) the class describes the shape of its instances,\nwhich means that the\nset of admissible instance variable names is the same for all instances of a\nclass.\nIn Python on the other hand, it is possible to add arbitrary attributes to an\ninstance at any point. The instance behaves like a dictionary mapping attribute\nnames (as strings) to the attribute values.\nThis is actually how CPython implements instances. Every instance has a\nreference to a dictionary that stores all the attributes of the instance. This\ndictionary can be reached via the .__dict__ attribute. To make things more\nfun, the dictionary can also be changed by writing to that attribute.\n\n\nExample\nAs an example, consider the following code:\nclass A(object):\n    pass\n\ninstance1 = A()\ninstance1.x = 4\ninstance1.y = 6\ninstance1.z = -1\n\ninstance2 = A()\ninstance2.x = 1\ninstance2.y = 2\ninstance2.z = 3\n\nThese two instances would look something like this in memory:\n\n(The picture glosses over a number of details, but it still shows the essential\nissues.)\nThis way of storing things is simple, but unfortunately rather inefficient. Most\ninstances of the same class have the same shape, i.e. the same set of instance\nattribute names. That means that the key part of all the dictionaries is\nidentical (shown grey here). Therefore storing that part repeatedly in all\ninstances is a waste. In addition, dictionaries are themselves rather large.\nSince they are typically implemented as hashmaps, which must not be too full to\nbe efficient, a dictionary will use something like 6 words on average per key.\n\n\nSlots\nSince normal instances are rather large, CPython 2.2 introduced slots, to make\ninstances consume less memory. Slots are a way to fix the set of attributes an\ninstance can have. This is achieved by adding a declaration to a class, like\nthis:\nclass B(object):\n    __slots__ = [\"x\", \"y\", \"z\"]\n\nNow the instances of B can only have x, y and z as attributes\nand don't have a dictionary at all. Instead, the instances of B get\nallocated with enough size to hold exactly the number of instance variables that\nthe class permits. This clearly saves a lot of memory over the dictionary\napproach, but has a number of disadvantages. It is obviously less flexible, as\nyou cannot add additional instance variables to an instance if you happen to\nneed to do that. It also introduces a set of rules and corner-cases that can\nbe surprising sometimes (e.g. instances of a subclass of a class with slots that\ndoesn't have a slots declaration will have a dict).\n\n\n\nUsing Maps for Memory-Efficient Instances\nAs we have seen in the diagram above, the dictionaries of instances of the same\nclass tend to look very similar and share all the keys. The central idea to use\nless memory is to \"factor out\" the common parts of the instance dictionaries\ninto a new object, called a \"map\" (because it is a guide to the landscape of the\nobject, or something). After that factoring out, the representation of the\ninstances above looks something like this:\n\nEvery instance now has a reference to its map, which describes what the instance\nlooks like. The actual instance variables are stored in an array (called\nstorage in the diagram). In the example here, the map describes that the\ninstances have three attributes x, y and z. The numbers after the\nattributes are indexes into the storage array.\nIf somebody adds a new attribute to one of the instances, the map for that\ninstance will be changed to another map that also contains the new attribute,\nand the storage will have to grow a field with the new attribute. The maps are\nimmutable, immortal and reused as much as possible. This means, that two\ninstances of the same class with the same set of attributes will have the same\nmap. This also means that the memory the map itself uses is not too important,\nbecause it will potentially be amortized over many instances.\nNote that using maps makes instances nearly as small as if the correct slots had\nbeen declared in the class. The only overhead needed is the indirection to the\nstorage array, because you can get new instance variables, but not new slots.\nThe concept of a \"map\" that describes instances is kind of old and comes from\nthe virtual machine for the Self programming language. The optimization was\nfirst described in 1989 in a paper by Chambers, Ungar and Lee with the title An\nEfficient Implementation of Self, a Dynamically-Typed Object-Oriented Language\nBased on Prototypes. A similar technique is used in Google's V8 JavaScript\nengine, where the maps are called hidden classes and in the Rhino\nJavaScript engine.\nThe rest of the post describes a number of further details that occur if\ninstances are implemented using maps.\n\nSupporting Dictionaries with Maps\nThe default instance representation with maps as shown above works without\nactually having a dictionary as part of each instance. If a dictionary is\nactually requested, by accessing the .__dict__ attribute, it needs to be\ncreated and cached. The dictionary is not a normal Python dictionary, but a thin\nwrapper around the object that forwards all operations to it. From the user's\npoint of view it behaves like a normal dictionary though (it even has the\ncorrect type).\nThe dictionary needs to be cached, because accessing .__dict__ several times\nshould always return the same dictionary. The caching happens by using a\ndifferent map that knows about the dictionary and putting the dictionary into\nthe storage array:\n\nThings become really complex if the fake dict is used in strange ways. As long\nas the keys are strings, everything is fine. If somebody adds other keys to the\ndict, they cannot be represented by the map any more (which supports only\nattributes, i.e. string keys in the __dict__). If that happens, all the\ninformation of the instance will move into the fake dictionary, like this:\n\nIn this picture, the key -1 was added to the instance's dictionary. Since\nusing the dictionary in arbitrary ways should be rare, we are fine with the\nadditional time and memory that the approach takes.\n\n\nSlots and Maps\nMaps work perfectly together with slots, because the slots can just be stored\ninto the storage array used by the maps as well (in practise there are some\nrefinements to that scheme).  This means that putting a __slots__ on a\nclass has mostly no effect, because the instance only stores the values of the\nattributes (and not the names), which is equivalent to the way slots are stored\nin CPython.\n\n\n\nImplementation Details\nIn the diagrams above, I represented the maps as flat objects. In practise this\nis a bit more complex, because it needs to be efficient to go from one map to\nthe next when new attributes are added. Thus the maps are organized in a tree.\nThe instances with their maps from above look a bit more like this in practise:\n\nEvery map just describes one attribute of the object, with a name and a an\nindex. Every map also has a back field, that points to another map\ndescribing what the rest of the object looks like. This chain ends with a\nterminator, which also stores the class of the object.\nThe maps also contain the information necessary for making a new object of\nclass A. Immediately after the new object has been created, its map is the\nterminator. If the x attribute is added, its maps is changed to the\nsecond-lowest map, and so on. The blue arrows show the sequence of maps that\nthe new object goes through when the attributes x, y, z are added.\nThis representation of maps as chains of objects sounds very inefficient if an\nobject has many attributes. The whole chain has to be walked to find the index.\nThis is true to some extent. The problem goes away in the presence of the JIT,\nwhich knows that the chain of maps is an immutable structure, and will thus\noptimize away all the chain-walking. If the JIT is not used, there are a few\ncaches that try to speed up the walking of this chain (similar to the method\ncache in CPython and PyPy).\n\n\nResults\nIt's hard to compare the improvements of this optimization in a fair way, as\nthe trade-offs are just very different. Just to give an impression, a million\nobjects of the same class with three fields on a 32bit system takes:\nwithout slots:\n\n182 MiB memory in CPython\n177 MiB memory in PyPy without maps\n40 MiB memory in PyPy with maps\n\nwith slots:\n\n45 MiB memory in CPython\n50 MiB memory in PyPy without maps\n40 MiB memory in PyPy with maps\n\nNote how maps make the objects a bit more efficient like CPython using slots.\nAlso, using slots has no additional effect in PyPy.\n\n\nConclusion\nMaps are a powerful approach to shrinking the memory used by many similar\ninstances. I think they can be pushed even further (e.g. by adding information\nabout the types of the attributes) and plan to do so in the following months.\nDetails will be forthcoming.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/efficiently-implementing-python-objects-3838329944323946932.html"
    },
    {
      "title": "Speeding up PyPy by donations",
      "text": "PyPy joins the Software Freedom Conservancy\n\nGood news. PyPy is now a member of the Software Freedom Conservancy (SFC),\nsee the SFC blog post.  This allows us to manage non-profit monetary aspects of\nthe project independently from a company or particular persons.   So we\ncan now officially receive donations both from people prefering right or\nleft sides, see the Donate buttons on our home page and our blog.\nAnd you can use PayPal or Google Checkout, Donations are tax-exempt in the\nUSA and hopefully soon in Europe as well.\nWhat's it going to get used for?  For the immediate future we intend to use\nthe donations for funding travels of core contributors to PyPy sprints\nwho otherwise can't afford to come.  So if you have no time but some\nmoney you can help to encourage coding contributors to care for PyPy.\nIf we end up with bigger sums we'll see and take suggestions.  Money\nspending decisions will be done by core PyPy people according to\nnon-profit guidelines.  And we'll post information from time to time\nabout how much we got and where the money went.\nIf you have any questions regarding the SFC membership or donations\nyou may send email to sfc at pypy.org  which will be observed\nby Carl Friedrich Bolz, Jacob Hallen and Holger Krekel - the initial\nPyPy SFC representatives on behalf of the PyPy team.  Many thanks go\nout to Bradley M. Kuhn for helping to implement the PyPy SFC membership.\ncheers,\nHolger & Carl Friedrich",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/speeding-up-pypy-by-donations-6035529829962326007.html"
    },
    {
      "title": "A snake which bites its tail: PyPy JITting itself",
      "text": "We have to admit: even if we have been writing for years about the fantastic\nspeedups that the PyPy JIT gives, we, the PyPy developers, still don't use it\nfor our daily routine.  Until today :-).\nReaders brave enough to run translate.py to translate PyPy by themselves\nsurely know that the process takes quite a long time to complete, about a hour\non super-fast hardware and even more on average computers.  Unfortunately, it\nhappened that translate.py was a bad match for our JIT and thus ran much\nslower on PyPy than on CPython.\nOne of the main reasons is that the PyPy translation toolchain makes heavy use\nof custom metaclasses, and until few weeks ago metaclasses disabled some of\nthe central optimizations which make PyPy so fast.  During the recent\nD\u00fcsseldorf sprint, Armin and Carl Friedrich fixed this problem and\nre-enabled all the optimizations even in presence of metaclasses.\nSo, today we decided that it was time to benchmark again PyPy against itself.\nFirst, we tried to translate PyPy using CPython as usual, with the following\ncommand line (on a machine with an \"Intel(R) Xeon(R) CPU W3580 @ 3.33GHz\" and\n12 GB of RAM, running a 32-bit Ubuntu):\n\n$ python ./translate.py -Ojit targetpypystandalone --no-allworkingmodules\n\n... lots of output, fractals included ...\n\n[Timer] Timings:\n[Timer] annotate                       ---  252.0 s\n[Timer] rtype_lltype                   ---  199.3 s\n[Timer] pyjitpl_lltype                 ---  565.2 s\n[Timer] backendopt_lltype              ---  217.4 s\n[Timer] stackcheckinsertion_lltype     ---   26.8 s\n[Timer] database_c                     ---  234.4 s\n[Timer] source_c                       ---  480.7 s\n[Timer] compile_c                      ---  258.4 s\n[Timer] ===========================================\n[Timer] Total:                         --- 2234.2 s\n\nThen, we tried the same command line with PyPy (SVN revision 78903, x86-32 JIT\nbackend, downloaded from the nightly build page):\n\n$ pypy-c-78903 ./translate.py -Ojit targetpypystandalone --no-allworkingmodules\n\n... lots of output, fractals included ...\n\n[Timer] Timings:\n[Timer] annotate                       ---  165.3 s\n[Timer] rtype_lltype                   ---  121.9 s\n[Timer] pyjitpl_lltype                 ---  224.0 s\n[Timer] backendopt_lltype              ---   72.1 s\n[Timer] stackcheckinsertion_lltype     ---    7.0 s\n[Timer] database_c                     ---  104.4 s\n[Timer] source_c                       ---  167.9 s\n[Timer] compile_c                      ---  320.3 s\n[Timer] ===========================================\n[Timer] Total:                         --- 1182.8 s\n\nYes, it's not a typo: PyPy is almost two times faster than CPython!\nMoreover, we can see that PyPy is faster in each of the individual steps apart\ncompile_c, which consists in just a call to make to invoke gcc.\nThe slowdown comes from the fact that the Makefile also contains a lot of\ncalls to the trackgcroot.py script, which happens to perform badly on PyPy\nbut we did not investigate why yet.\nHowever, there is also a drawback: on this specific benchmark, PyPy consumes\nmuch more memory than CPython.  The reason why the command line above contains\n--no-allworkingmodules is that if we include all the modules the\ntranslation crashes when it's complete at 99% because it consumes all the 4GB\nof memory which is addressable by a 32-bit process.\nA partial explanation if that so far the assembler generated by the PyPy JIT\nis immortal, and the memory allocated for it is never reclaimed.  This is\nclearly bad for a program like translate.py which is divided into several\nindependent steps, and for which most of the code generated in each step could\nbe safely be thrown away when it's completed.\nIf we switch to 64-bit we can address the whole 12 GB of RAM that we have, and\nthus translating with all working modules is no longer an issue.  This is the\ntime taken with CPython (note that it does not make sense to compare with the\n32-bit CPython translation above, because that one does not include all the\nmodules):\n\n$ python ./translate.py -Ojit\n\n[Timer] Timings:\n[Timer] annotate                       ---  782.7 s\n[Timer] rtype_lltype                   ---  445.2 s\n[Timer] pyjitpl_lltype                 ---  955.8 s\n[Timer] backendopt_lltype              ---  457.0 s\n[Timer] stackcheckinsertion_lltype     ---   63.0 s\n[Timer] database_c                     ---  505.0 s\n[Timer] source_c                       ---  939.4 s\n[Timer] compile_c                      ---  465.1 s\n[Timer] ===========================================\n[Timer] Total:                         --- 4613.2 s\n\nAnd this is for PyPy:\n\n$ pypy-c-78924-64 ./translate.py -Ojit\n\n[Timer] Timings:\n[Timer] annotate                       ---  505.8 s\n[Timer] rtype_lltype                   ---  279.4 s\n[Timer] pyjitpl_lltype                 ---  338.2 s\n[Timer] backendopt_lltype              ---  125.1 s\n[Timer] stackcheckinsertion_lltype     ---   21.7 s\n[Timer] database_c                     ---  187.9 s\n[Timer] source_c                       ---  298.8 s\n[Timer] compile_c                      ---  650.7 s\n[Timer] ===========================================\n[Timer] Total:                         --- 2407.6 s\n\nThe results are comparable with the 32-bit case: PyPy is still almost 2 times\nfaster than CPython.  And it also shows that our 64-bit JIT backend is as good\nas the 32-bit one.  Again, the drawback is in the consumed memory: CPython\nused 2.3 GB while PyPy took 8.3 GB.\nOverall, the results are impressive: we knew that PyPy can be good at\noptimizing small benchmarks and even middle-sized programs, but as far as we\nknow this is the first example in which it heavily optimizes a huge, real world\napplication.  And, believe us, the PyPy translation toolchain is complex\nenough to contains all kinds of dirty tricks and black magic that make Python\nlovable and hard to optimize :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/11/snake-which-bites-its-tail-pypy-jitting-5161284681004717142.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Report 2010",
      "text": "This years installment of the yearly PyPy D\u00fcsseldorf Sprint is drawing to a\nclose. As usual, we worked in the seminar room of the programming language\ngroup at the University of D\u00fcsseldorf. The sprint was different from previous\nones in that we had fewer people than usual and many actually live in\nD\u00fcsseldorf all the time.\nDavid spent the sprint working on the arm-backend branch, which is adding an\nARM backend to the JIT. With the help of Armin he added support for bridges in\nthe JIT and generally implemented missing operations, mostly for handling integers so far.\nRonny and Anto worked the whole week trying to come up with a scheme for\nimporting PyPy's SVN history into a mercurial repository without loosing too\nmuch information. This is a non-trivial task, because PyPy's history is gnarly.\nWe are nearly at revision 79000 and when we started using it, Subversion was at\nversion 0.1. All possible and impossible ways to mangle and mistreat a\nSubversion repository have been applied to PyPy's repo, so most of the\nimporting tools just give up. Ronny and Anto came up with a new plan and new\nhelper scripts every day, only to then discover another corner case that they\nhadn't thought of. Now they might actually have a final plan (but they said\nthat every day, so who knows?).The branch history of PyPy's repository (every box is a branch)Carl Friedrich and Lukas started working in earnest on memory benchmarks to\nunderstand the memory behaviour of Python code better. They have now\nimplemented a generic memory benchmark runner and a simple analysis that walks\nall objects and collects size information about them. They also added some\nbenchmarks that were proposed in the comments of the recent call for\nbenchmarks. As soon as some results from that work are there, we will post\nabout them.\nThere were also some minor tasks performed during the sprint. Armin implemented\nthe _bisect module and the dict.popitem method in RPython. Armin and\nCarl Friedrich made the new memory-saving mapdict implementation more suitable\nto use without the JIT (blog post should come about that too, at some point).\nThey also made classes with custom metaclasses a lot faster when the JIT is\nused.\nThe last three days of the sprint were spent working on H\u00e5kan's\njit-unroll-loops branch.  The branch is meant to move loop invariants out of\nthe loop, using techniques very similar to what is described in the recent post\non escape analysis across loop boundaries (see? it will soon stop being\nscience-fiction). Some of the ideas of this approach also come from LuaJIT\nwhich also uses very aggressive loop invariant code motion in its optimizers.\nMoving loop invariants outside of the loop is very useful, because many of the\nlookups that Python programs do in loops are loop invariants. An example is if\nyou call a function in a loop: The global lookup can often be done only once.\nThis branch fundamentally changes some of the core assumptions of the JIT, so\nit is a huge amount of work to make it fit with all the other parts and to\nadapt all tests. That work is now nearly done, some failing tests remain. The\nnext steps are to fix them and then do additional tests with the translated\nexecutable and look at the benchmarks.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/10/dusseldorf-sprint-report-2010-371223200425847723.html"
    },
    {
      "title": "The peace of green",
      "text": "No, we are not going to talk about the environment (i.e., the set of variables\nas printed by /usr/bin/env. What else? :-)).\nAfter months in which we had a couple of tests failing every day, we finally\nmanaged to turn (almost) everything green today, at least on Linux.  Enjoy\nthis screenshoot taken from the nightly build page:\n\n\n\n\nAs usual, the full buildbot results can be seen from the summary page.\ncheers,\nAnto",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/10/peace-of-green-4230271053903469504.html"
    },
    {
      "title": "PhD Thesis about PyPy's CLI JIT Backend",
      "text": "Hi all,\nfew months ago I finished the PhD studies and now my thesis is available,\njust in case someone does not have anything better to do than read it :-).\nThe title of the thesis is High performance implementation of Python for\nCLI/.NET with JIT compiler generation for dynamic languages, and its mainly\nbased on my work on the CLI backend for the PyPy JIT (note that the CLI JIT\nbackend is currently broken on trunk, but it's still working in the cli-jit\nbranch).\nThe thesis might be useful also for people that are not directly interested in\nthe CLI JIT backend, as it also contains general information about the inner\nworkings of PyPy which are independent from the backend: in particular,\nchapters 5 and 6 explain how the JIT frontend works.\n\nHere is the summary of chapters:\n\nIntroduction\nThe problem\nEnter PyPy\nCharacterization of the target platform\nTracing JITs in a nutshell\nThe PyPy JIT compiler generator\nThe CLI JIT backend\nBenchmarks\nConclusion and Future Work\n\n\n\ncheers,\nAnto",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/10/phd-thesis-about-pypys-cli-jit-backend-969267841095296323.html"
    },
    {
      "title": "Next PyPy sprint",
      "text": "Hi all,\n\nThe next PyPy sprint is scheduled for the end of the month, from the 25th to the 31st of October 2010.  It will be done at the university of D\u00fcsseldorf, Germany, where three of us are working.\n\nPlease see this link for more information.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/10/next-pypy-sprint-4850394963147107623.html"
    },
    {
      "title": "PyPy in Google's Summer of Code 2010",
      "text": "Hello.\nThis year we had a record of two and a half applications (one was on a cross\nsection of PyPy and numpy) accepted for the Google\nSoC program. Since it ended a couple of weeks ago, we wanted to present the results that\nwere achieved. All three projects were completed successfully, although the rate\nof success varied quite a bit.\nThe Numpy proposal progress significantly on making numpy compatible with\nPyPy's CPython's extension module support, but failed to bring PyPy's numpy\nimplementation into a usable shape (which is a somewhat ambitious goal, one\nmight argue). The experiments done during the projects are living on the\nmicronumpy branch.\nThe Fast ctypes proposal did some useful experiments on how to JIT external\ncalls from PyPy to C, however, the actual code as of now is not very\ninteresting and it's quite far from providing a full ctypes replacement (or\nequivalent).\nDefinitely the most successful proposal was a 64bit (x86_64) backend for PyPy's\nJIT. It not only includes working 64bit JIT (merged into PyPy trunk), but also\na working asmgcc for x86_64 linux platform, that makes it possible to run the JIT\non this architecture with our advanced garbage collectors. One can say that\nx64_64 is now no longer a second-class citizen for PyPy, although it definitely\ndidn't receive as much testing as the x86 platform. Expect this to be a major\nselling point for the next PyPy release :-)\nCheers,\nfijal & the PyPy team",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/09/pypy-in-googles-summer-of-code-2010-1267220161643618015.html"
    },
    {
      "title": "Using Escape Analysis Across Loop Boundaries for Specialization",
      "text": "This blog post is a successor to the one about escape analysis in PyPy's\nJIT. The examples from there will be continued here. This post is a bit\nscience-fictiony. The algorithm that PyPy currently uses is significantly more\ncomplex and much harder than the one that is described here. The resulting\nbehaviour is very similar, however, so we will use the simpler version (and we\nmight switch to that at some point in the actual implementation).\nIn the last blog post we described how escape analysis can be used to remove\nmany of the allocations of short-lived objects and many of the type dispatches\nthat are present in a non-optimized trace. In this post we will improve the\noptimization to also handle more cases.\nTo understand some more what the optimization described in the last blog post\ncan achieve, look at the following figure:\n\n\n\nThe figure shows a trace before optimization, together with the lifetime of\nvarious kinds of objects created in the trace. It is executed from top to\nbottom. At the bottom, a jump is used to execute the same loop another time.\nFor clarity, the figure shows two iterations of the loop.\nThe loop is executed until one of the guards in the trace fails, and the\nexecution is aborted.\nSome of the operations within this trace are new operations, which each create a\nnew instance of some class. These instances are used for a while, e.g. by\ncalling methods on them, reading and writing their fields. Some of these\ninstances escape, which means that they are stored in some globally accessible\nplace or are passed into a function.\nTogether with the new operations, the figure shows the lifetimes of the\ncreated objects. Objects in category 1 live for a while, and are then just not\nused any more. The creation of these objects is removed by the\noptimization described in the last blog post.\nObjects in category 2 live for a while and then escape. The optimization of the\nlast post deals with them too: the new that creates them and\nthe field accesses are deferred, until the point where the object escapes.\nThe objects in category 3 and 4 are in principle like the objects in category 1\nand 2. They are created, live for a while, but are then passed as an argument\nto the jump operation. In the next iteration they can either die (category\n3) or escape (category 4).\nThe optimization of the last post considered the passing of an object along a\njump to be equivalent to escaping. It was thus treating objects in category 3\nand 4 like those in category 2.\nThe improved optimization described in this post will make it possible to deal\nbetter with objects in category 3 and 4. This will have two consequences: on\nthe one hand, more allocations are removed from the trace (which is clearly\ngood). As a side-effect of this, the traces will also be type-specialized.\n\nOptimizing Across the Jump\nLet's look at the final trace obtained in the last post for the example loop.\nThe final trace was much better than the original one, because many allocations\nwere removed from it. However, it also still contained allocations:\n\n\n\nThe two new BoxedIntegers stored in p15 and p10 are passed into\nthe next iteration of the loop. The next iteration will check that they are\nindeed BoxedIntegers, read their intval fields and then not use them\nany more. Thus those instances are in category 3.\nIn its current state the loop\nallocates two BoxedIntegers at the end of every iteration, that then die\nvery quickly in the next iteration. In addition, the type checks at the start\nof the loop are superfluous, at least after the first iteration.\nThe reason why we cannot optimize the remaining allocations away is because\ntheir lifetime crosses the jump. To improve the situation, a little trick is\nneeded. The trace above represents a loop, i.e. the jump at the end jumps to\nthe beginning. Where in the loop the jump occurs is arbitrary, since the loop\ncan only be left via failing guards anyway. Therefore it does not change the\nsemantics of the loop to put the jump at another point into the trace and we\ncan move the jump operation just above the allocation of the objects that\nappear in the current jump. This needs some care, because the arguments to\njump are all currently live variables, thus they need to be adapted.\nIf we do that for our example trace above, the trace looks like this:\n\n\n\nNow the lifetime of the remaining allocations no longer crosses the jump, and\nwe can run our escape analysis a second time, to get the following trace:\n\n\n\nThis result is now really good. The code performs the same operations than\nthe original code, but using direct CPU arithmetic and no boxing, as opposed to\nthe original version which used dynamic dispatching and boxing.\nLooking at the final trace it is also completely clear that specialization has\nhappened. The trace corresponds to the situation in which the trace was\noriginally recorded, which happened to be a loop where BoxedIntegers were\nused. The now resulting loop does not refer to the BoxedInteger class at\nall any more, but it still has the same behaviour. If the original loop had\nused BoxedFloats, the final loop would use float_* operations\neverywhere instead (or even be very different, if the object model had\nuser-defined classes).\n\n\nEntering the Loop\nThe approach of placing the jump at some other point in the loop leads to\none additional complication that we glossed over so far. The beginning of the\noriginal loop corresponds to a point in the original program, namely the\nwhile loop in the function f from the last post.\nNow recall that in a VM that uses a tracing JIT, all programs start by being\ninterpreted. This means that when f is executed by the interpreter, it is\neasy to go from the interpreter to the first version of the compiled loop.\nAfter the jump is moved and the escape analysis optimization is applied a\nsecond time, this is no longer easily possible.  In particular, the new loop\nexpects two integers as input arguments, while the old one expected two\ninstances.\nTo make it possible to enter the loop directly from the intepreter, there\nneeds to be some additional code that enters the loop by taking as input\narguments what is available to the interpreter, i.e. two instances. This\nadditional code corresponds to one iteration of the loop, which is thus\npeeled off:\n\n\n\n\n\nSummary\nThe optimization described in this post can be used to optimize away\nallocations in category 3 and improve allocations in category 4, by deferring\nthem until they are no longer avoidable. A side-effect of these optimizations\nis also that the optimized loops are specialized for the types of the variables\nthat are used inside them.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/09/using-escape-analysis-across-loop-2887031293132023676.html"
    },
    {
      "title": "Escape Analysis in PyPy's JIT",
      "text": "The goal of a just-in-time compiler for a dynamic language is obviously to\nimprove the speed of the language over an implementation of the language that\nuses interpretation. The first goal of a JIT is thus to remove the\ninterpretation overhead, i.e. the overhead of bytecode (or AST) dispatch and the\noverhead of the interpreter's data structures, such as operand stack etc. The\nsecond important problem that any JIT for a dynamic language needs to solve is\nhow to deal with the overhead of boxing of primitive types and of type\ndispatching. Those are problems that are usually not present in statically typed\nlanguages.\nBoxing of primitive types means that dynamic languages need to be able to handle\nall objects, even integers, floats, etc. in the same way as user-defined\ninstances. Thus those primitive types are usually boxed, i.e. a small\nheap-structure is allocated for them, that contains the actual value.\nType dispatching is the process of finding the concrete implementation that is\napplicable to the objects at hand when doing a generic operation at hand. An\nexample would be the addition of two objects: The addition needs to check what\nthe concrete objects are that should be added are, and choose the implementation\nthat is fitting for them.\nLast year, we wrote a blog post  and a paper about how PyPy's meta-JIT\napproach works. These explain how the meta-tracing JIT can remove the overhead\nof bytecode dispatch. In this post (and probably a followup) we want to explain\nhow the traces that are produced by our meta-tracing JIT are then optimized to\nalso remove some of the overhead more closely associated to dynamic languages,\nsuch as boxing overhead and type dispatching. The most important technique to\nachieve this is a form of escape analysis that we call virtual objects.\nThis is best explained via an example.\n\nRunning Example\nFor the purpose of this blog post, we are going to use a very simple object\nmodel, that just supports an integer and a float type. The objects support only\ntwo operations, add, which adds two objects (promoting ints to floats in a\nmixed addition) and is_positive, which returns whether the number is greater\nthan zero. The implementation of add uses classical Smalltalk-like\ndouble-dispatching. These classes could be part of the implementation of a very\nsimple interpreter written in RPython.\nclass Base(object):\n    def add(self, other):\n        \"\"\" add self to other \"\"\"\n        raise NotImplementedError(\"abstract base\")\n    def add__int(self, intother):\n        \"\"\" add intother to self, where intother is a Python integer \"\"\"\n        raise NotImplementedError(\"abstract base\")\n    def add__float(self, floatother):\n        \"\"\" add floatother to self, where floatother is a Python float \"\"\"\n        raise NotImplementedError(\"abstract base\")\n    def is_positive(self):\n        \"\"\" returns whether self is positive \"\"\"\n        raise NotImplementedError(\"abstract base\")\n\nclass BoxedInteger(Base):\n    def __init__(self, intval):\n        self.intval = intval\n    def add(self, other):\n        return other.add__int(self.intval)\n    def add__int(self, intother):\n        return BoxedInteger(intother + self.intval)\n    def add__float(self, floatother):\n        return BoxedFloat(floatother + float(self.intval))\n    def is_positive(self):\n        return self.intval > 0\n\nclass BoxedFloat(Base):\n    def __init__(self, floatval):\n        self.floatval = floatval\n    def add(self, other):\n        return other.add__float(self.floatval)\n    def add__int(self, intother):\n        return BoxedFloat(float(intother) + self.floatval)\n    def add__float(self, floatother):\n        return BoxedFloat(floatother + self.floatval)\n    def is_positive(self):\n        return self.floatval > 0.0\n\nUsing these classes to implement arithmetic shows the basic problem that a\ndynamic language implementation has. All the numbers are instances of either\nBoxedInteger or BoxedFloat, thus they consume space on the heap. Performing many\narithmetic operations produces lots of garbage quickly, thus putting pressure on\nthe garbage collector. Using double dispatching to implement the numeric tower\nneeds two method calls per arithmetic operation, which is costly due to the\nmethod dispatch.\nTo understand the problems more directly, let us consider a simple function that\nuses the object model:\ndef f(y):\n    res = BoxedInteger(0)\n    while y.is_positive():\n        res = res.add(y).add(BoxedInteger(-100))\n        y = y.add(BoxedInteger(-1))\n    return res\n\nThe loop iterates y times, and computes something in the process. To\nunderstand the reason why executing this function is slow, here is the trace\nthat is produced by the tracing JIT when executing the function with y\nbeing a BoxedInteger:\n\n# arguments to the trace: p0, p1\n# inside f: res.add(y)\nguard_class(p1, BoxedInteger)\n    # inside BoxedInteger.add\n    i2 = getfield_gc(p1, intval)\n    guard_class(p0, BoxedInteger)\n        # inside BoxedInteger.add__int\n        i3 = getfield_gc(p0, intval)\n        i4 = int_add(i2, i3)\n        p5 = new(BoxedInteger)\n            # inside BoxedInteger.__init__\n            setfield_gc(p5, i4, intval)\n# inside f: BoxedInteger(-100)\np6 = new(BoxedInteger)\n    # inside BoxedInteger.__init__\n    setfield_gc(p6, -100, intval)\n\n# inside f: .add(BoxedInteger(-100))\nguard_class(p5, BoxedInteger)\n    # inside BoxedInteger.add\n    i7 = getfield_gc(p5, intval)\n    guard_class(p6, BoxedInteger)\n        # inside BoxedInteger.add__int\n        i8 = getfield_gc(p6, intval)\n        i9 = int_add(i7, i8)\n        p10 = new(BoxedInteger)\n            # inside BoxedInteger.__init__\n            setfield_gc(p10, i9, intval)\n\n# inside f: BoxedInteger(-1)\np11 = new(BoxedInteger)\n    # inside BoxedInteger.__init__\n    setfield_gc(p11, -1, intval)\n\n# inside f: y.add(BoxedInteger(-1))\nguard_class(p0, BoxedInteger)\n    # inside BoxedInteger.add\n    i12 = getfield_gc(p0, intval)\n    guard_class(p11, BoxedInteger)\n        # inside BoxedInteger.add__int\n        i13 = getfield_gc(p11, intval)\n        i14 = int_add(i12, i13)\n        p15 = new(BoxedInteger)\n            # inside BoxedInteger.__init__\n            setfield_gc(p15, i14, intval)\n\n# inside f: y.is_positive()\nguard_class(p15, BoxedInteger)\n    # inside BoxedInteger.is_positive\n    i16 = getfield_gc(p15, intval)\n    i17 = int_gt(i16, 0)\n# inside f\nguard_true(i17)\njump(p15, p10)\n\n(indentation corresponds to the stack level of the traced functions).\nThe trace is inefficient for a couple of reasons. One problem is that it checks\nrepeatedly and redundantly for the class of the objects around, using a\nguard_class instruction. In addition, some new BoxedInteger instances are\nconstructed using the new operation, only to be used once and then forgotten\na bit later. In the next section, we will see how this can be improved upon,\nusing escape analysis.\n\n\nVirtual Objects\nThe main insight to improve the code shown in the last section is that some of\nthe objects created in the trace using a new operation don't survive very\nlong and are collected by the garbage collector soon after their allocation.\nMoreover, they are used only inside the loop, thus we can easily prove that\nnobody else in the program stores a reference to them. The\nidea for improving the code is thus to analyze which objects never escape the\nloop and may thus not be allocated at all.\nThis process is called escape analysis. The escape analysis of\nour tracing JIT works by using virtual objects: The trace is walked from\nbeginning to end and whenever a new operation is seen, the operation is\nremoved and a virtual object is constructed. The virtual object summarizes the\nshape of the object that is allocated at this position in the original trace,\nand is used by the escape analysis to improve the trace. The shape describes\nwhere the values that would be stored in the fields of the allocated objects\ncome from. Whenever the optimizer sees a setfield that writes into a virtual\nobject, that shape summary is thus updated and the operation can be removed.\nWhen the optimizer encounters a getfield from a virtual, the result is read\nfrom the virtual object, and the operation is also removed.\nIn the example from last section, the following operations would produce two\nvirtual objects, and be completely removed from the optimized trace:\n\np5 = new(BoxedInteger)\nsetfield_gc(p5, i4, intval)\np6 = new(BoxedInteger)\nsetfield_gc(p6, -100, intval)\n\nThe virtual object stored in p5 would know that it is an BoxedInteger, and that\nthe intval field contains i4, the one stored in p6 would know that\nits intval field contains the constant -100.\nThe following operations, that use p5 and p6 could then be\noptimized using that knowledge:\n\nguard_class(p5, BoxedInteger)\ni7 = getfield_gc(p5, intval)\n# inside BoxedInteger.add\nguard_class(p6, BoxedInteger)\n# inside BoxedInteger.add__int\ni8 = getfield_gc(p6, intval)\ni9 = int_add(i7, i8)\n\nThe guard_class operations can be removed, because the classes of p5 and\np6 are known to be BoxedInteger. The getfield_gc operations can be removed\nand i7 and i8 are just replaced by i4 and -100. Thus the only\nremaining operation in the optimized trace would be:\n\ni9 = int_add(i4, -100)\n\nThe rest of the trace is optimized similarly.\nSo far we have only described what happens when virtual objects are used in\noperations that read and write their fields. When the virtual object is used in\nany other operation, it cannot stay virtual. For example, when a virtual object\nis stored in a globally accessible place, the object needs to actually be\nallocated, as it will live longer than one iteration of the loop.\nThis is what happens at the end of the trace above, when the jump operation\nis hit. The arguments of the jump are at this point virtual objects. Before the\njump is emitted, they are forced. This means that the optimizers produces code\nthat allocates a new object of the right type and sets its fields to the field\nvalues that the virtual object has. This means that instead of the jump, the\nfollowing operations are emitted:\n\np15 = new(BoxedInteger)\nsetfield_gc(p15, i14, intval)\np10 = new(BoxedInteger)\nsetfield_gc(p10, i9, intval)\njump(p15, p10)\n\nNote how the operations for creating these two instances has been moved down the\ntrace. It looks like for these operations we actually didn't win much, because\nthe objects are still allocated at the end. However, the optimization was still\nworthwhile even in this case, because some operations that have been performed\non the forced virtual objects have been removed (some getfield_gc operations\nand guard_class operations).\nThe final optimized trace of the example looks like this:\n\n# arguments to the trace: p0, p1\nguard_class(p1, BoxedInteger)\ni2 = getfield_gc(p1, intval)\nguard_class(p0, BoxedInteger)\ni3 = getfield_gc(p0, intval)\ni4 = int_add(i2, i3)\ni9 = int_add(i4, -100)\n\nguard_class(p0, BoxedInteger)\ni12 = getfield_gc(p0, intval)\ni14 = int_add(i12, -1)\n\ni17 = int_gt(i14, 0)\nguard_true(i17)\np15 = new(BoxedInteger)\nsetfield_gc(p15, i14, intval)\np10 = new(BoxedInteger)\nsetfield_gc(p10, i9, intval)\njump(p15, p10)\n\nThe optimized trace contains only two allocations, instead of the original five,\nand only three guard_class operations, from the original seven.\n\n\nSummary\nIn this blog post we described how simple escape analysis within the scope of\none loop works. This optimizations reduces the allocation of many intermediate\ndata structures that become garbage quickly in an interpreter. It also removes a\nlot of the type dispatching overhead. In a later post, we will explain how this\noptimization can be improved further.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/09/escape-analysis-in-pypys-jit-1780048403046080197.html"
    },
    {
      "title": "EuroPython 2010 Videos available",
      "text": "Hi all,\nthe videos of the talks from EuroPython 2010 are now available on\nblip.tv: in particular, there are the three videos of the PyPy talk.\nPart 1: What's news in PyPy 1.2 and 1.3 (by Antonio Cuni)\nPart 2: Just in Time compilation (by Armin Rigo)\nPart 3: cpyext (by Amaury Forgeot d'Arc)\nMoreover, here is Mark Shannon's talk which compares HotPy, Unladen Swallow\nand PyPy:",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/08/europython-2010-videos-available-8446190660370796142.html"
    },
    {
      "title": "Call for Benchmarks",
      "text": "As you know, a lot of PyPy's recent development effort has gone into speeding up\nexecution of Python programs. However, an additional good property of PyPy's\nPython interpreter is that most objects are represented in a much more compact\nway than in CPython. We would like to investigate some more advanced techniques\nto reduce the memory usage of Python programs further.\nTo do this it is necessary to investigate the memory behaviour of real programs\nwith large heaps. For speed measurements there are standard benchmarks, but for\nmemory improvements there is nothing comparable, the memory behaviour of large\nprograms is not that well understood. Therefore we are looking for programs that we\ncan study and use as benchmarks.\nSpecifically we are looking for Python programs with the following properties:\n\nlarge heaps of about 10MB-1GB\nshould have non-trivial runtime as well (in the range of a few seconds), to\njudge the speed impact of optimizations\nideally pure-Python programs that don't use extension modules so that they run\nunder both CPython and PyPy (this is optional, but makes my life much easier).\n\nWe are also rather interested in programs that do a lot of string/unicode\nprocessing.\nWe would be grateful for all ideas. Telling us about a program also has the\nadvantage that we will work on optimizing PyPy for it :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/08/call-for-benchmarks-2605012131351543912.html"
    },
    {
      "title": "PyOhio",
      "text": "This weekend I delivered a talk at PyOhio (an annual conference in Columbus, OH, USA) on PyPy and Unladen Swallow.  The talk covered reasons that Python, the language, is hard to optimize, why CPython is slow, and a few optimizations that PyPy and Unladen Swallow have implemented.  The slides from my talk are online, and the talk was recorded so a video will follow.  I gave a similar talk to ChiPy (the Chicago Python user group), which was also recorded and the video is available.  Both audiences were excited about the futures for PyPy and Unladen Swallow, and for the future of a faster Python.\nAlex",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/08/pyohio-2568618480482575546.html"
    },
    {
      "title": "Using virtualenv with PyPy",
      "text": "Thanks to the work that was recently done on the sys-prefix branch, it is\nnow possible to use virtualenv with PyPy.\nTo try it, you need:\n\n\na recent version of PyPy: PyPy 1.3 does not contain the necessary logic to\nwork with virtualenv, so you need a more recent PyPy from subversion\ntrunk. You can either build it by yourself or download one of our\nprecompiled nightly builds\na copy of virtualenv-pypy: this is a fork of virtualenv that contains\nall the patches needed to work with PyPy, and hopefully will be merged\nback at some point.  It should be totally compatible with the official\nversion of virtualenv, so it is safe to use it even to create non-PyPy\nenvironments.  If you notice some weird behavior that does not happen with\nthe standard virtualenv, please let us know.\n\n\nThe directory layout has been redesigned in a way that it is possible to use\nvirtualenv to install a PyPy both from a precompiled tarball or from an svn\ncheckout:\n\n# from a tarball\n$ virtualenv -p /opt/pypy-c-jit-76426-linux/bin/pypy my-pypy-env\n\n# from the svn checkout\n$ virtualenv -p /path/to/pypy-trunk/pypy/translator/goal/pypy-c my-pypy-env\n\nOnce the environment has been created, you can enter it as usual. Note that\nbin/python is now a symlink to bin/pypy.\nEnjoy it :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/08/using-virtualenv-with-pypy-7238942727709530503.html"
    },
    {
      "title": "A Play on Regular Expression",
      "text": "The paper where the algorithms we described in the recent blog posts come from is now available. It  is written as a play in three Acts with a cast of three and is very readable and funny. The Haskell code is at Sebastian Fischer's github pages.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/07/play-on-regular-expression-9014941705636345998.html"
    },
    {
      "title": "EuroPython 2010 report",
      "text": "So, EuroPython 2010 is over, I am flying home and it's time to write a report\nabout the conference from the PyPy point of view.\nAs usual, the conference was very interesting and went very well. The quality\nof the talks I attended to was high on average and most importantly I could\nmeet a lot of interesting people to discuss various things.\nOn the first day, Armin, Amaury and I presented the usual PyPy status talk\n(here are the slides):\nthe talk is an extended version of the one that I and Armin presented at\nPycon Italia in May and is divided in three parts: first I talked about the\ncurrent status of the project, what is the content of the recent 1.2 and 1.3\nreleases and showed a demo of a simple Django application that renders a\nMandelbrot fractal and is measurably faster on PyPy than on CPython.  In the\nsecond part of the talk, Armin gave an introduction about the ideas that stand\nbehind the JIT.  Finally, in the third part Amaury explained how the new\ncpyext module lets PyPy to compile and load existing CPython extensions\nwritten in C.\nI think that the talk was well received: the only drawback is that there was\nno time to answer questions at the end of the presentation.  However, we\nreceived a lot of \"offline\" questions after the talk finished and thorough the\nwhole conference: it is always great to see that people are interested in our\nwork, and I'd like to thank everybody for the feedback that they gave to us.\nPyPy was also mentioned in the interesting Mark Shannon's talk, where he\ncompared the optimization techniques used by PyPy, Unladen Swallow and\nHotPy, which is Mark's own PhD project.  Moreover, Henrik Vendelbo\ngave a talk about how to tweak PyPy to produce a standalone\nexecutable which embeds a whole python application to make deployment easier,\nwhile Andrew Francis explained his implementation of the Go select\nstatement based on the stackless.py module implemented in PyPy.  Personally,\nI am glad to see that people start to think of PyPy as a useful starting\npoint to experiment with new features and use cases that we did not think\nabout: after all, one of PyPy explicit goals is to be \"flexible and easy to\nexperiment with\".\nAfter the conference there were the usual post EuroPython sprints: this\nyear we had not planned a PyPy sprint, but some people showed interest\nin it and since Armin and I happened to be still around the day after the\nconference, we decided to do a mini 1-day sprint, with 6 or 7 people\npresent. Since there were only two core developers it was impossible to use\nour usual pairing scheme, in which every newcomer pairs with someone who is\nexperienced with the source code to gain knowledge of it.  However, I think it\nwas still a successful day of work, and we managed to fix a couple of bugs\nthat was standing in our issue tracker.  Again, I'd like to thank all the\npeople that came and worked with us during the sprint.\nIn conclusion I really enjoyed the EuroPython 2010 experience: the fact that I\nmanaged to find a place in Birmingham where to eat a good Italian-style \"gelato\"\nhelped a lot :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/07/europython-2010-report-7803731360759120212.html"
    },
    {
      "title": "CERN Sprint Report \u2013 Wrapping C++ Libraries",
      "text": "The last five days we have been sprinting in a meeting room in the Computing\nCenter at CERN in Gen\u00e8ve, Switzerland. Present are Armin Rigo, Antonio Cuni,\nCarl Friedrich Bolz and Wim Lavrijsen (LBL). The goal of the sprint was to use\nsome of the C++ technology developed at CERN to make it possible to use C++\nlibraries from PyPy's Python interpreter. For this we used the Reflex\nlibrary, which provides reflection information for C++ classes. We discussed\nusing Reflex in PyPy during the D\u00fcsseldorf sprint of 2008, please read\nthat blog post if you want some more details on how Reflex works. There is\nsupport for this sort of C++/Python integration also for CPython, using the\nPyROOT module.\nThe sprint was very successful. On Monday we had a few discussion about how\nReflex could best be integrated with PyPy. One of the goals of the sprint was to\nmake the approach JIT-friendly from the start, so that calls to C++ libraries\ncan be reasonably fast. After the discussion we started coding on the\nreflex-support branch. This branch adds a new cppyy builtin module to\nPyPy's Python interpreter (why we chose that name is left as an exercise to the\nreader). This module can be used to load C++ classes, construct instances and\ncall static and instance methods on them.\nThe work has just started, as of now, the argument and return types of the\nmethods are restricted to some simple C types, such as int, double and\nchar* and pointers to class instances. Most of the work necessary to\nproperly resolve overloaded methods is done, but default arguments are not.\nAs an example, suppose there is a C++ class like this:\nclass example01 {\nprivate:\n    static int count;\n    int somedata;\npublic:\n\n    example01(int a) : somedata(a) {\n        count++;\n    }\n    ~example01() {\n        count--;\n    }\n    static int getCount() {\n        return count;\n    }\n\n    int addDataToInt(int a) {\n        return somedata + a;\n    }\n};\nint example01::count = 0;\n\nYou can now use it from PyPy's Python interpreter in the following way, after\nyou have used Reflex to generate reflection information for the class:\nimport cppyy\ncppyy.load_lib(\"example01Dict.so\") # contains the Reflex information\nexample01_class = cppyy.gbl.example01\ninstance = example01_class(7)\nassert example01_class.getCount() == 1\nres = instance.addDataToInt(4)\nassert res == 11\nres = instance.addDataToInt(-4)\nassert res == 3\ninstance.destruct() # so far explicit destruction needed\nassert example01_class.getCount() == 0\n\nWe also did some very early JIT work and some early performance measurements.\nThe rough figures are that cppyy is two times faster at calling a simple C++\nmethod from Python than PyROOT. To get a feeling for how fast things could\ngo in the end, we also implemented a proof-of-concept for some more advanced JIT\ntechnology (which requires a patch for Reflex and uses a GCC extension). With\nthis, the speedup over PyROOT is a factor of 20. Of course, this is still a\nlot slower than a C++ to C++ method call (probably by at least an order of\nmagnitude).\nThe sprint was very productive because we managed to get the right people into\nthe same room working together. Wim has a lot of experience with C++ and Reflex,\nand is the author of PyROOT, and of course the others know a lot about PyPy\n(at the end of the sprint, Anto was very glad that he stopped using C++ a long\ntime ago). Also, working at CERN was very cool. The atmosphere is amazing, and\nwe got to visit the ATLAS control room. Extremely advanced technology, and\nalso research on a completely different scale than what we are used to.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/07/cern-sprint-report-wrapping-c-libraries-6547377950791793143.html"
    },
    {
      "title": "Comparing SPUR to PyPy",
      "text": "Recently, I've become aware of the SPUR project of Microsoft Research and\nread some of their papers (the tech report \"SPUR: A Trace-Based JIT Compiler\nfor CIL\" is very cool). I found the project to be very interesting and since\ntheir approach is in many ways related to what PyPy is doing, I now want to\ncompare and contrast the two projects.\n\nA Tracing JIT for .NET\nSPUR consist of two parts: On the one hand it is a VM for CIL, the\nbytecode of the .NET VM. This VM uses a tracing JIT compiler to compile the\nprograms it is running to machine code. As opposed to most existing VMs that\nhave a tracing JIT it does not use an interpreter at all. Instead it\ncontains various variants of a JIT compiler that produce different versions of\neach method. Those are:\n\na profiling JIT which produces code that does lightweight profiling when\nrunning the compiled method\na tracing JIT which produces code that produces a trace when running the\ncompiled method\na transfer-tail JIT which is used to produce code which is run to get from a\nfailing guard back to the normal profiling version of a method\nan optimizing JIT that actually optimizes traces and turns them into machine code\n\n\nOptimizations Done by the Optimizing JIT\nSPUR's optimizing JIT does a number of powerful optimizations on the traces before it\nturns them into machine code. Among them are usual compiler optimizations such\nas register allocation, common subexpression elimination, loop invariant code\nmotion, etc.\nIt also performs some optimizations that are specific to the tracing context and\nare thus not commonly found in \"normal\" compilers:\n\nguard implication: if a guard is implied by an earlier guard, it is removed\nguard strengthening: if there is a sequence of guards that become stronger\nand stronger (i.e. each guard implies the previous one), the first guard in\nthe sequence is replaced by the last one, and all others are removed. This can\ngreatly reduce the number of guards and is generally safe. It can shift a\nguard failure to an earlier point in the trace, but the failure would have\noccurred at some point in the trace anyway.\nload/store optimizations: this is an optimization for memory reads/writes.\nIf several loads from the same memory location occur without writes in\nbetween, all but the first one are removed. Similarly, if a write to a memory\nlocation is performed, this write is delayed as much as possible. If there is\na write to the same location soon afterwards, the first write can be removed.\nescape analysis: for allocations that occur in a loop, the optimizer checks\nwhether the resulting object escapes the loop. If not, the allocation is moved\nbefore the loop, so that only one object needs to be allocated, instead of one\nevery loop iteration.\nuser-controlled loop unrolling: not exactly an optimization, but an\ninteresting feature anyway. It is possible to annotate a CIL method with a\nspecial decorator [TraceUnfold] and then the tracing JIT will fully unroll\nthe loops it contains. This can be useful for loops than are known to run a\nsmall and fixed number of iterations for each call-site.\nuser controlled tracing: The user can also control tracing up to a point.\nMethods can be annotated with [NativeCall] to tell the tracer to never\ntrace their execution. Instead they appear as a direct call in the trace.\n\n\n\n\nA JavaScript Implementation\nIn addition to the tracing JIT I just described, SPUR also contains a JavaScript\nimplementation for .NET. The approach of this implementation is to translate\nJavaScript to CIL bytecode, doing some amount of type inference to detect\nvariables that have fixed types. All operations where no precise type could be\ndetermined are implemented with calls to a JavaScript runtime system, which does\nthe necessary type dispatching. The JavaScript runtime is implemented in C#.\nThe JavaScript implementation and the CLI VM with a tracing JIT sound quite\nunrelated at first, but together they amplify each other. The tracing JIT traces\nthe JavaScript functions that have been translated to CLI bytecode. Since the\nJavaScript runtime is in C#, it exists as CLI bytecode too. Thus it can be\ninlined into the JavaScript functions by the tracer. This is highly beneficial,\nsince it exposes the runtime type dispatching of the JavaScript operations to\nthe optimizations of the tracing JIT. Particularly the common expression\nelimination helps the JavaScript code. If a series of operations is performed on\nthe same object, the operations will all do the same type checks. All but the\ntype checks of the first operation can be removed by the optimizer.\n\nPerformance Results\nThe speed results of the combined JavaScript implementation and tracing JIT are\nquite impressive. It beats TraceMonkey for most benchmarks in SunSpider (apart\nfrom some string-heavy benchmarks that are quite slow) and can compete with V8\nin many of them. However, all this is steady-state performance and it seems\nSPUR's compile time is rather bad currently.\n\n\nFurther Possibilities\nA further (so far still hypothetical) advantage of SPUR is that the approach can\noptimize cases where execution crosses the border of two different systems. If\nsomebody wrote an HTML layout engine and a DOM in C# to get a web browser and\nintegrated it with the JavaScript implementation described above, the tracing\nJIT could optimize DOM manipulations performed by JavaScript code as well as\ncallbacks from the browser into JavaScript code.\nOf course the approach SPUR takes to implement JavaScript is completely\ngeneralizable. It should be possible to implement other dynamic languages in the\nsame way as JavaScript using SPUR. One would have to write a runtime system for\nthe language in C#, as well as a compiler from the language into CIL bytecode.\nGiven these two elements, SPUR's tracing JIT compiler would probably do a\nreasonable job at optimizing this other language (of course in practise, the\nlanguage implementation would need some tweaking and annotations to make it\nreally fast).\n\n\n\nComparison With PyPy\nThe goals of PyPy and SPUR are very similar. Both projects want to implement\ndynamic languages in an efficient way by using a tracing JIT. Both apply the\ntracing JIT \"one level down\", i.e. the runtime system of the dynamic language is\nvisible to the tracing JIT. This is the crucial point of the approach of both\nprojects. Since the runtime system of the dynamic language is visible to the\ntracing JIT, the JIT can optimize programs in that dynamic language. It does not\nitself need to know about the semantics of the dynamic language. This makes the\ntracing JIT usable for a variety of dynamic languages. It also means that the\ntwo halves can be implemented and debugged independently.\nIn SPUR, C# (or another language that is compilable to CIL) plays the role of\nRPython, and CIL is equivalent to the intermediate format that PyPy's\ntranslation toolchain uses. Both formats operate on a similar abstraction level,\nthey are quite close to C, but still have support for the object system of their\nrespective language and are garbage-collected.\nSPUR supports only a JavaScript implementation so far, which could maybe change in\nthe future. Thus JavaScript in SPUR corresponds to Python in PyPy, which was the\nfirst dynamic language implemented in PyPy (and is also the reason for PyPy's\nexistence).\nThere are obviously also differences between the two projects, although many of\nthem are only skin-deep. The largest difference is the reliance of SPUR on\ncompilers on all levels. PyPy takes the opposite approach of using interpreters\nalmost everywhere. The parts of PyPy that correspond to SPUR's compilers are (I\nwill use the Python implementation of PyPy as an example):\n\nthe JavaScript-to-CIL compiler corresponds to the Python interpreter of PyPy\nthe profiling JIT corresponds to a part of PyPy's translation toolchain\nwhich adds some profiling support in the process of turning RPython code into\nC code,\nthe tracing JIT corresponds to a special interpreter in the PyPy JIT which\nexecutes an RPython program and produces a trace of the execution\nthe transfer-tail JIT corresponds to PyPy's blackhole interpreter, also\ncalled fallback interpreter\nthe optimizing JIT corresponds to the optimizers and backends of PyPy's JIT\n\n\nPyPy's Optimizations\nComparing the optimizations that the two projects perform, the biggest\ndifference is that PyPy does \"trace stitching\" instead of fully supporting trace\ntrees. The difference between the two concerns what happens when a new trace\ngets added to an existing loop. The new trace starts from a guard in the\nexisting loop that was observed to fail often. Trace stitching means that the\nloop is just patched with a jump to the new trace. SPUR instead recompiles the\nwhole trace tree, which gives the optimizers more opportunities, but also makes\ncompilation a lot slower. Another difference is that PyPy does not perform\nloop-invariant code motion yet.\nMany of the remaining optimizations are very similar. PyPy supports guard\nimplication as well as guard strengthening. It has some load/store\noptimizations, but PyPy's alias analysis is quite rudimentary. On the other\nhand, PyPy's escape analysis is very powerful. PyPy also has support for the\nannotations that SPUR supports, using some decorators in the pypy.rlib.jit\nmodule. User-controlled loop unrolling is performed using the unroll_safe\ndecorator, tracing of a function can be disabled with the dont_look_inside\ndecorator.\nPyPy has a few more annotations that were not mentioned in the SPUR tech report.\nMost importantly, it is possible to declare a function as pure, using the\npurefunction decorator. PyPy's optimizers will remove calls to a function\ndecorated that way if the arguments to the call are all constant. In addition it\nis possible to declare instances of classes to be immutable, which means that\nfield accesses on constant instances can be folded away. Furthermore there is\nthe promote hint, which is spelled x = hint(x, promote=True). This will\nproduce a guard in the trace, to turn x into a constant after the guard.\n\n\n\nSummary\nGiven the similarity between the projects' goals, it is perhaps not so\nsurprising to see that PyPy and SPUR have co-evolved and reached many similar\ndesign decisions. It is still very good to see another project that does many\nthings in the same way as PyPy.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/07/comparing-spur-to-pypy-8835011873209414462.html"
    },
    {
      "title": "\"Blackhole\" interpreter",
      "text": "Hi all,\n\nHere are a few words about the JIT's \"great speedup in compiling\ntime\" advertized on the PyPy 1.3 release (see the\n\nprevious blog post).\nThe exact meaning behind these words needs a fair bit of\nexplanation, so here it is in case you are interested.\n\nIf you download a version of PyPy 1.3 that includes a JIT\ncompiler, you get an executable that could be qualified as rather\nfat: it actually contains three interpreters.  You have on the\none hand the regular Python interpreter.  It is here because it's\nnot possible to JIT-compile every single piece of Python code you\ntry to run; only the most executed loops are JIT-compiled.  They\nare JIT-compiled with a tracing interpreter that operates one\nlevel down.  This is the second interpreter.  This tracing step\nis quite slow, but it's all right because it's only invoked on\nthe most executed loops (on the order of 100 to 1000 times in\ntotal in a run of a Python script that takes anyway seconds or\nminutes to run).\n\nSo apart from the JIT compilation itself, we have two worlds in\nwhich the execution proceeds: either by regular interpretation,\nor by the execution of assembler code generated by the JIT\ncompiler.  And of course, we need to be able to switch from one\nworld to the other quickly: during regular interpretation we have\nto detect if we already have generated assembler for this piece\nof code and if so, jump to it; and during execution of the\nassembler, when a \"guard\" fails, i.e. when we meet a path of\nexecution for which we did not produce assembler, then we need to\nswitch back to regular interpretation (or occasionally invoke the\nJIT compiler again).\n\nLet us consider the cost of switching from one world to another.\nDuring regular interpretation, if we detect that we already have\nassembler corresponding to this Python loop, then we just jump to\nit instead of interpreting the Python loop.  This is fairly\ncheap, as it involves just one fast extra check per Python loop.\nThe reverse is harder because \"guard\" failures can occur at any\npoint in time: it is possible that the bit of assembler that we\nalready executed so far corresponds to running the first 4 Python\nopcodes of the loop and a half.  The guard that failed just now\nis somewhere in the middle of interpreting that opcode -- say,\nmultiplying these two Python objects.\n\nIt's almost impossible to just \"jump\" at the right place in the\ncode of the regular interpreter -- how do you jump inside a\nregular function compiled in C, itself in a call chain, resuming\nexecution of the function from somewhere in the middle?\n\nSo here is the important new bit in PyPy 1.3.  Previously, what\nwe would do is invoke the JIT compiler again in order to follow\nwhat needs to happen between the guard failure and the real end\nof the Python opcode.  We would then throw away the trace\ngenerated, as the only purpose was to finish running the current\nopcode.  We call this \"blackhole interpretation\".  After the end\nof the Python opcode, we can jump to the regular interpreter\neasily.\n\nDoing so was straightforward, but slow, in case it needs to be\ndone very often (as in the case in some examples, but not all).\nIn PyPy 1.3, this blackhole interpretation step has been\nredesigned as a time-critical component, and that's where the\nthird interpreter comes from.  It is an interpreter that works\nlike the JIT compiler, but without the overhead of tracing (e.g.\nit does not need to box all values).  It was designed from the\nground up for the sole purpose of finishing the execution of the\ncurrent Python opcode.  The bytecode format that it interprets is\nalso new, designed for that purpose, and the JIT compiler itself\n(the second interpreter) was adapted to it.\nThe old bytecode format in PyPy 1.2 is gone\n(it was more suited for the JIT compiler, but less for blackhole\ninterpretation).\n\nIn summary, it was a lot of changes in the most front-end-ish\nparts of the JIT compiler, even though it was mostly hidden\nchanges.  I hope that this longish blog post helped bring it a\nbit more to the light :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/06/blackhole-interpreter-2752965445510091289.html"
    },
    {
      "title": "PyPy 1.3 released",
      "text": "Hello.\nWe're please to announce the release of PyPy 1.3. This release has two major\nimprovements. First of all, we stabilized the JIT compiler since 1.2 release,\nanswered user issues, fixed bugs, and generally improved speed.\nWe're also pleased to announce alpha support for loading CPython extension\nmodules written in C. While the main purpose of this release is increased\nstability, this feature is in alpha stage and it is not yet suited for\nproduction environments.\n\nHighlights of this release\n\nWe introduced support for CPython extension modules written in C. As of now,\nthis support is in alpha, and it's very unlikely unaltered C extensions will\nwork out of the box, due to missing functions or refcounting details. The\nsupport is disabled by default, so you have to do:\n\nimport cpyext\n\nbefore trying to import any .so file. Also, libraries are source-compatible\nand not binary-compatible. That means you need to recompile binaries, using\nfor example:\n\npypy setup.py build\n\nDetails may vary, depending on your build system. Make sure you include\nthe above line at the beginning of setup.py or put it in your PYTHONSTARTUP.\nThis is alpha feature. It'll likely segfault. You have been warned!\n\nJIT bugfixes. A lot of bugs reported for the JIT have been fixed, and its\nstability greatly improved since 1.2 release.\n\nVarious small improvements have been added to the JIT code, as well as a great\nspeedup of compiling time.\n\n\n\n\nCheers,\nMaciej Fijalkowski, Armin Rigo, Alex Gaynor, Amaury Forgeot d'Arc and the PyPy team\n\n\nUpdate:The correct command to build extension is \"pypy setup.py build\", not \"python setup.py build\" as it was stated before.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2010/06/pypy-13-released-8546085566902489304.html"
    },
    {
      "title": "A JIT for Regular Expression Matching",
      "text": "This is part 2 of a series, see Part 1 for an introduction. In this post\nI want to describe how the JIT generator of the PyPy project can be used to turn\nthe elegant but not particularly fast regular expression matcher from the first\npart into a rather fast implementation. In addition, I will show some speed\nmeasurements against various regular expression implementations.\nAgain, note the disclaimer: This technology could not easily be used\nto implement Python's re-module.\n\nExample Expression and First Numbers\nThe regular expression I will use as an example in the rest of this paper is\nthe expression (a|b)*a(a|b){20}a(a|b)*. It matches all strings that have two\na with exactly 20 characters between them. This regular expression has\nthe property that the corresponding DFA needs 2**(n+1) different states. As\nan input string, we use a random string (of varying lengths) that does not\nmatch the regular expression. I will give all results as number of chars matched\nper second. While this is not a particularly typical regular expression, it\nshould still be possible to get some ballpark numbers for the speeds of various\nimplementations \u2013 as we will see, the differences between implementations are\nhuge anyway.\nAll the benchmarks were performed on my laptop, which has an Intel Core2 Duo\nP8400 processor with 2.26 GHz and 3072 KB of cache on a machine with 3GB RAM\nrunning Ubuntu Linux 10.04.\nTo get a feeling for the orders of magnitude involved, the CPython re module\n(which is implemented in C and quite optimized) can match 2'500'000 chars/s.\nGoogle's new re2 implementation still matches 550'000 chars/s. Google's\nimplementation is slower, but their algorithm gives complexity and space\nguarantees similar to our implementation in the last blog post.\nOn the other end of the performance scale is the pure-Python code from the last\nblog post running on CPython. It can match only 12'200 chars/s and is thus 200\ntimes slower than the re module.\n\n\nTranslating the Matcher\nThe code described in the last blog post is not only normal Python code, but\nalso perfectly valid RPython code. Nothing particularly dynamic is going on in\nthe code, thus it can be translated with PyPy's translation toolchain to C code.\nThe resulting binary is considerably faster and can match 720'000 chars/s, 60\ntimes faster than the untranslated version.\nAnother approach is to write equivalent versions of the algorithms in lower\nlevel languages. This has been done for C++ by Sebastian Fischer and for Java by\nBaltasar Tranc\u00f3n y Widemann. The algorithm is object-oriented enough to be\nmapped very closely to the respective languages. The C++ version is\na little bit faster than the RPython version translated to C, at 750'000 chars/s. That's\nnot very surprising, given their similarity. The Java version is more than twice\nas fast, with 1'920'000 chars/s. Apparently the Java JIT compiler is a lot\nbetter at optimizing the method calls in the algorithm or does some other\noptimizations. One reason for this could be that the Java JIT can assume that\nthe classes it sees are all there are (and it will invalidate the generated\nmachine code if more classes are loaded), whereas the C++ compiler needs to\ngenerate code that works even in the presence of more regular expression\nclasses.\n\n\nGenerating a JIT\nTo get even more performance out of the RPython code, it is possible to generate\na JIT for it with the help of the PyPy translation toolchain. To do this, the\nmatching code needs to be extended somewhat by some hints that tell PyPy's JIT\ngenerator how this is to be done. The JIT generator can automatically produce a\nJIT compiler from an RPython interpreter of the source language. In our case,\nwe view the regular expression matcher as an interpreter for regular\nexpressions. Then the match function corresponds to the\ndispatch loop of a traditional interpreter.\nOur regular expression matcher is a very peculiar interpreter. The matcher\nworks by running exactly one loop (the one in match) as many times as the\ninput string is long, irrespective of the \"program\", i.e. the particular\nregular expressions. In addition, within the loop there are no conditions (e.g.\nif statements) at all, it is just linear code. This makes it almost perfectly\nsuited\nto the JIT generator, which produces a tracing JIT. A tracing JIT compiles the\nhot loops of a program (i.e. regular expression) and has to do extra work if\nthere are conditions in the loop. In our case, there is exactly one loop per\nregular expression, without any condition.\n\nJIT Hints\nThe hints that are needed for the match function of the last blog post can\nbe seen here (the function is slightly rewritten, e.g. the JIT does only\nproperly support a while loop as the main dispatch loop):\njitdriver = jit.JitDriver(reds=[\"i\", \"result\", \"s\"], greens=[\"re\"])\n\ndef match(re, s):\n    if not s:\n        return re.empty\n    # shift a mark in from the left\n    result = re.shift(s[0], 1)\n    i = 1\n    while i < len(s):\n        jitdriver.can_enter_jit(i=i, result=result, s=s, re=re)\n        jitdriver.jit_merge_point(i=i, result=result, s=s, re=re)\n        # shift the internal marks around\n        result = re.shift(s[i], 0)\n        i += 1\n    re.reset()\n    return result\n\nThe jitdriver is an instance describing the data of the interpreter we are\ndealing with. The arguments to the constructor need to list all local variables\nof the dispatch loop. The local variables are classified into two classes, red\nones and green ones. The green ones hold the objects that make up the program\nthat the interpreter currently runs and which position in the program is\ncurrently being executed. In a typical bytecode interpreter, the bytecode object\nand the program counter would be green. In our case, the regular expression is\nthe program, so it is green. The rest of the variables are red.\nThe green variables are treated specially by the JIT generator. At runtime, for\na given value of the green variables, one piece of machine code will be\ngenerated. This piece of machine code can therefore assume that the value of\nthe green variable is constant.\nThere are two additional hints, which are method calls on the\njitdriver instance. The jit_merge_point method marks the beginning of\nthe main interpreter loop. The can_enter_jit function marks the point where\na loop in the user program can be closed, which in our case is trivial, it's\njust at the end of the interpreter loop (for technical reasons it is put at the beginning, because nothing must happen between the can_enter_jit and jit_merge_point invocations).\nThose are the hints that the JIT generator needs to function at all. We added\nsome additional hints, that give the JIT generator more information to work\nwith. Those hints are immutability information, which means that certain\ninstance fields can not be changed after the object has been constructed. Apart\nfrom the marked field, none of the fields of any of the Regex subclasses\ncan change. For example for the Char class this is expressed in the\nfollowing way:\nclass Char(Regex):\n    _immutable_fields_ = [\"c\"]\n    def __init__(self, c):\n        ...\n\nThese hints allow the generated JIT to constant-fold reads out of the immutable\nfields in some situations.\n\n\nAdaptions to the Original Code\nIn the introduction above I wrote that the code within the loop in match\nuses no conditions. It is indeed true that none of the _shift methods\nhave an if statement or similar. However, there are some hidden conditions\ndue to the fact that the and and or boolean operators are used, which\nare short-circuiting. Therefore the JIT-version of the code needs to be adapted\nto use the non-short-circuiting operators & and |.\n\n\nJIT Example\nTo get an impression of how the generated machine code looks like, consider the\nregular expression (a|b)*. As regular expression objects this would be\nRepetition(Alternative(Char('a'), Char('b'))). The machine code in its intermediate,\nmachine-independent form looks as follows (I have slightly cleaned it up and\nadded comments for clarity):\n# arguments of the loop\n# i0 is i in the match function\n# result0 is result in the match function\n# s0 is s in the match function\n[i0, result0, s0] # those are the arguments to the machine code\nchar = s0[i0] # read the character\n# read the current mark:\ni5 = ConstPtr(ptr_repetition).marked\ni7 = char == 'a' # is the character equal to 'a'\ni8 = i5 & i7\ni10 = char == 'b' # is the character equal to 'b'\ni11 = i5 & i10\n# write new mark\nConstPtr(ptr_chara).marked = i8\ni13 = i8 | i11\n# write new mark\nConstPtr(ptr_charb).marked = i11\n# write new mark\nConstPtr(ptr_alternative).marked = i13\n# increment the index\ni17 = i0 + 1\ni18 = len(s0)\n# write new mark\nConstPtr(ptr_repetition).marked = i13\n# check that index is smaller than the length of the string\ni19 = i17 < i18\nif not i19:\n    go back to normally running match\njump(i17, i13, s0) # start from the top again\n\nThe various ConstPtr(ptr_*) denote constant addresses of parts of the regular\nexpression tree:\n\nptr_repetition is the Repetition\nptr_chara is Char('a')\nptr_charb is Char('b')\nptr_alternative is the Alternative\n\nEssentially the machine code reads the next char out of the string, the current\nmark out of the Repetition and then performs some boolean operations on\nthose, writing back the new marks. Note in particular how the generated\nmachine code does not need to do any method calls to shift and _shift and\nthat most field reads out of the regular expression classes have been optimized\naway, because the fields are immutable. Therefore the machine code does not\nneed to deconstruct the tree of regular expression objects at all, it just\nknows where in memory the various parts of it are, and encodes that directly\ninto the code.\n\n\nPerformance Results With JIT\nWith the regular expression matcher translated to C and with a generated JIT,\nthe regular expression performance increases significantly. Our running example\ncan match 16'500'000 chars/s, which is more than six times faster than the\nre module. This is not an entirely fair comparison, because the re\nmodule can give more information than just \"matches\" or \"doesn't match\", but\nit's still interesting to see. A more relevant comparison is that between the\nprogram with and without a JIT: Generating a JIT speeds the matcher up by more\nthan 20 times.\n\n\n\nConclusion\nSo, what have we actually won? We translated the relatively simple and very slow\nregular expression matching algorithm from the last post to C and were thus able\nto speed it up significantly. The real win is gained by also generating a JIT\nfor the matcher, which can be regarded as a simple interpreter. The resulting\nmatcher is rather fast.\nThe lesson from these posts is not that you can or should write a practical\nand general regular expression module in this way \u2013 indeed, enhancing the\nalgorithm to support more features of the re module would be a lot of work\nand it is also unclear what the speed results for more realistic regular\nexpressions would be. However, it makes for a great case study of the JIT\ngenerator. It was relatively straightforward to generate a JIT for the regex\nmatcher, and the speed results were great (Admittedly I know rather a lot about\nPyPy's JIT though). This approach is generalizable to many programs that are\nsufficiently \"interpreter-like\" (whatever that exactly means).\nAll the results that appeared at various points in this blog post can be seen\nhere:\n\n\n\n\n\n\n\nImplementation\nchars/s\nspeedup over pure Python\n\nPure Python code\n12'200\n1\n\nPython re module\n2'500'000\n205\n\nGoogle's re2 implementation\n550'000\n45\n\nRPython implementation translated to C\n720'000\n59\n\nC++ implementation\n750'000\n61\n\nJava implementation\n1'920'000\n157\n\nRPython implementation with JIT\n16'500'000\n1352\n\n\n\n\nSources\nAll the source code can be found in my Subversion user directory on Codespeak.\n\nEdit: Armin is right (see first comment). I fixed the problem.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/06/jit-for-regular-expression-matching-3877859053629057968.html"
    },
    {
      "title": "PyPy in Google's Summer of Code 2010",
      "text": "Good news everyone.\nThis year, thanks to google generosity and PSF support, we got two and a\nhalf of students for PyPy's summer of code. We didn't cut any students, but one\nof the projects is a joint project of PyPy and numpy. Hereby I present\ndescriptions, in my own words with my own opinions and in arbitrary order.  For\nmore details please follow links to particular blogs.\n\nJason Creighton: 64bit JIT backend for PyPy\nIntel 64bit (and I mean x86_64) compatibility for JIT has been one of the top\nrequested features (along with GIL removal). While GIL removal is not really an\neasy task, having our JIT emit 64bit assembler is sort of easy, thanks to our\nJIT backend abstraction. It will likely be faster, thanks to abundance of\nregisters.\n\n\nBartosz Skowron: Fast ctypes for PyPy\nHistorically weak point of PyPy was compatibility with extension modules.  We\nhave progressed quite a bit in recent years, first introducing ctypes for\npypy then progressing towards CPython extension modules. However, ctypes is\nwell known to be slow (and it's even slower on PyPy) and writing CPython\nextension modules is ugly, and it's going to be only with compatibility layer\nthat'll keep this slow. What happens if we try to employ JIT technology to\nctypes? Maybe we can compile calls to C code from Python as a direct calls in\ncompiled assembler? Why not?\nThis project will look how the JIT technology can be employed to do some\nsort of FFI. There is no guarantee we'll get super-fast ctypes as a result,\nbut it's good to see progress in that area.\n\n\nDan Roberts: Numpy in PyPy\nThis is a joint project of numpy and PyPy. The main objective is to bring\nnumpy to PyPy, possibly fast. The official mentor for this project is\nStefan van der Walt from numpy community. During initial meeting it was\nagreed that probably the best way to go would be to support original numpy\nwith CPython extension compatibility and then provide a minimal native numpy\nframework for pypy. The former would retain full compatibility, while the\nlatter would have JIT integration, with line of our previous\nnumeric experiments. There would be an explicit interface from converting\none array to another for convinience.\n\nOverall, I'm very happy to see so much support for PyPy from SoC. I hope all\nthree proposals will be successful!\nCheers,\nfijal & pypy team.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/05/pypy-in-googles-summer-of-code-2010-5321939902318322352.html"
    },
    {
      "title": "An Efficient and Elegant Regular Expression Matcher in Python",
      "text": "Two weeks ago, I was at the Workshop Programmiersprachen und Rechenkonzepte,\na yearly meeting of German programming language researchers. At the workshop,\nFrank Huch and Sebastian Fischer gave a really excellent talk about an\nelegant regular expression matcher written in Haskell. One design goal of the\nmatcher was to run in time linear to the length of the input string (i.e.\nwithout backtracking) and linear in the size of the regular expression. The\nmemory use should also only be linear in the regular expression.\nDuring the workshop, some of the Haskell people and me then implemented the\nalgorithm in (R)Python. Involved were Frank, Sebastian, Baltasar Tranc\u00f3n y\nWidemann, Bernd Bra\u00dfel and Fabian Reck.\nIn this blog post I want to describe this implementation and show the code of\nit, because it is quite simple. In a later post I will show what optimizations\nPyPy can perform on this matcher and also do some benchmarks.\nA Note on terminology: In the rest of the post \"regular expression\" is meant\nin the Computer Science sense, not in the POSIX sense. Most importantly, that\nmeans that back-references are not allowed.\nAnother note: This algorithm could not be used to implement PyPy's re\nmodule! So it won't help to speed up this currently rather slow implementation.\n\nImplementing Regular Expression Matchers\nThere are two typical approaches to implement regular expression. A naive one is\nto use a back-tracking implementation, which can lead to exponential matching\ntimes given a sufficiently evil regular expression.\nThe other, more complex one, is to transform the regular expression into a\nnon-deterministic finite automaton (NFA) and then transform the NFA into a\ndeterministic finite automaton (DFA). A DFA can be used to efficiently match\na string, the problem of this approach is that turning an NFA into a DFA can\nlead to exponentially large automatons.\nGiven this problem of potential memory explosion, a more sophisticated approach\nto matching is to not construct the DFA fully, but instead use the NFA for\nmatching. This requires some care, because it is necessary to keep track of\nwhich set of states the automaton is in (it is not just one state, because the\nautomaton is non-deterministic).\nThe algorithm described here is essentially equivalent to this approach, however\nit does not need an intermediate NFA and represents a state of a corresponding\nDFA as marked regular expression (represented as a tree of nodes). For many\ndetails about an alternative approach to implement regular expressions\nefficiently, see Russ Cox excellent article collection.\n\n\nThe Algorithm\nIn the algorithm the regular expression is represented as a tree of nodes. The\nleaves of the nodes can match exactly one character (or the epsilon node, which\nmatches the empty string). The inner nodes of the tree combine other nodes in\nvarious ways, like alternative, sequence or repetition. Every node in the tree\ncan potentially have a mark. The meaning of the mark is that a node is marked,\nif that sub-expression matches the string seen so far.\nThe basic approach of the algorithm is that for every character of the input\nstring the regular expression tree is walked and a number of the nodes in the\nregular expression are marked. At the end of the string, if the top-level node\nis marked, the string matches, otherwise it does not. At the beginning of the\nstring, one mark gets shifted into the regular expression from the top, and then\nthe marks that are in the regex already are shifted around for every additional\ncharacter.\nLet's start looking at some code, and an example to make this clearer. The base\nclass of all regular expression nodes is this:\nclass Regex(object):\n    def __init__(self, empty):\n        # empty denotes whether the regular expression\n        # can match the empty string\n        self.empty = empty\n        # mark that is shifted through the regex\n        self.marked = False\n\n    def reset(self):\n        \"\"\" reset all marks in the regular expression \"\"\"\n        self.marked = False\n\n    def shift(self, c, mark):\n        \"\"\" shift the mark from left to right, matching character c.\"\"\"\n        # _shift is implemented in the concrete classes\n        marked = self._shift(c, mark)\n        self.marked = marked\n        return marked\n\nThe match function which checks whether a string matches a regex is:\ndef match(re, s):\n    if not s:\n        return re.empty\n    # shift a mark in from the left\n    result = re.shift(s[0], True)\n    for c in s[1:]:\n        # shift the internal marks around\n        result = re.shift(c, False)\n    re.reset()\n    return result\n\nThe most important subclass of Regex is Char, which matches one\nconcrete character:\nclass Char(Regex):\n    def __init__(self, c):\n        Regex.__init__(self, False)\n        self.c = c\n\n    def _shift(self, c, mark):\n        return mark and c == self.c\n\nShifting the mark through Char is easy: a Char instance retains a mark\nthat is shifted in when the current character is the same as that in the\ninstance.\nAnother easy case is that of the empty regular expression Epsilon:\nclass Epsilon(Regex):\n    def __init__(self):\n        Regex.__init__(self, empty=True)\n\n    def _shift(self, c, mark):\n        return False\n\nEpsilons never get a mark, but they can match the empty string.\n\nAlternative\nNow the more interesting cases remain. First we define an abstract base class\nBinary for the case of composite regular expressions with two children, and\nthen the first subclass Alternative which matches if either of two regular\nexpressions matches the string (usual regular expressions syntax a|b).\nclass Binary(Regex):\n    def __init__(self, left, right, empty):\n        Regex.__init__(self, empty)\n        self.left = left\n        self.right = right\n\n    def reset(self):\n        self.left.reset()\n        self.right.reset()\n        Regex.reset(self)\n\nclass Alternative(Binary):\n    def __init__(self, left, right):\n        empty = left.empty or right.empty\n        Binary.__init__(self, left, right, empty)\n\n    def _shift(self, c, mark):\n        marked_left  = self.left.shift(c, mark)\n        marked_right = self.right.shift(c, mark)\n        return marked_left or marked_right\n\nAn Alternative can match the empty string, if either of its children can.\nSimilarly, shifting a mark into an Alternative shifts it into both its\nchildren. If either of the children are marked afterwards, the Alternative\nis marked too.\nAs an example, consider the regular expression a|b|c, which would be\nrepresented by the objects Alternative(Alternative(Char('a'), Char('b')), Char('c')).\nMatching the string \"a\" would lead to the following marks in\nthe regular expression objects (green nodes are marked, white ones are\nunmarked):\n\n\nAt the start of the process, no node is marked. Then the first char is matched,\nwhich adds a mark to the Char('a') node, and the mark will propagate up the\ntwo Alternative nodes.\n\n\nRepetition\nThe two remaining classes are slightly trickier. Repetition is used to match\na regular expression any number of times (usual regular expressions syntax\na*):\nclass Repetition(Regex):\n    def __init__(self, re):\n        Regex.__init__(self, True)\n        self.re = re\n\n    def _shift(self, c, mark):\n        return self.re.shift(c, mark or self.marked)\n\n    def reset(self):\n        self.re.reset()\n        Regex.reset(self)\n\nA Repetition can always match the empty string. The mark is shifted into the\nchild, but if the Repetition is already marked, this will be shifted into\nthe child as well, because the Repetition could match a second time.\nAs an example, consider the regular expression (a|b|c)* matching the string\nabcbac:\n\nFor every character, one of the alternatives matches, thus the repetition matches\nas well.\n\n\nSequence\nThe only missing class is that for sequences of expressions, Sequence (usual\nregular expressions syntax ab):\nclass Sequence(Binary):\n    def __init__(self, left, right):\n        empty = left.empty and right.empty\n        Binary.__init__(self, left, right, empty)\n\n    def _shift(self, c, mark):\n        old_marked_left = self.left.marked\n        marked_left = self.left.shift(c, mark)\n        marked_right = self.right.shift(\n            c, old_marked_left or (mark and self.left.empty))\n        return (marked_left and self.right.empty) or marked_right\n\nA Sequence can be empty only if both its children are empty. The mark\nhandling is a bit delicate. If a mark is shifted in, it will be shifted to the\nleft child regular expression. If that left child is already marked before the\nshift, that mark is shifted to the right child. If the left child can match the\nempty string, the right child gets the mark shifted in as well.\nThe whole sequence matches (i.e. is marked), if the left child is marked after\nthe shift and if the right child can match the empty string, or if the right\nchild is marked.\nConsider the regular expression abc matching the string abcd. For the\nfirst three characters, the marks wander from left to right, when the d is\nreached, the matching fails.\n\n\n\nMore Complex Example\nAs a more complex example, consider the expression ((abc)*|(abcd))(d|e)\nmatching the string abcabcabcd.\n\nNote how the two branches of the first alternative match the first abc in\nparallel, until it becomes clear that only the left alternative (abc)* can\nwork.\n\n\nComplexity\nThe match function above loops over the entire string without going back and\nforth. Each iteration goes over the whole tree every time. Thus the complexity\nof the algorithm is O(m*n) where m is the size of the regular expression\nand n is the length of the string.\n\n\n\nSummary & Outlook\nSo, what have we achieved now? The code shown here can match regular expressions\nwith the desired complexity. It is also not much code. By itself, the Python\ncode shown above is not terribly efficient. In the next post I will show how the\nJIT generator can be used to make the simple matcher shown above really fast.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/05/efficient-and-elegant-regular-2727904462179540436.html"
    },
    {
      "title": "Running wxPython on top of pypy",
      "text": "Hello,\nThese last three weeks we have been busy working on the cpyext subsystem, which\nallows pypy to execute extension modules written with the Python C API.\nToday we hacked enough to have wxPython compile, and run its wonderful demo.\nThis:\n\ncannot be distinguished from the same run with a\nstandard python interpreter, but this:\n\nshows an exception that\nCPython never produces.\nwxPython is a big extension module: it has more than 500 classes and 7500\nfunctions, most of the code is automatically generated by swig.  It uses\nadvanced techniques, like \"Original Object Return\" and cross-platform\npolymorphism, that effectively allows the developer to seamlessly subclass C++\nobjects in Python and write GUI applications efficiently.\nThe demo application runs reasonably fast, it feels slower than with CPython,\nbut I did not activate the JIT option of pypy.  It still crashes in some places\n(the demo is very comprehensive and covers all the aspects of wxPython), and\nthreads are expected to not work at the moment.\nWe had to modify a little the code of wxPython, mainly because it often stores\nborrowed references into C++ objects.  This does not work well in pypy, where\nall other counted references can disappear, and allows the address of the object\nto change.  The solution is to use weak references instead.  The patch is here,\nit will eventually be merged into the upstream wxPython version.\nThis first real test proves that CPython extensions can be migrated to pypy\nwithout much pain.  It also points some places which can be improved, like\nbetter diagnostics in crashes, better support of distutils...\nAmaury Forgeot d'Arc",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/05/running-wxpython-on-top-of-pypy-52246787415886751.html"
    },
    {
      "title": "Using CPython extension modules with PyPy natively, or: PyPy can load .pyd files with CPyExt!",
      "text": "PyPy is now able to load\nand run CPython extension modules (i.e. .pyd and .so files) natively by using the new CPyExt\nsubsystem.\nUnlike the solution presented in another blog post (where extension modules like\nnumpy etc. were run on CPython and proxied through TCP), this solution does not require\na running CPython anymore. We do not achieve full binary compatiblity\nyet (like Ironclad), but recompiling the extension is generally enough.\nThe only prerequisite is that the necessary functions of the C API of CPython are already\nimplemented in PyPy. If you are a user or an author of a module and miss certain functions\nin PyPy, we invite you to implement them. Up until now, a lot of people (including a lot of\nnew committers) have stepped up and implemented a few functions to get their favorite module\nrunning. See the end of this post for a list of names.\nRegarding speed, we tried the following: even though there is a bit of overhead when running\nthese modules, we could run the regular expression engine of CPython (_sre.so) and execute\nthe spambayes benchmark of the Unladen Swallow benchmark suite (cf. speed.pypy.org) and\nexperience a speedup:\nIt became two times faster on pypy-c than with the built-in regular\nexpression engine of PyPy. From Amdahl's Law it follows that the _sre.so must run several\ntimes faster than the built-in engine.\nCurrently pursued modules include PIL and others. Distutils support is nearly ready.\nIf you would like to participate or want information on how to use this new feature, come and join\nour IRC channel #pypy on freenode.\nAmaury Forgeot d'Arc and Alexander Schremmer\nFurther CPyExt Contributors:\nAlex Gaynor\nBenjamin Peterson\nJean-Paul Calderone\nMaciej Fijalkowski\nJan de Mooij\nLucian Branescu Mihaila\nAndreas St\u00fchrk\nZooko Wilcox-O Hearn",
      "tags": "cpyext,CPython,extension modules,speed",
      "url": "https://www.pypy.org/posts/2010/04/using-cpython-extension-modules-with-5864754772659599217.html"
    },
    {
      "title": "PyPy on google open source blog",
      "text": "Hello\nBea D\u00fcring, from the PyPy team, wrote a post for google open source blog covering PyPy's 1.2 release. It's also the first public mention of the fact that google provided financial support for PyPy's 2.5 compatibility. Thanks!\nCheers\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/04/pypy-on-google-open-source-blog-1192495586835103069.html"
    },
    {
      "title": "Introducing nightly builds and ubuntu PPA",
      "text": "Hello.\n\nWe're pleased to announce two things that we were constantly asked for: Nightly builds and Ubuntu PPA for 1.2 release made by Bartosz Skowron. There are no nightly build ubuntu packages (yet).\n\n\nNightly builds are what they are - pure pypy executables with JIT compiled in (for linux only now). They require either a pypy checkout or a release download. The main difference is that by default display more debugging information than release builds and that they contain recent bugfixes and improvements of course :-)\n\nCheers\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/03/introducing-nightly-builds-and-ubuntu-3346203966988761264.html"
    },
    {
      "title": "Blog coverage of speed.pypy.org",
      "text": "If you want to read a detailed analysis about why speed.pypy.org is cool, head over to Saveen Reddy's blog at the MSDN.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/03/blog-coverage-of-speedpypyorg-2291955489972824511.html"
    },
    {
      "title": "Heroes of the 1.2 Release",
      "text": "Now that the release is done I wanted to list and to thank some people that\nwere essential in the process of getting it out of the door, particularly\nbecause the work of some of them is not very visible usually.\nArmin Rigo and Maciej Fija\u0142kowski tirelessly worked on most aspects of\nthe release, be it fixing the last known bugs and performance problems,\npackaging or general wizardry.\nAmaury Forgeot d'Arc made sure that PyPy 1.2 actually supports Windows as a\nplatform properly and compiled the Windows binaries.\nMiquel Torres designed and implemented our new speed overview page,\nhttps://speed.pypy.org which is a great tool for us to spot performance\nregressions and to showcase our improvements to the general public.\ntav designed the new user-oriented web page, https://pypy.org which is a lot\nnicer for people that only want to use PyPy as a Python implementation (and not\nbe confused by how PyPy is actually made).\nHolger Krekel fixed our main development server codespeak.net, even while\nbeing on vacation and not really having online connectivity. Without that, we\ncouldn't actually have released anything.\nBartosz Skowron worked a lot on making Ubuntu packages for PyPy, which is\nreally cool. Even though he didn't quite finish in time for the release, we will\nhopefully get them soon.\nThanks to all you guys!",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2010/03/heroes-of-12-release-7211722984024027191.html"
    },
    {
      "title": "Introducing the PyPy 1.2 release",
      "text": "We are pleased to announce PyPy's 1.2 release.\nThis version 1.2 is a major milestone and it is the first release to ship\na Just-in-Time compiler that is known to be faster than CPython\n(and unladen swallow) on some real-world applications (or the best benchmarks\nwe could get for them). The main theme for the 1.2 release is speed.\nThe JIT is stable and we don't observe crashes. Nevertheless we would\nrecommend you to treat it as beta software and as a way to try out the JIT\nto see how it works for you.\nHighlights:\n\nThe JIT compiler.\nVarious interpreter optimizations that improve performance as well as help\nsave memory. Read our various blog posts about achievements.\nIntroducing a new PyPy website at pypy.org made by tav and improved\nby the PyPy team.\nIntroducing speed.pypy.org made by Miquel Torres, a new service that monitors our performance\nnightly.\nThere will be ubuntu packages on PyPy's PPA made by Bartosz Skowron,\nhowever various troubles prevented us from having them as of now.\n\nKnown JIT problems (or why you should consider this beta software) are:\n\nThe only supported platform is 32bit x86 for now, we're looking for help with\nother platforms.\nIt is still memory-hungry.  There is no limit on the amount of RAM that\nthe assembler can consume; it is thus possible (although unlikely) that\nthe assembler ends up using unreasonable amounts of memory.\n\nIf you want to try PyPy, go to the download page on our excellent new site\nand find the binary for your platform. If the binary does not work (e.g. on\nLinux, because of different versions of external .so dependencies), or if\nyour platform is not supported, you can try building from the source.\nThe PyPy release team,\nArmin Rigo, Maciej Fijalkowski and Amaury Forgeot d'Arc\nTogether with\nAntonio Cuni, Carl Friedrich Bolz, Holger Krekel, Samuele Pedroni and many others.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2010/03/introducing-pypy-12-release-2791388655442447862.html"
    },
    {
      "title": "State of PyPy talk from Pycon",
      "text": "Hello.\n\nThe last PyPy video from pycon has been uploaded. It's a very short (less than 10 minutes) \"keynote\" talk about state of PyPy.\n\nEnjoy!\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/03/state-of-pypy-talk-from-pycon-6748503931490058986.html"
    },
    {
      "title": "Introducing speed.pypy.org",
      "text": "Hello.\nSome time ago, we introduced our nightly performance graphs. This was a quick\nhack to allow us to see performance regressions. Thanks to Miquel Torres,\nwe can now introduce https://speed.pypy.org, which is a Django-powered web\napp sporting a more polished visualisation of our nightly performance runs.\nWhile this website is not finished yet, it's already far better than our previous\napproach :-)\nDetails about announcement on pypy-dev are found here.\nIf you're are interested in having something similar for other benchmark runs, contact Miquel (tobami at gmail).\nQuoting Miquel: \"I would also like to note, that if other performance-oriented\nopensource projects are interested, I would be willing to see if we can set-up\nsuch a Speed Center for them. There are already people interested in\ncontributing to make it into a framework to be plugged into buildbots, software\nforges and the like. Stay tuned!\"",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/03/introducing-speedpypyorg-1822874891591164256.html"
    },
    {
      "title": "Benchmarking twisted",
      "text": "Hello.\nI recently did some benchmarking of twisted on top of PyPy. For the very\nimpatient: PyPy is up to 285% faster than CPython. For more patient people,\nthere is a full explanation of what I did and how I performed measurements,\nso they can judge themselves.\nThe benchmarks are living in twisted-benchmarks and were mostly written\nby Jean Paul Calderone. Even though he called them \"initial exploratory\ninvestigation into a potential direction for future development resulting\nin performance oriented metrics guiding the process of optimization and\navoidance of complexity regressions\", they're still much much better than\naverage benchmarks found out there.\nThe methodology was to run each benchmark for\nquite some time (about 1 minute), measuring number of requests each 5s.\nThen I looked at dump of data and subtracted some time it took\nfor JIT-capable interpreters to warm up (up to 15s), averaging\neverything after that. Averages of requests per second are in the table below (the higher the better):\n\n\n\n\n\n\n\n\nbenchname\nCPython\nUnladen swallow\nPyPy\n\nnames\n10930\n11940 (9% faster)\n15429 (40% faster)\n\npb\n1705\n2280 (34% faster)\n3029 (78% faster)\n\niterations\n75569\n94554 (25% faster)\n291066 (285% faster)\n\naccept\n2176\n2166 (same speed)\n2290 (5% faster)\n\nweb\n879\n854 (3% slower)\n1040 (18% faster)\n\ntcp\n105M\n119M (7% faster)\n60M (46% slower)\n\n\n\nTo reproduce, run each benchmark with:\n\nbenchname.py -n 12 -d 5\nWARNING: running tcp-based benchmarks that open new connection for each\nrequest (web & accept) can exhaust number of some kernel structures,\nlimit n or wait until next run if you see drops in request per second.\nThe first obvious thing is that various benchmarks are more or less amenable\nto speedups by JIT compilation. Accept and tcp getting smallest speedups, if at\nall. This is understandable, since JIT is mostly about reducing interpretation\nand frame overhead, which is probably not large when it comes to accepting\nconnections. However, if you actually loop around, doing something, JIT\ncan give you a lot of speedup.\nThe other obvious thing is that PyPy is the fastest python interpreter\nhere, almost across-the board (Jython and IronPython won't run twisted),\nexcept for raw tcp throughput. However, speedups can vary and I expect\nthis to improve after the release, as there are points, where PyPy can\nbe improved. Regarding raw tcp throughput - this can be a problem for\nsome applications and we're looking forward to improve this particular\nbit.\nThe main reason to use twisted for this comparison is a lot of support from\ntwisted team and JP Calderone in particular, especially when it comes to\nproviding benchmarks. If some open source project wants to be looked at\nby PyPy team, please provide a reasonable set of benchmarks and infrastructure.\nIf, however, you're a closed source project fighting with performance problems\nof Python, we're providing contracting for investigating opportunities, how\nPyPy and not only PyPy, can speed up your project.\nCheers,\nfijal\n\nBenchmark descriptions:\n\nnames - simple DNS server\nweb - simple http hello world server\npb - perspective broker, RPC mechanism for twisted\niterations - empty twisted loop\naccept - number of tcp connections accepted per second\ntcp - raw socket transfer throughput\n\nUsed interpreters:\n\nCPython 2.6.2 - as packaged by ubuntu\nUnladen swallow svn trunk, revision 1109\nPyPy svn trunk, revision 71439\n\nTwisted version used: svn trunk, revision 28580\nMachine: unfortunately 32bit virtual-machine under qemu, running ubuntu karmic,\non top of Quad core intel Q9550 with 6M cache. Courtesy of Michael Schneider.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2010/03/hello-5058108566628405592.html"
    },
    {
      "title": "Pycon 2010 report",
      "text": "Hello.\nGreetings to everybody from Pycon 2010 Atlanta. Right now I'm sitting in\na sprint room with people sprinting on various projects, like CPython,\ntwisted etc. The conference was really great, and I've seen some good talks,\nalthough I've been too exhausted from my own talks to go to too many.\nProbably I should stay away from proposing that many talks to next pycon :-)\nThe highlight of sprints was that we got a common mercurial repository at python.org for python benchmarks. We might be able to come up with\n\"the python benchmark suite\" which will mostly consist \nof simple benchmarks using large python libraries, rather than microbenchmarks.\nThe repository was started by the Unladen Swallow people and we already\nhave common commit access among PyPy, CPython, Unladen Swallow, Jython\nand Iron Python. We don't have yet a common place to run benchmarks,\nbut we should be able to fix that soon.\nRegarding the talks, there are online videos for\nHow to write cross-interpreter python programs and Speed of PyPy talks,\namong other talks from Pycon.\nThere should be a video for my short keynote shortly.\nThe talks were well received as there is interest in PyPy's progress.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/02/pycon-2010-report-6986911457623699520.html"
    },
    {
      "title": "Nightly graphs of PyPy's performance",
      "text": "Hello.\nIn the past few months, we made tremendous progress on the JIT front.\nTo monitor the progress daily, we introduced recently some cool graphs\nthat plot revision vs performance. They are based on unladen swallow\nbenchmark runner and they're written entirely in JavaScript, using canvas\nvia the JQuery and Flot libraries.\nIt's amazing what you can do in JavaScript these days... They are also\ntested via the very good oejskit plugin, that integrates py.test\nwith JavaScript testing, driven by the command line.\nAs you can probably see, we're very good on some benchmarks and not that\ngreat on others. Some of the bad results come from the fact that while we\ndid a lot of JIT-related work, other PyPy parts did not see that much\nlove. Some of our algorithms on the builtin data types are inferior to those\nof CPython. This is going to be an ongoing focus for a while.\nWe want to first improve on the benchmarks for a couple\nof weeks before doing a release to gather further feedback.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2010/01/nightly-graphs-of-pypys-performance-8360469412941669946.html"
    },
    {
      "title": "Accelerating PyPy development by funding",
      "text": "PyPy has recently made some great speed and memory progress towards providing the most efficient Python interpreter out there.  We also just announced\nour plans for the pypy-1.2 release.  Much of this is driven by personal\ncommitment, by individuals and companies investing time and money.\nNow we'd appreciate some feedback and help regarding getting money\ninto the PyPy project to help its core members (between\n5 and 15 people depending how you count) to sustain themselves.  We see\nseveral options:\n\nuse a foundation structure and ask for tax-exempt donations to the\nproject, its developers and infrastructure.  We just got\na letter from the Software Freedom Conservancy that they view\nour application favourably so this option becomes practical hopefully\nsoon.\noffer to implement certain features like a 64bit JIT-backend,\nNumpy for PyPy or a streamlined installation in exchange for money,\ncontributed in small portions/donations.  Do you imagine you or your\ncompany would sponsor PyPy on a small scale for efforts like this?\nAny other bits you'd like to see?\noffer to implement larger scale tasks by contracting PyPy related companies,\nnamely Open End and merlinux who have successfully done such\ncontracts in the past.  Please don't hesitate to contact\nholger@merlinux.eu and bea@openend.se if you want to start a\nconversation on this.\napply for public/state funding - in fact we are likely to get some\nfunding through Eurostars, more on that separately.  Such funding\nis usually only a 50-60% percentage of actual employment and\nproject costs, and is tied to research questions rather than\nto make PyPy a production-useable interpreter, though.\n\nAnything else we should look out for?\ncheers & thanks for any feedback,\nMaciej and Holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/12/accelerating-pypy-development-by-8973749020516679741.html"
    },
    {
      "title": "Planning a next release of PyPy",
      "text": "The PyPy core team is planning to make a new release before the next Pycon US.\nThe main target of the 1.2 release is packaging the good results\nwe have achieved applying our current JIT compiler generator to our\nPython interpreter. Some of that progress has been chronicled in\nrecent posts on the status blog. By releasing them in a\nrelatively stable prototype we want to encourage people to try them with their\nown code and to gather feedback in this way. By construction the JIT compiler\nshould support all Python features, what may vary are the speedups\nachieved (in some cases the JIT may produce worse results than the PyPy\ninterpreter which we would like to know) and the extra memory required\nby it.\nFor the 1.2 release we will focus on the JIT stability first, less on\nimproving non-strictly JIT areas. The JIT should be good at many things\nas shown by previous blog postings. We want the JIT compiler in the\nrelease to work well on Intel 32 bits on Linux, with Mac OS X and\nWindows being secondary targets.  Which compilation targets work will\ndepend a bit on contributions.\nIn order to finalize the release we intend to have a concentrated\neffort (\"virtual sprint\") from the 22nd to the 29th of\nJanuary. Coordination will happen as usual through the #pypy irc\nchannel on freenode. Samuele Pedroni will take the role of release\nmanager as he already did in the past.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2009/12/planning-next-release-of-pypy-4193252449406707091.html"
    },
    {
      "title": "Leysin Winter Sprint: reported",
      "text": "Update: the sprint has been reported to some later date.\n\nThe next PyPy sprint will probably still be in Leysin, Switzerland, for the\nseventh time.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/12/leysin-winter-sprint-23-30th-january-7768876505015446348.html"
    },
    {
      "title": "Using CPython extension modules with PyPy, or: PyQt on PyPy",
      "text": "If you have ever wanted to use CPython extension modules on PyPy,\nwe want to announce that there is a solution that should be compatible\nto quite a bit of the available modules. It is neither new nor written\nby us, but works nevertheless great with PyPy.\nThe trick is to use RPyC, a transparent, symmetric remote procedure\ncall library written in Python. The idea is to start a\nCPython process that hosts the PyQt libraries\nand connect to it via TCP to send RPC commands to it.\nI tried to run PyQt applications\nusing it on PyPy and could get quite a bit of the functionality of these\nworking. Remaining problems include regular segfaults of CPython\nbecause of PyQt-induced memory corruption and bugs because classes\nlike StandardButtons behave incorrectly when it comes to arithmetical operations.\nChanges to RPyC needed to be done to support remote unbound __init__ methods,\nshallow call by value for list and dict types (PyQt4 methods want real lists and dicts\nas parameters), and callbacks to methods (all remote method objects are wrapped into\nsmall lambda functions to ease the call for PyQt4).\nIf you want to try RPyC to run the PyQt application of your choice, you just\nneed to follow these steps. Please report your experience here in the blog\ncomments or on our mailing list.\n\n\nDownload RPyC from the RPyC download page.\nDownload this patch and apply it to RPyC by running\npatch -p1 < rpyc-3.0.7-pyqt4-compat.patch in the RPyC directory.\nInstall RPyc by running python setup.py install as root.\nRun the file rpyc/servers/classic_server.py using CPython.\nExecute your PyQt application on PyPy.\n\n\nPyPy will automatically connect to CPython and use its PyQt libraries.\nNote that this scheme works with nearly every extension library. Look\nat pypy/lib/sip.py on how to add new libraries (you need to create\nsuch a file for every proxied extension module).\nHave fun with PyQt\nAlexander Schremmer",
      "tags": "CPython,extension modules,PyQt4,RPyC",
      "url": "https://www.pypy.org/posts/2009/11/using-cpython-extension-modules-with-4951018896657992031.html"
    },
    {
      "title": "Some benchmarking",
      "text": "Hello.\n\nRecently, thanks to the surprisingly helpful Unhelpful, also known as Andrew Mahone,\nwe have a decent, if slightly arbitrary, set of performances graphs.\nIt contains a couple of benchmarks already\nseen on this blog as well as some taken from The Great Computer\nLanguage Benchmarks Game. These benchmarks don't even try to represent \"real applications\"\nas they're mostly small algorithmic benchmarks. Interpreters used:\n\n\n\nPyPy trunk, revision 69331 with --translation-backendopt-storesink, which is\nnow on by default\n\n\nUnladen swallow trunk, r900\n\nCPython 2.6.2 release\n\n\nHere are the graphs; the benchmarks and the runner script are available\n\n\n\n\nAnd zoomed in for all benchmarks except binary-trees and fannkuch.\n\n\n\nAs we can see, PyPy is generally somewhere between the same speed\nas CPython to 50x faster (f1int). The places where we're the same\nspeed as CPython are places where we know we have problems - for example generators are\nnot sped up by the JIT and they require some work (although not as much by far\nas generators & Psyco :-). The glaring inefficiency is in the regex-dna benchmark.\nThis one clearly demonstrates that our regular expression engine is really,\nreally, bad and urgently requires attention.\n\n\nThe cool thing here is, that although these benchmarks might not represent\ntypical python applications, they're not uninteresting. They show\nthat algorithmic code does not need to be far slower in Python than in C,\nso using PyPy one need not worry about algorithmic code being dramatically\nslow. As many readers would agree, that kills yet another usage of C in our\nlives :-)\n\nCheers,\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/11/some-benchmarking-9211261260383281459.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Report",
      "text": "While the D\u00fcsseldorf is dwindling off, we put our minds to the task of retelling\nour accomplishments. The sprint was mostly about improving the JIT and we         \nmanaged to stick to that task (as much as we managed to stick to anything). The   \nsprint was mostly filled with doing many small things.                        \n                                               \nInlining                                                                 \nCarl Friedrich and Samuele started the sprint trying to tame the JIT's inlining.\nUntil now, the JIT would try to inline everything in a loop (except other loops)  \nwhich is what most tracing JITs actually do. This works great if the resulting    \ntrace is of reasonable length, but if not it would result in excessive memory     \nconsumption and code cache problems in the CPU. So far we just had a limit on     \nthe trace size, and we would abort tracing when the limit was reached. This       \nwould happen again and again for the same loop, which is not useful at all. The   \nnew approach introduced is to be more clever when tracing is aborted by marking   \nthe function with the largest contribution to the trace size as non-inlinable. The\nnext time this loop is traced, it usually then gives a reasonably sized trace.\nThis gives a problem because now some functions that don't contain loops are not\ninlined, which means they never get assembler code for them generated. To remedy  \nthis problem we also make it possible to trace functions from their start (as     \nopposed to just tracing loops). We do that only for functions that can not be     \ninlinined (either because they contain loops or they were marked as               \nnon-inlinable as described above).                                            \nThe result of this is that the Python version telco decimal benchmark runs                                 \nto completion without having to arbitrarily increase the trace length limit.                    \nIt's also about 40% faster than running it on CPython. This is one of the first                 \nnon-tiny programs that we speed up.                                                         \n                                                                                          \n                                                 \nReducing GC Pressure                                                                   \nArmin and Anto used some GC instrumentation to find places in pypy-c-jit                     \nthat allocate a lot of memory. This is an endlessly surprising exercise, as                     \nusually we don't care too much about allocations of short-lived objects when                    \nwriting RPython, as our GCs usually deal well with those. They found a few                      \nplaces where they could remove allocations, most importantly by making one of                   \nthe classes that make up traces smaller.                                                    \n                                                                                          \n                                          \nOptimizing Chains of Guards                                                            \nCarl Friedrich and Samuele started a simple optimization on the trace level that             \nremoves superfluous guards. A common pattern in a trace is to have stronger                     \nand stronger guards about the same object. As an example, often there is first a                \nguard that an object is not None, later followed by a guard that it is exactly                  \nof a given class and then even later that it is a precise instance of that                      \nclass. This is inefficient, as we can just check the most precise thing in the                  \nplace of the first guard, saving us guards (which take memory, as they need resume data).       \nMaciek, Armin and Anto later improved on that by introducing a new guard that                   \nchecks for non-nullity and a specific class in one guard, which allows us to                    \ncollapse more chains.                                                                       \n                                                                                          \n                                         \nImproving JIT and Exceptions                                                           \nArmin and Maciek went on a multi-day quest to make the JIT and Python-level                  \nexceptions like each other more. So far, raising and catching exceptions would                  \nmake the JIT generate code that has a certain amusement value, but is not really                \nfast in any way. To improve the situation, they had to dig into the exception                   \nsupport in the Python interpreter, where they found various inefficiencies. They                \nalso had to rewrite the exceptions module to be in RPython (as opposed to                                                             \njust pure Python + an old hack). Another problems is that tracebacks give you                   \naccess to interpreter frames. This forces the JIT to deoptimize things, as                      \nthe JIT keeps some of the frame's content in CPU registers or on the CPU stack,                 \nwhich reflective access to frames prevents.                                                     \nCurrently we try to improve the simple cases where the traceback is never                       \nactually accessed. This work is not completely finished, but some cases are                     \nalready significantly faster.                                                               \n                                                                                          \n                                       \nMoving PyPy to use py.test 1.1                                                         \nHolger worked on porting PyPy to use the newly released py.test 1.1. PyPy                   \nstill uses some very old support code in its testing infrastructure, which makes                \nthis task a bit annoying. He also gave the other PyPy developers a demo of some                 \nof the newer py.test features and we discussed which of them we want to start                   \nusing to improve our tests to make them shorter and clearer. One of the things                  \nwe want to do eventually is to have less skipped tests than now.                            \n                                                                                          \n                           \nUsing a Simple Effect Analysis for the JIT                                             \nOne of the optimization the JIT does is caching fields that are read out of                  \nstructures on the heap. This cache needs to be invalidated at some points, for\nexample when such a field is written to (as we don't track aliasing much).\nAnother case is a call in the assembler, as the target function could\narbitrarily change the heap. This of course is imprecise, since most functions\ndon't actually change the whole heap, and we have an analysis that finds out\nwhich sorts of types of structs and arrays a function can mutate. During the\nsprint Carl Friedrich and Samuele integrated this analysis with the JIT, to help\nit invalidate caches less aggressively. Later Anto and Carl Friedrich also\nported this support to the CLI version of the JIT.\n\n\nMiscellaneous\nSamuele (with some assistance of Carl Friedrich) set up a buildbot slave on a\nMac Mini at the University. This should let us stabilize on the Max OS X. So far\nwe still have a number of failing tests, but now we are in a situation to\nsanely approach fixing them.\nAnto improved the CLI backend to support the infrastructure for producing the\nprofiling graphs Armin introduced.\nThe guinea-pigs that were put into Carl Friedrich's care have been fed (which\nwas the most important sprint task anyway).\n Samuele & Carl Friedrich",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/11/dusseldorf-sprint-report-2505348213879053352.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Started",
      "text": "The D\u00fcsseldorf sprint starts today. Only Samuele and me are there so far, but that should change over the course of the day. We will mostly work on the JIT during this sprint, trying to make it a lot more practical. For that we need to decrease its memory requirements some more and to make it use less aggressive inlining. We will post more as the sprint progresses.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/11/dusseldorf-sprint-started-7608527610228870250.html"
    },
    {
      "title": "PyPy on RuPy 2009",
      "text": "Hello.\n\nIt's maybe a bit late to announce, but there will be PyPy talk\nat Rupy conference this weekend in\nPoznan. Precisely, I'll be talking mostly about PyPy's JIT and\nhow to use it. Unfortunately the talk is on Saturday, at 8:30 in the morning.\n\n\nEDIT: Talk is online, together with examples\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/11/pypy-on-rupy-2009-5675275348619189353.html"
    },
    {
      "title": "Logging and nice graphs",
      "text": "Hi all,\n\nThis week I worked on improving the system we use for logging.  Well, it was not really a \"system\" but rather a pile of hacks to measure in custom ways timings and counts and display them.  So now, we have a system :-)\n\nThe system in question was integrated in the code for the GC and the JIT, which are two independent components as far as the source is concerned.  However, we can now display a unified view.  Here is for example pypy-c-jit running pystone for (only) 5000 iterations:\n\n\n\nThe top long bar represents time.  The bottom shows two summaries of the total time taken by the various components, and also plays the role of a legend to understand the colors at the top.  Shades of red are the GC, shades of green are the JIT.\n\nHere is another picture, this time on pypy-c-jit running 10 iterations of richards:\n\n\n\nWe have to look more closely at various examples, but a few things immediately show up.  One thing is that the GC is put under large pressure by the jit-tracing, jit-optimize and (to a lesser extent) the jit-backend components.  So large in fact that the GC takes at least 60-70% of the time there.  We will have to do something about it at some point.  The other thing is that on richards (and it's likely generally the case), the jit-blackhole component takes a lot of time.  \"Blackholing\" is the operation of recovering from a guard failure in the generated assembler, and falling back to the interpreter.  So this is also something we will need to improve.\n\nThat's it!  The images were generated with the following commands:\n\nPYPYLOG=/tmp/log pypy-c-jit richards.py\npython pypy/tool/logparser.py draw-time /tmp/log --mainwidth=8000 --output=filename.png\n\nEDIT: nowadays the command-line has changed to:python rpython/tool/logparser.py draw-time /tmp/log --mainwidth=8000 filename.png",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/11/hi-all-this-week-i-worked-on-improving-6515977421244851229.html"
    },
    {
      "title": "GC improvements",
      "text": "In the last week, I (Armin) have been taking some time off the\nJIT work to improve our GCs.  More precisely, our GCs now take\none or two words less for every object.  This further reduce the\nmemory usage of PyPy, as we will show at the end.\n\nBackground information: RPython object model\n\nWe first need to understand the RPython object model as\nimplemented by our GCs and our C backend.  (Note that the\nobject model of the Python interpreter is built on top of\nthat, but is more complicated -- e.g. Python-level objects\nare much more flexible than RPython objects.)\n\nConsider these two RPython classes:\n    \n\nclass A:\n    def __init__(self, x):\n        self.x = x\n    def f(self):\n        return self.x * 42\n\nclass B(A):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def f(self):\n        return self.x + self.y\n\n\nThe instances of A and B look like this in memory (all cells\nare one word):\n\n\nGC header\nvtable ptr of A\nhash\nx\n\n\n\nGC header\nvtable ptr of B\nhash\nx\ny\n\n\nThe first word, the GC header, describes the layout.  It\nencodes on half a word the shape of the object, including where it\ncontains further pointers, so that the GC can trace it.  The\nother half contains GC flags (e.g. the mark bit of a\nmark-and-sweep GC).\n\nThe second word is used for method dispatch.  It is similar to a\nC++ vtable pointer.  It points to static data that is mostly a\ntable of methods (as function pointers), containing e.g. the method f\nof the example.\n\nThe hash field is not necessarily there; it is only present in classes\nwhose hash is ever taken in the RPython program (which includes being\nkeys in a dictionary).  It is an \"identity hash\": it works like\nobject.__hash__() in Python, but it cannot just be the address of\nthe object in case of a GC that moves objects around.\n\nFinally, the x and y fields are, obviously, used to store the value\nof the fields.  Note that instances of B can be used in places that\nexpect a pointer to an instance of A.\n\nUnifying the vtable ptr with the GC header\n\nThe first idea of saving a word in every object is the observation\nthat both the vtable ptr and the GC header store information about\nthe class of the object.  Therefore it is natural to try to only have\none of them.  The problem is that we still need bits for the GC flags,\nso the field that we have to remove is the vtable pointer.\n\nThis means that method dispatch needs to be more clever: it\ncannot directly read the vtable ptr, but needs to compute it\nfrom the half-word of the GC header.  Fortunately, this can be\ndone with no extra instruction on the assembler level.  Here is\nhow things will look like in the end, assuming a 32-bit x86\nmachine (but note that as usual we just generate portable C).\n\nThe trick for achieving efficiency is that we store all\nvtables together in memory, and make sure that they don't take\nmore than 256 KB in total (16 bits, plus 2 bits of alignment).\nHere is how the assembler code (produced by the normal C\ncompiler, e.g. gcc) for calling a method looks like.  Before\nthe change:\n\n\nMOV EDX, [EAX + 4]               # load the vtable ptr from object EAX\nMOV EDX, [EDX + method_offset]   # load the function pointer from the vtable\nCALL EDX\n\n\nInstead, we now have:\n\n\nMOVZX EDX, [EAX]     # load the 16-bit part of the GC header from EAX\nMOV EDX, [vtable_start + 4*EDX + method_offset]\nCALL EDX\n\n\nNote that the complex addressing scheme done by the second MOV\nis still just one instruction: the vtable_start and\nmethod_offset are constants, so they are combined.  And as the\nvtables are anyway aligned at a word boundary, we can use\n4*EDX to address them, giving us 256 KB instead of just 64 KB\nof vtables.\n\nOptimizing the hash field\n\nIn PyPy's Python interpreter, all application-level objects\nare represented as an instance of some subclass of W_Root.\nSince all of these objects could potentially be stored in a\ndictionary by the application Python program, all these\nobjects need a hash field.  Of course, in practice, only a\nfraction of all objects in a Python program end up having\ntheir hash ever taken.  Thus this field of W_Root is wasted\nmemory most of the time.\n\n(Up to now, we had a hack in place to save the hash field\non a few classes like W_IntegerObject, but that meant that\nthe Python expression ``object.__hash__(42)'' would raise\na TypeError in PyPy.)\n\nThe solution we implemented now (done by some Java GCs, among\nothers) is to add a hash field to an object when the\n(identity) hash of that object is actually taken.  This means\nthat we had to enhance our GCs to support this.  When objects\nare allocated, we don't reserve any space for the hash:\n\nobject at 0x74B028\n\n...00...\nx\ny\n\n    \nWhen the hash of an object is taken, we use its current memory\naddress, and set a flag in the GC header saying that this\nparticular object needs a hash:\n\nobject at 0x74B028\n\n...01...\nx\ny\n\n\nIf the GC needs to move the object to another memory location,\nit will make the new version of the object bigger, i.e. it\nwill also allocate space for the hash field:\n\nobject at 0x825F60\n\n...11...\nx\ny\n0x74B028\n\n\nThis hash field is immediately initialized with the old memory\naddress, which is the hash value that we gave so far for the\nobject.  To not disturb the layout of the object, we always\nput the extra hash field at the end.  Of course, once set,\nthe hash value does not change even if the object needs to\nmove again.\n\nResults\n\nRunning the following program on PyPy's Python interpreter\nwith n=4000000:\n\n\ndef make_linked_list(n):\n    a = None\n    i = 0\n    while i < n:\n        b = X()\n        b.next = a\n        a = b\n        i += 1\n\n\nthe two optimizations together save 32 MB of RAM (i.e. 8 bytes\nper object).  The version of PyPy we measured this with was built\nas follows:\n\n\n./translate.py --gcremovetypeptr targetpypystandalone --objspace-std-withsharingdict\n\n\nThe total amount of RAM used on a 32-bit Linux is 247 MB,\ncompleting in 10.3 seconds.  On CPython, it consumes 684 MB\nand takes 89 seconds to complete...  This nicely shows that\nour GCs are much faster at allocating objects, and that our\nobjects can be much smaller than CPython's.\n\nArmin Rigo & Carl Friedrich Bolz",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/10/gc-improvements-6174120095428192954.html"
    },
    {
      "title": "First pypy-cli-jit benchmarks",
      "text": "As the readers of this blog already know, I've been working on porting the\nJIT to CLI/.NET for the last months.  Now that it's finally possible to get a\nworking pypy-cli-jit, it's time to do some benchmarks.\nWarning: as usual, all of this has to be considered to be a alpha version:\ndon't be surprised if you get a crash when trying to run pypy-cli-jit.  Of\ncourse, things are improving very quickly so it should become more and more\nstable as days pass.\nFor this time, I decided to run four benchmarks. Note that for all of them we\nrun the main function once in advance, to let the JIT recognize the hot\nloops and emitting the corresponding code.  Thus, the results reported do\nnot include the time spent by the JIT compiler itself, but give a good\nmeasure of how good is the code generated by the JIT.  At this point in time,\nI know that the CLI JIT backend spends way too much time compiling stuff, but\nthis issue will be fixed soon.\n\n\nf1.py: this is the classic PyPy JIT benchmark. It is just a function\nthat does some computational intensive work with integers.\nfloatdemo.py: this is the same benchmark involving floating point\nnumbers that have already been described in a previous blog post.\noodemo.py: this is just a microbenchmark doing object oriented stuff\nsuch as method calls and attribute access.\nrichards2.py: a modified version of the classic richards.py, with a\nwarmup call before starting the real benchmark.\n\n\nThe benchmarks were run on a Windows machine with an Intel Pentium Dual Core\nE5200 2.5GHz and 2GB RAM, both with .NET (CLR 2.0) and Mono 2.4.2.3.\nBecause of a known mono bug, if you use a version older than 2.1 you need\nto pass the option -O=-branch to mono when running pypy-cli-jit, else it\nwill just loop forever.\nFor comparison, we also run the same benchmarks with IronPython 2.0.1 and\nIronPython 2.6rc1.  Note that IronPython 2.6rc1 does not work with mono.\nSo, here are the results (expressed in seconds) with Microsoft CLR:\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\npypy-cli-jit\nipy 2.0.1\nipy 2.6\nipy2.01/ pypy\nipy2.6/ pypy\n\n\n\nf1\n0.028\n0.145\n0.136\n5.18x\n4.85x\n\nfloatdemo\n0.671\n0.765\n0.812\n1.14x\n1.21x\n\noodemo\n1.25\n4.278\n3.816\n3.42x\n3.05x\n\nrichards2\n1228\n442\n670\n0.36x\n0.54x\n\n\n\n\nAnd with Mono:\n\n\n\n\n\n\n\n\n\nBenchmark\npypy-cli-jit\nipy 2.0.1\nipy2.01/ pypy\n\n\n\nf1\n0.042\n0.695\n16.54x\n\nfloatdemo\n0.781\n1.218\n1.55x\n\noodemo\n1.703\n9.501\n5.31x\n\nrichards2\n720\n862\n1.20x\n\n\n\n\nThese results are very interesting: under the CLR, we are between 5x faster\nand 3x slower than IronPython 2.0.1, and between 4.8x faster and 1.8x slower\nthan IronPython 2.6.  On the other hand, on mono we are consistently faster\nthan IronPython, up to 16x.  Also, it is also interesting to note that\npypy-cli runs faster on CLR than mono for all benchmarks except richards2.\nI've not investigated yet, but I think that the culprit is the terrible\nbehaviour of tail calls on CLR: as I already wrote in another blog post,\ntail calls are ~10x slower than normal calls on CLR, while being only ~2x\nslower than normal calls on mono.  richads2 is probably the benchmark that\nmakes most use of tail calls, thus explaining why we have a much better result\non mono than CLR.\nThe next step is probably to find an alternative implementation that does not\nuse tail calls: this probably will also improve the time spent by the JIT\ncompiler itself, which is not reported in the numbers above but that so far it\nis surely too high to be acceptable. Stay tuned.",
      "tags": "cli,jit,pypy",
      "url": "https://www.pypy.org/posts/2009/10/first-pypy-cli-jit-benchmarks-6698484455072589492.html"
    },
    {
      "title": "PyPy's JIT now supports floats",
      "text": "Hello.\n\n\n\nWe've just merged branch which adds float support to x86 backend.\nThis means that floating point operations are now super fast\nin PyPy's JIT. Let's have a look at example, provided by \nAlex Gaynor\nand stolen from Factor blog.\n\n\n\nThe original version of the benchmark, was definitely tuned for the performance needs of CPython.\n\nFor running this on PyPy, I changed to a bit simpler version of the program,\nand I'll explain a few changes that I did, which the reflect current\nlimitations of PyPy's JIT. They're not very deep and they might be\nalready gone while you're reading it:\n\n\n\nUsage of __slots__. This is a bit ridiculous, but we spend quite a bit\n  of time to speed up normal instances of new-style classes which are\n  very fast, yet ones with __slots__ are slower. To be fixed soon.\n\nUsage of reduce. This one is even more obscure, but reduce is not\n  perceived as a thing producing loops in a program. Moving to\n  a pure-Python version of reduce fixes the problem.\n\nUsing x ** 2 vs x * x. In PyPy, reading a local variable is a\n  no-op when JITted (the same as reading local variable in C). However\n  multiplication is simpler operation that power operation.\n\n\n\nI also included the original Java benchmark. Please\nnote that original java version is similar to my modified one\n(not the one specifically tuned for CPython)\n\n\nThe performance figures below (for n = 1 000 000), average of 10 runs:\n\n\nCPython 2.6: 7.56s\nCPython & psyco 2.6: 4.44s\nPyPy: 1.63s\nJava (JVM 1.6, client mode): 0.77s\n\n\n\nand while JVM is much faster, it's very good that we can even compare :-)\n\n\nCheers\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/10/pypys-jit-now-supports-floats-7003493323596806737.html"
    },
    {
      "title": "First results of the JIT",
      "text": "Hi all,\n\nJust a quick note to tell you that we are progressing on the\nJIT front.  Here are the running times of the richards\nbenchmark on my laptop:\n\n8.18 seconds with CPython 2.5.2;\n\n2.61 seconds with pypy-c-jit (3x faster than CPython);\n\n1.04 seconds if you ignore the time spent making assembler (8x faster than CPython);\n\n1.59 seconds on Psyco, for reference (5x faster that CPython).\n\nYes, as this table shows, we are spending 1.57 seconds in the JIT\nsupport code.  That's too much -- even ridiculously so -- for anything but a\nlong-running process.  We are working on that :-)\n\nIf you want to build your own pypy-c-jit (for x86-32 only for now):\n\nyou need a Subversion checkout of trunk;\n\nrun pypy/translator/goal/translate.py with the -Ojit\n  option;\n\nas usual, wait a long time (and be sure you have more than 1GB of RAM).\n\nFor now pypy-c-jit spews a lot of debugging output and\nthere are a few known\nexamples where it crashes.  As we like to repeat, however, it's a complete JIT:\napart from the crashes (the bugs are probably in the JIT support code), it supports the whole Python language from the start -- in the sense of doing correct things.  Future work include\nPython-specific improvements by e.g. tweaking the data structures used to store Python objects so that they are more JIT-friendly.\n\nEDIT: Oh yes, fijal reminds me that CPython 2.6 is 30% faster than CPython 2.5 on this benchmark (which is mostly my \"fault\", as I extracted a small part of PyPy and submitted it as a patch to CPython that works particularly well for examples like richards).  It does not fundamentally change the fact that we are way faster though.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/09/first-results-of-jit-6674537807334018925.html"
    },
    {
      "title": "PyPy sprint in D\u00fcsseldorf, 6 Nov - 13 Nov",
      "text": "The next PyPy sprint will be held in the Computer Science department of\nHeinrich-Heine Universit\u00e4t D\u00fcsseldorf from the 6th to the 13th of\nNovember 2009. This is a fully public sprint, everyone is welcome to\njoin us.\n\nTopics and goals\nAt the sprint we intend to work on the JIT generator in PyPy and on\napplying it to PyPy Python interpreter.\nThe precise work that will be done is not fixed, as we don't know in\nwhich state the JIT will be in November.  However, possible areas of\nwork might include:\n\ntweaking the interpreter/objspace to be more JIT-friendly, e.g.\ninstance implementation code, call code\nif there is interest starting non x86-32 JIT backends\ntrying out existing software to find features where the optimizations\nof the JIT could be improved\nimproving our benchmarking infrastructure\n\nWe will give special priority to topics that \"non-core\" people find\ninteresting (as long as they are somehow JIT-related).\nFor an introduction of how our JIT-generation process works, please\nrefer to our blog:\nhttps://morepypy.blogspot.com/2009/03/jit-bit-of-look-inside.html\nThere is also a more dense academic paper about the subject:\nhttps://codespeak.net/svn/pypy/extradoc/talk/icooolps2009/bolz-tracing-jit-final.pdf\n\n\nLocation\nThe sprint will take place in a seminar room of the computer science\ndepartment.  It is in the building 25.12 of the university campus. For\ntravel instructions see\n\nhttps://stups.cs.uni-duesseldorf.de/anreise/esbahn.php\n\n\nRegistration\nIf you'd like to come, please subscribe to the pypy-sprint mailing\nlist and drop a note about your interests and post any questions.\nMore organisational information will be send to that list.  We'll keep a\nlist of people which we'll update (which you can do so yourself if\nyou have codespeak commit rights).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/09/pypy-sprint-in-dusseldorf-6-nov-13-nov-8153983964308175836.html"
    },
    {
      "title": "PyPy gets a new compiler",
      "text": "Today, I merged the parser-compiler branch, which I have been working on over the summer. It contained a total rewrite of both PyPy's Python parser and AST compiler. PyPy's old parser was (in)famous internally for being complicated and slow (with many algorithmic complexities greater than O(n)). The new parser is a simple as I could make it LL(1) parser like CPython (though it doesn't share the hacks of CPython's parser).\n\nThe new compiler is based on the Abstract Syntax Trees (AST) that CPython 2.5 introduced instead of PyPy's old AST based on the compiler package's. This means that Python code running on PyPy will be able to use the same _ast interface as CPython. PyPy's _ast implementation supports AST features that CPython 2.6 added, including compiling modified AST to bytecode and executing it. In this rewrite, some more obscure compiler features were added, too. For example, jumps in bytecode can now be greater than 65535 bytes! (That's like an if statement with 7000 lines of code in the body.)\n\nWhile the PyPy translation toolchain still has many obscure details and hacks, this merge completes the process of making the actual Python interpreter very clean. Hopefully, this will make adding new features much easier and make PyPy less frustrating to maintain as well as providing application level code with an improved AST interface!",
      "tags": "compiler,parser,speed",
      "url": "https://www.pypy.org/posts/2009/08/pypy-gets-new-compiler_25-6401910947439531107.html"
    },
    {
      "title": "Gothenburg JIT sprint report",
      "text": "Finally, we managed to squeeze in some time to write a report about what\nhas been going on the mysterious JIT sprint in Gothenburg, Sweden.\nThe main goals of the sprint were to lay down the groundwork for getting\nmore JIT work going in the next months and get more of PyPy developers\nup to speed with the current state of the JIT. One of the elements was\nto get better stability of the JIT, moving it slowly from being a prototype to\nactually work nicely on larger programs.\n\nThe secret goal of the sprint was to seek more speed, which Anto and\nCarl Friedrich did even during the break day:\n\n\nWe spent the first two days improving test coverage of the x86 backend\nand the optimizer. Now we have 100% coverage with unittests\n(modulo figleaf bugs), which does not mean anything, but it's better\nthan before.\n\nThen we spent quite some time improving the optimizer passes, so\nnow we generate far less code than before the sprint, because a lot of\nit is optimized away. On the interpreter side, we marked more objects\n(like code objects) as immutable, so that reading fields from them\ncan be constant-folded.\nAnother important optimization that we did is to remove consecutive\nreading of the same fields from the same structure, if no code in between\ncan change it.\nOur JIT is a hybrid environment, where only hot loops of code are jitted\nand the rest stays being interpreted. We found out that the performance\nof the non-jitted part was suboptimal, because all accesses to python\nframes went through an extra layer of indirection. We removed this layer\nof indirection, in the case where the jit and the interpreter cannot\naccess the same frame (which is the common case).\nWe also spent some time improving the performance of our x86 backend,\nby making it use more registers and by doing more advanced variable\nrenaming at the end of loops. It seems that using more registerd is not as\nmuch of a win as we hoped, because modern day processors are much\nsmarter than we thought.\nThe most mind bending part was finding why we loose performance by\nmaking the JIT see more of the interpreter. It took us two very frustrating\ndays and 36 gray hairs to find out that from the JIT we call a different malloc\nfunction in the Boehm GC, which is by far slower than the version that\nwe use from the interpreter. This meant that the more we jitted, the\nslower our code got, purely because of the mallocs.\nNow that this is fixed, the world makes much more sense again.\nA lot of the sprint's work is not directly measurable in the performance\nfigures, but we did a lot of work that is necessary for performance to\nimprove in the next weeks. After we have done a bit more work, we should\nbe able to provide some performance figures for programs that are\nmore realistic than just loops that count to ten millions (which are\nvery fast already :).\nNow we're going to enjoy a couple of days off to recover from the sprint.\nB\u00e4sta h\u00e4lsningar,\nCarl Friedrich, fijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/08/gothenburg-jit-sprint-report-3309138497953458138.html"
    },
    {
      "title": "PyPy numeric experiments",
      "text": "Because PyPy will be presenting at the upcoming euroscipy conference, I have been playing recently with the idea of NumPy and PyPy integration. My idea is to integrate PyPy's JIT with NumPy or at least a very basic subset of it.  Time constraints make it impossible to hand write a JIT compiler that understands NumPy. But given PyPy's architecture we actually have a JIT generator, so we don't need to write one :-)\n\n\n\nOur JIT has shown that it can speed up small arithmetic examples significantly. What happens with something like NumPy?\n\n\nI wrote a very minimal subset of NumPy in RPython, called micronumpy (only single-dimension int arrays that can only get and set items), and a benchmark against it. The point of this benchmark is to compare the performance of a builtin function (numpy.minimum) against the equivalent hand-written function, written in pure Python and compiled by our JIT.\n\n\nThe goal is to prove that it is possible to write algorithms in Python instead of C without loss of efficiency. Sure, we can write some functions (like minimum in the following example), but there is a whole universe of other ufuncs which would be cool to have in Python instead, assuming this could be done without a huge loss in efficiency.\n\n\nHere are the results. This is comparing PyPy svn revision 66303 in the pyjitpl5 branch against python 2.6 with NumPy 1.2.1. The builtin numpy.minimum in PyPy is just a naive implementation in RPython, which is comparable to the speed of a naive implementation written in C (and thus a bit slower than the optimized\nversion in NumPy):\n\n\n\nNumPy (builtin function)0.12s\nPyPy's micronumpy (builtin function)0.28s\nCPython (pure Python)11s\nPyPy with JIT (pure Python)0.91s\n\n\nAs we can see, PyPy's JIT is slower than the optmized NumPy's C version, but still much faster than CPython (12x).\n\n\nWhy is it slower? When you actually look at assembler, it's pretty obvious that it's atrocious. There's a lot of speedup to be gained out of just doing simple optimizations on resulting assembler. There are also pretty obvious limitations, like x86 backend not being able to emit opcodes for floats or x86_64 not being there. Those limitations are not fundamental in any sense and can be relatively straightforward to overcome. Therefore it seems we can get C-level speeds for pure Python implementations of numeric algorithms using NumPy arrays in PyPy. I think it's an interesting perspective that Python has the potential of becoming less of a glue language and more of a real implementation language in the scientific field.\n\nCheers,\nfijal",
      "tags": "numpy",
      "url": "https://www.pypy.org/posts/2009/07/pypy-numeric-experiments-2221073696038673235.html"
    },
    {
      "title": "ECOOP 2009",
      "text": "Last week (from 6th to 10th of July) Anto, Armin and me (Carl Friedrich) were in\nthe magnificent city of Genova, Italy at the ECOOP conference. In this blog\npost I want to give a (necessarily personal) account of what we did there.\n\nWorkshop days: ICOOOLPS\nThe first two days of the conference were the workshop days. On Monday we\nattended the ICOOOLPS workshop, (see the programme of the workshop). We\nhad gotten two papers accepted at the workshop (one about layering PyPy's JIT\non top of the CLR and one about the basic idea of PyPy's tracing JIT) and\nthus gave two presentations at the workshop, one was given by Anto, the other\nby me. Both went reasonably well, we got some positive feedback.\nNearly all the other talks were rather interesting as well. I particularly liked\nthe one by Hans Schippers, who presented a machine model built on delegation\ncalled delMDSOC.  The model is meant implement most features that a language\nwould need that makes it possible to separate cross-cutting concerns. In the\ntalk at ICOOOLPS he presented an extension to the model that adds concurrency\nsupport, using a combination of actors and coroutines. He then showed that the\nconcurrency mechanisms of Java, Salsa (and extension of Java adding actors) and\nIo can be mapped to this model.\nFurthermore there were two interesting invited talks, one by Andreas Gal\n(Mozilla), and one by Cliff Click (Azul Systems). Andreas explained how\nTraceMonkey works. This was very useful for me, because his talk was just before\nmine and I could thus kill most of my introduction about tracing JIT compilers\nand have more time for the really interesting stuff :-).  Cliff talked about\nimplementing other languages on top of the JVM and some of the pitfalls in\ngetting them perform well.\nAll in all, ICOOOLPS was a very enjoyable workshop, also with many interesting\ndiscussions.\nOn Tuesday there were more workshops, but also the PyPy tutorial, so I only went\nto a few talks of the COP workshop and spent the rest of the morning\npreparing the tutorial (see next section).\n\n\nTutorial\nOn Tuesday afternoon we gave a PyPy Tutorial, as part of the ECOOP summer\nschool. The first lesson we learned was that (as opposed to a community\nconference) people don't necessarily want to actually take their laptop out and\ntry stuff. We gave a slow walk-through about the full life-cycle of development\nof a dynamic language interpreter using PyPy's tool-chain: Starting from writing\nyour interpreter in RPython, testing it on top of CPython to translating it to\nC, .NET or Java to actually adding hints to get a JIT inserted.\nThere were about seven people attending the tutorial, a couple of which were\nvery interested and were asking questions and discussing. Some of the\ndiscussions were even very technical, e.g. one about the details of our\ntype-inference algorithm for RPython and why we cannot do a bottom-up analysis\nbut have to use forward-propagation instead.\nJan Vitek of Purdue University told of some of the problems of the OVM\nproject, which is (among other things) a Java implementation in Java (OVM also\nwants to support implementing VMs for other languages with it, if I understood\ncorrectly). He said that the project has\nessentially gotten too large and complicated, which means that it is very hard\nfor new people to get into the project. While PyPy doesn't have some of the\nproblems of a full Java implementation (e.g. right now our concurrency support\nis minimal) I definitely think that some of these risks apply to PyPy as well\nand we should find ways to improve the situation in this regard. Channeling\nSamuele: Somewhere inside the large lumbering blob of PyPy there is an elegant\ncore trying to get out.\n\n\nMain Conference\nFrom Wednesday till Friday the main conference was happening. Many of the\ntalks were not all that interesting for me, being quite Java centric. One talk\nthat I liked a lot was \"Making Sense of Large Heaps\", which was presented by\nNick Mitchell (IBM). He presented a tool called \"Yeti\" that can be used to\nanalyze large heaps of Java programs. The tool uses some clever algorithms and\nheuristics to summarize the heap usage of data structures in intelligent ways to\nmake it easier to find possible memory-wasters in a program. Nick also gave Anto\nand me a demo of the tool, where we tried to apply it to pypy-jvm (we found\nout that a fifth of the static data in there belongs to the parser/compiler :-(\n).\nOn each of the days of the conference there was a keynote. I missed the one by\nSimon Peyton-Jones on Wednesday about type classes in Haskell. On Thursday,\nDavid Ungar was awarded the Dahl-Nygaard-Prize for his work on the Self\nprogramming language. Subsequently he gave a really inspiring keynote with the\ntitle \"Self and Self: Whys and Wherefores\" where he recollected Self's history,\nboth on a technical as well as on a social level. Parts of the talk were\nsnippets from the movies Self: The Movie and Alternate Reality Kit, both\nof which I highly recommend.\nThe keynote on Friday was by Cliff Click with the title \"Java on 1000 Cores:\nTales of Hardware/Software Co-design\". He described the custom CPU architecture\nthat Azul Systems has developed to run Java server applications on hundreds of\ncores. The talk mostly talked about the hardware, which I found very interesting\n(but some people didn't care for too much). Azul's CPU is essentially 54 in-order\nRISC cores in a single processor. The cores have a lot of extensions that make\nit easier to run Java on them, e.g. hardware read- and write-barriers,\nhardware-transactional-memory and hardware escape-detection (!).\nIn addition to the talks, there is of course always the hallway track (or coffee\ntrack) which is the track where you stand in the hallway and discuss with\npeople. As usual, this was the most interesting part of the conference. One of\nthose talks was Anto and me giving a PyPy demo to David Ungar. We had a very\ninteresting discussion about VM implementation in general and the sort of\ndebugging tools you need to write in particular. He liked PyPy a lot, which\nmakes me very happy. He also liked the fact that I have actually read most Self\npapers :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/07/ecoop-2009-8415055006373020774.html"
    },
    {
      "title": "EuroPython",
      "text": "EuroPython is coming.  We have two 30-minutes talks that we will present.  In addition, the sprint takes place the 29th of June (there will be no-one from the team on the 28th of June), as well as on the 3rd and 4th of July.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/06/europython-8318355560715932819.html"
    },
    {
      "title": "JIT progress",
      "text": "In the last days I finally understood how to do virtualizables.  Now the frame overhead is gone. This was done with the help of discussion with Samuele, porting ideas from PyPy's first JIT attempt.\n\n\nThis is of course work in progress, but it works in PyPy (modulo a few XXXs, but no bugs so far).  The performance of the resulting code is quite good: even with Boehm (the GC that is easy to compile to but gives a slowish pypy-c), a long-running loop typically runs 50% faster than CPython.  That's \"baseline\" speed, moreover: we will get better speed-ups by applying optimizations on the generated code.  Doing so is in progress, but it suddenly became easier because that optimization phase no longer has to consider virtualizables -- they are now handled earlier.\n\nUpdate:Virtualizables is basically a way to avoid frame overhead. The frame object\nis allocated and has a pointer, but the JIT is free to unpack it's fields (for example python\nlevel locals) and store them somewhere else (stack or registers). Each external (out of jit) access\nto frame managed by jit, needs to go via special accessors that can ask jit where those variables\nare.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/06/jit-progress-7289127796450840053.html"
    },
    {
      "title": "News from the jit front",
      "text": "As usual, progress is going slower then predicted,\nbut nevertheless, we're working hard to make some progress.\n\n\nWe recently managed to make our nice GCs cooperate with our JIT. This is\none point from our detailed plan. As of now, we have a JIT with GCs and\nno optimizations. It already speeds up some things, while slowing down\nothers. The main reason for this is that the JIT generates assembler which is kind\nof ok, but it does not do the same level of optimizations gcc would do.\n\n\nSo the current status of the JIT is that it can produce assembler out\nof executed python code (or any interpreter written in RPython actually),\nbut the results are not high quality enough since we're missing optimizations.\n\n\nThe current plan, as of now, looks as follows:\n\n\nImprove the handling of GCs in JIT with inlining of malloc-fast\n  paths, that should speed up things by a constant, not too big factor.\n\n\nWrite a simplified python interpreter, which will be a base for experiments\n  and to make sure that our JIT does correct things with regard to\n  optimizations. That would work as mid-level integration test.\n\n\nThink about ways to inline loop-less python functions into their parent's loop.\n\n\nGet rid of frame overhead (by virtualizables)\n\n\nMeasure, write benchmarks, publish\n\n\nProfit\n\n\n\nCheers,\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/06/news-from-jit-front-367552118380842303.html"
    },
    {
      "title": "ICOOOLPS Submissions",
      "text": "Both of the papers that people from the PyPy team submitted to ICOOOLPS have\nbeen accepted. They are:\n\n\n\"Faster than C#: efficient implementation of dynamic languages on .NET\"\n(pdf1) by Armin, Anto and Davide Ancona, who is Anto's Ph.D. advisor\n\"Tracing the Meta-Level: PyPy\u2019s Tracing JIT Compiler\" (pdf2) by Carl\nFriedrich, Armin, Anto and Maciek\n\n\n(the pdfs are obviously the submitted versions, not the final ones).\nThis year ICOOOLPS (Implementation, Compilation, Optimization of\nObject-Oriented Languages, Programs and Systems) is being held on July the 6th\nat ECOOP 2009 in Genova, Italy.  Other than these two papers, Anto and Carl\nFriedrich will also present a PyPy tutorial, on July the 7th.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/05/icooolps-submissions-6705901656116873587.html"
    },
    {
      "title": "4 weeks of GDB",
      "text": "Hello.\n\nSo, according to our jit\nplan we're mostly done with point 1, that is to provide a JIT that compiles\npython code to assembler in the most horrible manner possible but doesn't\nbreak. That meant mostly 4 weeks of glaring at GDB and megabytess of assembler\ngenerated by C code generated from python code. The figure of 4 weeks proves\nthat our approach is by far superior to the one of psyco, since Armin says it's\n\"only 4 weeks\" :-)\n\n\nRight now, pypy compiled with JIT can run the whole CPython test suite\nwithout crashing, which means we're done with obvious bugs and the only\nones waiting for us are really horrible.  (Or they really don't exist.\nAt least they should never be about obscure Python corner cases: they can\nonly be in the 10'000 lines of relatively clear code that is our JIT\ngenerator.)\n\n\nBut... the fun thing is that we can actually concentrate on optimizations!\nSo the next step is to provide a JIT that is correct *and* actually speeds\nup python. Stay tuned for more :-)\n\nCheers,\nfijal, armin & benjamin\n\nUPDATE: for those of you blessed with no knowledge of C, gdb stands for GNU debugger, a classic debugger for C. (It's also much more powerful than python debugger, pdb, which is kind of surprising).",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/04/4-weeks-of-gdb-522864241041643529.html"
    },
    {
      "title": "1.1 final released",
      "text": "We just released PyPy 1.1 final. Not much changed since the beta, apart\nfrom some more fixed bugs. Have fun with it!",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2009/04/11-final-released-225813777919757859.html"
    },
    {
      "title": "Roadmap for JIT",
      "text": "Hello.\n\n\nFirst a disclaimer. This post is more about plans for future than current\nstatus. We usually try to write about things that we have done, because\nit's much much easier to promise things than to actually make it happen,\nbut I think it's important enough to have some sort of roadmap.\n\n\nIn recent months we came to the point where the 5th generation of\nJIT prototype was working as nice\nor even a bit nicer than 1st one back in 2007. Someone might ask \"so why\ndid you spend all this time without going forward?\". And indeed, we spend\na lot of time moving sideways, but as posted, we also spent a lot of time\ndoing some other things, which are important as well.\nThe main advantage of current JIT incarnation is much much simpler than\nthe first one. Even I can comprehend it, which is much of an improvement :-)\n\n\nSo, the prototype is working and gives very nice speedups in range of 20-30x\nover CPython. We're pretty confident this prototype will work and will\nproduce fast python interpreter eventually. So we decided that now we'll\nwork towards changing prototype into something stable and solid. This\nmight sound easy, but in fact it's not. Having stable assembler backend\nand optimizations that keep semantics is not as easy as it might sound.\n\n\nThe current roadmap, as I see it, looks like as following:\n\n\n Provide a JIT that does not speedup things, but produce assembler without\n  optimizations turned on, that is correct and able to run CPython's library\n  tests on a nightly basis.\n\n\n Introduce simple optimizations, that should make above JIT a bit faster than\n  CPython. With optimizations disabled JIT is producing incredibly dumb\n  assembler, which is slower than correspoding C code, even with removal\n  of interpretation overhead (which is not very surprising).\n\n\n Backport optimizations from JIT prototype, one by one, keeping an eye\n  on how they perform and making sure they don't break anything.\n\n\n Create new optimizations, like speeding up attribute access.\n\n\n Profit.\n\n\n\nThis way, we can hopefully provide a working JIT, which gives fast python\ninterpreter, which is a bit harder than just a nice prototype.\n\n\nTell us what you think about this plan.\n\nCheers,\nfijal & others.",
      "tags": "jit,pypy,roadmap,speed",
      "url": "https://www.pypy.org/posts/2009/04/roadmap-for-jit-377358891902851723.html"
    },
    {
      "title": "Leysin Sprint Report",
      "text": "The Leysin sprint is nearing its end, as usual here is an attempt at a summary\nof what we did.\nRelease Work\nLarge parts of the sprint were dedicated to fixing bugs. Since the easy bugs\nseem to have been fixed long ago, those were mostly very annoying and hard bugs.\nThis work was supported by our buildbots, which we tried to get free of\ntest-failures. This was worked on by nearly all participants of the sprint\n(Samuele, Armin, Anto, Niko, Anders, Christian, Carl Friedrich). One\nparticularly annoying bug was the differences in the tracing events that PyPy\nproduces (fixed by Anders, Samuele and Christian). Some details about larger\ntasks are in the sections below.\nThe work culminated in the beta released on Sunday.\n\nStackless\nA large number of problems came from our stackless features, which do some\nadvanced things and thus seem to contain advanced bugs. Samuele and Carl\nFriedrich spent some time fixing tasklet pickling and unpickling. This was\nachieved by supporting the (un)pickling of builtin code objects. In addition\nthey fixed some bugs in the finalization of tasklets. This needs some care\nbecause the __del__ of a tasklet cannot run at arbitrary points in time, but\nonly at safe points. This problem was a bit subtle to get right, and popped up\nnearly every morning of the sprint in form of a test failure.\nArmin and Niko added a way to restrict the stack depth of the RPython-level\nstack. This can useful when using stackless, because if this is not there it is\npossible that you fill your whole heap with stack frames in the case of an\ninfinite recursion. Then they went on to make stackless not segfault when\nthreads are used at the same time, or if a callback from C library code is in\nprogress. Instead you get a RuntimeError now, which is not good but better\nthan a segfault.\n\n\n\nKilling Features\nDuring the sprint we discussed the fate of the LLVM and the JS backends. Both\nhave not really been maintained for some time, and even partially untested\n(their tests were skipped). Also their usefulness appears to be limited. The JS\nbackend is cool in principle, but has some serious limitations due to the fact\nthat JavaScript is really a dynamic language, while RPython is rather static.\nThis made it hard to use some features of JS from RPython, e.g. RPython does not\nsupport closures of any kind.\nThe LLVM backend had its own set of problems. For\na long time it produced the fastest form of PyPy's Python interpreter, by first\nusing the LLVM backend, applying the LLVM optimizations to the result, then\nusing LLVM's C backend to produce C code, then apply GCC to the result :-).\nHowever, it is not clear that it is still useful to directly produce LLVM\nbitcode, since LLVM has rather good C frontends nowadays, with llvm-gcc and\nclang. It is likely that we will use LLVM in the future in our JIT (but that's\nanother story, based on different code).\nTherefore we decided to remove these two backends from SVN, which Samuele and\nCarl Friedrich did. They are not dead, only resting until somebody who is\ninterested in maintaining them steps up.\n\n\nWindows\nOne goal of the release is good Windows-support. Anders and Samuele set up a new\nwindows buildbot which revealed a number of failures. Those were attacked by\nAnders, Samuele and Christian as well as by Amaury (who was not at the sprint,\nbut thankfully did a lot of Windows work in the last months).\n\n\nOS X\nChristian with some help by Samuele tried to get translation working again under\nMac OS X. This was a large mess, because of different behaviours of some POSIX\nfunctionality in Leopard. It is still possible to get the old behaviour back,\nbut whether that was enabled or not depended on a number of factors such as\nwhich Python is used. Eventually they managed to successfully navigate that maze\nand produce something that almost works (there is still a problem remaining\nabout OpenSSL).\n\n\nDocumentation\nThe Friday of the sprint was declared to be a documentation day, where (nearly)\nno coding was allowed. This resulted in a newly structured and improved getting\nstarted document (done by Carl Friedrich, Samuele and some help of Niko) and\na new document describing differences to CPython (Armin, Carl Friedrich) as\nwell as various improvements to existing documents (everybody else). Armin\nundertook the Sisyphean task of listing all talks, paper and related stuff\nof the PyPy project.\n\n\n\nVarious Stuff\n\nJava Backend Work\nNiko and Anto worked on the JVM backend for a while. First they had to fix\ntranslation of the Python interpreter to Java. Then they tried to improve the\nperformance of the Python interpreter when translated to Java. Mostly they did a\nlot of profiling to find performance bottlenecks. They managed to improve\nperformance by 40% by overriding fillInStackTrace of the generated exception\nclasses. Apart from that they found no simple-to-fix performance problems.\n\n\nJIT Work\nArmin gave a presentation about the current state of the JIT to the sprinters as\nwell as Adrian Kuhn, Toon Verwaest and Camillo Bruni of the University of Bern\nwho came to visit for one day. There was a bit of work on the JIT going on too;\nArmin and Anto tried to get closer to having a working JIT on top of the CLI.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/04/leysin-sprint-report-1416905818217912359.html"
    },
    {
      "title": "Beta for 1.1.0 released",
      "text": "Today we are releasing a beta of the upcoming PyPy 1.1 release. There\nare some Windows and OS X issues left that we would like to address\nbetween now and the final release but apart from this things should be\nworking. We would appreciate feedback.\nThe PyPy development team.\n\nPyPy 1.1: Compatibility & Consolidation\nWelcome to the PyPy 1.1 release - the first release after the end of EU\nfunding. This release focuses on making PyPy's Python interpreter more\ncompatible with CPython (currently CPython 2.5) and on making the\ninterpreter more stable and bug-free.\nPyPy's Getting Started lives at:\n\nhttps://codespeak.net/pypy/dist/pypy/doc/getting-started.html\n\nHighlights of This Release\n\n\nMore of CPython's standard library extension modules are supported,\namong them ctypes, sqlite3, csv, and many more. Most of these extension modules\nare fully supported under Windows as well.\nhttps://codespeak.net/pypy/dist/pypy/doc/cpython_differences.html\nhttps://morepypy.blogspot.com/2008/06/pypy-improvements.html\n\nThrough a large number of tweaks, performance has been improved by\n10%-50% since the 1.0 release. The Python interpreter is now between\n0.8-2x (and in some corner case 3-4x) slower than CPython. A large\npart of these speed-ups come from our new generational garbage\ncollectors.\nhttps://codespeak.net/pypy/dist/pypy/doc/garbage_collection.html\n\nOur Python interpreter now supports distutils as well as\neasy_install for pure-Python modules.\n\nWe have tested PyPy with a number of third-party libraries. PyPy can\nrun now: Django, Pylons, BitTorrent, Twisted, SymPy, Pyglet, Nevow,\nPinax:\nhttps://morepypy.blogspot.com/2008/08/pypy-runs-unmodified-django-10-beta.html\nhttps://morepypy.blogspot.com/2008/07/pypys-python-runs-pinax-django.html\nhttps://morepypy.blogspot.com/2008/06/running-nevow-on-top-of-pypy.html\n\nA buildbot was set up to run the various tests that PyPy is using\nnightly on Windows and Linux machines:\nhttps://codespeak.net:8099/\n\nSandboxing support: It is possible to translate the Python\ninterpreter in a special way so that the result is fully sandboxed.\nhttps://codespeak.net/pypy/dist/pypy/doc/sandbox.html\nhttps://blog.sandbox.lt/en/WSGI%20and%20PyPy%20sandbox\n\n\n\n\n\nOther Changes\n\n\nThe clr module was greatly improved. This module is used to\ninterface with .NET libraries when translating the Python\ninterpreter to the CLI.\nhttps://codespeak.net/pypy/dist/pypy/doc/clr-module.html\nhttps://morepypy.blogspot.com/2008/01/pypynet-goes-windows-forms.html\nhttps://morepypy.blogspot.com/2008/01/improve-net-integration.html\n\nStackless improvements: PyPy's stackless module is now more\ncomplete. We added channel preferences which change details of the\nscheduling semantics. In addition, the pickling of tasklets has been\nimproved to work in more cases.\n\nClassic classes are enabled by default now. In addition, they have\nbeen greatly optimized and debugged:\nhttps://morepypy.blogspot.com/2007/12/faster-implementation-of-classic.html\n\nPyPy's Python interpreter can be translated to Java bytecode now to\nproduce a pypy-jvm. At the moment there is no integration with\nJava libraries yet, so this is not really useful.\n\nWe added cross-compilation machinery to our translation toolchain to\nmake it possible to cross-compile our Python interpreter to Nokia's\nMaemo platform:\nhttps://codespeak.net/pypy/dist/pypy/doc/maemo.html\n\nSome effort was spent to make the Python interpreter more\nmemory-efficient. This includes the implementation of a mark-compact\nGC which uses less memory than other GCs during collection.\nAdditionally there were various optimizations that make Python\nobjects smaller, e.g. class instances are often only 50% of the size\nof CPython.\nhttps://morepypy.blogspot.com/2008/10/dsseldorf-sprint-report-days-1-3.html\n\nThe support for the trace hook in the Python interpreter was\nimproved to be able to trace the execution of builtin functions and\nmethods. With this, we implemented the _lsprof module, which is\nthe core of the cProfile module.\n\nA number of rarely used features of PyPy were removed since the previous\nrelease because they were unmaintained and/or buggy. Those are: The\nLLVM and the JS backends, the aspect-oriented programming features,\nthe logic object space, the extension compiler and the first\nincarnation of the JIT generator. The new JIT generator is in active\ndevelopment, but not included in the release.\nhttps://codespeak.net/pipermail/pypy-dev/2009q2/005143.html\nhttps://morepypy.blogspot.com/2009/03/good-news-everyone.html\nhttps://morepypy.blogspot.com/2009/03/jit-bit-of-look-inside.html\n\n\n\n\n\nWhat is PyPy?\nTechnically, PyPy is both a Python interpreter implementation and an\nadvanced compiler, or more precisely a framework for implementing dynamic\nlanguages and generating virtual machines for them.\nThe framework allows for alternative frontends and for alternative\nbackends, currently C, Java and .NET.  For our main target \"C\", we can\n\"mix in\" different garbage collectors and threading models,\nincluding micro-threads aka \"Stackless\".  The inherent complexity that\narises from this ambitious approach is mostly kept away from the Python\ninterpreter implementation, our main frontend.\nSocially, PyPy is a collaborative effort of many individuals working\ntogether in a distributed and sprint-driven way since 2003.  PyPy would\nnot have gotten as far as it has without the coding, feedback and\ngeneral support from numerous people.\nHave fun,\n\nthe PyPy release team, [in alphabetical order]\nAmaury Forgeot d'Arc, Anders Hammerquist, Antonio Cuni, Armin Rigo,\nCarl Friedrich Bolz, Christian Tismer, Holger Krekel,\nMaciek Fijalkowski, Samuele Pedroni\nand many others:\nhttps://codespeak.net/pypy/dist/pypy/doc/contributor.html",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2009/04/beta-for-110-released-4604559533184706699.html"
    },
    {
      "title": "Leysin Sprint Started",
      "text": "The Leysin Sprint started today. The weather is great and the view is wonderful, as usual. Technically we are working on the remaining test failures of the nightly test runs and are generally trying to fix various long-postponed bugs. I will try to give more detailed reports as the sprint progresses.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/04/leysin-sprint-started-4551365436232104640.html"
    },
    {
      "title": "Pycon videos are online",
      "text": "Hi.\n\nWe didn't yet write full pycon summary, but both of our talks are now online: PyPy status talk and python in a sandbox.\nUpdate:\nSlides are also available: PyPy status talk and Python in a sandbox.\n\n\nEnjoy!\nfijal & holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/04/pycon-videos-are-online-909873128878039557.html"
    },
    {
      "title": "VM summit: nice to see friendly competition",
      "text": "So Google has launched the unladen swallow project\nwith this first goal: \n\n\n    Produce a version of Python at least 5x faster than CPython.\n\n\nWe discussed some details with Collin Winter,  Jeffrey Yasskin and Thomas Wouters\nduring the VM summit yesterday. We were a bit confused about usage\nof the term JIT, because as far as we understood, it's going to be upfront\ncompilation into LLVM.  In the past we have looked into LLVM\n \u2013  at one point PyPy extensively use it but it\nwasn't clear how we could make good use to it. \nThey also consider changing to something else than LLVM.  It's gonna be \ninteresting to see how this works out. \n\n\nIt's good to see friendly competition, and we think that should take up\nthe challenge and see if we can produce faster pickling, run 2to3 and \nDjango faster than what they can come up with.  We also talked \nto IronPython and Jython developers and all agreed that some\ncommon benchmarks would be good.  And maybe do weekly\npress releases about small speed increases? :) \n\n\nThe idea of the VM summit here in Chicago was to bring together implementors\nof various virtual machine languages.  There were members of the communities of\nIronPython, CPython, GemStone's MagLev, Rubinius, Mozilla's TraceMonkey, Parrot, \nSun's Da Vinci Machine, Microsoft's DLR, Jython and JRuby.\nEverybody got to talk 5-10 minutes on their current status and \nchallenges.  It is clear that you cannot begin to cover the \ncomplexities and architectures of the involved projects. \nBut that wasn't too much of a problem because the rest of\nthe day everybody freely and dynamically grouped on their\nissues of choice.  We established some more personal contacts,\nwas great to chat with people like Andreas Gal from the University of \nCalifornia, Irvine, who have a very similar idea about the JIT\nthat we have.  Actually, we could probably haved mixed our\ntwo presentations and nobody would have actually noticed :-).\n\n\nAt the end of the presentation part, John Rose presented his\nslides. John is a Hotspot developer, and while not precisely a dynamic\nlanguage implementor, he has a lot of experience in virtual\nmachine implementation. It's very good to see the JVM being extended towards\nsupporting dynamic-language specific things, in order to be something\nmore than just a good platform for Java.  We'll probably have \nsome extra meetup with him the next days. \n\ncheers, \nholger and fijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/03/vm-summit-nice-to-see-friendly-8755773725359396485.html"
    },
    {
      "title": "PyPy talk at OpenBossa 09",
      "text": "Yesterday i gave my PyPy status/mobile perspectives at OpenBossa, Nokia's developer conference for embedded platforms in Brazil.  Found it a bit of a tough task to do that in 50 minutes.  I had some 50, later more developers attending the talk and was happy with the questions and the feedback.  Guess it's a good sign if the number of people grows during a talk :)  It was the first time i tried to work more with pictures and actually used some devianart photos from Marikaz to mark section transitions.  I summarize/highlight some key points here in the post.\nAfter intro and 2.5 compatibility status, i talked about our measurements of PyPy's Python on Nokia's N810 internet tablet. The best bit is that for almost all Python data structures PyPy has smaller memory representations than CPython.  Particularly good are class instances which often score at 50% of CPython's sizes.  Startup time is also often better and can be improved.  On the bad side, PyPy's quite large base interpreter size and its bytecode execution is often worse. In the talk i also outline ideas for \"perfect PYC files\" for minimizing module import times and maximizing sharing across interpreter processes. I also briefly discussed the PyPy situation with extension modules and regarding C++ libs.  Most of these ideas arose from sprint discussions last year.  In the morning i also had some good talk with Stefan Seefeld about Boost Python and the new QT4 bindings.   Maybe to use Boost Python is also a good opportunity - but PyPy does not currently have a C-level or C++ level API.\nIn subsequent lunch discussions people agreed that PyPy has three main interesting areas currently:\n\nthe Python Just-In-Time Compiler\na virtualized, sandboxed Python interpreter\nan efficient Python interpreter for small devices\n\nI think our upcoming 1.1 release will be a good point in time for many people to look some more into PyPy.  I hope we are crossing the chasm soon.  It's been a while since the project started :)  Getting some more sponsoring to sustain and increase our current efforts probably wouldn't hurt.\nNow i am off to spend my last day in Recife / Brazil, fly back to Germany in the evening and then spend time on preparing for Pycon 2009. And I guess i am going to enjoy some naturally cold air - at least my two jogging sessions at Brazillian beaches, at a sustained 30 degrees celsius, were tough.  I guess i shouldn't complain, though :)\nWas great meeting all the brazillian guys and the few women - just had breakfeast with Kate Alhola, kernel hacker and working on the new \"Freemantle\" graphical platform.  Many thanks go to Marcio Marcedo and the Python team at INDT who invited me here.  Hope to come again next year and eventually talk more about the Zone VM :)\nIf you are interested in some more not so pypy-specific bits about the conference and what i experienced, you might head over to my tetamap blog.\nholger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/03/pypy-talk-at-openbossa-09-5135830287297423499.html"
    },
    {
      "title": "Good news everyone!",
      "text": "A quick update from the JIT front. As of yesterday, we're now able to translate\na highly-experimental Python interpreter that contains JIT. It mostly crashes\nimmediately, mostly due to some unsupported operations in the assembler backend,\nbut for a carefully crafted program, we're able to get massive speedups.\nFor something as complex as:\n\n\n  i = 0\n  while i < 10000000:\n   i = i + 1\n\n\nour JIT is about 20x faster than CPython. That's still about 3x slower than\nPsyco, but looking at assembler code it's obvious that we can speed it up\na lot. These are very good news, since we don't encode python semantics at\nall in the JIT. The JIT is automatically generated from the Python interpreter\nsource code. This means we should be able to expand it to handle more complex\npython programs relatively quickly (interested assembler experts needed!).\n\n\nThis is actually the fifth incarnation of JIT that happened over the last\ntwo years. It's by far simpler and more promising than any of the previous\napproaches. Expect more details soon!\n\nCheers,\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/03/good-news-everyone-421421336094214242.html"
    },
    {
      "title": "JIT - a bit of look inside",
      "text": "The previous post about our JIT explained a bit from the 1000 km\nperspective how the tracing JIT would approach a language like Python.\n\n\nI would like to step a bit inside and give a zoom to some of its features that\nare already working.\nWhile probably not the most innovative, I think it's very nice to look\nat the way we work with the JIT and what tools we use.\n\n\nThe main cool thing is that you can work on and try the JIT (including trying\nit on the Python interpreter!) without even generating a single bit of\nassembler. How? Let's start with something very simple. Let's take\na simple interpreter for language X.\n\n\nLanguage X has 3 opcodes: CO_INCREASE, CO_DECREASE and CO_JUMP_BACK_3.\nCO_INCREASE increase the accumulator by one, CO_DECREASE decrease\nit by one, CO_JUMP_BACK_3 jump 3 opcodes back, if the accumulator is smaller\nthan 100 (this is only to maintain some halting conditions possible).\nThe interpreter for language X looks like this::\n\n\n    jitdriver = JitDriver(greens = ['i'], reds = ['res', 'a'])\n    code = [CO_INCREASE, CO_INCREASE, CO_INCREASE,\n            CO_JUMP_BACK_3, CO_INCREASE, CO_DECREASE]\n            \n    def add(res, a):\n        return res + a\n\n    def sub(res, a):\n        return res - a\n\n    def main_interpreter_loop(a):\n        i = 0\n        res = 0\n        c = len(code)\n        while i < c:\n            jitdriver.jit_merge_point(res=res, i=i, a=a)\n            elem = code[i]\n            if elem == CO_INCREASE:\n                res = add(res, a)\n            elif elem == CO_DECREASE:\n                res = sub(res, a)\n            else:\n                if res > 100:\n                    pass\n                else:\n                    i = i - 3\n                    jitdriver.can_enter_jit(res=res, i=i, a=a)\n                    continue\n            i = i + 1\n        return res\n\n\nAll very simple code, expect the jitdriver hints, which instruct JIT how to\nbehave (they are the equivalent of the ``add_to_position_key`` of last the blog\npost).\n\n\nLet's look how this code is processed. This will also give a glance\nat how we work in this code. This particular piece can be found\non a branch in pypy/jit/metainterp/test/test_loop.py\nand can be run with ./test_all.py jit/metainterp/test/test_loop.py -k test_example -s --view from pypy directory. The -s option lets you see the debugging output, while\n--view will show you some graphs. So, let's look at graphs in order:\n\n\n\nAnd the same picture with a bit of zoom for the first block:\n\n\n\n\nThis is the call graph of an interpreter loop, nothing magic so far. This is an\nintermediate representation of translation toolchain input. If you look around\nyou can follow how the opcodes are dispatched (with a chain of ifs) and helpers\ncalled. Next graph is very boring, because it's a bit lower level representation\nof the same thing (you exit with q or escape btw :).\n\n\nWhen we exit the graph viewer, we can see the trace generated by interpreting\nthis graph with a given bytecode (variable code in paste above). It's something\nlike:\n\n\n\n        [compiler] ENTER\n        [runner:cpu]    call__4 [(''), * GCREF hidden, 0] -> 0\n        [runner:cpu]    int_eq [0, 0] -> True\n        [runner:cpu]    int_add [9, 1] -> 10\n        [runner:cpu]    int_add [0, 1] -> 1\n        [runner:cpu]    int_lt [1, 6] -> True\n        [runner:cpu]    call__4 [(''), * GCREF hidden, 1] -> 0\n        [runner:cpu]    int_eq [0, 0] -> True\n        [runner:cpu]    int_add [10, 1] -> 11\n        [runner:cpu]    int_add [1, 1] -> 2\n        [runner:cpu]    int_lt [2, 6] -> True\n        [runner:cpu]    call__4 [(''), * GCREF hidden, 2] -> 0\n        [runner:cpu]    int_eq [0, 0] -> True\n        [runner:cpu]    int_add [11, 1] -> 12\n        [runner:cpu]    int_add [2, 1] -> 3\n        [runner:cpu]    int_lt [3, 6] -> True\n        [runner:cpu]    call__4 [(''), * GCREF hidden, 3] -> 1\n        [runner:cpu]    int_eq [1, 0] -> False\n        [runner:cpu]    int_eq [1, 2] -> False\n        [runner:cpu]    int_gt [12, 100] -> False\n        [runner:cpu]    int_sub [3, 3] -> 0\n        [compiler] LEAVE\n\n\nIt's entering JIT, doing some primitive operations for bytecode dispatching\nand repeating the loop. Note that at the end of the interpreted loop\n(not to be confused with the interpreter loop), we see int_sub [3, 3]\nwhich resets the bytecode position to the beginning. At this time JIT\n(instructed by can_enter_jit hint) notices that all green variables\nare the same (here only i),\nhence we can compile the efficient loop from this point.\n\n\n\n\nThe loop contains 3 additions and a check (for i < 100), exactly\nthe same as our interpreted program would do, but completely without\ninterpretation overhead!\n\n\nAs you might have noticed, there is no assembler involved so far. All of this\ninstruction execution is done directly, in pure python. In fact, the\ncode for executing instructions is located in jit/backend/llgraph\nwhich directly interprets instructions. This is by far simpler (and easier\nto debug) than x86 assembler.\n\n\nAnd this is basically it: the very simple interpreter and a jit for it.\nOf course we actually can generate assembler for that. Also the missing\npiece is optimizing the generated graphs. While for this example,\nby removing the interpretetation overhead, we're done, with more complex\nexamples it's important to further optimize traces. Hopefully this and\nhow we actually generate assembler will be topics for next blog posts.\n\nCheers,\nfijal",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/03/jit-bit-of-look-inside-7472130507462677287.html"
    },
    {
      "title": "PyPy on Mobiles, at OpenBossa",
      "text": "Next week i am going to give a talk on PyPy at OpenBossa, a developer conference on embedded platforms.  I've written up a bit more of my background and why i find it very interesting to go there on my blog.  Probably will mostly follow up there or on twitter and not much here on the PyPy blog because it's not all about PyPy.  To summarize how i see it: i think there is great potential for Python and PyPy on mobiles and am thrilled to hear about what's going on currently and to discuss opportunities.\ncheers, holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/03/pypy-on-mobiles-at-openbossa-845760004725129519.html"
    },
    {
      "title": "Applying a Tracing JIT to an Interpreter",
      "text": "After I had failed once more to explain to someone on IRC what the idea behind\nthe current JIT generator work of PyPy, I decided to just write a blog post to\nexplain it. Here it is :-). The post turned out to be a bit long, so please bear\nwith me.\nThe goal of the post is to give an understanding of how PyPy's JIT generator is\ngoing to work. To do this, I will look at what happens when you write an\ninterpreter in Java and apply a completely normal tracing JIT to it (for this\nreason all the code examples will be in some sort of pseudo-Java). The\nresulting generated machine code is bad, so I will explain a way to fix the\noccurring problem.\nThe techniques I describe here are conceptually similar to what we are doing in\nPyPy. The details (as usual) are different. The reasons why I am trying to\nexplain things in this way is that I can start from tracing JITs, which are a\nknown existing technique.\nTo understand the following, it is helpful to already know a bit how a normal\ntracing JIT works. I will give a reminder of how it is working, but there also\nexist a couple of more thorough introductions on the web already.\nI also will leave out a lot of details about the more detailed workings of\ntracing JITs and only explain the things that are relevant to what I am trying\nto get to here.\nTracing JITs\nTracing JITs are an idea explored by the Dynamo project in the context of\ndynamic optimization of machine code at runtime. The techniques were then\nsuccessfully applied to Java VMs and are now being used by Mozilla's\nTraceMonkey JavaScript VM. They are built on some basic assumptions:\n\n\nprograms spend most of their runtime in loops\nseveral iterations of the same loop are likely to take similar code paths\nthe best way to gain information about the behaviour of a program is to\nobserve it\n\n\nThe basic approach of a tracing JIT is to only generate machine code for\ncommonly executed loops and to interpret the rest of the program. The code for\nthose common loops however should be highly optimized, including aggressive\ninlining.\nThe generation of loops works as follows: At first, everything is interpreted.\nThe interpreter does a bit of lightweight profiling to figure out which loops\nare run often. When a common loop is identified, the interpreter enters a\nspecial mode (called tracing mode). When in tracing mode, the interpreter\nrecords a history (the trace) of all the operations it executes, in addition\nto actually performing the operations. During tracing, the trace is repeatedly\nchecked whether the interpreter is at a position in the program that it had seen\nearlier in the trace. If this happens, the trace recorded corresponds to a loop\nin the program that the tracing interpreter is running. At this point, this loop\nis turned into machine code by taking the trace and making machine code versions\nof all the operations in it.\nThis process assumes that the path through the loop that was traced is a\n\"typical\" example of possible paths (which is statistically likely). Of course\nit is possible that later another path through the loop is taken, therefore the\nmachine code will contain guards, which check that the path is still the same.\nIf during execution of the machine code a guard fails, the machine code is left\nand execution falls back to using interpretation (there are more complex\nmechanisms in place to still produce more code for the cases of guard failures,\nbut they are of no importance for this post).\nIt is important to understand when the tracer considers a loop in the trace to\nbe closed. This happens when the position key is the same as at an earlier\npoint. The position key describes the position of the execution of the program,\ne.g. usually contains things like the function currently being executed and the\nprogram counter position of the tracing interpreter.\nLet's look at a small example. Take the following code:\n\nint sum_1_to_n(int n) {\n    int result = 0;\n    while (n >= 0) {\n        result += n;\n        n -= 1;\n    }\n    return result;\n}\n\nThe tracing JIT will at one point trace the execution of the while loop in\nsum_1_to_n. The trace might look as follows:\n\nguard_true(n >= 0);\nresult += n;\nn -= 1;\n<loop_back>\n\nThis trace will then be turned into machine code. Note that the machine code\nloop is by itself infinite and can only be left via a guard failure.\nA slightly more complex example:\n\nint f(int a, int b) {\n    if (b % 46 == 41)\n        return a - b;\n    else\n        return a + b;\n}\n\nint strange_sum(int n) {\n    int result = 0;\n    while (n >= 0) {\n        result = f(result, n);\n        n -= 1;\n    }\n    return result;\n}\n\nThe trace of the loop in strange_sum would maybe look like this:\n\nguard_true(n >= 0);\na = result;\nb = n;\nguard_false(b % 46 == 41);\nresult = a + b;\nn -= 1;\n<loop_back>\n\nThis would then be turned into machine code. Note how f was inlined into the\nloop and how the common else case was turned into machine code, while the\nother one is implemented via a guard failure.\nApplying a Tracing JIT to an Interpreter\nIn the rest of the post we will explore what happens when the program that is\nbeing executed/compiled by the tracing JIT is itself a (bytecode) interpreter\nfor another language.\nA stylized bytecode interpreter for a simple programming language could look as\nfollows:\n\nW_Object interpret(String bytecode, ...) {\n    Stack<W_Object> stack = new Stack<W_Object>();\n    int pc = 0;\n    while (true) { // bytecode dispatch loop\n        char instruction = bytecode.charAt(pc);\n        pc += 1;\n        switch (instruction) {\n            case ADD:\n                W_Object arg2 = stack.pop();\n                W_Object arg1 = stack.pop();\n                stack.push(do_addition(arg1, arg2));\n                break;\n            case SUB:\n                W_Object arg2 = stack.pop();\n                W_Object arg1 = stack.pop();\n                stack.push(do_substraction(arg1, arg2));\n                break;\n            case RETURN:\n                return stack.pop();\n            case JUMP_BACKWARD:\n                pc -= (int)bytecode.charAt(pc);\n                break;\n            case LOAD_INTEGER:\n                int value = (int)bytecode.charAt(pc);\n                pc += 1;\n                stack.push(new W_Integer(value));\n                break;\n            case PRINT:\n                do_print(stack.pop());\n                break;\n            case DUP:\n                stack.push(stack.peek());\n                break;\n            case JUMP_IF_TRUE:\n                ...\n            ...\n        }\n    }\n\nIf we apply a tracing JIT to this function, it will trace and compile the\nexecution of one bytecode, because after one bytecode the bytecode dispatch loop\nis closed. E.g. it might trace and produce machine code for the execution of a\nSUB. (Sidenote: this interpret function is an example where one of the\nassumptions of a tracing JIT break down: two iterations of the bytecode dispatch\nloop are rarely going to follow the same code path, because usually two\nconsecutive bytecodes encode different instructions).\nThe important bit to remember here is that the tracing JIT will produce a\nmachine code loop that corresponds to the bytecode dispatch loop in the\ninterpret function. Let's see how we can change that.\nImproving the Generated Code\nIf we want to make use of the fact that the program that is being jitted is\nitself an interpreter, we need to change the tracing JIT a bit. To be more\nprecise we add a way for the user of the tracing JIT to add information to the\nposition key that the tracing JIT uses to decide when a loop is closed. This is\ndone by a call to a magic function add_to_position_key. This allows the\nprogram writer to influence the tracing JIT's behaviour.\nThe semantics of add_to_position_key is as follows: The method itself does\nnot do anything. It has an effect only when it is seen during tracing. If it is\nseen during tracing, the tracer adds the argument of the call to the position\nkey that the tracer is using to find out whether a loop was closed or not.\nIn the example of the interpret function above, we would add a call to this\nfunction into the while loop as follows:\n\nW_Object interpret(String bytecode, ...) {\n    Stack stack = new Stack();\n    int pc = 0;\n    while (true) { // bytecode dispatch loop\n        add_to_position_key(pc);\n        add_to_position_key(bytecode);\n        char instruction = bytecode.charAt(pc);\n        pc += 1;\n        switch (instruction) {\n            case ADD:\n    ...\n\nWhen the modified tracing JIT traces now the interpret function executing a\nSUB, something interesting happens. When the bytecode loop is closed, the\nmodified tracing JIT does not consider the trace to be a loop, because the value of\npc has been increased by one, so the position key differs. Instead it\ncontinues to trace, effectively unrolling the bytecode dispatch loop of\ninterpret.\nThe only way for a loop to be considered closed is if the pc variable has\nthe same value a second time.  This can only happen after a JUMP_BACKWARD\ninstruction has been executed.  A JUMP_BACKWARD instruction will only be in\nthe bytecode when the bytecode represents a loop. This means that the modified\ntracing JIT will trace the interpret function and will only consider that\nthe trace represents a loop when the bytecode itself represents a loop! Thus, a\nmachine code loop will eventually be created that corresponds to the loop in the\nbytecode.\nLet's look at at example. If we have a bytecode that corresponds to the\nfollowing instructions:\n\npc |   instruction\n---+---------------------\n0  |  LOAD_INTEGER 0\n2  |  DUP\n3  |  PRINT\n4  |  LOAD_INTEGER 1\n6  |  ADD\n7  |  JUMP_BACKWARD 6\n\nThis loop will print integers starting from 0 and going on from there. The\nmodified tracing JIT will unroll the bytecode dispatch until it sees the\nJUMP_BACKWARD bytecode. After that bytecode the pc will be 2 again. Thus\nthe earlier position key is repeated, which means that the loop will be closed.\nThe produced machine code will do the equivalent of the following Java code:\n\n...\nguard_true(pc == 2)\nguard_true(bytecode == \"... correct bytecode string ...\")\nwhile (true) {\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == DUP);\n    stack.push(stack.peek());\n\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == PRINT);\n    do_print(stack.pop());\n\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == LOAD_INTEGER)\n    value = (int)bytecode.charAt(pc);\n    pc += 1\n    stack.push(W_Integer(value))\n\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == ADD)\n    arg2 = stack.pop()\n    arg1 = stack.pop()\n    stack.push(do_addition(arg1, arg2))\n\n    instruction = bytecode.charAt(pc);\n    pc += 1;\n    guard_true(instruction == JUMP_BACKWARD)\n    pc -= (int)bytecode.charAt(pc);\n}\n\nThis is machine code that essentially does what the bytecode above did. Of\ncourse the code still remains some remnants of the interpreter (like the program\ncounter manipulations, the stack handling, etc), which would have to be removed\nby some clever enough optimization step. If this were done, result would look a\nlot more natural.\nSummary\nIf a tracing JIT is enhanced by a way to influence its loop-closing behaviour we\ncan significantly improve its performance when the jitted program is itself an\ninterpreter. The result is that in such a case the produced machine code\nwill correspond to the functions that are being interpreted, not to the code of\nthe interpreter itself.\nNow, what does all this have to do with PyPy? What we are working on since a\nwhile is a sort of tracing JIT for RPython which allows to be customized with a\nfunction very similar to the add_to_position_key described above. This will\nmake it possible to make the tracing JIT generate code that corresponds to the\ncode that the interpreter interprets. For example, we would add a call to\nadd_to_position_key to SPy, PyPy's Smalltalk VM. Then the tracing JIT will\nproduce machine code for Smalltalk-level loops, with all the usual benefits of a\ntracing JIT (like inlining of intermediate methods, constant-folding, ...).\nThis JIT differs from normal tracing JITs in that it also supports very powerful\nconstant-folding and allocation-removal optimizations. Those optimizations will\n(hopefully) be the content of a later blog post.\nThe basics of this process have been working fine since quite a while. What the\nwork currently focuses on is to improve the optimizers to remove not only the\nbytecode manipulation code, but also the stack handling, and a large number of\nother inefficiencies.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2009/03/applying-tracing-jit-to-interpreter-3287844903778799266.html"
    },
    {
      "title": "The next Leysin Winter Sprint",
      "text": "PyPy Leysin Winter Sprint (14-21th April 2009)\n\nThe next PyPy sprint will be in Leysin, Switzerland, for the\nsixth time.  This sprint will take place immediately after\nEaster.  This is a fully public sprint: newcomers and topics\nother than those proposed below are welcome.\n\n\n\n\n\n\n\nThe overall idea of the sprint is to continue working on making PyPy ready\nfor general use.  There are a few tasks left in there.  In parallel, we\nwill continue the work on the JIT, if there is general interest.  And as\nusual, we are ready to add any other task -- please mention on the mailing\nlist what you would like to work on; the list of task is not really fixed.\nAnd as usual, the main side goal is to have fun in winter sports :-)\nWe can take a day off for ski until Sunday, the 19th; afterwards, the\ninstallations close.  (There was quite a lot of snow this winter, so\nthere should be some left even though it's relatively late in the season.)\n\n\n\n\n\nFor more information see the announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/03/next-leysin-winter-sprint-1791506307881043273.html"
    },
    {
      "title": "Wroclaw 2009 sprint progress report",
      "text": "Hello.\n\nWe have just finished probably the smallest sprint ever\nin PyPy history. For most of the time it was just me\nand Armin pairing together.\n\nWe also had a chance to work a bit with people from\nthe University, but there were definitely not enough\ncore developers to organize the work in a reasonable\nmanner. At some point we ended up having two pairs containing\nfour people each.\n\nJakub and Bartosz (who were our gentle hosts) worked\non getting PyPy's sandbox integrated with django.\nIt's still just an example what you can do (ie you\ncan do much more), but it's already interesting to look\nat. The code can be found in user dir. This server (not yet online anywhere, sorry)\nis able to run untrusted python code provided by user inside\na fully configurable sandbox.\n\nWe also implemented missing peepholer optimizations from\nCPython, finding out that some peepholer tests were failing,\njust because PyPy is optimizing better :-)\n\nThe main part of the sprint was work on JIT (most notable the fifth\ngeneration of the JIT), which was moved\nfrom the obscure directory in Carl's user in svn (which contains\nbranches these days!) into a PyPy branch. It's still very much\nwork in progress and a lot of pen and paper or handwaving was\ninvolved, but we were able to implement a lot of basics in record time.\n\nRight now we need a lot of rest after the exhaustive sprint,\nbut after that, stay tuned for more information about\nprogressing JIT!\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/02/wroclaw-2009-sprint-progress-report-2510073170049635489.html"
    },
    {
      "title": "Wroclaw 2009 PyPy sprint and talk",
      "text": "The next PyPy sprint will be held in Wroc\u0142aw, Poland 7-14th February 2009. This is fully public\nsprint and all newcomers are welcomed. Preceeding the sprint there\nwill be a talk at University of Technology in Wroc\u0142aw held at 22nd of January.\n\nFor detailed info about the sprint, look here.\n\nThe talk will be a general, high-level overview about PyPy project. There is a very nice poster, made by Jakub Gustak and Bartosz Skowron (in polish):\n\n\n\nTalk details:\n\nLocation: Politechnika Wroc\u0142awska, budynek C-13, sala 0.31\nDate: 22nd January 2009, 19:00\nLanguage: very likely polish, although talk can be as well in english if some non-polish native would show up.\n\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2009/01/wroclaw-2009-pypy-sprint-and-talk-8240928228677982487.html"
    },
    {
      "title": "Pycon 2009",
      "text": "Hello.\n\nBoth of our PyPy talks has been accepted for Pycon US 2009. Although both\nare somehow related to PyPy, they're vastly different in\ntopics, attitude and target audience.\n\nThe first one is a classic PyPy status talk - we'll mostly talk about\nour achievements from the last year (readers of this blog are aware of most,\nbut not all :) as well as some general introduction and plans for the future.\n\n\nThe second one is about PyPy's sandboxing features. This is in my opinion\na very underestimated feature, also by us, because it's not really well\nadvertised or documented. The main purpose of the talk is to present\nto the general public how this works and how to use it.  Hopefully we will\nget to work and publish about this a bit more ahead of Pycon already. \nUnlike Zope's Restricted Python, it provides you with the full python\nlanguage, inside a fully\nvirtualized sandbox, controlled from an external process by a custom\nsecurity policy. Stay tuned for more :-)\n\n\nSee you at Pycon 2009!\n\n\nCheers,\nfijal and holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/12/pycon-2009-9090464449197911432.html"
    },
    {
      "title": "Porting the JIT to CLI (part 3)",
      "text": "In my two previous posts, we talked about the PyPy JIT generator, seeing\nthat it can produce huge speedups and how its backend-independent frontend\nworks.\nIn this post, we will look closer at the internals of the CLI JIT backend; in\nparticular, we will see how we work around some serious limitations of the\nplatform, and why these workarounds didn't have any serious impact on the\nperformances of our toy virtual machine.\n\nGraphs, blocks, links\n\n\n\n\nOne of the core aspect of PyPy translator is the concept of flow graph: a\nflow graph is a data structure that represents the code we are operating on.\nIt is composed by a set of basic blocks, each block containing a sequence\nof operations; blocks are connected together by links, and each link can\ncarry a variable number of arguments whose value is passed to the target\nblock.  In case a block contains more than one outgoing links, the one to\nfollow is selected by looking at the value of a designated variable (the\nexitswitch), thus making possible to implement conditional jumps.  To have\na more complete description of the flow graphs model, check the documentation.\n\nAs we saw in the previous post, the generated JIT compiler makes heavy use of\nflexswitches to generate efficient code, continuously intermixing\nJIT-compile time and runtime.\nIn terms of graphs, we can think of a flexswitch as a special block whose\nlinks change over time.  In particular, adding a new case to the flexswitch is\nequivalent to create a link whose target is a new block where the just\ngenerated code starts.  Thus, the graphs grows over the time, as showed by\nthe following images:\n\n\n\n\n\n\nIn the images above, the block containing the flexswitch is colored in\ncyan. In the first picture, there is only one block connected to the\nflexswitch: this block contains the code to restart the JIT compilation.  The\nsecond picture shows the graph after the first case has been added: you can\nclearly see that a new block has been created and attached to the flexswitch.\nFinally, the third picture shows the graph after a while, with a lot of new\nblocks attached.\n\n\nTranslate graphs to CLI\nConceptually, the goal of the CLI JIT backend is to express these graphs in\nterms of CLI bytecode.\nTranslating the single block is easy, as it is just a list of sequential\noperation, and it's straightforward to map each operation to the equivalent\nCLI opcode or to a call to a helper method.  Moreover, we need a way to\nexpress links between the various basic blocks: if the links are known in\nadvance, render them is as easy as emitting a (potentially conditional) jump to\nthe target block.  Thus, we won't discuss this part in detail, as it is quite\nstraightforward.\nThe hard part is how to implement flexswitches: at the time when we are\nemitting the code, some of the blocks of this growable graph don't even exist:\nhow can we make a jump to a non existent block of code?  For backends that\nemit assembly code, it is rather easy: when they need to add a new case to the\nflexswitch, they can just patch the existing code to insert a jump to a\nnewly allocated area of the memory, where the new code is being generated in.\nFor CLI this approach is not feasible, as the VM will never allow us to modify\nexisting code. Thus, we need to think of a different approach.\n\n\nGraphs and methods\nIn .NET, the basic unit of compilation is the method: the only way to\nexecute some bytecode is to wrap it into a method.  Moreover, it is not\npossible to execute a method until it has been completed, and after this point\nit is no longer possible to add new code.\nBecause of all these constraints we cannot simply map each graph to its own\nmethod, since we saw that our graphs can grow after they have already been\nexecuted few times.\nHence, we need to distinguish between the two concepts:\n\n\na graph is the logical unit of code as seen by the JIT compiler:\nconcretely, the CLI JIT backend renders it as one or more methods;\na method is a collection of basic blocks; each method has the so\ncalled parent graph, i.e. the graph its blocks logically belongs to.\n\n\nThe first method of a graph is called main method (which has\nnothing to do with the Main static methods found in .exe files); other\nmethods are called children methods.\nWhen we want to add a new case to the flexswitch, we create a method\ncontaining all the new code; then we wrap the method inside a delegate (the\n.NET equivalent of a function pointer) and pass it to the flexswitch, so that\nit can later invoke it.\n\n\nThe hard bit: non-local links\nUsing this approach, after a while the blocks of our original graph are\nscattered over a lot of different methods; however, there are no constraints\nabout how these blocks can be linked together, so it happens to have links\nbetween blocks which are not in the same method. In the following, we will\nrefer to them as non-local links.\nIf the non-local block we want to jump to happens to be at the beginning of\nits containing method, it is enough to invoke the method; but, what if we want\nto jump somewhere in the middle?  What we really want is to produce a method\nwhich has multiple entry-points; again, doing it in assembly would be\ntrivial, but the virtual machine does not provide any support for it, so we\nneed a work around.\nEach method in a graph is assigned an unique 16 bit method id; each block in\na method is assigned a progressive 16 bit block number.  From this two\nnumbers, we can compute the block id as an unsigned integer, by storing\nthe method id in the first 16 bits and the block number in the second 16 bits.\nBy construction, the block id is guaranteed to be unique in the graph.\nThe following picture shows a graph composed of three methods; the id of each\nmethod is shown in red, while the block ids are shown in red (for the method\nid part) and black (for the block number part).  The graph contains three\nnon-local links; in particular, note the link between blocks 0x00020001\nand 0x00010001 which connects two block that resides in different methods.\n\n\n\nEvery method contains a special dispatch block, (not shown in the picture above) whose goal is to jump to\nthe specified block number inside the method itself.  The first argument of a\nchild method is always a block id; when the method starts, it immediately\njumps to the dispatch block, and thus to the desired block.For example, suppose to have a method which contains 3 blocks numbered 0, 1,\n2; here is how its dispatch blocks looks like; for simplicity it is shown as\nC# code, but it is actually generated as IL bytecode:\n\n// dispatch block\nint methodid = (blockid & 0xFFFF0000) >> 16); // take the first 16 bits\nint blocknum = blockid && 0x0000FFFF;         // take the second 16 bits\n\nif (methodid != MY_METHOD_ID) {\n// jump_to_unknown block\n...\n}\n\nswitch(blocknum) {\ncase 0:\ngoto block0;\ncase 1:\ngoto block1;\ncase 2:\ngoto block2;\ndefault:\nthrow new Exception(\"Invalid block id\");\n}\n\nWhenever we want to jump to a non-local block, it is enough to store the block\nid in the appropriate variable and jump to the dispatch block.  If the block\nresides in a different method, the jump_to_unknown block is entered; this\nspecial block is implemented differently by the main method and the child\nmethods, as we will see soon.\nEach time a new method is added to the graph, we build a delegate\nfor it, and store it in a special array\ncalled method_map; since we assign the method id sequentially starting\nfrom 0, we are sure that to fetch the method whose id is n we can simply\nload the n-th element of the array.\nThe jump_to_unknown block of the main method uses this array to select the\nright method, and calls it (FlexSwitchCase is the type of delegates for\nall children methods):\n\n// jump_to_unknown block of the main method\nFlexSwitchCase meth = method_map[methodid];\nblockid = meth(blockid, ...); // execute the method\ngoto dispatch_block;\n\nEach child method returns a block id specifying the next block to jump to;\nafter its execution, we assign the return value to the blockid variable,\nand jump again to the dispatch block, which will jump again to the appropriate\nblock.\nKeeping this in mind, it is straightforward to implement the\njump_to_unknown block of children methods: it is enough to return the\ntarget block id to the caller, and let its dispatch loop do the right thing.\nIf the caller is also a child method, it will return it again, until we reach\nthe dispatch loop of the main method, which will finally do the jump.  In\ntheory, we could implement things differently and jumping directly from a\nchild method to another one, but in that case the call stack could grows\nindefinitely in case of a tight loop between two blocks residing in different\nmethods.\nTo implement the dispatch block we can exploit the switch opcode of the\nCLI; if the .NET JIT is smart enough, it can render it using an indirect jump;\noverall, jumping to a non-local block consists of an indirect function call\n(by invoking the delegate) plus an indirect jump (by executing the switch\nopcode); even if this is more costly than a simple direct jump, we will see in\nthe next section that this not the main source of overhead when following a\nnon-local link.\nObviously, the slow dispatching logic is needed only when we want to jump to a\nnon-local block; if the target block happens to reside in the same method as\nthe current one, we can directly jump to it, completely removing the overhead.\nMoreover, the dispatch blocks are emitted only if needed, i.e. if the parent\ngraph contains at least one flexswitch; graphs without flexswitches are\nrendered in the obvious way, by making one method per graph.\n\n\nThe slow bit: passing arguments\nJumping to the correct block is not enough to follow a link: as we said\nbefore, each link carries a set of arguments to be passed from the source to\nthe target block.  As usual, passing arguments across local links is easy, as\nwe can just use local variables to hold their values; on the other hand,\nnon-local links make things more complex.\nThe only way to jump to a block is to invoke its containing method, so the\nfirst solution that comes to mind is to specify its input arguments as\nparameter of the method; however, each block has potentially a different\nnumber (and different types) of input arguments than every other block, so we\nneed to think of something else.\nAn alternative solution could be to compute the union of the sets of input\narguments of all the blocks in the method, and use this set as a signature\nfor the method; this way, there would be enough space to specify the input\narguments for every block we might want to jump to, each block ignoring the\nexceeding unused parameters.\nUnfortunately, all the children methods must have the very same signature,\nas they are all called from the same calling site in the dispatch block of the\nmain method.  Since the union of the set of input arguments (and hence the\ncomputed signature) varies from method to method, this solution cannot work.\nWe might think to determine the signature by computing the union of input\narguments of all blocks in the graph; this way, all the children methods\nwould have the same signature.  But as we said above, the graph grows new\nblocks at runtime, so we cannot determine in advance which set of input\narguments we will need.\nTo solve the problem we need a way to pass a variable number of arguments\nwithout knowing in advance neither their number nor their types.  Thus, we use\nan instance of this class:\n\npublic class InputArgs {\npublic int[] ints;\npublic float[] floats;\npublic object[] objs;\n...\n}\n\nSince the fields are arrays, they can grow as needed to contain any number of\narguments; arguments whose type is primitive are stored in the ints or\nfloats array, depending on their type; arguments whose type is a reference\ntype are stored in the objs array: it's up to each block to cast each\nargument back to the needed type.\nThis solution impose a huge overhead on both writing and reading arguments:\n\n\nwhen writing, we need to make sure that the arrays are big enough to\ncontains all the arguments we need; if not, we need to allocate a bigger\narray.  Moreover, for each argument we store into the array the virtual\nmachine performs a bound-check, even if we know the index will never be\nout of bounds (because we checked the size of the array in advance);\nwhen reading, the same bound-check is performed for each argument read;\nmoreover, for each value read from the objs array we need to insert a\ndowncast.\n\n\nTo mitigate the performance drop, we avoid to allocate a new InputArgs\nobject each time we do a non-local jump; instead, we preallocate one at the\nbeginning of the main method, and reuse it all the time.\nOur benchmarks show that passing arguments in arrays is about 10 times slower\nthan passing them as real parameter of a method.  Unfortunately, we couldn't\ncome up with anything better.\n\n\nImplement flexswitches\nNow, we can exploit all this machinery to implement flexswitches, as this is\nour ultimate goal.  As described above, the point is to be able to add new\ncases at runtime, each case represented as a delegate.  Here is an excerpt\nof the C# class that implements a flexswitch that switches over an integer\nvalue:\n\npublic class IntLowLevelFlexSwitch:\n{\npublic uint default_blockid = 0xFFFFFFFF;\npublic int numcases = 0;\npublic int[] values = new int[4];\npublic FlexSwitchCase[] cases = new FlexSwitchCase[4];\n\npublic void add_case(int value, FlexSwitchCase c)\n{\n...\n}\n\npublic uint execute(int value, InputArgs args)\n{\nfor(int i=0; i<numcases; i++)\nif (values[i] == value) {\n return cases[i](0, args);\n}\nreturn default_blockid;\n}\n}\n\nFor each case, we store both the triggering value and the corresponding\ndelegate; the add_case method takes care to append value and c to\nthe values and cases arrays, respectively (and resize them if\nnecessary).  The interesting bit is the execute method: it takes a value\nand a set of input arguments to be passed across the link and jumps to the\nright block by performing a linear search in the values array.\nAs shown by previous sections, the first argument of a FlexSwitchCase is\nthe block id to jump to; since when we go through a flexswitch we always want\nto jump to the first block of the method, we pass the special value 0 as a\nblock id, which precisely means jump to the first block.  This little\noptimization let us not to have to explicitly store the block id for the first\nblock of all the cases.\nThe value returned by execute is the next block id to jump to; if the\nvalue is not found in the values array, we return the default_blockid,\nwhose value has been set before by the JIT compiler; default_blockid\nusually points to a block containing code to restart the JIT compiler again;\nwhen the JIT compiler restarts, it emits more code for the missing case, then\ncalls add_case on the flexswitch; from now on, the new blocks are wired\ninto the existing graph, and we finally managed to implement growable\ngraphs.\n\n\nPerformances\nAs we saw, implementing growable graphs for CLI is a pain, as the virtual machine\noffers very little support, so we need an incredible amount of workarounds.\nMoreover, the code generated is much worse than what an assembly backend could\nproduce, and the cost of following a non-local link is very high compared to\nlocal links.\nHowever, our first blog post showed that we still get very good\nperformances; how is it possible?\nAs usual in computer science, most of the time of a running program in\nspent in a tiny fraction of the code; our benchmark is no exception, and the\nvast majority of the time is spent in the inner loop that multiplies numbers;\nthe graph is built in such a way that all the blocks that are part of the\ninner loop reside in the same method, so that all links inside are local (and\nfast).\nFlexswitches and non-local links play a key role to select the right\nspecialized implementation of the inner loop, but once it is selected they are\nnot executed anymore until we have finished the computation.\nIt is still unclear how things will look like when we will compile the full\nPython language instead of a toy one; depending on the code, it could be\npossible to have non-local links inside the inner loop, thus making\nperformance much worse.\n\n\nAlternative implementations\nBefore implementing the solution described here, we carefully studied a lot of\npossible alternatives, but all of them either didn't work because of a\nlimitation of the virtual machine or they could work but with terrible\nperformances.\nIn particular, in theory it is possible to implement non-local links using\ntail calls, by putting each block in its own method and doing a tail call\ninstead of a jump; this would also solve the problem of how to pass arguments,\nas each method could have its own signature matching the input args of the\nblock.  I would like to explain this solution in a more detailed way as I\nthink it's really elegant and nice, but since this post is already too long,\nI'll stop here :-).\nIn theory, if the .NET JIT were smart enough it could inline and optimize away\nthe tail calls (or at least many of those) and give us very efficient code.\nHowever, one benchmark I wrote shows that tail calls are up to 10 times\nslower (!!!) than normal calls, thus making impractical to use them for our\npurposes.\n\n\nConclusion\nDespite the complexity of the implementation, our result are extremely good;\nthe speedup we got is impressive, and it proves that PyPy's approach to JIT\ncompiler can work well also on top of object oriented virtual machines like\n.NET or the JVM.\nGenerating bytecode for those machine at runtime is not a new idea; Jython,\nIronPython, JRuby and other languages have been doing this for years.\nHowever, Jython and IronPython do only a simple \"static\" translation, which\ndoesn't take advantage of the informations gathered at runtime to generate\nbetter, faster and specialized code.  Recently, JRuby grew a new strategy to\nJIT-compile only hotspots, taking advantage of some informations gathered\nwhile interpreting the code; this is still a \"one-shot\" compilation, where the\ncompiled code does not change over time.\nTo my knowledge, PyPy brings the first example of a\nlanguage which implements a truly JIT compiler on top of the underlying JIT\ncompiler of the virtual machine, emitting bytecode that changes and adapts\nover the time.  If someone knows other languages doing that, I would really\nlike to know more.\nBeing so innovative, the problem of this approach is that the current virtual\nmachines are not designed to support it in a native way, and this forces us to\nput a lot of workarounds that slow down the generated code.  The hope is that\nin the future the virtual machines will grow features that help us to generate\nsuch kind of code.  The experimental Da Vinci VM seems to go in the right\ndirection, so it is possible that in the future I will try to write a JIT\nbackend for it.\nAt the moment, the CLI JIT backend is almost complete, and all the hardest\nproblems seems to be solved; the next step is to fix all the remaining bugs\nand implement some minor feature that it's still missing, then try to apply it\nto the full Python language and see what is the outcome.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/12/porting-jit-to-cli-part-3-3519327524638923621.html"
    },
    {
      "title": "Porting the JIT to CLI (part 2)",
      "text": "In my previous post, we saw that PyPy JIT generator can produce huge\nspeedups when applied to the tlc toy language.\nIn this post we will dive a bit into the internals of PyPy JIT, to see how it\nmanages to do so. Note that this is a very high level overview of how the\nJIT works, and applies to all backends.  Then, in the third post of this\nseries, we will look closer at the CLI JIT backend, seeing how it works around\nsome .NET limitations and how the generated code looks like.\n\nPyPy JIT for dummies\nAs you surely know, the key idea of PyPy is that we are too lazy to write a\nJIT of our own: so, instead of passing nights writing a JIT, we pass years\ncoding a JIT generator that writes the JIT for us :-).\nI'm not going to explain how the JIT generator does its job, (perhaps this\nwill be the subject of another blog post), but how the generated JIT\nworks.\nThere are values that, if known at compile-time (i.e., when the JIT compiler\nruns), let the JIT to produce very efficient code.  In a dynamic language,\ntypes are the primary example: for instance, suppose you are a compiler and\nyou have to compile to following Python function:\n\ndef mysum(a):\n  return a + 1\n\nAt compile time, you don't have any knowledge about the type of the parameter:\nit could be integer, float, an user defined object, etc.  In this situation,\nthe only safe choice is to emit code which does the usual, slow, full lookup\nto know how to perform the operations.\nOn the other hand, suppose that you knew in advance that the parameter is an\ninteger: this time, you could emit code that exploits this extra\nknowledge, by performing directly a fast integer addition.\nThe idea behind PyPy JIT is that if you don't have enough knowledge to\ngenerate efficient code, you stop compiling and wait until you know\nexactly what you need.  Concretely, you emit code that runs until the point\nwhere you stopped the compilation, then it triggers a special procedure that\nrestarts the compiler.  This time the JIT compiler knows everything\nyou need, because you can inspect the state of the running program.\nLet's see an example: the first time the JIT compiles mysum, it produces\nsomething like this pseudo-code:\n\nPyObject mysum_compiled(PyObject a)\n{\n  Type a_type = a.GetType();\n  switch(a_type) {\n      default: continue_compilation(a_type, <position>);\n  }\n}\n\nIf you call mysum(41), the execution goes in the default branch of the\nswitch, thus calling continue_compilation: its job is to restart the JIT\ncompiler, which now can emit fast code because it knows the exact type of\na; then, it modifies the original mysum_compiled function, in\norder to make it executing the newly generated code the next time it\nencounters an integer at that point:\n\nPyObject mysum_compiled(PyObject a)\n{\n  Type a_type = a.GetType();\n  switch(a_type) {\n      PyInteger: return new PyInteger(a.value+1); // fast path!\n      default: continue_compilation(a_type, <position>);\n  }\n}\n\nFrom now on, every time we call mysum with an integer argument, the JIT\ncompiler is not called anymore and the fast path is directly executed; if we\nhappen to call mysum with a float arguments, the switch goes again in the\ndefault branch, and the JIT compiler is started once more to produce fast\ncode also for this case.  What happens in practice is that compile-time and\nruntime are continuously intermixed, until the switches are stable enough and\nthe compiler is not needed anymore.\nIn PyPy jargon, this kind of \"growable switch\" is called flexswitch, and\nit's one of the most important concept of our JIT generator.\n\nPromotion\nHow can the JIT generator know which values are useful to know to generate\nefficient code and which aren't?  Unfortunately it can't, or at least our JIT\ngenerator is not smart enough at the moment.\nTo get the best from it, the developers of the VM need to instruct it by\nannotating the variables on which we want the JIT to stop until it knows the\nactual values; this is done by using particular hints, called promote\nand promote_class; variables annotated with such hints are said to be\npromoted. If something is promoted, a flexswitch is used to gain\ninformation about it, as seen in the last section.\nFor an example, let's look at an excerpt from main dispatch loop of the tlc\nvirtual machine:\n\nelif opcode == ADD:\n  a, b = stack.pop(), stack.pop()\n  hint(a, promote_class=True)\n  hint(b, promote_class=True)\n  stack.append(b.add(a))\n\nThis the implementation of the ADD opcode: first, it pops two values from\nthe stack; then, it computes the result; finally, it push the result to the\nstack again.  In between, both the classes of a and b have been\npromoted: this means that when the JIT emits the code for b.add(a), it\nknows exactly what is happening: if it sees that both are instances of the\nIntObj class, it inlines the method call and emits a fast integer addition\ninstead.\n\nVirtuals\nThe other important concept of the JIT is the presence of virtual\nstructures, virtual lists, and virtual dictionaries.  Again, I'm not\ngoing to explain in depth how they work, but only why they are so important for\ngenerating highly efficient code.\nThe essence of virtuals is that you don't allocate objects until you really\nneed to do it, e.g. because they are being passed as an argument to some\nexternal function.  Instead, we store all the informations we need as local\nvariables; e.g., in the case of a virtual structure, we create as many local\nvariables as the number of its fields: if the structure escapes the local\nscope, we force it to a real object, by allocating memory on the heap and\ninitializing it after the current value of the local variables.\nThis technique allows the JIT to avoid the allocation of many temporary\nobjects that hold intermediate results; consider for example the following\nPython loop:\n\nresult = 0\nfor i in range(N):\n  result += i\nreturn result\n\nWithout the JIT, at each iteration, a new int object is created and bound\nto the result variable, while the previous one is discarded and not needed\nanymore.  By combining virtuals and promotion, the JIT can emit code that does\nthe whole computation locally, and allocates a real object only at the end,\nwhen it escapes from the local scope because it is returned from the\nfunction.\n\nPutting it all together\nThis is, essentially, how PyPy's generated JITs work.  To summarize, our JITs\nemit multiple versions of each chunk of code: each version is specialized\nand optimized for one particular case.\nThe cost of selecting the right specialization to use (through flexswitches)\nis almost always negligible compared to how much time you save by running the\nfast version instead of the more-general-but-slow one.  Moreover, each\nspecialized version knows the exact shape of the objects it's dealing with, so\nthey can be virtualized to make the generated code even more efficient.\nAt the end, the actual code generation is done by one of the JIT backends:\nthe backends exploit all the knowledge gathered by the previous steps to\nproduce highly efficient code, but this will be the subject of the next blog\npost.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/11/porting-jit-to-cli-part-2-2456826431882963884.html"
    },
    {
      "title": "Porting the JIT to CLI (part 1)",
      "text": "As the readers of this blog already know, I have been working on the CLI\nJIT backend for some months: last Friday, it reached an important milestone,\nas it is now able to produce huge speedups for a little dynamic language.  To\nknow how huge the speedup is, read on :-).\nThe goal of PyPy JIT generator is to take an interpreter and, with the help of\nfew annotations, automatically generate a JIT compiler for it.  In this post,\nwe will talk about the tlc virtual machine: while tlc it is just a toy\nlanguage, it contains some features that make it an interesting target for our\nJIT generator.\n\nThe tlc virtual machine\ntlc is executed by a stack based, dynamically typed virtual machine (for\nthose who knows a bit about the Python VM: does it sound familiar? :-)).\nThere are three types of objects: integers, nil, and cons cells (i.e.\nlisp-like pairs of objects).\nAs the VM is very simple, it provides only few opcodes:\n\n\nopcodes to manipulate the stack, like PUSH, POP, etc.\ninteger operations, like ADD, MUL, all the comparisons, etc.:\nthese operations can only be applied to integers;\nlist operations, like CONS, CAR, CDR: these operations can\nonly be applied to lists;\nother operations, including jumps and conditional jumps.\n\n\nThe VM is interesting for our purposes because it has a lot of similarities\nwith Python (though on a smaller scale, of course):\n\n\nit has to do type-checks at runtime before doing most of the operations;\nevery time you do an arithmetic operation, it has to unbox the operand,\ndo the computation, and the box the result again.\n\n\nThis means that even if you have a program which only uses integers, you are\npaying a lot of overhead.\nTo know more about this toy VM, look at its source code: the interesting\nbits are the classes used to represent objects, and the interp_eval\nfunction, which contains the main loop of the virtual machine.  As you can\nsee, the implementation is quite straightforward; all the hint calls you\nsee are the special annotations needed by the JIT generator to produce better\ncode.\n\n\nLet's JIT it!\nSo, the whole point is to generate a JIT compiler from it, isn't it?\nFirst, checkout a fresh copy of the oo-jit branch:\n\n$ svn co https://codespeak.net/svn/pypy/branch/oo-jit\n\nThen, go to the oo-jit/pypy/jit/tl directory, and compile the tlc VM\nwith the CLI backend and JIT enabled:\n\n$ cd oo-jit/pypy/jit/tl/\n$ ../../translator/goal/translate.py -b cli --jit --batch targettlc\n...\nlot of texts\n...\n\nIf everything went OK, you now have a targettlc-cli executable, which\naccepts two arguments: the name of the file containing the tlc program we\nwant to run, and an integer to be passed to it.\nLuckily, in the same directory we have a factorial.tlc file that contains\nthe bytecode for a function that -- guess? -- computes the factorial of a\ngiven integer; let's try it:\n\n$ ./targettlc-cli factorial.tlc 5\nNon jitted:    120 (0.009371 seconds)\nWarmup jitted: 120 (0.208954 seconds)\nWarmed jitted: 120 (0.000323999999999991 seconds)\n\nCool, it seems that the result was computed correcly :-). As you can see from\nthe output, we ran the program three times:\n\n\nby plain interpretation, without any jitting;\nwith the jit enabled: this run includes the time spent by doing the\ncompilation itself, plus the time spent by running the produced code;\nagain with the jit enabled, but this time the compilation has already\nbeen done, so we are actually measuring how good is the code we produced.\n\n\nSo, it's time to run a benchmark: let's try to compute the factorial of a very\nbig number; the result will be 0, because obviously after a while we overflow,\nbut after all we are interested in the time spent, not in the result:\n\n$ ./targettlc-cli factorial.tlc 5000000\nNon jitted:    0 (19.93247 seconds)\nWarmup jitted: 0 (0.293229999999998 seconds)\nWarmed jitted: 0 (0.0494239999999984 seconds)\n\n$ python -c 'print 19.93247/0.0494239999999984'\n403.295362577\n\nAnd no, I didn't make any mistake in copying&pasting: the jitted version is\nreally 400 times faster that the non jitted one!\nWarning: my laptop seems to be not very well suited for benchmarks, as the\nresults vary a lot from run to run; I've run the benchmarks a lot of times,\nand I got speedup factors up to 500 times, so your results may be different.\n\n\nMore benchmarks\nIt's also interesting to compare the result with a manual written C#\nversion of the factorial, to see how good is code we produced; to get\nreasonable results, we need to compute a larger factorial, to let to code to\nrun a bit more:\n\n$ ./targettlc-cli --onlyjit factorial.tlc 100000000\nWarmup jitted: 0 (0.980856 seconds)\nWarmed jitted: 0 (0.769716 seconds)\n\n$ mono factorial.exe 100000000\nC#:            0 (0.153777 seconds)\n\n$ python -c 'print 0.769716/0.153777'\n5.00540392907\n\nWe know that the generated code is far from being optimal, but probably the\nfactor of five is at least partially due to the fact that Mono's own JIT is optimized for\nC#-like code, and our code has a completely different shape.\nAll the benchmarks above were run under Linux, with Mono 1.9.1.  Here are the\nresults for the same benchmarks, but run with Microsoft CLR (on a different\nmachine, so the absolute values are not comparable):\n\n$ ./targettlc-cli factorial.tlc 5000000\nNon jitted:    0 (15,640625 seconds)\nWarmup jitted: 0 (0,4375 seconds)\nWarmed jitted: 0 (0,03125 seconds)\n\n$ python -c 'print 15.640625/0.03125'\n500.5\n\n$ ./targettlc-cli --onlyjit factorial.tlc 100000000\nWarmup jitted: 0 (0,90625 seconds)\nWarmed jitted: 0 (0,515625 seconds)\n\n$ ./factorial.exe 100000000\nC#:            0 (0,34375 seconds)\n\n$ python -c 'print 0.515625/0.34375'\n1.5\n\nThe results are even better than before; this is probably thanks to CLR's JIT,\nthat does a better job than Mono when faced to something which is different\nthan the usual C#-like code.\n\n\nConclusions (for now)\nThis is a very important result, because it proves that PyPy's approach to JIT\ncompilers can be applied effectively also to OO virtual machines; the result\nis even better than what I expected, because when generating code for .NET we\nhave much less freedom than when generating assembly code, and I had to play\nsome tricks to work around some .NET limitations.\nMoreover, it worked at the first try :-). I tried to compile the tlc\nvirtual machine as soon as all the related JIT tests were passing, and\nsurprisingly everything worked just fine, even if it was the very first time I\nwas trying to apply some features of the JIT to something bigger than a test:\nI think this is yet another prove that Test Driven Development just works!\nEven if this is a major milestone, the CLI JIT backend is not yet completed:\nas a consequence it can't still be used for the full PyPy, but all the\nhardest problems should have been solved now.\nSince a lot of readers asked for more technical details, especially about the\nJIT, I will try to soon write a second blog post explaining how the CLI backend works\ninternally, with a brief look to the generated code to see how it looks like.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/11/porting-jit-to-cli-part-1-8712941279840156635.html"
    },
    {
      "title": "One year PyPy Blog",
      "text": "Last Friday the PyPy Status Blog had its first anniversary. Yay! After not\nreally buying into any of this new-fangled \"blog\" stuff for a long time we just\nbit the bullet and got started. Totally surprisingly it even worked. We posted\n76 post in the last year, more than one per week. By now we have more than 800\nsubscribers (according to feedburner), which is quite cool for a rather niche\nblog.\nTo make our blog even more interesting, I would like to ask for some feedback\nvia the comments:\n\n\nWhich posts did you like in particular?\nWhat sort of posts would you be interested in getting more of?\nAny other improvements we could make?",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/11/one-year-pypy-blog-3267056180369310162.html"
    },
    {
      "title": "Sprint Discussions: JIT Generator Planning",
      "text": "Background\nFinally, the JIT post :-). First some background: Despite our plans at the end\nof the EU period, PyPy's Python interpreter didn't get a good and widely\napplicable JIT in the last year. The reason for that was that we discovered that\nalthough the basic idea to generate JIT compilers is good, the concrete\nprototype made during the EU period is basically flawed. It could have\nbeen pushed a bit farther, but would have run into deep troubles eventually. One\nof the problems would have been performance instability: change a seemingly\nunrelated bit in your source program, and the performance changes in unexpected\nways, which is clearly not desirable. Another problem with that old approach is\nthat too much assembler code is generated, leading to memory problems, and also\nthat the generated assembler is bad in various ways, e.g. it is hard in that\napproach to do proper register allocation.\nTherefore we decided that it would be worthless to pursue this direction much\nfurther. Instead we tried to research approaches to fixing the inherent\nproblems. This research was largely done in Prolog and I eventually wrote my\nMaster's thesis about it. From the Prolog work we got some good insights into\nwhat needs to be done and what sorts of techniques are needed. Also, it inspired\nArmin to do some more exploration on a small Python prototype which used the\nlessons learned from Prolog and also some additional ideas from tracing JITs. So\nfar, however, the prototype is neither in RPython, nor much integrated with\nPyPy.\nThis research is not the only thing happening in the JIT-area. During the last\nyear, Antonio Cuni was working on bringing the JIT to pypy-cli. This\nconsisted mostly of writing a .NET backend for the old JIT-generator. Some\nfurther work is being done since August by John Witulski, who is writing an\nAMD64 backend for the JIT-generator for his Bachelor's thesis.\n\nWhere to go from thereDuring the sprint we discussed in which directions we should continue now. We\nplan to work quite a bit on the JIT in the coming months. Both Armin and Anto\nare in D\u00fcsseldorf for four months, and them and me plan to mostly work on the\nJIT (as well as giving a lecture on \"Dynamic Programming Languages\", trying to\nensnare some more students).\nThe first step will be to experiment a bit more with Armin's prototype. So far\nit looks rather promising, but there are some unsolved issues that we need to\nlook into first. The first issue is to think a bit about how to efficiently do\nprofiling to compile only important code paths. The other large issue are\nso-called \"virtualizables\". Roughly speaking, they are the frame objects of the\ninterpreter from which the JIT is generated. They need special treatment,\nbecause on the one hand it is important that they get optimized away to make the\ncode fast, since the frames are accessed all the time for the local variables;\non the other hand they should still be usable for introspection if code is\naround that is trying to look into them.\nWhen this is done, the prototype needs to be ported to RPython, which is a\nnon-trivial task, since it is rather dynamic so far (it is rather important that\nthe unresolved issues are done before the porting, because once the prototype is\nin RPython, experimentation will be harder). The porting has the potential to be\ntedious, but in a sense it is \"just work\", as opposed to unclear research.\nAt this point it will become important to think about the backend interface. The\ninterface that the old frontend used to produce assembler code won't be usable\nfor the new approach, so things need to be rearranged slightly. Afterwards the\nbackends will have more information and be invoked at a slightly higher level,\nwhich should allow them to produce better code.\nWhen all this is done, the JIT generator will be in a rather good state and it\nshould become possible (modulo a lot of details, of course), to use it on the\nPython interpreter.\nConclusion\nI am intentionally not attaching any time estimates to this blog post. So far\nour time estimates have not been very accurate when it comes to the JIT, which\nonly lead to disappointment when the JIT failed to materialize. We hope that we\nwill progress in interesting ways in the next four months, but who knows. Note\nthat we are really quite disappointed ourselves that it took so much longer than\nwe planned and hoped. The reason for this is mostly that this work really is\nresearch and sometimes it is just hard to predict what sort of problems turn up.\nPartial evaluation (the basis for our JIT generator) is a 30 years old technique\nthat was always just promising and never really successful, so the fact that we\nthink we can solve its problems in a few years is very much hubris anyway :-).\nOn the positive side, we think that we now know these problems much better than\never before and that we have a plan that has a chance to succeed.\nAlso we are still convinced that our approach has huge potential, despite the\ndifficulties. If we manage to pull it off, it should be significantly simpler to\nsupport new language features in the JIT and also to get speedups on some rather\ninteresting bits of the language. Some ideas we are having include generating a\nJIT for the regex engine or speed up ctypes-bindings to be nearly as fast as an\nextension module (or faster?). Also the JIT will be such that by construction\nthe JIT-generated code behaves identical to the original code, which isn't\nalways true for Psyco, for example.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/10/sprint-discussions-jit-generator-3301578822967655604.html"
    },
    {
      "title": "Sprint Discussions: C++ Library Bindings",
      "text": "At the beginning of this year, PyPy grew ctypes support, thanks to generous\nsupport by Google. This made it possible to interface with C libraries from\nour Python interpreter, something that was possible but rather tedious before.\nWhat we are lacking so far is a way to interface to large C++ libraries (like\nGUI libraries). During the sprint we had a brainstorming session about possible\napproaches for fixing this shortcoming.For CPython there are a number of approaches in common use:\n\n\nSIP, mainly used for PyQT\nSWIG\nBoost.Python\n\n\nThose all have the property that they produce some code that is then compiled\nwith a compiler to produce a CPython extension. The produced code also uses\nfunctions from CPython's C-API. This model is not simple to use for PyPy in its\ncurrent state. Since PyPy generates C code automatically, a fixed C-level API\ndoes not exist (it is not unlikely that at one point in the future we might have\nto provide one, but not yet). At the moment, PyPy very much has a \"Don't call\nus, we call you\"-approach.\nA very different approach is followed by the Reflex package, which is\ndeveloped at CERN (which has an incredible amount of C++ libraries). It is not\nmainly intended for writing Python bindings for C++ libraries but instead\nprovides reflection capabilities for C++. The idea is that for every C++ shared\nlibrary, an additional shared library is produced, which allows together with\nReflex to introspect properties of C++ classes, methods, etc. at runtime. These\nfacilities are then used for writing a small generic CPython extension module,\nthat allows CPython to use any C++ library for which this reflection information\nwas generated.\nThis approach is a bit similar to the ctypes module, apart from the fact\nthat ctypes does not use any reflection information, but the user has to\nspecify the data structures that occur in the C code herself. This makes it\nsometimes rather burdensome to write cross-platform library bindings.\nFor PyPy the approach seems rather fitting: We would need to implement only the\ngeneric extension module and could then use any number of C++ libraries. Of\ncourse some more evaluation is needed (e.g. to find out whether there are any\nrestrictions for the C++ code that the library can use and how bothersome it is\nto get this reflection information for a large library) but so far it seems\npromising.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/10/sprint-discussions-c-library-bindings-249141169883996521.html"
    },
    {
      "title": "Sprint Discussions: Release Planning",
      "text": "One of the discussions that happened during the sprint was about how to approach\nthe next PyPy release. There hasn't been a release since the end of the EU\nperiod, which is not an optimal situation. Therefore we plan to make a 1.1\nrelease at the beginning of next year, ideally before Pycon US. We'd also like\nto move towards time-based releases. This will be greatly helped by the\nnew buildbot infrastructure, which allows us to decide when the\nstate of the codebase is stable enough to release.\nAnother goal of the release is to involve more people from the wider PyPy\ncommunity by having bugdays and generally asking for more support. This will be\nparticularly useful for bugs on platforms that no one of the core developers\ngroup is using.\nFeature-wise the release will mostly contain CPython 2.5 language support,\nincluding some new extension modules (like ctypes, expat, sqlite).\nIn addition we plan to make it easier to actually install and use the PyPy\nPython interpreter, which means some sort of proper installation procedure and\nsupporting distutils on top of PyPy. Another part of the release will be\nsupport for fully sand-boxing an interpreter.\nAdditionally there were also a large number of improvements on several levels\nsince the last release, like optimizations, faster oldstyle-classes, better\nGCs, correct finalization behaviour, lots and lots of bugfixes, better\nthreading support (still with the GIL), some work on improving memory\nbehaviour,  ...\nIn contrast to our last release, we will focus mainly on PyPy's Python\nIntepreter and more particularly its C-version.   There are also various\nexperimental interpreters that PyPy contains, like for Prolog, Smalltalk,\nJavaScript and Scheme. We also don't intend to put the LLVM and Javascipt\nbackends in the release, since they are essentially unmaintained and at least\npartially broken.  If anybody is particularly interested in one of these\ncomponents, please feel free to step up and take responsibility for them.\nAnother thing that the release won't contain is a JIT.  I plan to make another\nblog-post about this soon, stay tuned.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2008/10/sprint-discussions-release-planning-7097053444808236145.html"
    },
    {
      "title": "D\u00fcsseldorf Sprint Report Days 1-3",
      "text": "The D\u00fcsseldorf sprint is currently in full progress and this post will try to\nsummarize what progress has been made in the last days. We are (again) sprinting\nat the STUPS group of the D\u00fcsseldorf University. You can find the sprint\nannouncement and the daily planning file.\nHolger and Samuele put quite some effort over several days into setting up and\nimproving PyPy's testing infrastructure. PyPy has a variety of tests. On the one\nhand, there are of course our own tests. But then we also have the CPython tests\nthat should be run on top of pypy-c. Up to now we used a custom-made pile of\nhacks, held together by lots of duct-tape. It consisted of a variety of\ndifferent machines running different things with different reporting solutions.\nSome of the old test-results can still be found on wyvern. Now we are moving\nto a buildbot based solution together with a custom reporter to have a view\nsimilar to the old one. Some details are not quite finished yet, but most of the\nthings are already working rather well (currently all the results displayed\nare from the 2.5-merge branch).\nAnother large (and ongoing) topic of work is the 2.5 branch. It contains the\nwork done by our Summer-of-Code student, Bruno Gola, of adding CPython 2.5\nfeatures to PyPy's Python interpreter. While Bruno implemented most language\nfeatures and imported the 2.5 stdlib into PyPy, a lot of details were still\nmissing. In the last days nearly everybody worked on fixing small issues and\nfailing stdlib tests. While doing that we tried to categorize some CPython tests\nas implementation dependant so that we can skip them when running on PyPy.\n\nMemory Improvements\nOne goal of the sprint is to measure and to reduce the memory behaviour of our\nPython interpreter. The idea is to make pypy-c a realistic option for use on\nembedded devices. By memory behaviour we mean both the\ndynamic memory usage (how much bytes does a dict or an instance take) as well as\nthe size of the executable and details of the GC strategy.\nAlexander, Carl Friedrich and Antonio did some work on analyzing the static data\nthat a pypy-c executable contains. Our executables have the tendency to be\nrather large, both due to a lot of code and due to a large amount of static\ndata. The analysis didn't give any really surprising results, the problem is\nmostly that we have a lot of static data originating from a bit everywhere in\nour program. Two big offenders are the unicodedata-module with about 750 KB\nof static data and the multimethod-tables with about 150 KB of data.\nArmin, Iko, Anto and Maciek worked on a new approach to malloc-removal. This is\n(for PyPy) a crucial optimization of the translation toolchain that performs\nescape analysis to find out which objects don't outlive the frame they were\nallocated in. Since RPython is garbage-collected we usually have a lot of\nallocations, so it is important to statically get rid of many of them. To\nsuccessfully do that, some inlining is needed to give the analysis more context.\nThis leads to the fact that we have rather aggressive inlining-settings to allow\nas much malloc-removal as possible. The new approach tries to inline functions\nonly if this actually leads to the successful removal of a malloc operation. The\ncode is not finished quite yet, so it remains to be seen how successful it will\nbe.\nBefore the sprint Maciek had started to work on a mark-compact GC for PyPy. The\nidea is that it is better for memory-constrained-environments because it does\nnot double the memory-requirements during collections. During the sprint Armin\nand Maciek worked on cleaning up the code a bit and then merging the branch.\nAn interesting property of the mark-compact GC is that after a collection all\nthe memory that is not currently used by the program is returned to the\noperating system. Right now the GC is not as fast as our more mature ones, but\nit probably will be the basis for future tweaking.\nA small thing that was done by Alexander and Carl Friedrich to make objects smaller is\nto enable shared instance dictionaries also for instances of old-style\nclasses. Before it worked only for instances of new-style classes. Shared\ninstance dictionaries are a way to reduce the memory-usage of instances. In the\noptimal case, it gives the same memory-savings that __slots__ are giving,\nbut without any behavioural changes. Conceptually it is very similar e.g. to\nthe notion of \"map\" in the Self project, or the hidden classes that Google Chrome's V8\nis using (click on the link, there are nice graphics). The\ndifference is that for them it is mostly a way to get faster attribute access,\nand PyPy is so far only using it form memory savings (but that might change in\nthe future).\nIn parallel to all the other work, John Witulski worked tirelessly on advancing\nthe AMD64-JIT-backend. John has the implementation of this backend as the topic\nof his Bachelor's thesis. He is progressing quite well (especially also\nconsidering that this is his first sizeable Python project ever), just sometimes\nbeing impaired by such annoyances as errors in the official Intel documentation.\nBy now the backend is supporting many integer operations and control flow.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/10/dsseldorf-sprint-report-days-1-3-5256639868851086032.html"
    },
    {
      "title": "Prolog-JIT Master's-Thesis Finished",
      "text": "As we already blogged, in the last half-year or so, Michael Leuschel, Armin\nand me did a lot of JIT generator work on a Prolog prototype. The idea was to\nexperiment more quickly with some techniques than what would have been possible\nwith RPython. These experiments were quite successful in themselves. With very\nlittle code we managed to get a JIT that is not doing too badly when compared to\nexisting projects for Prolog.\nThis Prolog work was also the subject of my Master's thesis. I finished the\nthesis about two weeks ago (and since then have been mostly sleeping and then\nsprinting). The thesis should be self-contained when it comes to explaining the\nJIT concepts but needs knowledge of Prolog to be understandable.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2008/10/prolog-jit-masters-thesis-finished-5462132148241449867.html"
    },
    {
      "title": "PyPy/Python at the Maemo summit",
      "text": "Maciej and me visited the Maemo Summit in Berlin -\na community meetup around Nokia's Linux based\nmobile platform.  We spontaneously did a lightning\ntalk about a first running pypy-c on Maemo\nand got nice feedback.  \n\nWe also had a nice lunch with guys from the INDT in Brazil, including Marcio Marcedo and Marcelo Eduardo.  It turns out that Python is used a lot on Maemo, for example the nice Canola UI is done with it.  Will be interesting to see how this shapes up in relation to the iPhone and Android.\n\nA lot of Nokia engineers were around and they announced that from October on they are going for weekly new releases of their SDK for the new Fremantle (Maemo-5) debian-based platform until the SDK becomes final - if we got this right.  \n\nFunnily enough, we met Marius Gedminas from the Programmers of Vilnius - he gave a lightning talk on his impressions as a community member.  We think python programmers really should go much more to non-Python centric conferences.\n\nThe whole event took place at the C-Base - was a bit\ncrammed in some of the sessions with something like 200 people attending.\n\ncheers, Maciej and Holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/09/pypypython-at-maemo-summit-6115106472056714072.html"
    },
    {
      "title": "Pycon UK, Javascript and the GIL",
      "text": "Just got back from Pycon UK 2008 - here are some impressions. \nBoth the keynote speakers Mark Shuttleworth (Canonical) and \nTed Leung (Sun Microsystems) expressed their concerns about\nJavascript becoming so fast and prominent that it could displace\nPython in the future.  They also highlighted the fact that\nMulti-core systems get cheaper and more popular also on \ndesktop computers or notebooks.  They challenged the community\nto advance Python implementations to exploit it.  Question was up \nwhat PyPy can do here.  As it stands, PyPy still uses the good old\nGlobal Interpreter Lock (GIL) but our approaches should indeed \nlend itself well to do experimentation with free threading.  \n\nDuring the 2-day conference we met many interesting people, most \nnotably the guys from Resolver, among them William Reade who is working on\nIronClad -- which implements a fake python25.dll on top of\nIronPython.  He presented some good results for Numpy in his\nlightning talk.  This approach is surely something to follow\nclosely and potentially use for PyPy. \n\nWe also had lunch and a couple of chats with Jacob Kaplan-Moss from\nDjango fame - he is apparently up to try use PyPy's sandboxing features\nfor one of his projects, cool!\n\nConference itself was well organized for the 230 attending people - although\nthe venue might be a bit small for next year's EuroPython.  Ah, and\nwe gave three well attended talks, find the slides here:\n\n\nPyPy status and 1.1 plans\nPyPy JIT\npy.test tutorial\n\ncheers,\nHolger, Maciej, Anto (associated through merlinux, btw)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/09/pycon-uk-javascript-and-gil-8387247619202094916.html"
    },
    {
      "title": "D\u00fcsseldorf PyPy sprint 5-13th October, 2008",
      "text": "The PyPy team is happy to announce the next sprint, which will take place in\nthe Computer Science Department of the University of D\u00fcsseldorf, Germany.\nSprinting will start on the 6th of October and go on till the 12th. Please\narrive on the day before if you want to come.\nTopics of the sprint will be aiming at a 1.1 release and to work on integrating PyPy better \nwith small devices. Other topics are also welcome!\nWe will try to find a hotel with group rates, so if you are interested, please\nsign up soon! See the announcement for more details.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/09/dsseldorf-pypy-sprint-5-13th-october-8919978872121664955.html"
    },
    {
      "title": "pylib/py.test 0.9.2 released",
      "text": "PyPy and its 14638 automated tests use the py.test tool which is also used by many other projects.  PyPy developers have actually driven and contributed a lot to its development.  \n\nI just released version 0.9.2 of the py lib mainly fixing Windows issues and providing better packaging and integration with setuptools.  It's usable completely independently from PyPy - \"easy_install py\" gives you the py.test command line. Of course you can run py.test on top of a translated PyPy version as well. Here is a quick summary of what the py lib provides besides py.test:\n\npy.execnet: ad-hoc code distribution to SSH, Socket and local sub processes\npy.magic.greenlet: micro-threads on standard CPython (\"stackless-light\") and PyPy\npy.path: path abstractions over local and subversion files\npy.code: dynamic code compile and traceback printing support\ntested against Linux, Win32, OSX, works on python 2.3-2.6\n\nGood general entry points for installation and documentation:\n\nPypi pages\nDownload/Install\nDocumentation/API\n\nhave fun, holger krekel",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2008/08/pylibpytest-092-released-6233865913406513469.html"
    },
    {
      "title": "New translation option: --opt",
      "text": "Hi all,\n\nA few command-line options for translate.py have changed.\nMost interesting is that optimization levels are selected with\nthe option --opt, or -O for short.  This replaces --allopts,\nwhich was also called --faassen in reference to a person who\nis actually not involved in PyPy (so that was a bit of a\nstrange joke).  Also, --allworkingmodules is the default\nnowadays, and can be cancelled with --no-allworkingmodules.\nThreads are also included in --allworkingmodules now.\n\nExamples:\n\ntranslate.py (reasonable default, corresponds to --opt=2)\n    translate.py --opt=3 (best, maybe 10-20% faster)\n    translate.py --opt=1 (translation is faster and less RAM-hungry)\n\n\nFor more information, see:\n    \nGetting started\n    List of optimization levels",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/08/new-translation-option-opt-7737733390438084418.html"
    },
    {
      "title": "PyPy runs unmodified django 1.0 beta",
      "text": "This is just a quick update post to previous post - django folks commited all\noutstanding tickets and we are able to run unmodified django\non top of pypy-c. Instructions how to do it are well explained\non django wiki entry\n\nenjoy,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/08/pypy-runs-unmodified-django-10-beta-7105507436425430319.html"
    },
    {
      "title": "Europython 2008 PyPy talks and sprint sum up",
      "text": "The EuroPython 2008 conference and sprints have finished - it certainly was \na very eventful and successful conference for PyPy.  And many very interesting \nnon-PyPy talks as well.  PyPy presentations are available online: PyPy status talk\nPyPy for the rest of us, PyPy behind the scenes.  Armin and Maciej also did a well-attended \ntalk about PyPy's garbage collection, but that was quite interactive, no slides. \n\nThe talks were all well visited and we got good questions.  However, we still \nneed to work on sorting out the \"PyPy technology cloud\" and how to present\nit to different audiences.  Anyway, we are happy to hear feedback or questions\nabout the talks!\n\nAfter the conference there was a three-day PyPy sprint. Despite \nthe fact that most PyPy core developers were zombies, \nwe made good progress.  Particularly our newcomers did very well.  \nHere are some results: \n\n itertools rewritten in RPython for performance by Jakub\n  Gustak and Andrew Durdin \n\n a new ctypes based dbm and hashlib module, both by Gasper Zejn \n  with support from Henrik Vendelbo, they also got ctypes to nicely work on OSX. (sorry for lack of proper letters in names :)\n\n implement builtin function call profiling by Stephan Diehl, Antonio and Armin. \n\n running\n  Pinax on top of pypy-c, by Henrik, Holger, Gasper. \n\n Jim Baker started a _rawffi.py for Jython using JNA aiming\n  to provide support to run PyPy's ctypes on top of Jython. \n  When Jython gets this to run, PyPy's JVM backend should be \n  able to use it. Talk about Code Reuse :) \n\n oldstyle classes are now the default, this makes \n  PyPy mimick very closely cpython's 2.5 object model. \n\n Andrew started a port of the Malbolge \n  interpreter written in Python to RPython (obviously the only missing \n  link for PyPy to take over the world). \n\n various cleanups (a new option \"--lonepycfiles\" helps with\n  saner imports, remove int-float comparison shortcuts, ...) \n\nAt the end of the sprint we also discussed initial plans for a 1.1 release which we'd like to make happen this year.   So we are generally looking forward to a busy rest of 2008 and luckily this starts by many of us taking a good vacation first :) \n\nCheers,\nfijal & holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/07/europython-2008-pypy-talks-and-sprint-2255727845041197411.html"
    },
    {
      "title": "Finding Bugs in PyPy with a Fuzzer",
      "text": "Last week I played a bit with Fusil, which is a fuzzing framework.  The idea is\nto feed the interpreter code that calls the functions of a module with random values\nof various types as arguments in the hope that one hits an unchecked case. This is\ndone until a problem is hit , the most common problem being a segfault.  Victor Stinner,\nthe author of Fusil, is a regular in the PyPy IRC channel and thankfully helped me\ngetting started with Fusil. I used his project description for CPython as a starting\npoint and tweaked it a bit.  Reason is that PyPy is harder to segfault and so\nI tweaked Fusil to also count uncaught RPython-level exceptions as such a problem.\n(RPython has full exception support, and if an RPython-exception escapes to the top\nlevel, the Python interpreter aborts.  One should not be able to exploit this but\nbut for a user it is bad enough, because such exceptions cannot be caught from\nPython code.)\nUsing Fusil I found a number of cases where such exceptions happened (in some\npickle support-code, in the expat parser, in the os and in the termios\nmodule) and also one or two segfaults (in the parser module, of all places).\nI fixed all these problems so that by\nnow the fuzzer just runs for a very long time and only finds things that take\ntoo long (so they count as a way to do a DoS attack) like\npow(12355123123L, 12351512123121L) or round(1, 1000000000) (the latter\nshould probably be fixed). This probably just means that the fuzzer is not good\nenough, because there are certainly segfaults left in PyPy. However, the fact\nthat it is rather hard to find them validates our approach of using a\nhigh-level memory-managed language for our interpreter. Victor tells me that it\nis rather easy to find segfaults in CPython this way, he already found quite\nsome problems.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/07/finding-bugs-in-pypy-with-fuz-7503072572107631526.html"
    },
    {
      "title": "PyPy's Python runs Pinax / Django",
      "text": "During the EP2008 sprint we got Pinax running on top of PyPy. At our play1 server we have it running on top of pypy-c.  Not that you'll notice many differences to the original site but that's the point, isn't it? ... Well, in fact i am too lazy to customize our play1 version now - i rather spent a nice evening with the other sprint guys :) \n\nPinax integrates numerous reusable Django apps to take care of the things that many sites have in common. Many thanks particularly to Henrik Vendelbo who sorted out various Pinax and PyPy issues, and wrote up a nice DjangoAndPyPy wiki page describing the installation process.\n\ngreetings from Vilnius (Lithunia), Holger",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/07/pypys-python-runs-pinax-django-1265543049596913506.html"
    },
    {
      "title": "EP2008: PyPy meets Jython",
      "text": "One of the great events at EuroPython 2008 were our chats and meetings with the Jython and Sun people.  The Jython people recently are pushing into releasing Python version 2.5 and they currently pursue many interesting sub projects.  Coincidentally, PyPy also has tons of interesting areas and results :)  So we eventually got into brainstorming a number of possible technical collab ideas.  Further below is a first list as i wrote it down from our 10 people PyPy / Jython 30 minute close up meeting yesterday.\n\nIt felt great to be able to talk to the Jython people this way - kudos to Sun for their clear commitments and open ways to go about things!  I sense a genuine interest on fair collaboration with non-java developer communities.  Seems like they are serious about not focusing on \"Java this\", \"Java that\" anymore  but rather focus on the JVM platform.  Good!  And about language\nindependent interest in ambitious technology. Even Better!  I am tensed to see how things go from here.\n\nSo here the list of technical collab ideas:\nctypes - try to create _rawffi module in Java for Jython, which will enable Jython to reuse our existing ctypes implementation (and have PyPy use the Jython-rawffi for its own for PyPy.JVM) generally see to share work / (continue) collaborate regarding extension modulesJython/PyPy (and eventually IronPython): document known differences to CPython, maybe in a PEPPython Interpreter for Jython (in order to run CPython's .pyc files): re-use pypy's bytecode evaluator, implement a \"Jython object space\". re-use rpython-extension modules for jython (e.g. SRE), by compiling them to Java and reusing as a native library.collaborate on testing framework / benchmarking, have a common site to show test resultsmake py.test compatible with jythoncome up with a set of \"pure Python language\" tests, which would gather and refactor tests from CPython, PyPy and Jython. look into using java types / jython approaches for implementing free threading.share knowledge regarding JIT / psyco\nIf you have any more ideas, comments or would like to join efforts, let us know!\n\nCheers and thanks to Ted Leung, Frank Wierzbiki, Jim Baker and Tobias Ivarsson from Sun and Jython fame respectively,\n\nHolger",
      "tags": "ep2008,jython,pypy,sun",
      "url": "https://www.pypy.org/posts/2008/07/ep2008-pypy-meets-jython-1107070144380217881.html"
    },
    {
      "title": "PyPy at the EuroPython 2008",
      "text": "Greetings from Vilnius, Lithuania. There were already\ntwo pypy talks, one performed by Jacob Hallen\nPyPy for the rest of us and second\nby Maciej Fijalkowski PyPy status talk. The thing that\nwe forgotten to tell is that PyPy sandboxing feature\ncan also easily limit CPU and RAM usage as well as\nany other possible resource (like network transfer).\nFor anyone who would like to join, there is a PyPy\nsprint after the conference.\n\nCheers,\narigo & fijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/07/pypy-at-europython-2008-1488914968455397674.html"
    },
    {
      "title": "JIT in Prolog",
      "text": "Hi all,\n\nSome news from the JIT front.  Progress on the JIT has been low-profile\nin the past few months.  No big results to announce yet, but we have\nplayed with some new ideas, and they are now documented as a draft\nresearch paper: Towards Just-In-Time Compilation and Specialisation of Prolog.\n\nProlog?  Yes.  To understand this slightly unusual choice of programming\nlanguage, here is first some background about our JIT.\n\nPyPy contains not a JIT but a JIT generator, which means that we\nonly write an interpreter for a language (say, the complete Python\nlanguage), and we get a JIT \"for free\".  More precisely, it's not for\nfree: we had to write the JIT generator, of course, as well as some\namount of subtle generic support code.  The JIT generator preprocesses\nthe (complete Python) interpreter that we wrote and links the result\nwith the generic support code; the result is a (complete Python) JIT.\n\nThe way that this works so far gives us a generated JIT that is very\nsimilar to Psyco in the way\nit works.\nBut Psyco has issues (and so the current PyPy JITs have the same issues):\nit can sometimes produce too much machine code,\ne.g. by failing to notice that two versions of the machine code are\nclose enough that they should really be one; and it can also sometimes\nfail in the opposite way, by making a single sub-efficient version of\nthe machine code instead of several efficient specialized versions.\n\nA few months ago we have chosen to experiment with improving this\ninstead of finishing and polishing what we had so far.  The choice was\nmostly because we were (and still are) busy finishing and polishing\neverything else in PyPy, so it was more fun to keep at least the JIT on\nthe experimental side.  Besides, PyPy is now getting to a rather good\nand complete state, and it is quite usable without the JIT already.\n\nAnyway, enough excuses.  Why is this about Prolog?\n\nIn PyPy, both the (complete Python) interpreter and the JIT support code\nare in RPython.  Now RPython is not\nan extremely complicated language, but still, it is far from the top on a\nminimalism scale.  In general, this is a good in practice (or at least I\nthink so): it gives\na reasonable balance because it is convenient to write interpreters\nin RPython, while not being so bloated that it makes our translation\ntoolchain horribly complicated (e.g. writing garbage collectors for\nRPython - or even JIT generators - is reasonable).  Still, it is not the\nbest choice for early research-level experimentation.\n\nSo what we did instead recently is hand-write, in Prolog, a JIT that\nlooks similar to what we would like to achieve for RPython with our JIT\ngenerator.  This gave much quicker turnaround times than we were used to\nwhen we played around directly with RPython.  We wrote tiny example\ninterpreters in Prolog (of course not a complete Python interpreter).\nSelf-inspection is trivial in Prolog, and generating Prolog code at\nruntime is very easy too.  Moreover, many other issues are also easier\nin Prolog: for example, all data structures are immutable \"terms\".\nOther languages than Prolog would have worked, too, but it happens to be\none that we (Carl Friderich, Michael Leuschel and myself) are familiar\nwith -- not to mention that it's basically a nice small dynamic\nlanguage.\n\nOf course, all this is closely related to what we want to do in PyPy.\nThe fundamental issues are the same.  Indeed, in PyPy, the major goals\nof the JIT are to remove, first, the overhead of allocating objects all\nthe time (e.g. integers), and second, the overhead of dynamic dispatch\n(e.g. finding out that it's integers we are adding).  The equivalent\ngoals in Prolog are, first, to avoid creating short-lived terms, and\nsecond, to remove the overhead of dispatch (typically, the dispatching\nto multiple clauses).  If you are familiar with Prolog you can find more\ndetails about this in the paper. So far we already played with many possible solutions\nin the Prolog JIT, and the paper describes the most mature one; we have\nmore experimentation in mind.  The main point here is that these are\nmostly language-independent techniques (anything that works both in\nProlog and in RPython has to be language-independent, right? :-)\n\nIn summary, besides the nice goal of speeding up Prolog, we are trying\nto focus our Prolog JIT on the issues and goals that have equivalents in\nthe PyPy JIT generator.  So in the end we are pretty convinced that it\nwill give us something that we can backport to PyPy -- good ideas about\nwhat works and what doesn't, as well as some concrete algorithms.",
      "tags": "jit",
      "url": "https://www.pypy.org/posts/2008/06/hi-all-some-news-from-jit-front-7534695765973581706.html"
    },
    {
      "title": "PyPy code swarm",
      "text": "Following the great success of code_swarm, I recently produced a\nvideo that shows the commit history of the PyPy project.\nThe video shows the commits under the dist/ and branch/\ndirectories, which is where most of the development happens.\nIn the first part of the video, you can see clearly our sprint based\napproach: the video starts in February 2003, when the first PyPy\nsprint took place in Hildesheim: after a lot of initial activity, few\ncommits happened in the next two months, until the second PyPy sprint,\nwhich took place in Gothenburg in late May 2003; around the minute\n0:15, you can see the high commit rate due to the sprint.\nThe next two years follow more or less the same pattern: very high\nactivity during sprints, followed by long pauses between them; the\nmost interesting breaking point is located around the minute 01:55;\nit's January 2005, and when the EU project starts, the number of\ncommits just explodes, as well as the number of people involved.\nI also particularly appreciated minute 03:08 aka March 22, 2006: it's\nthe date of my first commit to dist/, and my nickname magically\nappears; but of course I'm biased :-).\nThe soundtrack is NIN - Ghosts IV - 34: thanks to xoraxax for\nhaving added the music and uploaded the video.\n                  PyPy Codeswarm from solse@trashymail.com on Vimeo.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/pypy-code-swarm-7038411918926116477.html"
    },
    {
      "title": "Funding of some recent progress by Google's Open Source  Programs",
      "text": "As readers of this blog already know, PyPy development has\nrecently focused on getting the code base to a more usable state. One\nof the most important parts of this work was creating an\nimplementation of the ctypes module for PyPy, which\nprovides a realistic way to interface with external libraries. The\nmodule is now fairly complete (if somewhat slow), and has generated a\ngreat deal of community interest.  One of the main reasons this work\nprogressed so well was that we received funding from Google's Open\nSource Programs Office.  This is\nreally fantastic for us, and we cannot thank Google and Guido enough for helping PyPy progress\nmore rapidly than we could have with volunteer-only time!\nThis funding opportunity arose from the PyPy US road trip at the end\nof last year, which included a visit to Google. You\ncan check out the video\nof the talk we gave during our visit.  We wrapped up our day with\ndiscussions about the possibility of Google funding some PyPy work and\nsoon after a we were at work on the proposal for improvements we'd\nsubmitted.\nOne nice side-effect of the funding is indeed that we can use some of\nthe money for funding travels of contributors to our sprint meetings.\nThe next scheduled Google funding proposal also aims at making our\nPython interpreter more usable and compliant with CPython. This will be done by trying to\nfully run Django on top of PyPy.  With\nmore efforts like this one we're hoping that PyPy can start to be used\nas a CPython replacement before the end of 2008.\nMany thanks to the teams at merlinux and Open End for making this development possible, including\nCarl Friedrich Bolz, Antonio Cuni, Holger Krekel, Maciek Fijalkowski\nat merlinux, Samuele Pedroni and yours truly at Open End.\nWe always love to hear feedback from the community, and you can get\nthe latest word on our development and let us know your thoughts here in the comments.\nBea D\u00fcring, Open End AB\n\nPS: Thanks Carl Friedrich Bolz for drafting this post.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/pypy-improvements-5272963843122158791.html"
    },
    {
      "title": "Pdb++ and rlcompleter_ng",
      "text": "When hacking on PyPy, I spend a lot of time inside pdb; thus, I tried\nto create a more comfortable environment where I can pass my nights\n:-).\nAs a result, I wrote two modules:\n\n\npdb.py, which extends the default behaviour of pdb, by adding\nsome commands and some fancy features such as syntax highlight and\npowerful tab completion; pdb.py is meant to be placed somewhere in\nyour PYTHONPATH, in order to override the default version of pdb.py\nshipped with the stdlib;\nrlcompleter_ng.py, whose most important feature is the ability\nto show coloured completions depending on the type of the objects.\n\n\nTo find more informations about those modules and how to install them,\nhave a look at their docstrings.\nIt's important to underline that these modules are not PyPy specific,\nand they work perfectly also on top of CPython.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/pdb-and-rlcompleterng-2414105295687348881.html"
    },
    {
      "title": "Running Nevow on top of PyPy",
      "text": "Another episode of the \"Running Real Application of top of PyPy\" series:\n\nToday's topic: Divmod's Nevow. Nevow (pronounced as the French \"nouveau\", or \"noo-voh\") is a web application construction kit written in Python.  Which means it's just another web framework, but this time built on top of Twisted.\nWhile, due to some small problems we're not yet able to pass full Twisted test suite on top of pypy-c, Nevow seems to be simple enough to work perfectly (959 out of 960 unit tests passing, with the last one recognized as pointless and about to be deleted).  Also, thanks to\nexarkun, Nevow now no longer relies on ugly details like refcounting.\n\nAs usual, translate pypy using:\n\ntranslate.py --gc=hybrid --thread targetpypystandalone --faassen --allworkingmodules --oldstyle\n\nOf course, obligatory to the series, screenshot:\n\n\nThis is Nevow's own test suite.\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/running-nevow-on-top-of-pypy-58891137802412513.html"
    },
    {
      "title": "Next sprint: Vilnius/Post EuroPython, 10-12th of July",
      "text": "As happened in the last years, there will be a PyPy sprint just after\nEuroPython.  The sprint will take place in the same hotel as the\nconference, from 10th to 12th of July.\nThis is a fully public sprint: newcomers are welcome, and on the first\nday we will have a tutorial session for those new to PyPy development.\nSome of the topics we would like to work on:\n\n\ntry out Python programs and fix them or fix PyPy or fix performance bottlenecks\nsome JIT improvement work\nport the stackless transform to ootypesystem\n\n\nOf course, other topics are also welcome.\nFor more information, see the full announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/next-sprint-vilniuspost-europython-10-3844544842675903586.html"
    },
    {
      "title": "German Introductory Podcast About Python and PyPy",
      "text": "During the Berlin Sprint Holger was interviewed by Tim Pritlove for Tim's\nPodcast \"Chaosradio Express\". The whole thing is in German, so only\ninteresting to German-speakers. The PyPy episode can be found here. The\ninterview is touching on a lot of topics, starting with a fairly general intro\nabout what Python is and why it is interesting and then moving to explaining and\ndiscussing PyPy. The bit about PyPy starts after about 45 minutes. There is also\na comment page about the episode.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/german-introductory-podcast-about-3836017753197345761.html"
    },
    {
      "title": "Running Pylons on top of PyPy",
      "text": "The next episode of the \"Running Real Applications on Top of PyPy\" series: \nYesterday, we spend some time with Philip Jenvey on tweaking Pylons and PyPy to cooperate with each other. While doing this we found some pretty obscure details, but in general things went well.\n\nAfter resolving some issues, we can now run all (72) Pylons tests on\ntop of pypy-c compiled with the following command:\n\n\ntranslate.py --gc=hybrid --thread targetpypystandalone --faassen --allworkingmodules --oldstyle\n\nand run some example application. Here is the obligatory screenshot (of course\nit might be fake, as usual with screenshots). Note: I broke application on purpose to showcase cool debugger, default screen is just boring:\n\nPlease note that we run example application without DB access, since\nwe need some more work to get SQLAlchemy run on top of pypy-c together with\npysqlite-ctypes. Just one example of an obscure details that sqlalchemy is\nrelying on in the test suite:\n\n\n  class A(object):\n  \u00a0\u00a0locals()[42] = 98\n\n\nUpdate:This is only about new-style classes.\n\nThis works on CPython and doesn't on PyPy.\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/running-pylons-on-top-of-pypy-3234492105090025733.html"
    },
    {
      "title": "List comprehension implementation details",
      "text": "List comprehensions are a nice feature in Python. They are, however, just\nsyntactic sugar for for loops. E.g. the following list comprehension:\n\ndef f(l):\n    return [i ** 2 for i in l if i % 3 == 0]\n\nis sugar for the following for loop:\n\ndef f(l):\n    result = []\n    for i in l:\n        if i % 3 == 0:\n            result.append(i ** 2)\n    return result\n\nThe interesting bit about this is that list comprehensions are actually\nimplemented in almost exactly this way. If one disassembles the two functions\nabove one gets sort of similar bytecode for both (apart from some details, like\nthe fact that the append in the list comprehension is done with a special\nLIST_APPEND bytecode).\nNow, when doing this sort of expansion there are some classical problems: what\nname should the intermediate list get that is being built? (I said classical\nbecause this is indeed one of the problems of many macro systems). What CPython\ndoes is give the list the name _[1] (and _[2]... with nested list\ncomprehensions). You can observe this behaviour with the following code:\n\n$ python\nPython 2.5.2 (r252:60911, Apr 21 2008, 11:12:42)\n[GCC 4.2.3 (Ubuntu 4.2.3-2ubuntu7)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> [dir() for i in [0]][0]\n['_[1]', '__builtins__', '__doc__', '__name__', 'i']\n>>> [[dir() for i in [0]][0] for j in [0]][0]\n['_[1]', '_[2]', '__builtins__', '__doc__', '__name__', 'i', 'j']\n\nThat is a sort of nice decision, since you can not reach that name by any\n\"normal\" means. Of course you can confuse yourself in funny ways if you want:\n\n>>> [locals()['_[1]'].extend([i, i + 1]) for i in range(10)]\n[0, 1, None, 1, 2, None, 2, 3, None, 3, 4, None, 4, 5, None, 5, 6, None, 6, 7, None, 7, 8, None, 8, 9, None, 9, 10, None]\n\nNow to the real reason why I am writing this blog post. PyPy's Python\ninterpreter implements list comprehensions in more or less exactly the same way,\nwith on tiny difference: the name of the variable:\n\n$ pypy-c-53594-generation-allworking\nPython 2.4.1 (pypy 1.0.0 build 53594) on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n``the globe is our pony, the cosmos our real horse''\n>>>> [dir() for i in [0]][0]\n['$list0', '__builtins__', '__doc__', '__name__', 'i']\n\n\nNow, that shouldn't really matter for anybody, should it? Turns out it does. The\nfollowing way too clever code is apparently used a lot:\n\n__all__ = [__name for __name in locals().keys() if not __name.startswith('_') '\n               or __name == '_']\n\nIn PyPy this will give you a \"$list0\" in __all__, which will prevent the\nimport of that module :-(. I guess I need to change the name to match CPython's.\nLesson learned: no detail is obscure enough to not have some code depending\non it. Mostly problems on this level of obscurity are the things we are fixing\nin PyPy at the moment.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/list-comprehension-implementation-5289956690288817225.html"
    },
    {
      "title": "Better Profiling Support for PyPy",
      "text": "As PyPy is getting more and more usable, we need better tools to use to work on certain applications running on top of PyPy. Out of this interest, I spent some time implementing the _lsprof module, which is a part of the standard library since Python2.5. It is necessary for the cProfile module, which can profile Python programs with high accuracy and a lot less overhead than the older, pure-python profile module. Together with the excellent\nlsprofcalltree script, you can display this data using kcachegrind, which gives you great visualization possibilities for your profile data.\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/06/better-profiling-support-for-pypy-1848129914083462080.html"
    },
    {
      "title": "Threads and GCs",
      "text": "Hi all,\n\nWe can now compile a pypy-c that includes both thread support\nand one of our semi-advanced garbage collectors.  This means\nthat threaded Python programs can now run not only with a\nbetter performance, but without the annoyances of the Boehm\ngarbage collector.  (For example, Boehm doesn't like too much\nseeing large numbers of __del__(), and our implementation of\nctypes uses them everywhere.)\n\nMagic translation command (example):\n\n   translate.py --thread --gc=hybrid targetpypystandalone --faassen --allworkingmodules\n\nNote that multithreading in PyPy is based on a global\ninterpreter lock, as in CPython.  I imagine that we will get\nrid of the global interpreter lock at some point in the future\n-- I can certainly see how this might be done in PyPy, unlike\nin CPython -- but it will be a lot of work nevertheless.  Given\nour current priorities, it will probably not occur soon unless\nsomeone steps in.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/threads-and-gcs-1126087726480790112.html"
    },
    {
      "title": "Progresses on the CLI JIT backend front",
      "text": "In the last months, I've actively worked on the CLI backend for PyPy's\nJIT generator, whose goal is to automatically generate JIT compilers\nthat produces .NET bytecode on the fly.\nThe CLI JIT backend is far from be completed and there is still a lot\nof work to be done before it can handle the full PyPy's Python\ninterpreter; nevertheless, yesterday I finally got the first .NET\nexecutable that contains a JIT for a very simple toy language called\ntlr, which implements an interpreter for a minimal register based\nvirtual machine with only 8 operations.\nTo compile the tlr VM, follow these steps:\n\n\nget a fresh checkout of the oo-jit branch, i.e. the branch\nwhere the CLI JIT development goes on:\n\n$ svn co https://codespeak.net/svn/pypy/branch/oo-jit\n\n\ngo to the oo-jit/pypy/jit/tl directory, and compile the tlr VM\nwith the CLI backend and JIT enabled:\n\n$ cd oo-jit/pypy/jit/tl/\n$ ../../translator/goal/translate.py -b cli --jit --batch targettlr\n\n\n\n\nThe goal of our test program is to compute the square of a given\nnumber; since the only operations supported by the VM are addition and\nnegation, we compute the result by doing repetitive additions; I won't\ndescribe the exact meaning of all the tlr bytecodes here, as they are\nquite self-documenting:\n\nALLOCATE,    3,   # make space for three registers\nMOV_A_R,     0,   # i = a\nMOV_A_R,     1,   # copy of 'a'\n\nSET_A,       0,\nMOV_A_R,     2,   # res = 0\n\n# 10:\nSET_A,       1,\nNEG_A,\nADD_R_TO_A,  0,\nMOV_A_R,     0,   # i--\n\nMOV_R_A,     2,\nADD_R_TO_A,  1,\nMOV_A_R,     2,   # res += a\n\nMOV_R_A,     0,\nJUMP_IF_A,  10,   # if i!=0: goto 10\n\nMOV_R_A,     2,\nRETURN_A          # return res\n\nYou can find the program also at the end of the tlr module; to get an\nassembled version of the bytecode, ready to be interpreted, run this\ncommand:\n\n$ python tlr.py assemble > square.tlr\n\nNow, we are ready to execute the code through the tlr VM; if you are\nusing Linux/Mono, you can simply execute the targettlr-cli script\nthat has been created for you; however, if you use Windows, you have\nto manually fish the executable inside the targettlr-cli-data\ndirectory:\n\n# Linux\n$ ./targettlr-cli square.tlr 16\n256\n\n# Windows\n> targettlr-cli-data\\main.exe square.tlr 16\n256\n\nCool, our program computed the result correctly! But, how can we be\nsure that it really JIT compiled our code instead of interpreting it?\nTo inspect the code that it's generated by our JIT compiler, we simply\nset the PYPYJITLOG environment variable to a filename, so that the\nJIT will create a .NET assembly containing all the code that has been\ngenerated by the JIT:\n\n$ PYPYJITLOG=generated.dll ./targettlr-cli square.tlr 16\n256\n$ file generated.dll\ngenerated.dll: MS-DOS executable PE  for MS Windows (DLL) (console) Intel 80386 32-bit\n\nNow, we can inspect the DLL with any IL disassembler, such as\nilasm or monodis; here is an excerpt of the disassembled code,\nthat shows how our square.tlr bytecode has been compiled to .NET\nbytecode:\n\n.method public static  hidebysig default int32 invoke (object[] A_0, int32 A_1)  cil managed\n{\n    .maxstack 3\n    .locals init (int32 V_0, int32 V_1, int32 V_2, int32 V_3, int32 V_4, int32 V_5)\n\n    ldc.i4 -1\n    ldarg.1\n    add\n    stloc.1\n    ldc.i4 0\n    ldarg.1\n    add\n    stloc.2\n    IL_0010:  ldloc.1\n    ldc.i4.0\n    cgt.un\n    stloc.3\n    ldloc.3\n    brfalse IL_003b\n\n    ldc.i4 -1\n    ldloc.1\n    add\n    stloc.s 4\n    ldloc.2\n    ldarg.1\n    add\n    stloc.s 5\n    ldloc.s 5\n    stloc.2\n    ldloc.s 4\n    stloc.1\n    ldarg.1\n    starg 1\n\n    nop\n    nop\n    br IL_0010\n\n    IL_003b:  ldloc.2\n    stloc.0\n    br IL_0042\n\n    ldloc.0\n    ret\n}\n\nIf you know a bit IL, you can see that the code generated is not\noptimal, as there are some redundant operations like all those\nstloc/ldloc pairs; however, while not optimal, it is still quite good\ncode, not much different to what you would get by writing the square\nalgorithm directly in e.g. C#.\nAs I said before, all of this is still work in progress and there is\nstill much to be done. Stay tuned :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/progresses-on-cli-jit-backend-front-1021772190959551376.html"
    },
    {
      "title": "More windows support",
      "text": "Recently, thanks to Amaury Forgeot d'Arc and Michael Schneider, Windows became more of a first-class platform for PyPy's Python interpreter. Most RPython extension modules are now considered working (apart from some POSIX specific modules). Even CTypes now works on windows!\n\n\nNext step would be to have better buildbot support for all supported platforms (Windows, Linux and OS X), so we can control and react to regressions quickly. (Buildbot is maintained by JP Calderone)\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/more-windows-support-1747028151130099034.html"
    },
    {
      "title": "S3-Workshop Potsdam 2008 Writeup",
      "text": "Trying to give some notes about the S3 Workshop in Potsdam that several\nPyPyers and Spies (Armin, Carl Friedrich, Niko, Toon, Adrian) attended before\nthe Berlin sprint.  We presented a paper about SPy there. Below are some mostly\nrandom note about my (Carl Friedrich's) impressions of the conference and some\ntalk notes. Before that I'd like to give thanks to the organizers who did a\ngreat job. The workshop was well organized, the social events were wonderful (a\nvery relaxing boat trip in the many lakes around Potsdam and a conference\ndinner).\nVideo recordings of all the talks can be found on the program page.\n\nInvited Talks\n\"Late-bound Object Lambda Architectures\" by Ian Piumarta was quite an inspiring\ntalk about VPRI's attempt at writing a flexible and understandable computing\nsystem in 20K lines of code. The talk was lacking a bit in technical details, so\nwhile it was inspiring I couldn't really say much about their implementation.\nApart from that, I disagree with some of their goals, but that's the topic of\nanother blog post.\n\"The Lively Kernel \u2013 A Self-supporting System on a Web Page\" by Dan Ingalls. Dan\nIngalls is one of the inventors of the original Smalltalk and of Squeak. He was\ntalking about his latest work, the attempts of bringing a Squeak-like system to\na web browser using JavaScript and SVG. To get some feel for what exactly The\nLively Kernel is, it is easiest to just try it out (only works in Safari\nand Firefox 3 above Beta 5 though). I guess in a sense the progress of the\nLively Kernel over Squeak is not that great but Dan seems to be having fun.  Dan\nis an incredibly enthusiastic, friendly and positive person, it was really great\nmeeting him. He even seemed to like some of the ideas in SPy.\n\"On Sustaining Self\" by Richard P. Gabriel was a sort of deconstructivist\nmulti-media-show train wreck of a presentation that was a bit too weird for my\ntaste. There was a lot of music, there were sections in the presentation\nwhere Richard discussed with an alter ego, whose part he had recorded in advance\nand mangled with a sound editor. There was a large bit of a documentary\nabout Levittown. Even the introduction and the questions were weird, with Pascal\nConstanza staring down the audience, without saying a word (nobody dared to ask\nquestions). I am not sure I saw the point of the presentation, apart from\ngetting the audience to think, which probably worked. It seems that there are\npeople (e.g. Christian Neukirchen) that liked the presentation, though.\n\n\nResearch Papers\n\"SBCL - A Sanely Bootstrappable Common Lisp by Christophe Rhodes described the\nbootstrapping process of SBCL (Steel Bank Common Lisp). SBCL can be bootstrapped\nby a variety of Common Lisps, not just by itself. SBCL contains a complete\nblueprint of the initial image instead of always getting the new image by\ncarefully mutating the old one. This bootstrapping approach is sort of similar\nto that of PyPy.\n\"Reflection for the Masses\" by Charlotte Herzeel, Pascal Costanza, and Theo\nD'Hondt retraced some of the work of Brian Smith on reflection in Lisp. The\ntalk was not very good, it was way too long (40 min), quite hard to understand\nbecause Charlotte Herzeel was talking in a very low voice. The biggest mistake\nin her talk was in my opinion that she spent too much time explaining a more or\nless standard meta-circular interpreter for Lisp and then running out of time\nwhen she was trying to explain the modifications. I guess it would have been a\nfair assumptions that large parts of the audience know such interpreters, so\nglossing over the details would have been fine. A bit of a pity, since the paper\nseems interesting.\n\"Back to the Future in One Week - Implementing a Smalltalk VM in PyPy\"\nby Carl Friedrich Bolz, Adrian Kuhn, Adrian Lienhard, Nicholas D. Matsakis,\nOscar Nierstrasz, Lukas Renggli, Armin Rigo and Toon Verwaest, the paper with\nthe longest author list. We just made everybody an author who was at the sprint\nin Bern. Our paper had more authors than all the other papers together :-). I\ngave the presentation at the workshop, which went quite well, judging from the\nfeedback I got.\n\"Huemul - A Smalltalk Implementation\" by Guillermo Adri\u00e1n Molina. Huemul is a\nSmalltalk implementation that doesn't contain an interpreter but directly\ncompiles all methods to assembler (and also saves the assembler in the image).\nIn addition, as much functionality (such as threading, GUI) as possible is\ndelegated to libraries instead of reimplementing them in Smalltalk\n(as e.g. Squeak is doing). The approach seems to suffer from the usual problems\nof manually writing a JIT, e.g. the VM seems to segfault pretty often. Also I\ndon't agree with some of the design decisions of the threading scheme, there is\nno automatic locking of objects at all, instead the user code is responsible for\npreventing concurrent accesses from messing up things (which even seems to lead\nto segfaults in the default image).\n\"Are Bytecodes an Atavism?\" by Theo D'Hondt argued that using AST-based\ninterpreters can be as fast as bytecode-based interpreters which he proved by\nwriting two AST-interpreters, one for Pico and one for Scheme. Both of these\nimplementations seem to perform pretty well. Theo seems to have many similar\nviews as PyPy, for example that writing simple straightforward interpreters is\noften preferable than writing complex (JIT-)compilers.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/s3-workshop-potsdam-2008-writeup-6610637452403831794.html"
    },
    {
      "title": "Berlin Sprint Finished",
      "text": "The Berlin sprint is finished, below some notes on what we worked on during\nthe last three days:\n\n\nCamillo worked tirelessly on the gameboy emulator with some occasional input\nby various people. He is making good progress, some test ROMs run now on the\ntranslated emulator. However, the graphics are still not completely working\nfor unclear reasons. Since PyBoy is already taken as a project name, we\nconsidered calling it PyGirl (another name proposition was \"BoyBoy\", but the\nimplementation is not circular enough for that).\n\n\n\n\nOn Monday Armin and Samuele fixed the problem with our multimethods so that\nthe builtin shortcut works again (the builtin shortcut is an optimization\nthat speeds up all operations on builtin non-subclassed types quite a bit).\nAntonio and Holger (who hasn't been on a sprint in a while, great to have you\nback!) worked on writing a conftest file (the plugin mechanism of py.test)\nthat would allow us to run Django tests using py.test, which seems to be not\ncompletely trivial. They also fixed some bugs in PyPy's Python interpreter,\ne.g. related to dictionary subclassing.\nKarl started adding sound support to the RPython SDL-bindings, which will be\nneeded both by the Gameboy emulator and eventually by the SPy VM.\nArmin and Maciek continued the work that Maciek had started a while ago of\nimproving the speed of PyPy's IO operation. In the past, doing IO usually\ninvolved copying lots of memory around, which should have improved now. Armin\nand Maciek improved and then merged the first of the two branches that\ncontained IO improvements, which speeds up IO on non-moving GCs (mostly the\nBoehm GC). Then they continued working on the hybrid-io branch which is\nsupposed improve IO on the hybrid GC (which was partially designed exactly\nfor this).\nToon, Carl Friedrich finished cleaning up the SPy improvement branch and\nfixed all warnings that occur when you translate SPy there. An obscure bug in\nan optimization prevented them from getting working executables, which at\nthis moment blocks the merging of that branch.\n\n\nBy now everybody is home again (except for Anto, who booked his return flight\ntwo days too late, accidentally) and mostly resting. It was a good sprint, with\nsome interesting results and several new people joining. And it was definitely\nthe most unusual sprint location ever :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/berlin-sprint-finished-1597243123548564657.html"
    },
    {
      "title": "Berlin Sprint Day 1 + 2",
      "text": "After having survived the S3-Workshop which took place in Potsdam on Thursday\nand Friday (a blog-post about this will follow later) we are now sitting in the\nc-base in Berlin, happily sprinting. Below are some notes on what progress we\nmade so far:\n\n\nThe Gameboy emulator in RPython that Camillo Bruni is working on for his\nBachelor project at Uni Bern does now translate. It took him (assisted by\nvarious people) a while to figure out the translation errors (essentially\nbecause he wrote nice Python code that passed bound methods around, which the\nRTyper doesn't completely like).  Now that is fixed and the Gameboy emulator\ntranslates and runs a test ROM.  You cannot really see anything yet, because\nthere is no graphics support in RPython.\nTo get graphics support in RPython Armin and Karl started writing SDL\nbindings for RPython, which both the Gameboy emulator and the SPy VM need.\nThey have basic stuff working, probably enough to support the Gameboy\nalready.\nAlexander, Armin, Maciek and Samuele discussed how to approach separate\ncompilation for RPython, which isn't easy because the RPython type analysis\nis a whole-program analysis.\nStephan, Peter and Adrian (at least in the beginning) worked on making PyPy's\nstackless module more complete. They added channel preferences which\nchange details of the scheduling semantics.\nToon, Carl Friedrich and Adrian (a tiny bit) worked on SPy. There is a branch\nthat Toon started a while ago which contains many improvements but is also\nquite unclear in many respects. There was some progress in cleaning that up.\nThis involved implementing the Smalltalk process scheduler (Smalltalk really\nis an OS). There is still quite some work left though. While doing so, we\ndiscovered many funny facts about Squeak's implementation details (most of\nwhich are exposed to the user) in the process. I guess we should collect them\nand blog about them eventually.\nSamuele and Maciek improved the ctypes version of pysqlite that Gerhard\nH\u00e4ring started.\nArmin, Samuele and Maciek found an obscure bug in the interaction between the\nbuiltin-type-shortcut that Armin recently implemented and our multimethod\nimplementation. It's not clear which of the two are to blame, however it\nseems rather unclear how to fix the problem: Armin and Samuele are stuck in a\ndiscussion about how to approach a solution since a while and are hard to\ntalk to.\nStijn Timbermont, a Ph.D. student at the Vrije Universiteit Brussel who is\nvisiting the sprint for two days was first looking at how our GCs are\nimplemented to figure out whether he can use PyPy for some experiments. The\nanswer to that seems to be no. Today he was hacking on a Pico interpreter\n(without knowing too much about Python) and is making some nice progress, it\nseems.\n\n\nWill try to blog more as the sprint progresses.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/berlin-sprint-day-1-2-8761821946764492267.html"
    },
    {
      "title": "General performance improvements",
      "text": "Hi all,\n\nDuring the past two weeks we invested some more efforts on the\nbaseline performance of pypy-c.  Some of the tweaks we did\nwere just new ideas, and others were based on actual\nprofiling.  The net outcome is that we now expect PyPy to be\nin the worst case twice as slow than CPython on real\napplications.  Here are some small-to-medium-size benchmark\nresults.  The number is the execution time, normalized to 1.0\nfor CPython 2.4:\n\n1.90 on templess (a simple templating language)\n1.49 on gadfly (pure Python SQL database)\n1.49 on translate.py (pypy's own translation toolchain)\n1.44 on mako (another templating system)\n1.21 on pystone\n0.78 on richards\n\n\n(This is all without the JIT, as usual.  The JIT is not ready yet.)\n\nYou can build yourself a pypy-c with this kind of speed with\nthe magic command line (gcrootfinder is only for a 32-bit\nLinux machine):\n\n    pypy/translator/goal/translate.py --gc=hybrid --gcrootfinder=asmgcc targetpypystandalone --allworkingmodules --faassen\n\nThe main improvements come from:\n    \nA general shortcut for any operation between built-in objects:\nfor example, a subtraction of two integers or floats now dispatches\ndirectly to the integer or float subtraction code, without looking up\nthe '__sub__' in the class.\nA shortcut for getting attributes out of instances of user classes\nwhen the '__getattribute__' special method is not overridden.\nThe so-called Hybrid Garbage Collector is now a\nthree-generations collector.\n\nMore about our GCs...\nSome profiling showed bad performance in our implementation of\nthe built-in id() -- a trivial function to write in CPython, but a lot\nmore fun when you have a moving GC and your object's real address can\nchange.\nThe bytecode compiler's parser had a very slow linear search\nalgorithm that we replaced with a dictionary lookup.\n\n\nThese benchmarks are doing CPU-intensive operations. You can expect\na similar blog post soon about the I/O performance, as the\nio-improvements branch gets closer to being merged\n:-)  The branch could also improve the speed of\nstring operations, as used e.g. by the templating systems.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/general-performance-improvements-838741900863354293.html"
    },
    {
      "title": "Next Sprint: Berlin, May 17-22nd May",
      "text": "Our next PyPy sprint will take place in the crashed c-base space station, Berlin, Germany, Earth, Solar System.  This is a fully public sprint: newcomers (from all planets) are welcome.  Suggestion of topics (other topics are welcome too):\n\n\nwork on PyPy's JIT generator: we are refactoring parts of the\n    compiling logic, in ways that may also allow generating better\n    machine code for loops (people or aliens with knowledge on\n    compilers and SSA, welcome)\n\nwork on the SPy VM, PyPy's Squeak implementation, particularly the\n    graphics capabilities                                             \n\nwork on PyPy's GameBoy emulator, which also needs graphics support\n                                                                      \ntrying some large pure-Python applications or libraries on PyPy and\n    fixing the resulting bugs. Possibilities are Zope 3, Django and\n    others.\n\n\nFor more information, see the full announcement.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/05/next-sprint-berlin-may-17-22nd-may-5362899847460267375.html"
    },
    {
      "title": "Google's Summer of Code",
      "text": "PyPy got one proposal accepted for Google's Summer of Code under the Python\nSoftware Foundation's umbrella.  We welcome Bruno Gola into the PyPy\ncommunity. He will work on supporting all Python 2.5 features in PyPy and will\nalso update PyPy's standard library to support the modules that were modified\nor new in Python 2.5.\nRight now PyPy supports only Python 2.4 fully (some Python 2.5 features have\nalready sneaked in, though).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/googles-summer-of-code-4911168632727441622.html"
    },
    {
      "title": "Float operations for JIT",
      "text": "Recently, we taught the JIT x86 backend how to produce code for the x87 floating point coprocessor. This means that JIT is able to nicely speed up float operations (this this is not true for our Python interpreter yet - we did not integrate it yet). This is the first time we started going beyond what is feasible in psyco - it would take a lot of effort to make floats working on top of psyco, way more than it will take on PyPy.\n\nThis work is in very early stage and lives on a jit-hotpath branch, which includes all our recent experiments on JIT compiler generation, including tracing JIT experiments and huge JIT refactoring.\n\nBecause we don't encode the Python's semantics in our JIT (which is really a JIT generator), it is expected that our Python interpreter with a JIT will become fast \"suddenly\", when our JIT generator is good enough. If this point is reached, we  would also get fast interpreters for Smalltalk or JavaScript with relatively low effort.\n\nStay tuned.\n\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/float-operations-for-jit-6499693696246367083.html"
    },
    {
      "title": "Wrapping pyrepl in the readline API",
      "text": "If you translate a pypy-c with --allworkingmodules and start it, you will probably not notice anything strange about its prompt - except when typing multiline statements. You can move the cursor up and continue editing previous lines. And the history is multiline-statements-aware as well. Great experience! Ah, and completion using tab is nice too.\n\nTruth be told, there is nothing new here: it was all done by Michael Hudson's pyrepl many years ago.  We had already included pyrepl in PyPy some time ago.  What is new is a pure Python readline.py which exposes the most important parts of the API of the standard readline module by wrapping pyrepl under the hood, without needing the GNU readline library at all. The PyPy prompt is based on this, benefitting automagically from pyrepl's multiline editing capabilities, with minor tweaks so that the prompt looks much more like CPython's than a regular pyrepl prompt does.\n\nYou can also try and use this multiline prompt with CPython: check out pyrepl at https://codespeak.net/svn/pyrepl/trunk/pyrepl and run the new pythoni1 script.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/wrapping-pyrepl-in-readline-api-362730784820949868.html"
    },
    {
      "title": "Other April's Fools Ideas",
      "text": "While discussing what to post as an April Fool's joke yesterday, we had a\ncouple of other ideas, listed below. Most of them were rejected because they are\ntoo incredible, others because they are too close to our wish list.\n\nquantum computer backend\nPerl6 interpreter in RPython\nRuby backend to allow run \"python on rails\"\nmandatory static typing at app-level, because it's the only way to increase\nperformances\nrewrite PyPy in Haskell, because we discovered that dynamic typing is just\nnot suitable for a project of this size\na C front-end, so that we can interpret the C source of Python C extensions\nand JIT it. This would work by writing an interpreter for LLVM bytecode in\nRPython.\nan elisp backend\na TeX backend (use PyPy for your advanced typesetting needs)\nan SQL JIT backend, pushing remote procedures into the DB engine",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/other-aprils-fools-ideas-955926452383759016.html"
    },
    {
      "title": "Trying to get PyPy to run on Python 3.0",
      "text": "As you surely know, Python 3.0 is coming; recently, they released\nPython 3.0 alpha 3, and the final version is expected around\nSeptember.\nAs suggested by the migration guide (in the PEP 3000), we started by applying\n2to3 to our standard interpreter, which is written in RPython (though\nwe should call it RPython 2.4 now, as opposed to RPython 3.0 -- see\nbelow).\nConverting was not seamless, but most of the resulting bugs were due to the\nnew dict views, str/unicode changes and the missing \"reduce\" built-in.\nAfter forking and refactoring both our interpreter and the 2to3 script,\nthe Python interpreter runs on Python 3.0 alpha 3!\nNext step was to run 2to3 over the whole translation toolchain,\ni.e. the part of PyPy which takes care of analyzing the interpreter in\norder to produce efficient executables; after the good results we got\nwith the standard interpreter, we were confident that it would have\nbeen relatively easy to run 2to3 over it: unfortunately, it was not\n:-(.\nAfter letting 2to3 run for days and days uninterrupted, we decided to\nkill it: we assume that the toolchain is simply too complex to be\nconverted in a reasonable amount of time.\nSo, we needed to think something else; THE great idea we had was to\nturn everything upside-down: if we can't port PyPy to Py3k, we can\nalways port Py3k to PyPy!\nUnder the hood, the 2to3 conversion tool operates as a graph\ntransformer: it takes the graph of your program (in the form of Python\n2.x source file) and returns a transformed graph of the same program\n(in the form of Python 3.0 source file).  Since the entire translation\ntoolchain of PyPy is based on graph transformations, we could reuse it\nto modify the behaviour of the 2to3 tool.  We wrote a general\ngraph-inverter algorithm which, as the name suggests, takes a graph\ntransformation and build the inverse transformation; then, we applied\nthe graph inverter to 2to3, getting something that we called 3to2: it\nis important to underline that 3to2 was built by automatically\nanalysing 2to3 and reversing its operation with only the help of a few\nmanual hints. For this reason and because we are not keeping generated\nfiles under version control, we do not need to maintain this new tool in\nthe Subversion repository.\nOnce we built 3to2, it was relatively easy to pipe its result to our\ninterpreter, getting something that can run Python 3.0 programs.\nPerformance-wise, this approach has the problem of being slower at\nimport time, because it needs to run (automatically) 3to2 every time\nthe source is modified; in the future, we plan to apply our JIT\ntechniques also to this part of the interpreter, trying to mitigate the\nslowdown until it is not noticeable anymore to the final user.\nIn the next weeks, we will work on the transformation (and probably publish\nthe technique as a research paper, with a title like \"Automatic Program\nReversion on Intermediate Languages\").\nUPDATE: In case anybody didn't guess or didn't spot the acronym: The above\nwas an April Fool's joke. Nearly nothing of it is true.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/04/trying-to-get-pypy-to-run-on-python-30-5082015544752137606.html"
    },
    {
      "title": "Py-Lib 0.9.1 released",
      "text": "The Py-Lib 0.9.1 release is out! The Py-Lib is a very important support\nlibrary that PyPy uses for a lot of things \u2013 most importantly it contains\npy.test, which PyPy uses for testing.\nThis is mostly a bugfix release, with a couple of new features sneaked in.\nMost important changes:\n\nsome new functionality (authentication, export, locking) in py.path's\nSubversion APIs\nnumerous small fixes in py.test's rsession (experimental pluggable session)\nand generative test features\nsome fixes in the py.test core\n\nDownload/Install:   https://codespeak.net/py/0.9.1/download.html\nDocumentation/API:  https://codespeak.net/py/0.9.1/index.html\nUPDATE: the py-lib is now easy-installable with:\n\neasy_install py",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2008/03/py-lib-091-released-1654797401128918376.html"
    },
    {
      "title": "PyPy Summer of Code Participation",
      "text": "As in the last years, PyPy will again participate in Google's Summer of Code\nprogram under the umbrella of the Python Software Foundation. Unfortunately we\nwere a bit disorganized this year, so that our project ideas are only put up\nnow. The list of project ideas of PyPy can be found here.\nAny interested student should mail to our mailing list or just come to the\n#pypy channel on irc.freenode.net to discuss things.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/03/pypy-summer-of-code-participation-3403842530060519982.html"
    },
    {
      "title": "ctypes configuration tool",
      "text": "As a part of implementing ctypes, we decided to make coding using ctypes better on its own (irrelevant what python interpreter you use). The concrete problem we're trying to solve is to make ctypes code more platform-independent than it is. Say you want to create a ctypes type for size_t: ctypes itself provides no mechanism for doing that, so you need to use a concrete integer type (c_int, c_long, c_short etc.). Your code either becomes platform dependent if you pick one of them or is littered with conditionals for all sorts of platforms. We created a small library, called ctypes_configure (which is actually a variation of something we use somewhere in the PyPy source tree), which tries to solve some platform dependencies by compiling and running small chunks of C code through a C compiler. It's sort of like configure in the Linux world, except for Python using ctypes.\n\nTo install the library, you can just type easy_install ctypes_configure. The code is in an svn repository on codespeak and there is even some documentation and sample code. Also, even though the code lives in the pypy repository, it depends only on pylib, not on the whole of pypy.\n\nThe library is in its early infancy (but we think it is already rather useful). In the future we could add extra features, it might be possible to check whether the argtypes that are attached to  the external functions are consistent with what is in the C headers), so that the following code wouldn't segfault but give a nice error\n\nlibc = ctypes.CDLL(\"libc.so\")\ntime = libc.time\ntime.argtypes = [ctypes.c_double, ctypes.c_double]\ntime(0.0, 0.0)\n\n\nAlso, we plan to add a way to install a package that uses ctypes_configure in such a way that the installed library doesn't need to call the C compiler any more later.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/03/ctypes-configuration-tool-7414864595600362988.html"
    },
    {
      "title": "Bittorrent on PyPy",
      "text": "Hi all,\n\nBittorrent now runs on PyPy! I tried the no-GUI BitTornado version (btdownloadheadless.py). It behaves correctly and I fixed the last few obvious places which made noticeable pauses.  (However we know that there are I/O performance issues left: we make too many internal copies of the data, e.g. in a file.read() or os.read().)\n\nWe are interested in people trying out other real-world applications that, like the GUI-less Bittorrent, don't have many external dependencies to C extension modules. Please report all the issues to us!\n\nThe current magic command line for creating a pypy-c executable with as many of CPython's modules as possible is:\n\n\n  cd pypy/translator/goal\n  ./translate.py --thread targetpypystandalone.py --allworkingmodules --withmod-_rawffi --faassen\n\n\n(This gives you a thread-aware pypy-c, which requires the Boehm gc library.  The _rawffi module gives you ctypes support but is only tested for Linux at the moment.)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/03/bittorrent-on-pypy-7984272143557948160.html"
    },
    {
      "title": "As fast as CPython (for carefully taken benchmarks)",
      "text": "Good news everyone. A tuned PyPy compiled to C is nowadays as fast as CPython on the richards benchmark and slightly faster on the gcbench benchmark.\n\nIMPORTANT:  These are very carefully taken benchmarks where we expect pypy to be fast!  PyPy is still quite slower than CPython on other benchmarks and on real-world applications (but we're working on it).  The point of this post is just that for the first time (not counting JIT experiments) we are faster than CPython on *one* example :-)\n\nThe exact times as measured on my notebook (which is a Core Duo machine) are here:\n\nCompiled pypy with options:\n\n\n./translate.py --gcrootfinder=asmgcc --gc=generation targetpypystandalone.py --allworkingmodules --withmod-_rawffi --faassen\n\n(allworkingmodules and withmod-_rawffi are very likely irrelevant to those benchmarks)\n\nCPython version 2.5.1, release.\n\n\nrichards 800ms pypy-c vs 809ms cpython (1% difference)\ngcbench 53700ms pypy-c vs 60215ms cpython (11% difference)\n\nPyPy shines on gcbench, which is mostly just about allocating and freeing many objects.  Our gc is simply better than refcounting, even though we've got shortcomings in other places.\n\n\nAbout richards, there is a catch. We use a method cache optimization, and have an optimization which helps to avoid creating bound methods each time a method is called. This speeds up the benchmark for about 20%. Although method cache was even implemented for CPython, it didn't make its way to the core because some C modules directly modify the dictionary of new-style classes. In PyPy, the greater level of abstraction means that this operation is just illegal.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/03/as-fast-as-cpython-for-carefully-taken-1984440931984637179.html"
    },
    {
      "title": "Running Pyglet on PyPy",
      "text": "As part of our efforts of making PyPy's Python interpreter usable we put quite some effort into interfacing with external libraries. We were able, in quite a short amount of time (I think beginning really from Leysin sprint, or slightly earlier) to provide a prototype of the ctypes library. It is written in completely normal Python, at applevel, based on a very thin wrapper around the libffi library. This makes development a lot easier, but it makes the resulting ctypes implementation rather slow. The implementation is not complete yet and it will still need quite some effort to make it feature-complete (ctypes has lots of details and special cases and\ndo-what-I-mean magic). Yet another point will be to make it faster, but that's for much later.\n\nThe implementation is good enough to run those parts of Pyglet that don't depend on PIL (which PyPy doesn't have).  Here are a few pictures of running Pyglet demos on top of compiled pypy-c.\n\n\n\nTo compile a version of PyPy that supports ctypes, use this highly sophisticated command line\n\n\n./translate.py --gc=generation ./targetpypystandalone.py --allworkingmodules --withmod-_rawffi\n\nNote: this works on linux only right now.\n\nThe list of missing small ctypes features is quite extensive, but I consider the current implementation to be usable for most common cases. I would love to hear about libraries written in pure python (using ctypes), to run them on top of PyPy and use them as test cases. If someone knows such library, please provide a link.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/02/running-pyglet-on-pypy-3191536711417589549.html"
    },
    {
      "title": "Python Finalizers Semantics, Part 2: Resurrection",
      "text": "Continuing the last blog post about GC semantics in Python.\nAnother consequence of reference counting is that resurrection is easy to\ndetect. A dead object can resurrect itself if its finalizer stores it into a\nglobally reachable position, like this:\n\nclass C(object):\n    def __init__(self, num):\n        self.num = num\n    def __del__(self):\n        global c\n        if c is None:\n            c = self\nc = C(1)\nwhile c is not None:\n    c = None\n    print \"again\"\n\nThis is an infinite loop in CPython: Every time c is set to None in the\nloop, the __del__ method resets it to the C instance again (note that\nthis is terribly bad programming style, of course. In case anybody was wondering\n:-)). CPython can detect resurrection by checking whether the reference count\nafter the call to __del__ has gotten bigger.\nThere exist even worse examples of perpetual resurrection in particular in\ncombination with the cycle GC. If you want to see a particularly horrible one,\nsee this discussion started by Armin Rigo. In the ensuing thread Tim Peters\nproposes to follow Java's example and call the finalizer of every object at most\nonce.\nIn PyPy the resurrection problem is slightly more complex, since we have GCs\nthat run collection from time to time and don't really get to know at which\nprecise time an object dies. If the GC discovers during a collection that an\nobject is dead, it will call the finalizer after the collection is finished. If\nthe object is then dead at the next collection, the GC does not know whether\nthe object was resurrected by the finalizer and then died in the meantime or\nwhether it was not resurrected. Therefore it seemed sanest to follow Tim's\nsolution and to never call the finalizer of an object a second time, which has\nmany other benefits as well.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/02/python-finalizers-semantics-part-2-2748812428675325525.html"
    },
    {
      "title": "Python Finalizers Semantics, Part 1",
      "text": "Python's garbage collection semantics is very much historically grown and\nimplementation-driven. Samuele Pedroni therefore likes to call it the \"'there\nis no such thing as too much chocolate'-approach to GC semantics\" :-). In this\ntwo-part post series I am going to talk about the semantics of finalization\n(__del__ methods) in CPython and PyPy.\nThe current behaviour is mostly all a consequence of the fact that CPython uses\nreference counting for garbage collection. The first consequence is that if\nseveral objects die at the same time, their finalizers are called in a\nso-called topological order, which is a feature that some GCs have that\nCPython offers by chance.  This ensures, that in a __del__ method, all the\nattributes of the object didn't get their __del__ called yet. A simple\nexample:\n\nclass B(object):\n    def __init__(self, logfile):\n        self.logfile = logfile\n    def __del__(self):\n        self.logfile.write(\"done doing stuff\")\nb = B(file(\"logfile.txt\", \"w\"))\n\nIf the instance of B dies now, both it and the logfile are dead. They will\nget their __del__``s called and it's important that the file's ``__del__\ngets called second, because otherwise the __del__ of B would try to\nwrite to a closed file.\nThe correct ordering happens completely automatically if you use reference\ncounting: Setting b to None will decref the old value of b. This reduces\nthe reference count of this instance to 0, so the finalizer will be called.\nAfter the __del__ has finished, this object will be freed and all the\nobjects it points to decrefed as well, which decreases the reference count of\nthe file to 0 and call its `` __del__`` as well, which closes the file.\nThe behaviour of PyPy's semispace and generational GCs wasn't very nice so far:\nit just called the finalizers in an essentially random order. Last week Armin\ncame up with a somewhat complicated algorithm that solves this by emulating\nCPython's finalization order, which we subsequently implemented. So PyPy does\nwhat you expect now! The Boehm GC does a topological ordering by default, so it\nwasn't a problem there.\nA small twist on the above is when\nthere is a cycle of objects involving finalizers:\nIn this case a topological ordering is not possible, so that CPython refuses to\nguess the finalization order and puts such cycles into gc.garbage. This\nwould be very hard for PyPy to do, since our GC implementation is essentially\nindependent from the Python interpreter. The same GCs work for our other\ninterpreters after all too. Therefore we decided to break such a cycle at an\narbitrary place, which doesn't sound too insane.  The insane thing is for\na Python program to create a cycle of objects with finalizers and depend\non the order in which the finalizers are called.  Don't do that :-)  (After\nall, CPython wouldn't even call the finalizers in this case.)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/02/python-finalizers-semantics-part-1-1196956834543115766.html"
    },
    {
      "title": "PyPy presence on various conferences in the near future",
      "text": "Hello! I will have the pleasure of presenting PyPy on various conferences in the near future. They're (in chronological order):\n\n\nStudencki Festiwal Informatyczny in Krakow, POLAND 6-8 March 2008. I think this might be only interesting for polish people (website, in polish)\n\nPycon Chicago, IL, USA. 14-17 March 2008. There should be also a PyPy sprint afterwards, including newbie-friendly tutorial, everybody is welcome to join us!  (Provided that I'll get the US visa, which seems to be non-trivial issue for a polish citizen)\n RuPy, Poznan, POLAND 13-14 April 2008 (website). This is small, but very friendly Ruby and Python conference. Last year was amazing, I can strongly recommend to go there (Poznan is only 2h by train from Berlin also has its own airport).\n\n\nHope to see you at those places!\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/02/pypy-presence-on-various-conferences-in-6584680808789191759.html"
    },
    {
      "title": "Buildbots and Better Platform Support",
      "text": "In the last days we improved platform-support of PyPy's Python interpreter.\nJean-Paul Calderone has been tirelessly working for some time now on setting up a\nbuildbot for translating and testing PyPy. So far the basic mechanisms are\nworking and the buildbot is running on various machines, including some that\nMichael Schneider (bigdog) lets us use, one of them being a Windows machine,\nthe other one with a 64bit Linux (lots of thanks to those two, you are\nawesome!).\nWhat is still missing is a nice way to visualize the test results to quickly see\nwhich tests have started failing on which platforms. There is a prototype\nalready, which still needs some tweaking.\nThe availability of these machines has triggered some much-needed bug-fixing in\nPyPy to make our Python interpreter work better on Windows and on 64 bit Linux.\nMaciek and Michael Schneider worked on this quite a bit last week, with the\nresult that PyPy supports many more extension modules now on Windows and 64 bit\nLinux. Since we now have the buildbot the hope is that the support also won't\ndisappear soon :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/buildbots-and-better-platform-support-6965497451398110731.html"
    },
    {
      "title": "PyPy Keyboard Heatmap",
      "text": "Today I saw the keyboard heatmap generator on the Blended Technologies\nblog. I threw all the PyPy code at it to see whether the heatmap looks any\ndifferent than normal Python code. It doesn't:\n\nSo now the excuse \"I can't contribute to PyPy because it needs all those special\nPyPy-keys\" isn't working anymore :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/pypy-keyboard-heatmap-4950995633665492453.html"
    },
    {
      "title": "RPython can be faster than C",
      "text": "(yes, C as in language, not c as in speed of light). I looked recently at the great computer language shootout, for some benchmarks and to make some speed comparisons. I use this benchmark, modified it to be rpythonic-enough and compared speeds. The code is here (the only change from the Python version was to create a class instead of tuple, so actually this version is more OO). Also the benchmark is very likely flawed because it favours better GCs :).\nSo, here we go:\n\n\nLanguage:Time of run (for N=14):\nPython version running on Python 2.5.1, distribution25.5s\nPython version running on PyPy with generational GC45.5\nPython with psyco20s\nRPython translated to C using PyPy's generational GC0.42s\ncompiling the Haskell version with GHC 6.6.11.6s\ncompiling the C version with gcc 4.1.2 -O3 -fomit-frame-pointer0.6s\n\n\n\nAlso worth noticing is that when using psyco with the original version (with tuples) it is very fast (2s).\n\nSo, PyPy's Python interpreter is 80% slower than CPython on this (not too horrible), but RPython is 40% faster than gcc here. Cool. The result is mostly due to our GC, which also proves that manual memory-management can be slower than garbage collection in some situations. Please note that this result does not mean that RPython is meant for you. It requires a completely different mindset than the one used to program in Python. Don't say you weren't warned! :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/rpython-can-be-faster-than-c-2559071147541131237.html"
    },
    {
      "title": "PyPy.NET goes Windows Forms",
      "text": "After having spent the last few days on understanding PyPy's JIT,\ntoday I went back hacking the clr module.  As a result, it is now\npossible to import and use external assemblies from pypy-cli,\nincluding Windows Forms\nHere is a screenshot of the result you get by typing the following at\nthe pypy-cli interactive prompt:\n\n>>>> import clr\n>>>> clr.AddReferenceByPartialName(\"System.Windows.Forms\")\n>>>> clr.AddReferenceByPartialName(\"System.Drawing\")\n>>>> from System.Windows.Forms import Application, Form, Label\n>>>> from System.Drawing import Point\n>>>>\n>>>> frm = Form()\n>>>> frm.Text = \"The first pypy-cli Windows Forms app ever\"\n>>>> lbl = Label()\n>>>> lbl.Text = \"Hello World!\"\n>>>> lbl.AutoSize = True\n>>>> lbl.Location = Point(100, 100)\n>>>> frm.Controls.Add(lbl)\n>>>> Application.Run(frm)\n\nUnfortunately at the moment you can't do much more than this, because\nwe still miss support for delegates and so it's not possibile to\nhandle events. Still, it's a step in the right direction :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/pypynet-goes-windows-forms-7031406830502864570.html"
    },
    {
      "title": "Improve .NET Integration",
      "text": "A while ago Amit Regmi, a student from Canada, started working on the\nclr module improvements branch as a university project.\nDuring the sprint Carl Friedrich, Paul and me worked more on it and\nbrought it to a mergeable state.\nIt adds a lot of new features to the clr module, which is the\nmodule that allows integration between pypy-cli (aka PyPy.NET) and\nthe surrounding .NET environment:\n\n\nfull support to generic classes;\na new importer hook, allowing things like from System import\nMath and so on;\n.NET classes that implements IEnumerator are treated\nas Python iterators; e.g. it's is possile to iterate over them\nwith a for loop.\n\n\nThis is an example of a pypy-cli session:\n\n>>>> from System import Math\n>>>> Math.Abs(-42)\n42\n>>>> from System.Collections.Generic import List\n>>>> mylist = List[int]()\n>>>> mylist.Add(42)\n>>>> mylist.Add(43)\n>>>> mylist.Add(\"foo\")\nTraceback (most recent call last):\n  File \"<console>\", line 1, in <interactive>\nTypeError: No overloads for Add could match\n>>>> mylist[0]\n42\n>>>> for item in mylist: print item\n42\n43\n\nThis is still to be considered an alpha version; there are few known\nbugs and probably a lot of unknown ones :-), so don't expect it to\nwork in every occasion. Still, it's a considerable step towards real\nworld :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/improve-net-integration-2239651503641931440.html"
    },
    {
      "title": "Crashing Other People's Compilers",
      "text": "Over the years PyPy has (ab?)used various external software for different\npurposes, and we've discovered bugs in nearly all of them, mostly by pushing them\nto their limits. For example, many compilers are not happy with 200MB of\nsource in one file. The Microsoft C compiler has a limit of 65536 lines of code\nper file and the CLI was raising \"System.InvalidProgramException: Method\npypy.runtime.Constants:.cctor () is too complex.\", where too complex probably\nmeans \"too long\". Just for fun, today we collected all projects we could think of\nin which we found bugs:\n\n\nCPython (lots)\nPyPy and the py-lib (surpise)\nctypes\nTCC (we gave up on it)\nBoehm\nGraphviz\nMono\nLLVM (lots)\nPython.net\nthe Microsoft IL assembler\nMicrosoft's C compiler\nJasmin\nJPype\nnucular\nTwisted\nthe JVM, maybe\npygame or SDL\n\n\nSo one could say that PyPy is really just the most expensive debugging tool\never :-).",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/crashing-other-peoples-compilers-4574453763254909150.html"
    },
    {
      "title": "Leysin Winter Sport Sprint Started",
      "text": "The Leysin sprint has started since yesterday morning in the usual location. The view is spectacular (see photo) the weather mostly sunny. The following people are sprinting:\nMaciej FijalkowskiArmin RigoToby WatsonPaul deGrandisAntonio CuniCarl Friedrich BolzSo it is a rather small sprint.We started working on various features and performance improvements for the high level backends (JVM and .NET) and on implementing ctypes for PyPy. Later this week we plan to spend a few days on the JIT, because Anto and I both need to get into it for our respective university projects.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/leysin-winter-sport-sprint-started-5478612778498579467.html"
    },
    {
      "title": "Finding GC roots: using LLVM or parsing assembler files from GCC",
      "text": "PyPy contains a framework for writing custom Garbage Collectors, and a few simple GCs have been written in this framework. A common issue with all these GCs is how to find all the stack roots, i.e. all the pointers to live GC-managed objects currently stored in local variables, in all the callers of the current function. The current solution is to maintain a custom shadow stack of roots, where all functions push and pop copies of their local variables of type \"GC pointer\". Clearly this is an overhead. Can we remove it?\n\nLLVM has recently grown some support for this. By emitting markers in the LLVM source and with the help of a bit of custom C++ code, we can generate stack maps for the functions compiled by LLVM.  Then, with 100% non-portable code in our framework GC's root finding algorithm, we can walk the machine stack and locate where in each stack frame LLVM stores the GC pointers. (Yes, I mean non-portable: LLVM offers no help for doing that.  Maybe it will at some point, though I didn't manage to explain why this is an issue to people working on  this in LLVM so far...).  I've tried that approach in the llvmgcroot branch.  Over the manually-managed shadow stack, this gives speed improvements which are, very roughly, on the order of 5%.\n\nNote that this prevents some optimizations in LLVM, because it forces it to allocate all local variables of type \"GC pointer\" in the stack; it cannot keep them in registers and it must assume that they can be changed more or less at any time (as moving GCs do). Can we do better?\n\nActually, yes.  We can even do better in the C backend, using a GCC hack.  GCC has this nice extension:\nasm(\"bla\", constrains);\nThis is meant to generate assembler instructions directly from C. Internally, GCC considers the whole asm() as a single regular instruction of its intermediate language; the constrains are expressed in the same way as the constrains for all the prebuilt intermediate language instructions. They express things like input and output operands of the instruction, whether they can live in memory or in registers, whether the whole instruction has side-effects, etc. The nice thing about asm() is that it doesn't kill any optimization whatsoever in GCC - it's your job to make sure that you use the correct constrains.\n\nSo what I've tried in the asmgcroot branch is to use asm() as markers. In this branch, the C backend produces code like this after each function call, for each local variable containing a live GC pointer:\n\nasm(\"/* GCROOT %0 */\" : \"=g\"(localvar) : \"0\"(localvar) : \"memory\");\n\nThis causes GCC to emit the following line in the assembler file it generates:\n\n/* GCROOT register-or-memory-containing-localvar */\n\nI won't go in the details of the asm() line above - the constrains are just enough to make sure that GCC doesn't optimize too much, but don't prevent most optimizations from occurring. For example, the localvar can be in a register.\n\nThe assembler will just ignore the line above; it is a comment. But what we can do is write our own tool parsing the assembler files. This tool locates the /* GCROOT */ comments and follows where the register or memory location in the comment comes from (to do this it must follow the control flow and data flow of the function). This allows it to build a stack map: for each call instruction it knows exactly which registers and frame stack locations contain a live GC pointer. The stack map is then emitted in an extra assembler file that we link with the rest. As with LLVM above, the stack map is then used at run-time by non-portable code written in our GC's stack root tracker.\n\nYes, that's rather insane. But at least, we don't need to modify the assembler file - just read it. If GCC is too clever in its optimizations, the custom parser will get lost and complain cleanly; but I think that it is relatively safe in the sense that GCC optimizations should not be able to make the custom parser produce wrong results.\n\nThe branch is not merged because it's probably too insane to merge (not to mention, it's probably not portable to non-GCC compilers, and it is completely platform-specific). Still, it gives good results, better that the pure LLVM approach - on the order of 10% to 25% speed-ups for pypy-c.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/finding-gc-roots-using-llvm-or-parsing-1980376164990001937.html"
    },
    {
      "title": "Visualizing a Python tokenizer",
      "text": "Armin and me have been working on PyPy's parser and bytecode compiler for the Python language in the last days. Armin implemented several bytecode optimizations that CPython has since a while whereas I tried to refactor our tokenizer and parser (because our existing parser is rather slow and also not very nice code). Armin is mostly done whereas the new parser is not very far yet. What is done, however, is the Python tokenizer. It is implemented in the usual way, by using a set of regular expressions to generate a deterministic finite automaton (DFA). This automaton is then turned into a big function which does the actual tokenization. Of course the picture is not quite as simple for Python, because it is not possible to tokenize Python using only regular expressions. To generate the proper \"indent\" and \"dedent\" tokens it would be necessary to keep state (the previous indentation levels) which a DFA cannot do. This is solved by postprocessing the tokens that the tokenizer produces to turn whitespace tokens into the proper indent and dedent tokens.\nFor debugging purposes I implemented a visualization tool for DFAs using PyPy's pygame-based graph viewer. The graph viewer is able to visualize interactively any graph given in the graph-description language of Graphviz. Looking at the tokenizing DFA for Python is rather instructive, both for understanding how tokenizing works and (maybe) for understanding the Python language. To try it, download the dot file of the DFA and run from a pypy checkout:\n$ python pypy/bin/dotviewer.py tokenizer.dotThe following is a screenshot of the graphviewer:\n\nFor people who don't want do checkout PyPy I generated a (rather big) png for the DFA.\nNext thing I would like to do (apart from actually finishing the parser, of course :-) ) is visualize the Python grammar itself using syntax diagrams or something similar. So far I couldn't really find a program to do that, though.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2008/01/visualizing-python-tokenizer-5020282079473796926.html"
    },
    {
      "title": "PyPy Winter Sports Sprint from 12-19th of January in Leysin, Switzerland",
      "text": "The next PyPy sprint will be held in Leysin, Switzerland, for\nthe fifth time.  The overall idea of the sprint is to continue\nworking on making PyPy ready for general use.\nThe proposed topics are: ctypes, JIT, testing, LLVM.  This is\na fully public sprint, so newcomers and other topics are\nwelcome.  And like previous winters, the main side goal is to\nhave fun in winter sports :-) See the sprint announcement\nfor details.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/pypy-winter-sports-sprint-from-12-19th-5592383212609773292.html"
    },
    {
      "title": "(German) Slides of Talk at Python User Group Munich Available",
      "text": "Georg Brandl has put up the slides of the PyPy talk he gave at the Python User Group Munich.  The slides are in German.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/german-slides-of-talk-at-python-user-3715884461725333051.html"
    },
    {
      "title": "Various Performance Improvements",
      "text": "A few days ago, Armin discovered Gnuplot. He wrote a script that turns the results of the nightly benchmark runs into plots (lower is always better, all the numbers of the microbenchmarks are \"times slower than CPython\"). The corresponding microbenchmarks can be found in the repository. Staring at the plots revealed a strange performance regression around the revision 45000. After some investigation Armin found that an mostly unrelated change had disabled our method cache, which caused the regression. This was fixed.\n\nIn addition, Armin did a few other small tweaks in the interpreter main loop, making sure that small bytecodes are inlined into the main loop. This gave another few percent of performance increase. Together with the GC improvements two weeks ago this leads to the fastest non-JIT PyPy ever. Unfortunately \"fastest\" is not really very fast yet in absolute terms, with realistic apps being around 3-4 times slower than CPython. Especially calls (in all its variants) are quite slow, which is something we should look into.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/various-performance-improvements-7027210611565246190.html"
    },
    {
      "title": "Faster implementation of classic classes merged",
      "text": "Old-style classes have so far been a bit neglected by PyPy's Python interpreter. By default, PyPy makes all classes new-style and you have to use a command-line switch (--oldstyle) at startup or at translation time to change that default. Then you would get an pure-Python implementation of classic classes. This implementation was extremely slow (around 20 times slower than classic classes in CPython). In the past we had hoped that we could get away with mostly only supporting new-style classes, however it seems that real-world software seems to rely on them quite a bit, so we decided to offer a better migration path.\n\nA while ago I therefore started a re-implementation of classic classes in RPython to speed them up. This work is now finished, the branch I worked on got merged today. Speed for the old-style class benchmarks was improved greatly and I found quite a number of bugs in the old implementation too. New-style classes are still a bit faster than old-style in PyPy though, and this is unlikely to change.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/faster-implementation-of-classic-1021557618590043616.html"
    },
    {
      "title": "Profiling for fun with valgrind",
      "text": "Recently I've been doing a lot of profiling on the PyPy executables to find speed bottlenecks. Valgrind (the original page seems to be down) is an extremely nice tool for doing this. It has several built-in tools that give you different types of profiles. The callgrind mode provides you with a lot of information including relative call costs. The cachegrind tool gives you less information, but what it gives you (e.g. cache misses) is much more accurate. The obvious choice would be to have a way to combine the results of two profiling runs to have both. In the last days I wrote a script that does this. It's available at my user's svn and has a pretty intuitive command line interface. The combining calculation are not perfect yet, total costs of functions can still be a bit bogus (they can sum up to whatever) but at least the relative figures are good. This means that we can stop looking at two different types of graphs now.\n\nAn awesome tool for analyzing the profile data is kcachegrind.\n\n\n\nWhich also proves that my 12'' display is to small at least for some things :-).\n\n\nUpdate: pygrind is available under the MIT license.",
      "tags": "kcachegrind,profiling,valgrind",
      "url": "https://www.pypy.org/posts/2007/12/profiling-for-fun-with-valgrind-3215121784705288400.html"
    },
    {
      "title": "PyPy Talk at the Python User Group Munich",
      "text": "Tomorrow evening there will be an introductory talk about PyPy at the Python User Group Munich. The talk will be given by CPython and PyPy contributor Georg Brandl and will be in German.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/pypy-talk-at-python-user-group-munich-1952379593354367249.html"
    },
    {
      "title": "PyPy tasks in GHOP",
      "text": "In the latest bunch of tasks that Titus released on Friday for the Google Highly Open Participation Contest there are several that are related to PyPy. Some of them are about presenting PyPy to a technical audience: Task 187, Task 188, Task 189, Task 190.\n\nThen there are some three about Ropes, which are all rather challenging:\nSolving the first three section of the last ICFP contest with PyPy's ropes implementation:Task 248.Implementing nice wrapper classes around PyPy's ropes algorithms to make their use convenient: Task 239.Implementing the Ropes algorithms in C as a CPython extension module: Task 218 (already taken).\nIn addition there is a task to use PyPy's sandboxing features to provide an interactive Python tutorial on a web page: Task 220.\n\nWe're really looking forward to working together with some bright students!",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/pypy-tasks-in-ghop-5130253260153218709.html"
    },
    {
      "title": "faster than c",
      "text": "Of course being \"faster than c\" means being faster than light. What did you think it means? :-)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/faster-than-c-8057790636822502084.html"
    },
    {
      "title": "Good news from the garbage collection front",
      "text": "It seems that we can do better! Armin fixed a bug in our generational garbage collector, which caused variable sized objects (e.g. arrays) to be allocated outside  of the nursery. This resulted in 50% speedup on synthetic benchmarks and about 10-20% on real world ones. Doing some preliminary measures, it seems that we spend roughly 10% of the time in garbage collection, which is good (and there is still some room for improvements!)",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/12/good-news-from-garbage-collection-front-2678138026363485439.html"
    },
    {
      "title": "PyPy Google Tech Talk",
      "text": "The Google Tech Talk that Samuele, Armin, Jacob and Laura gave during the US trip is now on YouTube: https://www.youtube.com/watch?v=GnPmErtqPXk",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/pypy-google-tech-talk-9082134238390123890.html"
    },
    {
      "title": "Sprint Pictures",
      "text": "The obligatory sprint picture post...\n\n\n\n\nAlexander Schremmer, Armin Rigo, Maciek Fijalkowski, Antonio Cuni\n\nAnders Chrigstr\u00f6m, Samuele Pedroni, Laura Creighton, Jacob Hall\u00e9n, Carl Friedrich Bolz, Richard Emslie, Maciek Fijalkowski, Armin Rigo\n\nHolger Krekel\n\nWhiteboard with \"real world goals\" dependencies.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/sprint-pictures-3151912856495869652.html"
    },
    {
      "title": "Sprint Discussions: Wrapping External Libraries",
      "text": "A more technical discussion during the sprint was about the next steps for the external module problem (minutes). One of PyPy's biggest problems in becoming more generally useful are C extension modules, which can't work with PyPy's Python interpreter. We already reimplemented many of the more commonly used extension modules in CPython's standard library in Python or RPython. However, there are more missing and there is no way to implement all the extension modules that other people have written.\nWhiteboard after the discussion.\n\nTherefore we need a different approach to this problem. Extension modules are commonly written for two different reasons, one being speed, the other being wrapping non-Python libraries. At the moment we want mostly to approach a solution for the latter problem, because we hope that the JIT will eventually make it possible to not have to write extension modules for speed reasons any more. There are two rough ideas to approach this problem in the near future (there are other, more long-term ideas that I am not describing now): One of them is to add the ctypes module to PyPy's Python interpreter, which would mean re-implementing it since the existing implementation is written in C. The other way would be to work on the existing way to get extensions in that PyPy provides, which are \"mixed modules\". Mixed modules are written in a combination of RPython and normal Python code. To then wrap C libraries you would use rffi, which is the foreign function interface of RPython.The discussion round: Maciek Fijalkowski, Armin Rigo, Richard Emslie, Alexander Schremmer.Both approaches have problems: With ctypes you have no built-in way to query C header files for structure layouts and constants which requires you to hard-wire them, which is highly platform dependant. Mixed modules are not really fun to write, since they need to be RPython and we currently don't have a way to do separate compilation, so you always need to translate PyPy's whole Python interpreter to see whether your module is correct. In the meeting it was decided to first go for a ctypes replacement. The replacement would be written in pure Python, we already have a very thin wrapper around libffi which the new ctypes implementation would use.  The goal to reach would be to get the pygame implementation in ctypes to run on PyPy. To make ctypes more useful in general to write this kind of wrappers, we will probably extract some code that we have already written for PyPy's own usage: it gives a way to write \"imprecise\" declarations (\"a structure with at least fields called x and y which are of some kind of integer type\") and turn them into exact ctypes declarations, internally using the C compiler to inspect the platform headers. After this is done we should approach separate compilation so that developing modules in RPython has a quicker turnaround time. This is somewhat involved to implement for technical reasons. There are ideas how to implement it quickly to make it usable for prototyping, but it's still a lot of work.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/sprint-discussions-wrapping-external-8731011170537270161.html"
    },
    {
      "title": "Sprint Discussions: Releases, Testing",
      "text": "During the sprint we had various discussions about technical issues as well as planning discussions about how we want to go about things. One of them was about the stability of PyPy, how to ensure stability, how to handle releases and approaches to being more \"usable\". I will describe this discussion in this post (there are also minutes of the meeting).\n\n\n\nThe Meetings whiteboard\n\nTesting\n  First we discussed the current situation in terms of testing. PyPy has been extremely testing-oriented from the start, it is being developed almost exclusively in test-driven-development style. To deal with the large number of tests we already have some infrastructure in place:   we run all of PyPy's tests nightly on a Linux machine we translate a PyPy Python interpreter every night and use that to run the CPython compliance tests against it, also on a Linux machine we translate several Python interpreters every night and run benchmarks against them on a PowerPC running Mac OS X   As you can see, we are lacking in the Windows testing area, which is an even worse problem because none of the currently active developers has Windows as his primary OS. We should improve this by finding a Windows machine where the tests are run nightly and where we can log in to try bug-fixes quickly. The latter bit is important, we had a nightly windows test run before (thanks to Scott Dial) but it didn't help, because even if you tried to fix a bug you would have to wait until the next night to see whether it worked. Another very serious problem is that of aggregation: we have these various test runs that all have a web interface to check for errors but there is no easy way to find out which tests failed. You have to go to each page and even some sub-pages to see what needs fixing, which is a tedious process. The idea for solving this is aggregate all the available information into some sort of testing-entry-point page that gives a quick overview of the regressions that happened during the night. It's not clear whether we can achieve that with existing tools (buildbots or whatever), but we will investigate that.\n  Releases\nThe discussion about releases was more on a fundamental and less on a concrete level (especially when it comes to time-frames). We discussed what it means to make a release, because obviously it is more than just taking an SVN revision and putting a tarball of it onto the webpage. During the EU period we were required to make several releases, but those were not really meant to be more than technology previews for the brave adventurers to try. In the future we have the goal to release things that are more stable and hopefully more practically useful. The plan is to use medium-sized Python applications that have a chance to run on top of PyPy because they don't use too many extension modules (web apps being likely candidates) and that have good unit-tests themselves. The first step would be to find some applications that fit this description, fix the bugs that prevents PyPy from running them and from then on run them nightly on one of the testing machines to check for regressions. This would allow us to be more confident when stating that \"PyPy works\". Another thing to keep in mind for releases is the special features that our Python interpreter provides (e.g. the thunk and the taint object space, our stackless features, transparent proxies, sandboxing, special object implementations). Those features are neither tested by the CPython tests nor by any existing applications. Therefore we cannot really be confident that these features work and don't have too many bugs (in fact, the first time somebody really use the become feature of the thunk space in earnest he found a serious bug that is not fixed so far). To get around this problem, we plan to write small-to-medium sized example applications for each of these features (for stackless we can maybe use one of the existing stackless examples). This will hopefully find bugs and will also make it possible to evaluate whether the features make sense from a language design point of view. A minor thing to make releases easier is to be able to not only have the tests be run once a night but also be able to trigger them manually on the release branch before doing the release.Publishing Cool Things\n  Since we decided that the releases we make should be stable and usable, we also discussed how we would go about making new \"cool things\" like features, experiments etc. better known. The consensus was that this blog is probably the best forum for doing this. In addition we discussed having a stabler snapshot of the trunk made to ensure that people wanting to play around with these features don't accidentally get\na broken version.Helping Out\nRight now we are still in cleanup mode (the cleanup sprint is nearly done, but we haven't finished all the cleanups yet), so we won't be able to start on the above things right now. However, they will have a strong focus soon. So if you are interested in trying out to run programs on top of PyPy or writing new ones that use the new features you are most welcome to do so and we will try to fix the bugs or help you doing it (of course some tolerance against frustration is needed when you do that, because the bugs that turn up tend to be obscure). We have not been perfect at this in the past, but this will have to change.",
      "tags": "release",
      "url": "https://www.pypy.org/posts/2007/11/sprint-discussions-releases-testing-1126468258904483211.html"
    },
    {
      "title": "Ropes branch merged",
      "text": "This afternoon we merged the ropes branch that I have been working on on the side for a while (also to cut down the number of currently active branches a bit, since we are doing major cleanups right now). It contained a new (optional) implementation of the unicode type using the rope data structure. Ropes essentially use concatenation trees to represent strings. The leaves of the trees contain either byte arrays or arrays of unicode characters.\n\n\nOf course the fact that ropes are used is mostly completely transparent to the user (as usual in the pypy world :) ). Normal and unicode strings are implemented with them, but just from the behavior of these types the user has a hard time noticing. Of course there are significant changes in performance (in both directions).\n\nUsing ropes to implement strings has some interesting effects. The most obvious one is that string concatenation, slicing and repetition is really fast (I suspect that it is amortized O(1),   but haven't proved it). This is probably not helping most existing Python programs because people tend to code in such a way that these operations are not done too often. However, with ropes it is possible to do something like this:\nPython 2.4.1 (pypy 1.0.0 build 48942) on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>> import sys\n>>>> a = \"a\" * sys.maxint\n>>>> hash(a)\n-768146060\n\n\nSo somebody who is targeting a Python implementation that has ropes could write his code in such a way that this is taken into account. Another interesting feature is that ropes try to share as much data as possible with each other, so if you create a large slice of a large string, the slice is not going to take much additional memory.\n\nOne of the most interesting use-cases of ropes are together with unicode. The leaf nodes of a rope unicode string can be either a byte array or an array of unicode characters. This means that a unicode string that uses only characters that are latin-1 or ascii will use one byte of memory per character. If a unicode string contains mostly only unicode characters that are latin-1 and a few that are not, it will still use 1 byte for most of the latin-1 characters. This property also allows really fast encoding and decoding of unicode strings as long as they don't contain non-latin-1 characters (only with certain encodings of course):\n>>>> s = \"a\" * sys.maxint\n>>>> u = s.decode(\"ascii\")\n>>>> u = s.decode(\"latin-1\")\n>>>> u = s.decode(\"utf-8\")\nAgain, encoding and decoding strings that contain a few non-latin-1 characters is again efficient:\n>>>> u = \"a\" * 100000000 + u\"\\uffff\"\n>>>> s = u.encode(\"utf-8\")\n>>>> len(s)\n100000003\nI am not completely certain how useful this behaviour is for real-life applications, but it's kind of cool :-). It saves memory for european languages that contain few non-ascii characters.\n\nOf course there is at least one down-side to all of this, which is that string indexing is not O(1) any longer, because we have to walk down the tree to find the correct leaf where the character is actually in. I have not measured much, but I expect it to be quite fast in practice, because the trees are never deeper than 32 nodes.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/ropes-branch-merged-8782576892496878598.html"
    },
    {
      "title": "PyPy cleanup sprint startup",
      "text": "The following week we will have a sprint in Gothenburg to clean up the PyPy codebase and make it ready for future developments. So far, only a few people are here, the others will arrive this afternoon.\n\nThe \u00c4lvsborgsbron in Gothenburg from the ferry I took to get there.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/pypy-cleanup-sprint-startup-4429006224971155209.html"
    },
    {
      "title": "Unicode support in RPython",
      "text": "In the recent days we (Carl Friedrich, Anto and me) implemented native unicode support for RPython. This means that now you can write u'xxxx' directly in your RPython program, as well as unicode(some_string_variable) and most of the unicode methods should work as well. The things that don't work, are operations that require the unicode database (such as .upper() and friends) and encodings (unicode(x, encoding) for example). Right now our python interpreter does not use this at all, but that's the next step.\n\nCheers,\nfijal",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/unicode-support-in-rpython-in-recent-1444449848043047640.html"
    },
    {
      "title": "The PyPy Road Show (1): New York and IBM",
      "text": "We're slowly getting adjusted to the jet-lag (except maybe Samuele). Time to blog... The past two days at IBM, in New York, have been quite interesting.  The place is a research center.  Feels University-like, but meetings rooms have no windows and climatization fixed on \"polar\" settings.  The building is of course heated at this time of the year, and then the meeting rooms are climatized... I guess that just doesn't make sense to me. We gave a 1h30 talk to a general audience first.  Then we had a compact schedule of meetings with various people or groups of people.  In the early preparations for this trip we planned to stay only one day, but Martin Hirzel, our host, found too many people that wanted to talk with us :-) I think that both us and most of the people we talked with got interesting things out of the meetings.  On our side, let me point a few highlights. We asked two people that worked on the GCs for the Jikes RVM if reusing them for RPython programs would make sense.  They didn't scream \"you're mad!\", so I guess the answer is yes.  Apparently, it has been done before, too.  I'm still not sure I got this right, but it seems that Microsoft paid someone money to integrate them with Rotor...  Then the real-time garbage-collection guys explained to us the things that we need to take care about when writing a VM: real-time GC needs not only write barriers and read barriers, but pointer-equality-comparison barriers...  They have bad memories of trying to add a posteriori this kind of barrier into existing VMs, so it took us a bit of explaining to make them realize that adding new kinds of barriers is mostly trivial for us (I'm still not 100% sure they got it... bad memories can stick hard). Then we had discussions with JIT people.  Mostly, this allowed us to confirm that Samuele has already got a good idea about what Java JITs like Hotspot can do, and in which kind of situation they work well.  As expected, the most difficult bit for a PyPy-like JIT that would run on top of a JVM would be the promotion.  We discussed approaches like first generating fall-back cases that include some instrumentation logic, and regenerating code with a few promoted values after some time if it seems like it will be a gain.  Replacing a method with a new version is difficult to do in a way that is portable across Java VMs. There are still possible workarounds, but it also means that if we really want to explore this seriously, we should consider experimenting with specifics VMs - e.g. the Jikes RVM gives (or could be adapted to give) hooks to replace methods with new versions of them, which is something that the JVM's own JIT internally does all the time. We showed the taint object space and the sandboxed PyPy to several groups of security people.  I won't say much about it here, beyond the fact that they were generally interested by the fact that the corresponding code is very short and easy to play with.  They are doing a lot on security in Java and... PHP, for web sites.  Someone could write a PHP interpreter (!) in PyPy to get the same kind of results. But as Laura and Samuele put it, there are things in life you do for fun, and things you do for money :-) We're in Vancouver today and tomorrow.  More about this later... Armin Rigo",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/pypy-road-show-1-new-york-and-ibm-7837076523877011699.html"
    },
    {
      "title": "The PyPy Road Show",
      "text": "Armin Rigo, Samuele Pedroni, Laura Creighton and Jacob Hall\u00e9n are on a two-week-trip through the USA and Canada, to present PyPy to various companies and institutions. The next few blog entries will cover our experiences and adventures.\n\nHere is a glimpse of our schedule (all November 2007):\n4th: Chigaco5th-6th:  New York7th-8th:  Vancouver9th-18th: San Francisco and the Bay Area\nNotably, we meet with IBM Research in New York and give a Google Talk in the Bay Area.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/11/pypy-road-show-5790414147905233059.html"
    },
    {
      "title": "First Post",
      "text": "Welcome to the PyPy status blog. After we got a lot of positive feedback about the blog coverage of our Squeak/PyPy sprint in Bern  we decided that having a general PyPy blog sounds like a good idea. We will try to periodically post about what is going on in the PyPy project, cover sprints and other events where PyPyers are present. If you have any wishes about things we should write about, feel free to leave a comment.",
      "tags": "",
      "url": "https://www.pypy.org/posts/2007/10/first-post-8150793557471983289.html"
    },
    {
      "title": "Search",
      "text": "Search results appear here.",
      "tags": "",
      "url": "https://www.pypy.org/search.html"
    }
  ]
};